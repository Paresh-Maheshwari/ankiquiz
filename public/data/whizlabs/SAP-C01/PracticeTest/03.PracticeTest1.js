// 3. Practice Test I
var PracticeTest1 = {
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": 28373,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Your company is hosting a web application on AWS (with an autoscaling group already in place). According to the architectural best practices, the application must be highly available, scalable, cost-effective, with high-performance, and require minimal human intervention. You have deployed the web servers and database servers in the public and private subnet of the VPC, respectively. While testing the application via a web browser, you noticed that the application is not accessible.</p>\r\n\r\n<p>Which of the following&nbsp;configuration settings can help you to tackle this issue?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answers &ndash; C</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because (a) NAT instance is ideally used to route traffic from a private subnet to the internet via a public subnet, (b) NAT instance is not managed by AWS and requires to be configured and maintained by the user; hence, adding to the overhead, and (c) if not scaled, can cause a performance bottleneck. NAT Gateway is a preferred option over NAT instances.</li>\r\n\t<li>Option B recommends using AWS CloudFront and configure the distributions Origin to the web server and then use an AWS Route 53 &lsquo;CNAME&rsquo; for the CloudFront Distribution. CloudFront is highly available and accessible to the Internet. It would work better if the Origin for the AWS CloudFront Distribution were&nbsp;pointed to an AWS ELB rather than the Web Server itself.&nbsp;CloudFront doesn&acute;t support IPs as origins.</li>\r\n</ul>\r\n\r\n<p>Since the Origin would only be a Web Server, if this server goes offline for a period of time, the website would become unavailable. The content is not cached at the Edge location or if the TTL for the content expires.&nbsp;</p>\r\n\r\n<p>So, Option B is incorrect as well.</p>\r\n\r\n<ul>\r\n\t<li>Option C is CORRECT. Because, (a) if the web servers are behind an ELB, the load on the web servers will be uniformly distributed. Hence, if any of the web servers goes offline or becomes non-responsive, the traffic would be routed to other online web servers; making the application highly available, and (b) You can use Route53 to set the ALIAS record that points to the ELB endpoint.</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_q5rxpd.png\" style=\"height:534px; width:522px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option D is incorrect. Because the Auto Scaling group may terminate and relaunch a new instance. It is not suitable to configure the recordset that points to EIPs.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18490,
          "question_id": 28373,
          "answers": [
            {
              "choice": "<p>Configure a NAT instance in your VPC and create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Place all your web servers behind ELB. Configure a Route53 ALIAS-Record to point to the ELB DNS name.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Assign EIP&#39;s to all web servers. Configure a Route53 A-Record set with all EIPs with health checks and DNS failover.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28382,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You have developed an application that processes a massive amount of process logs generated by web site and mobile app. This application requires the ability to analyze petabytes of unstructured data using Amazon Elastic MapReduce. The resultant data is stored on Amazon S3. You have deployed the c4.8xlarge Instance type, whose CPUs are mostly idle during the data processing. Which of the below options would be the most cost-efficient way to reduce the log processing job&#39;s runtime?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\r\n\r\n<p>Option A is incorrect even though storing the files on an S3 storage class such as RRS would reduce the cost. The problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.&nbsp;</p>\r\n\r\n<p>Option B is incorrect as adding more of the c4.8xlarge instance type in the task instance group would create more idle resources, which is - in fact - more costly.</p>\r\n\r\n<p>Option C is CORRECT because, since the CPU&rsquo;s are mostly idle, it means that you have provisioned a larger instance that is under-utilized. A better cost-efficient solution would be to use smaller instances. For batch processing jobs such as the one mentioned in this scenario, you can use multiple t2 instances - which support the concept of CPU bursts - are ideal for situations where there are bursts of CPU during certain periods of time only.</p>\r\n\r\n<p>Option D is incorrect even though storing the files on an S3 storage class such as RRS would reduce the cost. The problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>For more information on resizing of the EC2 instances, please visit the URL given below-</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\" target=\"_blank\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a></p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18499,
          "question_id": 28382,
          "answers": [
            {
              "choice": "<p>Create log files with smaller sizes and store them on Amazon S3. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Add additional c4.8xlarge instances by introducing a task instance group. The network performance of 10 Gigabit per EC2 instance would increase the processing speed; thus reducing the load on the EMR cluster.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use smaller instances that have higher aggregate I/O performance.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create fewer, larger log files. Compress and store them on the Amazon S3 bucket. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28383,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your department creates a regular analytics report from your company&#39;s log files stored in S3. The daily EMR jobs produce the PDF reports and the aggregated tables in CSV format as an input to the Redshift data warehouse, and these are accessed frequently. You notice that the average EMR hourly usage is more than 25% but less than 50%.</p>\r\n\r\n<p>Your CFO requests you to optimize the cost structure for this system. Which of the following alternatives will lower costs without compromising the system&#39;s average performance or data integrity for the raw data?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - B</p>\r\n\r\n<p>Options A, C, and D are invalid. We need to access the PDF and CSV files daily. So S3-IA and glacier are not suitable for this purpose as both of these storage options are not the best for frequent access to files.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18500,
          "question_id": 28383,
          "answers": [
            {
              "choice": "<p>Use Glacier to store PDF and CSV Data. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift.&nbsp;</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use standard S3 to store PDF and CSV data. Use a combination of Spot Instances and Reserved Instances for EMR and Reserved Instances for RedShift.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use Glacier to store PDF and CSV Data. Add Spot Instances to Amazon EMR jobs. Use Spot Instances for Amazon Redshift.&nbsp;</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use S3-IA to store PDF and CSV Data. Use&nbsp;Reserved Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift.&nbsp;</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28384,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>You are the new IT architect in a company that operates a mobile sleep tracking application. When activated at night, the mobile app sends&nbsp;collected data points of 1 KB every 5 minutes to your middleware. The middleware layer takes care of authenticating the user and writing the data points into an Amazon DynamoDB table. Every morning, you scan the table to extract and aggregate last night&rsquo;s data on a per-user basis and store the results in Amazon S3. Users are notified via Amazon SMS mobile push notifications that new data is available, parsed, and visualized by the mobile app. The old data is not required by the end-users. Currently, you have around 100k users. You have been tasked to optimize the architecture of the middleware system to lower the cost. What would you recommend?<br />\r\n(Select TWO)</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A and C</strong></p>\r\n\r\n<p><strong>Option A is CORRECT</strong> because (a) The data stored would be old/obsolete anyways and need not be stored; hence, lowering the cost, and (b) Storing the data in DynamoDB is expensive. Hence, you can set an expiry date so that the data gets deleted automatically.</p>\r\n\r\n<p><strong>Option B is incorrect</strong> because&nbsp;(a) Storing the data in DynamoDB is more expensive than S3, and (b) giving the app access to DynamoDB to read the data is an operational overhead.</p>\r\n\r\n<p><strong>Option C is CORRECT</strong> because (a) it uses SQS which reduces the provisioned output cutting down on the costs, and (b) acts as a buffer that absorbs sudden higher load, eliminating going over the provisioned capacity.</p>\r\n\r\n<p><strong>Option D is incorrect</strong> because the data is only read once before it is stored to S3. The cache would only be useful if you read things multiple times. Also, in this scenario optimizing &quot;write&quot; operations is most desired, not &quot;read&quot; ones.</p>\r\n\r\n<p><strong>Option E is incorrect </strong>because (a) Amazon Redshift cluster is primarily used for OLAP transactions, not OLTP; hence, not suitable for this scenario, and (b) moving the storage to Redshift cluster means deploying a large number of EC2 instances that are continuously running, which is not a cost-effective solution.</p>\r\n\r\n<p>For complete guidelines on working with DynamoDB, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html\" target=\"_blank\">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html</a></li>\r\n\t<li><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 80228,
          "question_id": 28384,
          "answers": [
            {
              "question_id": "28384",
              "choice": "<p>Store the data in the DynamoDB table with a Time to Live (TTL) and the data will be deleted automatically.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28384",
              "choice": "<p>Have the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28384",
              "choice": "<p>Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28384",
              "choice": "<p>Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28384",
              "choice": "<p>Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28385,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high-resolution MP4 format. Your workforce is distributed globally, often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise, and it required that you may need to pay for a consultant. How would you implement the most cost-efficient architecture without compromising the high availability and quality of video delivery?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>There are four most important design considerations here: (a)&nbsp;video transcoding expertise, (b) global distribution of the content, (c) cost-effective solution, and (d) no compromise with the high availability and quality of the video delivery.</p>\r\n\r\n<p>Amazon Elastic Transcoder is a media transcoding service in the cloud. It is designed to be a highly scalable, easy-to-use, and cost-effective way for developers and businesses to convert (or &ldquo;transcode&rdquo;) media files from their source format into versions that will playback on various devices like smartphones, tablets, and PCs.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because (a) it uses Amazon Elastic Transcoder that converts from MP4 to HLS, (b) S3 Object Lifecycle Management reduces the cost by archiving the files to Glacier, and (c) CloudFront - which is a highly available service - enables the global delivery of the video without compromising the video delivery speed or quality.</li>\r\n\t<li>\r\n\t<p>Option B is incorrect because (a) it necessitates the overhead of infrastructure provisioning. i.e, deploying of EC2 instances, auto-scaling, SQS queue/pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost-efficient solution.</p>\r\n\t</li>\r\n\t<li>\r\n\t<p>Option C is incorrect because the use of EBS snapshots is not a cost-effective solution compared to S3 Object Lifecycle Management.</p>\r\n\t</li>\r\n\t<li>Option D is incorrect because&nbsp;(a) it necessitates the overhead of infrastructure provisioning. i.e, deploying EC2 instances, auto-scaling, SQS queue/pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost-efficient solution, and (d)&nbsp;the use of EBS snapshots is not a cost-effective solution compared to S3 Object Lifecycle Management.</li>\r\n</ul>\r\n\r\n<p>For more information on Elastic transcoder, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/elastictranscoder/\" target=\"_blank\">https://aws.amazon.com/elastictranscoder/</a></li>\r\n</ul>\r\n\r\n<p>CloudFront can then be used to deliver the content to the users from its various edge locations.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18502,
          "question_id": 28385,
          "answers": [
            {
              "choice": "<p>Elastic Transcoder to transcode original high-resolution MP4 videos to HLS.&nbsp;Use S3 to host videos and use&nbsp;CloudFront to serve HLS transcoded videos from S3.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. Use S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. Use CloudFront to serve HLS transcoding videos from Glacier.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. Use CloudFront to serve HLS transcoded videos from EC2.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. Use CloudFront to serve HLS transcoded videos from EC2.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28386,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You&rsquo;ve been hired to enhance the overall security posture for a large e-commerce site. They have a well-architected, multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier with static assets served directly from S3. They are using a combination of RDS and DynamoDB for their dynamic data and then archiving nightly into S3 for further processing with EMR. They are concerned because they found questionable log entries and a flood of superfluous requests for accessing the resources. You suspect that someone is performing a DDoS attack. How would you mitigate this kind of attack in the easiest and most cost-efficient way?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\r\n\r\n<p>In such scenarios where you are designing a solution to prevent the DDoS attack, you can use a Web Application Firewall (WAF).</p>\r\n\r\n<p>AWS WAF is a web application firewall that helps protect your web&nbsp;applications from common web exploits that could affect application&nbsp;availability, compromise security, or consume excessive resources. AWS&nbsp;WAF gives you control over which traffic to allow or block your web&nbsp;applications by defining customizable web security rules. You can use AWS&nbsp;WAF to create custom rules that block common attack patterns, such as SQL&nbsp;injection or cross-site scripting, and rules designed for your specific application. New rules can be deployed within minutes, letting you respond&nbsp;quickly to changing traffic patterns.&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because, although this option could work, the setup is very complex and is not a cost-effective solution.</li>\r\n\t<li>Option B is incorrect because (a) even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and (b) it does not prevent the attack from the new source of threat.</li>\r\n\t<li>Option C is CORRECT because you can use AWS WAF web access control lists (web ACLs) to help minimize the effects of a distributed denial of service (DDoS) attack.&nbsp;Reference can be found in <a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a>.</li>\r\n\t<li>Option D is incorrect because there is no such thing as the Advanced Protocol Filtering feature for ELB.</li>\r\n</ul>\r\n\r\n<p>For more information on WAF, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/waf/\" target=\"_blank\">https://aws.amazon.com/waf/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18503,
          "question_id": 28386,
          "answers": [
            {
              "choice": "<p>Recommend that they lease space at a DirectConnect partner location and establish a 1G DirectConnect connection to their VPC. Then they would establish Internet connectivity into their space, filter the traffic in hardware Web Application Firewall (WAF) and then pass the traffic through the DirectConnect connection into their application running in their VPC.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Add previously identified host file source IPs as an explicit INBOUND DENY NACL to the web tier subnet.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Enable AWS WAF to protect the application from the DDoS attack.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Remove all but TLS 1 &amp; 2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28387,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You currently operate a web application in the AWS US-East region. The application runs on an auto-scaled layer of EC2 instances and an RDS Multi-AZ database. Your IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your EC2, IAM, and RDS resources. The solution must ensure the integrity and confidentiality of your log data. Which of the below solutions would you recommend?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>For the scenarios where the application is tracking (or needs to track) the changes made by any AWS service, resource, or API, always think about AWS CloudTrail service.</p>\r\n\r\n<p>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticated AWS API calls and AWS sign-in events and collected this event information in files delivered to Amazon S3 buckets.&nbsp;</p>\r\n\r\n<p>The most important points in this question are (a) Use of a single&nbsp;S3 bucket, (b) CloudTrail with the&nbsp;option that applies trail to specific regions, (b) Data integrity, and (d) Confidentiality.</p>\r\n\r\n<p>Option A is CORRECT because (a) it uses AWS CloudTrail with the option that applies trail to the US-East region, (b) a single new S3 bucket and IAM Roles so that it has the confidentiality, (c) log file validation is enabled. See the AWS CloudTrail setting below which sets the option that applies trail to all regions:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_mggfij.png\" style=\"height:184px; width:808px\" /></p>\r\n\r\n<p>Options B is incorrect because (a) although&nbsp;it uses AWS CloudTrail,&nbsp;the option that applies trail to all regions is not enabled, and (b) SNS notifications can be overhead in this situation.</p>\r\n\r\n<p>Option C is incorrect because (a) as an existing S3 bucket is used, it may already be accessed to the user, hence not maintaining confidentiality, and (b) it is not using IAM roles.</p>\r\n\r\n<p>Option D is incorrect because (a)&nbsp;although&nbsp;it uses AWS CloudTrail,&nbsp;the option that applies trail to all regions is not enabled, and (b) three S3 buckets are not needed.</p>\r\n\r\n<p>For more information on Cloudtrail, please visit the below URLs-</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events\" target=\"_blank\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18504,
          "question_id": 28387,
          "answers": [
            {
              "choice": "<p>Create a new CloudTrail trail with one new S3 bucket to store the logs in the US-East region. Enable log file validation. Use IAM roles and S3 bucket policies to control the permissions on the S3 bucket.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create a new CloudTrail with one new S3 bucket to store the logs. Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the option that applies trail to all regions selected. Use S3 ACLs to control the permissions on the S3 bucket.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs, and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28388,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An enterprise wants to use a 3rd party SaaS application hosted by another AWS account. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise&rsquo;s account. The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege. There must be controls in place to ensure that any other third party cannot use the SaaS vendor&#39;s credentials.</p>\r\n\r\n<p>Which of the following options would meet all of these conditions?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\r\n\r\n<p>When a user, a resource, an application, or any service needs to access an AWS service or resource, always prefer creating an appropriate role with the least privileged access or only required access, rather than using any other credentials such as keys.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because you should never share your access and secret keys.</li>\r\n\t<li>Option B is incorrect because (a) when a user is created, even though it may have the appropriate policy attached to it, its security credentials are stored in the EC2 which can be compromised, and (b) creation of the appropriate role is always the better solution rather than creating a user.</li>\r\n\t<li>Option C is CORRECT because AWS&#39;s role creation allows cross-account access to the application to access the necessary resources. See the image and explanation below.</li>\r\n</ul>\r\n\r\n<p>Many SaaS platforms can access AWS resources via Cross-account access created in AWS. If you go to Roles in your identity management, you will see the ability to add a cross-account role.</p>\r\n\r\n<p>&nbsp;<img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_nhcxl7.png\" style=\"height:476px; width:1140px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option D is incorrect because the role is to be assigned to the application and its resources, not the EC2 instances.</li>\r\n</ul>\r\n\r\n<p>For more information on the cross-account role, please visit the&nbsp;below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18505,
          "question_id": 28388,
          "answers": [
            {
              "choice": "<p>From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM user within the enterprise account and&nbsp;assign a user policy to the IAM user that allows only the actions required by the SaaS application. Create new access and secret key for the user and provide these credentials to the SaaS provider.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM role for cross-account access that allows the SaaS provider&rsquo;s account to assume the role and assign it a policy that allows only the actions required by the SaaS application.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM role for EC2 instances, assign it a policy that allows only the actions required for the Saas application to work,&nbsp;provide the role ARN&nbsp;to the SaaS provider&nbsp;to be used when launching their application instances.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28389,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You are designing a data leak prevention solution for your VPC environment. You want your VPC Instances to be able to access software distribution servers on the Internet for product updates. The servers are accessible via the third party via their DNS names. You want to make sure that the instances can only access these known servers based on their URLs. Which of the following options would you consider?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - A</strong></p>\r\n\r\n<p>There are 3 main considerations in this scenario: (a) the instances in your VPC need internet access, (b) the access should be restricted for product updates only, and (c) all other outbound connection requests must be denied.</p>\r\n\r\n<p>With such scenarios, you should not put your instances in the public subnet as they would have access to the internet without any restrictions. So, you should put them in a private subnet, and since there is a need for logic for filtering the requests from client machines, configure a proxy server.</p>\r\n\r\n<p><strong>What is a Proxy Server?</strong></p>\r\n\r\n<p>A proxy server is a server that acts as a mediator between the client(s) that sends requests and the server that receives the requests and replies back. If any client requires any resources, it connects to the proxy server, and the proxy server evaluates the request based on its filtering rules. If the requests are valid, it connects to the server which receives the request and replies. The proxy server also maintains cache; i.e., if any subsequent requests from the same or other clients are received, it returns the result from the cache, saving the trip to and from the server. Hence, proxy servers tend to improve performance. See the diagram below.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_9ih19l.png\" style=\"height:300px; width:600px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because a proxy server (a) filters requests from the client and allows only those related to the product updates, and (b) in this case helps to filter all other requests except the ones for the product updates.</li>\r\n\t<li>Option B is incorrect because a security group cannot filter requests based on URLs and you cannot specify deny rules. Security groups are used only for IPs and NOT for static DNS names.</li>\r\n\t<li>Option C is incorrect because even though moving the instances in a private subnet is a good idea, the routing table does not have the filtering logic.&nbsp;</li>\r\n\t<li>Option D is incorrect. NACL is stateless. The default network ACL is configured to allow all traffic to flow in and out of the subnets to which it is associated.&nbsp;Option D is only specifying an Inbound rule. But for an Inbound rule, it should specify the Source rather than the&nbsp;destination.&nbsp;</li>\r\n</ul>\r\n\r\n<p>An example of setting up a proxy server can be found via the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/articles/6463473546098546\" target=\"_blank\">https://aws.amazon.com/articles/6463473546098546</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18506,
          "question_id": 28389,
          "answers": [
            {
              "choice": "<p>Place EC2 instances in private subnets and direct the egress traffic to a web proxy server in the public subnet and enforce URL based rules for outbound access.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Implement security groups and configure outbound rules to only permit traffic to software servers.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Move all your instances into private VPC subnets. Remove default routes from all routing tables and add specific routes to the software servers only.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Implement network access control lists to allow traffic from specific destinations, with an implicit deny as a rule.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28390,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An administrator uses Amazon CloudFormation to deploy a three-tier web application that consists of a web tier and application tier that will utilize Amazon DynamoDB for storage. While creating the CloudFormation template, which of the following would allow the application instance access to the DynamoDB tables without exposing API credentials?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\n\n<p>The scenario requires the instance to have access to DynamoDB tables without having to use the API credentials. In such scenarios, always think of creating IAM Roles rather than IAM Users.</p>\n\n<ul>\n\t<li>Option A is incorrect because the IAM Role needs both the read and the write access to the DynamoDB table.</li>\n\t<li>Option B is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, and (b) using IAM Role is the more secure way of accessing the resources than using IAM Users with security credentials.</li>\n\t<li>Option C is CORRECT. Instance profiles (with the correct roles) can be referenced in cloud formation templates. These instance profiles are created manually with the correct roles associated with them and can be reused on multiple instances. Pre-building instance profiles can provide standardization in naming across the entire infrastructure.</li>\n</ul>\n\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_cqqamr.png\" style=\"height:622px; width:635px\" /></p>\n\n<p>&nbsp;</p>\n\n<ul>\n\t<li>Option D is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, (b) using IAM Role is the more secure way of accessing the resources than using IAM Users with security credentials.</li>\n\t<li>For more information on granting access to AWS resources via EC2 instance profile property, please visit the below URL-\n\t<ul>\n\t\t<li><a href=\"https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html\" target=\"_blank\">https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html</a></li>\n\t</ul>\n\t</li>\n\t<li>For more information on adding IAM roles in CloudFormation templates, please visit the below URL-\n\t<ul>\n\t\t<li><a href=\"http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html\" target=\"_blank\">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html</a></li>\n\t</ul>\n\t</li>\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18507,
          "question_id": 28390,
          "answers": [
            {
              "choice": "<p>Create an IAM role&nbsp;that only has the read permissions for&nbsp;the required DynamoDB table and associate the Role to the application instances by referencing an instance profile.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use the Parameter section in the Cloud Formation template to have the user input Access and Secret Keys from an already created IAM user that has permissions required to read and write from the required DynamoDB table.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM Role that has the required permission to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create an Identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28391,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An AWS customer is deploying an application in a separate, highly constrained execution environment ( enclaves ) composed of an auto-scaling group of EC2 Instances. The customer&#39;s security policy requires that every outbound connection from these instances to any other service within the customer&#39;s Virtual Private Cloud must be authenticated using a unique X.509 certificate that contains the specific instance ID.&nbsp;</p>\r\n\r\n<p>Which of the following configurations will support these requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\r\n\r\n<p>This scenario requires (a) an x.509 certificate per instance created in the auto-scaling group, (b) the certificate should be unique that contains the instance id.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because (a) storing the signed certificate in S3 is a bad idea as it will not be unique for each instance id, and (b) S3 is not a key management service and cannot generate such certificates.</li>\r\n\t<li>Option B is incorrect because you need to generate the instance id first before generating the certificate that will be unique for that instance id. Therefore, embedding a certificate in the image and then launching the instance will not be useful at all.</li>\r\n\t<li>Option C is correct&nbsp;because once the new instance is launched, the ASG is configured to send the notification through SNS to the ACM. The ACM then generates the new certificate.</li>\r\n</ul>\r\n\r\n<p>Nitro enclaves and ACM:</p>\r\n\r\n<ul>\r\n\t<li>Manager (ACM) for Nitro Enclaves allows you to use public and private SSL/TLS certificates with your web applications and web servers running on Amazon EC2 instances with AWS Nitro Enclaves. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the internet and resources on private networks.</li>\r\n\t<li>Previously, when running a web server on an EC2 instance, you would have created SSL certificates and stored them as plaintext on your instance. With ACM for Nitro Enclaves, you can now bind AWS Certificate Manager certificates to an enclave and use those certificates directly with your web server without exposing the certificates in plaintext form to the parent instance and its users.</li>\r\n\t<li>ACM for Nitro Enclaves removes the time-consuming and error-prone manual process of purchasing, uploading, and renewing SSL/TLS certificates. ACM for Nitro Enclaves creates secure private keys, distributes the certificate and its private key to your enclave, and manages certificate renewals. With ACM for Nitro Enclaves, the certificate&#39;s private key remains isolated in the enclave, preventing the instance, and its users, from accessing it.</li>\r\n</ul>\r\n\r\n<p>Please also check https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf#nitro-enclave-refapp (page 39) on how to use ACM for Nitro Enclaves.</p>\r\n\r\n<ul>\r\n\t<li>Option D is incorrect because (a) the onus is on the EC2 instances to generate the signed certificate, (b) the requirement is to use a key management service to generate the signed certificate, and (c)&nbsp; AWS KMS does not have any feature to &#39;poll&#39; any service.</li>\r\n</ul>\r\n\r\n<p>Please check below AWS Docs for more details.</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-refapp.html\">https://docs.aws.amazon.com/enclaves/latest/user/nitro-enclave-refapp.html</a></p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf#nitro-enclave-refapp\">https://docs.aws.amazon.com/enclaves/latest/user/enclaves-user.pdf#nitro-enclave-refapp</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18508,
          "question_id": 28391,
          "answers": [
            {
              "choice": "<p>Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure an Auto Scaling group to launch instances with this role. Have the instances bootstrap, get the certificate from Amazon S3 upon first boot.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group Have the launched instances, generate a certificate signature request with the instance&rsquo;s assigned instance-id to the AWS KMS for signature.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure the AutoScaling group to send an SNS notification of the launch of a new instance to the AWS Certificate Manager. Create a signed certificate using AWS Certificate Manager (ACM).</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure the launched instances to generate a new certificate upon first boot. Have the AWS KMS poll the Auto Scaling group for associated instances and send new instances a certificate signature (that contains the specific instance-id).</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28374,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are given the task of moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately, this app requires access to several on-premises services, and no one who configured the app still works for your company. Even worse, there&rsquo;s no documentation for it. Which of the following options can help you to move the application from on-premises to VPC?</p>\r\n\r\n<p>Choose 4&nbsp;options below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer: A, D, E and F</strong></p>\r\n\r\n<p>The scenario requires you to connect your on-premise server/instance with Amazon VPC. When such scenarios are presented, always think about Direct Connect, VPN, and VM Import and Export as they help either connect the instances from a different location or import them from one location to another.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because Direct Connect sets up a dedicated connection between on-premises data-center and Amazon VPC and provides you with the ability to connect your on-premise servers with the instances in your VPC.</li>\r\n\t<li>Option B is incorrect as you normally create a VPN connection based on a customer gateway and a virtual private gateway (VPG) in AWS.&nbsp;</li>\r\n\t<li>Option C is incorrect as EIPs are not needed as the instances in the VPC can communicate with on-premise servers via their private IP address.</li>\r\n\t<li>Option D is CORRECT because there should not be a conflict between the IP address of on-premise servers and the instances in VPC for them to communicate.</li>\r\n\t<li>Option E is CORRECT because we need to configure Route-53 resolver&nbsp;to forward queries via Direct Connect to the On-Prem DNS server. Route 53 alone will not be able to move the application from on-premises to VPC.</li>\r\n\t<li>Option F is CORRECT because the VM Import Export service helps you import the virtual machine images from the data center to the AWS platform as EC2 instances&nbsp;and export them back to your on-premises environment. This offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances.&nbsp;Once the VM import is done.&nbsp;Then the application running inside the VPC can reach out to on-premises services.</li>\r\n</ul>\r\n\r\n<p><strong>Note:</strong><br />\r\n<br />\r\nVMWare import can help us moving machines from on-premise to ec2 instances inside VPC.<br />\r\n<br />\r\nRecently there is an announcement from AWS regarding Route53 Support for resolving on-premise dependency:</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-route-53-announces-resolver-with-support-for-dns-resolution-over-direct-connect-and-vpn/\" target=\"_blank\">https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-route-53-announces-resolver-with-support-for-dns-resolution-over-direct-connect-and-vpn/</a></li>\r\n</ul>\r\n\r\n<p><br />\r\nAs you know, the latest features/announcements take around 6 months to get reflected in the actual exam.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18491,
          "question_id": 28374,
          "answers": [
            {
              "choice": "<p>An AWS Direct Connect link between the VPC and the datacenter network.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>An Internet Gateway to allow a VPN connection.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>An Elastic IP address on the VPC instance.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>An IP address space that does not conflict with the one on-premises.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure Route-53 resolver and make entries in Amazon Route 53 that allow the Instances to resolve its dependencies&rsquo; IP addresses.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>A VM Import of the current virtual machine.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28392,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Your company has recently extended its on-premises data center into a VPC on AWS to add burst computing capacity as needed. The data center has already used an identity provider (IDP).&nbsp;Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You don&rsquo;t want to create new IAM users for each member and make those users sign in again to the AWS Management Console. Which option below will meet the needs of your NOC members?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\n\n<p>This scenario has two requirements: (a) temporary access to AWS resources be given to certain users or applications (NOC members in this case), and (b) you are not supposed to create new IAM users for the NOC members to log into AWS console.&nbsp;</p>\n\n<p>This scenario is handled by a concept named &quot;Federated Access&quot;. Read this for more information on federated access:&nbsp;<a href=\"https://aws.amazon.com/identity/federation/\" target=\"_blank\">https://aws.amazon.com/identity/federation/</a> .</p>\n\n<p>Read this article for more information on how to establish federated access to the AWS resources.</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/\" target=\"_blank\">https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/</a></p>\n\n<p>&nbsp;</p>\n\n<p>Option A is incorrect because OAuth 2.0 is not applicable in this scenario as we are not using Web Identity Federation. It is used with public identity providers such as Facebook, Google, etc.</p>\n\n<p>Option B is incorrect because the key point here is that you need to give access to AWS Management Console to only the members of your Network Operations Center using on-premises SSO to avoid signing in again. The users should not be using Facebook or Google IDs to login.</p>\n\n<p>Option C is CORRECT because (a) it gives federated access to the NOC members to AWS resources by using SAML 2.0 identity provider, and (b) it uses on-premises single sign on (SSO) endpoint to authenticate users and gives them access tokens before providing the federated access.</p>\n\n<p>Option D is incorrect because, even though it uses SAML 2.0 identity provider, we need to grant SSO access to the AWS management console and retrieving temporary security credentials is irrelevant here.</p>\n\n<p>See this diagram that explains the Federated Access using SAML 2.0.&nbsp;</p>\n\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_ncagbh.png\" style=\"height:423px; width:700px\" /></p>\n\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18509,
          "question_id": 28392,
          "answers": [
            {
              "choice": "<p>Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your members to sign in to the AWS Management Console.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use Web Identity Federation to retrieve AWS temporary security credentials to enable your members to sign in to the AWS Management Console.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use your on-premises SAML 2.0-compliant identity provider (IDP) to retrieve temporary security credentials to enable members to sign in to the AWS Management Console.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28393,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are designing an SSL/TLS solution that requires HTTPS clients to be authenticated by a&nbsp;web server using client certificate authentication.&nbsp;Which of the following options would you consider for configuring the webserver infrastructure?</p>\r\n\r\n<p>(Choose 2 options from the list given below, each one being an independent solution to the scenario)</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answers - A and B</strong></p>\r\n\r\n<p>This scenario requires you to set up the web servers so that the HTTPS clients must be authenticated by the client-side certificate (not the server-side certificate). There are two ways of architecting this - with ELB and without ELB. (a) With ELB, if you use HTTPS listeners, you have to deploy the server-side certificate - which is not desired. So, you need to use the TCP listener so that the HTTPS client requests do not terminate at the ELB. They pass through ELB and terminate at the webserver instances. (b) Alternatively, without ELB, you can directly use the webserver to communicate with the clients or set up a Route53 Record Set with the public IP address of the webserver(s) such that the client requests would be directly routed to the webserver(s).</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because it uses TCP (443) listener that relays requests to the backend instances as-is. This terminates the client certificate on the webserver instances rather than the ELB.</li>\r\n\t<li>Option B is CORRECT because it uses Route53 Record Set with the public IP address of the webserver(s) such that the client requests would be directly routed to the webserver(s).</li>\r\n\t<li>Option C is INCORRECT because if you use HTTPS listeners, it&nbsp;does not support client certificates. HTTPS connections are terminated on the ELB rather than the backend web server instances.</li>\r\n\t<li>Option D is incorrect because this setting should be an &quot;origin&quot; and not a &quot;target&quot;. The phrase should be &quot;Configure your web servers as the origins for a CloudFront distribution&quot;</li>\r\n</ul>\r\n\r\n<p>Please check below AWS docs for more details</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html\" target=\"_blank\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/ssl-server-cert.html</a></li>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-create-https-ssl-load-balancer.html#create-https-lb-console\" target=\"_blank\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-create-https-ssl-load-balancer.html#create-https-lb-console</a></li>\r\n</ul>\r\n\r\n<p>Please refer to the following link to TCP to pass through traffic to the ELB &quot;as-is&quot;</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/https-tcp-passthrough.html\" target=\"_blank\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/https-tcp-passthrough.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18510,
          "question_id": 28393,
          "answers": [
            {
              "choice": "<p>Configure ELB with TCP listeners on TCP/443 and place the Web servers behind it.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure your Web servers with EIPs. Place the Web servers&#39;&nbsp;IPs as endpoints of Route53 Record Sets&nbsp;and configure health checks against all Web servers.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure ELB with HTTPS listeners, and place the Web servers behind it.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure your web servers as the targets&nbsp;for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28394,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are designing a connectivity solution between on-premises infrastructure and Amazon VPC. Your servers on-premises will be communicating with your VPC instances. You will be establishing IPSec tunnels over the internet. You will be using Virtual Private&nbsp;Gateways and terminating the IPsec tunnels on AWS-supported customer gateways.</p>\r\n\r\n<p>You need to use IPSec tunnels for your connectivity between your on-premises environment and your VPC instances.</p>\r\n\r\n<p>Which of the following statements are correct concerning IPSec ( choose 4 )?</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C, D, E and F</strong></p>\r\n\r\n<p>IPSec is designed to provide authentication, integrity, and confidentiality of the data that is being transmitted. IPSec operates at the network layer of the OSI model. Hence, it only protects the data that is in transit over the internet. For the full security of the data transmission, it is essential that both the sender and receiver need to be IPSec-aware.</p>\r\n\r\n<p>See the diagram of this scenario:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_grbwxj.png\" style=\"height:560px; width:660px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because (a) IPSec operates at the network layer of the OSI model. Hence, it only protects the data that is in transit ( encryption of data in transit ONLY) over the internet, and (b) both the source and the destination (client and server) may not be IPSec aware.</li>\r\n\t<li>Option B is incorrect because the identity authentication of the origin of the data has to be done at the application layer, not the network layer.</li>\r\n\t<li>Option C is CORRECT because the data that is transiting via the IPSec tunnel is encrypted.</li>\r\n\t<li>Option D is CORRECT because IPSec protects the data in transit over the internet (fundamental responsibility of IPSec tunnel).</li>\r\n\t<li>Option E is CORRECT because, in this scenario, it is a pre-requisite to have the Peer identity authentication between VP&nbsp;gateway and customer gateway for implementing an IPsec VPN tunnel. The IPSec tunnel is established between the VP&nbsp;Gateway (VPG) and Customer Gateway (CGW) whose identity gets authenticated during the IPSec tunnel setup.&nbsp;</li>\r\n</ul>\r\n\r\n<p><br />\r\n<strong>Since it is a pre-requisite even for establishing this connection we cannot term that as an objective that we have achieved via IPSec implementation.</strong><br />\r\n&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>Option F is CORRECT because - as mentioned earlier - the integrity of the data that is transiting via the IPSec tunnel is always preserved (fundamental responsibility of IPSec tunnel).</li>\r\n</ul>\r\n\r\n<p>For more information on IPSec tunnel, please refer to</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://techgenix.com/securing_data_in_transit_with_ipsec/\" target=\"_blank\">http://techgenix.com/securing_data_in_transit_with_ipsec/</a></li>\r\n</ul>\r\n\r\n<p>The below link provides an article on the general working of an IPSec tunnel which outlines the advantages of an IPSec tunnel which includes:</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html\" target=\"_blank\">http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18511,
          "question_id": 28394,
          "answers": [
            {
              "choice": "<p>&nbsp;</p>\r\n\r\n<p>End-to-end protection of data.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>End-to-end Identity authentication.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Data encryption across the Internet.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Protection of data in transit over the Internet.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Peer identity authentication between Virtual Private&nbsp;Gateway and customer gateway is achieved as it is imperative for its implementation.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Data integrity protection across the Internet.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28395,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You are designing an intrusion detection prevention (IDS/IPS) solution for a customer&#39;s web application in a single VPC. You are considering the options for implementing IDS/IPS protection for traffic coming from the Internet. Which of the following options would you consider?</p>\r\n\r\n<p>Choose 2 options from the below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A and D</p>\r\n\r\n<p>The main responsibility of Intrusion Detection Systems (IDS) / Intrusion Prevention Systems (IPS) is to (a) detect the vulnerabilities in your EC2 instances, (b) protect your EC2 instances from attacks, and (c) respond to intrusion or attacks against your EC2 instances.</p>\r\n\r\n<p>The IDS is an appliance that is installed on the EC2 instances that continuously monitors the VPC environment to see if any malicious activity is happening and alerts the system administrator if such activity is detected. On the other hand, IPS is an appliance installed on the EC2 instances that monitors and analyzes the incoming and outgoing network traffic for any malicious activities and prevents the malicious requests from reaching the instances in the VPC.</p>\r\n\r\n<p>This scenario is asking you how you can set up IDS/IPS in your VPC. There are few well-known ways: (a) install the IDS/IPS agents on the EC2 instances of the VPC so that the activities of that instance can be monitored, (b) set up IDS/IPS on a proxy server/NAT through which the network traffic is flowing, or (c) set up a Security-VPC that contains EC2 instances with IDS/IPS capability and peer that VPC with your VPC and always accept the traffic from Security-VPC only.</p>\r\n\r\n<p>Option A is CORRECT because it implements the IDS/IPS agents on each EC2 instance in the VPC.</p>\r\n\r\n<p>Option B is incorrect because AWS does not support the promiscuous mode.</p>\r\n\r\n<p>Option C is incorrect because ELB with SSL does not have the intrusion detection/prevention capability.</p>\r\n\r\n<p>Option D is CORRECT because a reverse proxy server through which the traffic from instances inside VPC flows outside of it has the IDS/IPS agent installed.</p>\r\n\r\n<p>For more information on intrusion detection systems in AWS, please refer to the below link-</p>\r\n\r\n<p><a href=\"https://awsmedia.s3.amazonaws.com/SEC402.pdf\" target=\"_blank\">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18512,
          "question_id": 28395,
          "answers": [
            {
              "choice": "<p>Implement IDS/IPS agents on each Instance running In VPC.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Implement Elastic Load Balancing with SSL listeners in front of the web applications.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28396,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are designing a photo-sharing mobile app. The application will store all pictures in a single Amazon S3 bucket. Users will upload pictures from their mobile devices directly to Amazon S3 and will be able to view and download their own pictures directly from Amazon S3. You want to configure security to handle the potential users in the most secure manner possible.</p>\n\n<p>What should be done by your server-side application when a new user registers on the photo-sharing mobile application?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B</p>\n\n<p>This scenario requires the mobile application to have access to the S3 bucket. There are potentially millions of users and a proper security measures should be taken. It is suggested to set up a web identity federation through AWS Cognito.</p>\n\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_t35pmz.png\" style=\"height:373px; width:700px\" />You can let users sign in using a well-known third-party identity provider such as log in with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider. You can exchange the credentials from that provider for temporary permissions to use resources in your AWS account. This is known as the&nbsp;<em>web identity federation</em>&nbsp;approach to temporary access. When you use web identity federation for your mobile or web application, you don&#39;t need to create custom sign-in code or manage your own user identities. Using a web identity federation helps you keep your AWS account secure because you don&#39;t have to distribute long-term security credentials, such as IAM user access keys, with your application.</p>\n\n<p>Option A is incorrect because you should always grant the short-term or temporary credentials for the mobile application. This option asks to create long-term credentials.</p>\n\n<p>Option B is CORRECT because it configures the web identity federation through AWS Cognito by generating&nbsp;temporary security credentials using STS &quot;AssumeRole&quot; function.</p>\n\n<p>Option C is incorrect because, even though the setup is very similar to option B, it does not create IAM Role with proper permissions, which is essential.</p>\n\n<p>Option D is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create an IAM Role to access the AWS Resource via the &quot;AssumeRole&quot; function.</p>\n\n<p>Option E is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create an IAM Role to access the AWS Resource via the &quot;AssumeRole&quot; function.</p>\n\n<p>For more information on AWS temporary credentials, please refer to the below links-</p>\n\n<p><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a><br />\n<a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\" target=\"_blank\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18513,
          "question_id": 28396,
          "answers": [
            {
              "choice": "<p>Create a set of long-term credentials using the AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Set up web identity federation through Amazon Cognito for the mobile app. Use Cognito API operations to get a Cognito token and request temporary security credentials from AWS STS. Use the temporary credentials to access Amazon S3.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Record the user&rsquo;s Information In Amazon DynamoDB. When the user uses their mobile app create temporary credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app&rsquo;s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM user. Assign appropriate permissions to the IAM user Generate an access key and secret key for the IAM user, store them in the mobile app, and use these credentials to access Amazon S3.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IAM user. Update the bucket policy with appropriate permissions for the IAM user. Generate an Access Key and Secret Key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28397,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": 0,
      "question_text": "<p>You have an application running on an EC2 Instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - C</strong></p>\r\n\r\n<p>An IAM&nbsp;<em>role</em>&nbsp;is similar to a user. In that, it is an AWS identity with permission policies that determine what the identity can perform in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. If a user is assigned a role, access keys are created dynamically and provided to the user.</p>\r\n\r\n<p>You can use roles to delegate access to users, applications, or services that don&#39;t normally have access to your AWS resources.</p>\r\n\r\n<p>Whenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User. The reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service. This has security concerns. Whereas, when an IAM Role is created, it has all the necessary policies attached to it. So, the use of the access key and the secret key is not needed. This is the preferred approach.</p>\r\n\r\n<p><strong>Option A is incorrect</strong> because you should use the IAM Role.&nbsp;See the explanation given above.</p>\r\n\r\n<p><strong>Option B is incorrect </strong>because instead of IAM User, you should use the IAM Role. See the explanation given above.</p>\r\n\r\n<p><strong>Option C is CORRECT </strong>because (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.</p>\r\n\r\n<p><strong>Option D is incorrect</strong> because instead of IAM User, you should use the IAM Role. See the explanation given above.</p>\r\n\r\n<p>For more information on IAM roles, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 72540,
          "question_id": 28397,
          "answers": [
            {
              "question_id": "28397",
              "choice": "<p>Use the AWS account access Keys. The application retrieves the credentials from the source code of the application.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28397",
              "choice": "<p>Create an IAM user for the application with permissions that allow list access to the S3 bucket, launch the instance as the IAM user, and retrieve the IAM user&rsquo;s credentials from the EC2 instance user data.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28397",
              "choice": "<p>Create an IAM role for EC2 that allows permission to list objects in the S3 bucket. Launch the instance with that&nbsp;role and assume&nbsp;the role&rsquo;s credentials from the EC2 Instance metadata.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28397",
              "choice": "<p>Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28398,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are designing a social media site and are considering how to mitigate distributed denial-of-service (DDoS) attacks. Which of the below can be used to notify and mitigate the attacks?&nbsp;(Select THREE).</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C, D, and E</strong></p>\r\n\r\n<p>This question is asking you to select some of the most recommended and widely used DDoS mitigation techniques.</p>\r\n\r\n<p><strong>What is a DDoS Attack?</strong></p>\r\n\r\n<p>A Distributed Denial of Service (DDoS) attack is an attack orchestrated by distributed multiple sources that make your web application unresponsive and unavailable for the end-users.</p>\r\n\r\n<p><strong>DDoS Mitigation Techniques</strong></p>\r\n\r\n<p>Some of the recommended techniques for mitigating the DDoS attacks are&nbsp;</p>\r\n\r\n<p>(i) building the architecture using the AWS services and offerings that can protect the application from such attacks. e.g. CloudFront, WAF, Autoscaling, Route53, VPC, etc.</p>\r\n\r\n<p>(ii) defending the infrastructure layer by over-provisioning capacity and deploying DDoS mitigation systems.</p>\r\n\r\n<p>(iii) defending&nbsp;the application layer by using WAF and operating at scale by using autoscale so that the application can withstand the attack by scaling and absorbing the traffic.</p>\r\n\r\n<p>(iv) minimizing the surface area of attack</p>\r\n\r\n<p>(v) obfuscating the AWS resources</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because ENIs do not help in increasing the network bandwidth.</li>\r\n\t<li>Option B is incorrect because having dedicated instances performing at maximum capacity will not help mitigate the DDoS attack. What is needed is instances behind auto-scaling so that the traffic can be absorbed while actions are being taken on the attack and the application can continue responding to the clients.</li>\r\n\t<li>Option C is CORRECT because WAF can protect against DDoS attacks and users can define rules to allow or block traffic.</li>\r\n\t<li>Option D is CORRECT because ELB helps distribute the traffic to the auto-scaling instances (helps to absorb the traffic).</li>\r\n\t<li>Option E is CORRECT because CloudWatch alarms can be used to trigger an SNS notification so that the team can be alerted of the high traffic.</li>\r\n</ul>\r\n\r\n<p>Note:&nbsp;Advanced Shield would be a better solution. There is a cost factor attached to it.</p>\r\n\r\n<p>It is very important to read the AWS Whitepaper on Best Practices for DDoS Resiliency.</p>\r\n\r\n<p><a href=\"https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\" target=\"_blank\">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18515,
          "question_id": 28398,
          "answers": [
            {
              "choice": "<p>Add multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use dedicated instances to ensure that each instance has the maximum performance possible.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Deploy AWS WAF on an Amazon CloudFront distribution to prevent DDoS attacks to the application.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use an Elastic Load Balancer with Auto Scaling Groups at the Web Application layer to scale up when the traffic is increasing.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create Amazon CloudWatch alarms to alert for a high network in and CPU utilization and notify the team with an SNS notification.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28399,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A benefits enrollment company hosts a 3-tier web application running in a VPC on AWS which includes a NAT (Network Address Translation) instance in the public Web tier. There is enough provisioned capacity for the expected workload for the new fiscal year benefit enrollment period plus some extra overhead. Enrollment proceeds nicely for two days, but&nbsp;the web tier becomes unresponsive upon investigation using CloudWatch and other monitoring tools. It is discovered that there is a huge and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80 from a country where the benefits company has no customers. The web tier instances are so overloaded that benefit enrollment administrators cannot even SSH into them. Which activity would be useful in defending against this attack?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; D</p>\r\n\r\n<p>In this scenario, the attack comes from a set of certain IP addresses over a specific port from a specific country. You are supposed to defend against this attack.&nbsp;</p>\r\n\r\n<p>In such questions, always think about two options: Security groups and Network Access Control List (NACL). Security Groups operate at the individual instance level, whereas NACL operates at the subnet level. You should always fortify the NACL first, as it is encounter first during the communication with the instances in the VPC.</p>\r\n\r\n<p>Option A is incorrect because IP addresses cannot be blocked using the route table or IGW.</p>\r\n\r\n<p>Option B is incorrect because changing the NAT instance&#39;s EIP cannot block the incoming traffic from a particular IP address.</p>\r\n\r\n<p>Option C is incorrect because (a) you cannot deny port access using security groups, and (b) by default all requests are denied; you open access for a particular IP address or range. You cannot deny access to particular IP addresses using security groups.</p>\r\n\r\n<p>Option D is CORRECT because (a) you can add deny rules in NACL and block access to certain IP addresses. See an example below.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_gak95i.png\" style=\"height:326px; width:762px\" /></p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html\" target=\"_blank\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18516,
          "question_id": 28399,
          "answers": [
            {
              "choice": "<p>Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (Internet Gateway).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route Table with the new EIP.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create 15 Security Group rules to block the attacking IP addresses over port 80.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an inbound NACL (Network Access Control List) associated with the web tier subnet with deny rules to block the attacking IP addresses.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28400,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your fortune 500 company has undertaken a TCO analysis evaluating the use of Amazon S3 versus acquiring more hardware. The outcome was that all employees would be granted access to use Amazon S3 for storage of their personal documents. Which of the following will you need to consider so that you can set up a solution that incorporates a single sign-on from your corporate AD or LDAP directory?</p>\r\n\r\n<p>Choose 3 options from the below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A, B, and D</strong></p>\r\n\r\n<p>In questions like this where an application or user needs to be given access using Single Sign On (SSO), the following steps are very important.</p>\r\n\r\n<p>(i) setting up an identity provider for federated access</p>\r\n\r\n<p>(ii) authenticating users using corporate data store or&nbsp;active directory-user-attributes.</p>\r\n\r\n<p>(iii) getting temporary access tokens/credentials using AWS STS</p>\r\n\r\n<p>(iv) creating the IAM Role that has access to the needed AWS Resources</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because as mentioned above, setting up an identity provider for federated access is needed.</li>\r\n\t<li>Option B is CORRECT because as mentioned above, getting temporary access tokens/credentials using AWS STS is needed.</li>\r\n\t<li>Option C is incorrect because tagging each folder in a bucket does not help in this scenario.</li>\r\n\t<li>Option D is CORRECT because as mentioned above, creating the IAM Role that has access to the needed AWS Resources is needed.</li>\r\n\t<li>Option E is incorrect because you should be creating IAM Roles rather than IAM Users.&nbsp;</li>\r\n</ul>\r\n\r\n<p>The diagram below showcases how authentication is carried out when having an identity broker. This is an example of a SAML connection, but the same concept holds true for getting access to an AWS resource.</p>\r\n\r\n<p>&nbsp;<img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_cl3rab.png\" style=\"height:423px; width:700px\" /></p>\r\n\r\n<p>For more information on federated access, please visit the below link-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18517,
          "question_id": 28400,
          "answers": [
            {
              "choice": "<p>Setting up a federation proxy or identity provider.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Using AWS Security Token Service to generate temporary tokens.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Tagging each folder in the bucket.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configuring IAM roles that have suitable permissions to the related S3 resources.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Setting up a matching IAM user for every user in your corporate directory that needs access to a folder in the bucket.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28401,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your company policies require encryption of sensitive data at rest. You consider the possible options for protecting data while storing it at rest on an EBS data volume attached to an EC2 instance. Which of these options would allow you to encrypt your data at rest?</p>\r\n\r\n<p>Choose 3 options from the below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A, C,&nbsp; and D</p>\r\n\r\n<p>You can encrypt the data at rest by either using native data encryption, using a third-party encrypting tool or just encrypt the data before storing it on the volume.</p>\r\n\r\n<p>Option A CORRECT because it uses a third-party volume encryption tool.</p>\r\n\r\n<p>Option B is incorrect because EBS volumes are not encrypted by default.</p>\r\n\r\n<p>Option C is CORRECT as it encrypts the data before storing it on EBS.</p>\r\n\r\n<p>Option D is CORRECT as it uses native data encryption.</p>\r\n\r\n<p>Option E is incorrect as SSL/TLS is used to secure the data in transit, not at rest.</p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18518,
          "question_id": 28401,
          "answers": [
            {
              "choice": "<p>Implement third party volume encryption tools</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Do nothing as EBS volumes are encrypted by default</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Encrypt data inside your applications before storing it on EBS</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Encrypt data using native data encryption drivers at the file system level</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Implement SSL/TLS for all services running on the server</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28375,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are migrating a legacy client-server application to AWS. The application responds to a specific DNS domain (e.g., www.example.com) and has a 2-tier architecture, with multiple application servers and a database server. Remote clients use TCP to connect to the application servers. The application servers need to know the clients&#39; IP address to function properly and are currently taking that information from the TCP socket. A decision is made to use a multi-AZ RDS MySQL instance for the database. During the migration, you can change the application code. But you have to file a change request.</p>\r\n\r\n<p>Which of the following options would help&nbsp;to maximize scalability and high-availability for the architecture on AWS?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p><br />\r\nAWS ELB has support for Proxy Protocol. It simply depends on a humanly readable header with the client&#39;s connection information to the TCP data sent to your server.&nbsp;As per the AWS documentation, the Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections. Because load balancers intercept traffic between clients and your instances, the access logs from your instance contain the load balancer&#39;s IP address instead of the originating client. You can parse the first line of the request to retrieve your client&#39;s IP address and the port number.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because it implements the proxy protocol and uses ELB with a TCP listener. A change request is needed to support the proxy protocol in the application. With the proxy protocol in ELB, the backend servers can get the IP addresses of the clients.</li>\r\n\t<li>Option B is incorrect because, although implementing cross-zone load balancing provides high availability, it states to use TCP forwarding, which does not support X-Forwarded-For.</li>\r\n\t<li>Option C is incorrect because Route53 latency-based routing does not give the IP address of the clients.</li>\r\n\t<li>Option D is incorrect because the Route53 Alias record does not give the IP address of the clients.</li>\r\n</ul>\r\n\r\n<p>For more information on ELB enabling support for TCP, please refer to the links given below.</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/\" target=\"_blank\">https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/</a></li>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html\" target=\"_blank\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18492,
          "question_id": 28375,
          "answers": [
            {
              "choice": "<p>File a change request to implement Proxy Protocol Support in the application. Use&nbsp;an ELB with a TCP Listener and Proxy Protocol enabled to distribute the load on two application servers in different AZs.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>File a change request to Implement Cross-Zone support in the application. Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled to distribute the load on two application servers in different AZs.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>File a change request to implement Latency Based Routing support in the application. Use Route 53 with Latency Based Routing enabled to distribute the load on two application servers in different AZs.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>File a change request to implement Alias Resource Support in the application.&nbsp;Use a Route 53 Alias Resource Record to distribute the load on two application servers in different AZs.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28402,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You have an application that&nbsp;analyzes images within files.&nbsp;For each of the files in the input stream, the application writes&nbsp;to some&nbsp;number&nbsp;of output files. The number of input files processed each day is&nbsp;high and concentrated within a few hours of the day.</p>\r\n\r\n<p>You currently have an EC2 instance with a single large EBS volume that hosts the input data. It takes almost 20 hours per day to sync the monolithic EBS volume.</p>\r\n\r\n<p>What service(s) could be used to reduce the sync time and improve the solution&#39;s availability?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>The scenario in this question is that (a) there are EC2 instances that need to process a high number of input files, (b) currently the processing takes 20 hrs a day, which needs to be reduced, (c) the availability needs to be improved.</p>\r\n\r\n<p>First, let&#39;s see whether we should choose S3 or EBS with PIOPS. It appears that all the options have auto-scaling in common.&nbsp;There will be multiple EC2 instances working in parallel on the input data.&nbsp;This should reduce the overall elaboration time, satisfying one of the requirements.</p>\r\n\r\n<p>You can enable Multi-Attach on Amazon EBS Provisioned IOPS io1 volumes to allow a single volume to be concurrently attached to up to sixteen AWS Nitro System-based Amazon Elastic Compute Cloud (Amazon EC2) instances within the same Availability Zone which&nbsp;does not guarantee the availability. Whereas S3 provides high availability, which satisfies the other requirements.&nbsp;</p>\r\n\r\n<p>Second, SQS is a great option to do autonomous tasks and can queue service requests, and can be scaled to meet the high demand. SNS is a mere notification service and would not hold the tasks. Hence, SQS is certainly the correct choice.&nbsp;</p>\r\n\r\n<p>Option A is CORRECT because, as mentioned above, it provides high availability and can store a massive amount of data. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps distribute the commands/tasks to EC2 instances.</p>\r\n\r\n<p>Option B is incorrect because, as mentioned above, neither EBS nor SNS is a valid choice in this scenario.</p>\r\n\r\n<p>Option C is incorrect because, as mentioned above, SNS is not a valid choice in this scenario.</p>\r\n\r\n<p>Option D is incorrect because, as mentioned above, EBS is not a valid choice in this scenario.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18519,
          "question_id": 28402,
          "answers": [
            {
              "choice": "<p>Use S3 to store the files. Use SQS to store the elaboration commands. Configure an Auto Scaling group to process the messages in the queue and dynamically adjust the size of the group depending on the length of the SQS queue.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SNS to distribute elaboration commands to a group of hosts working in parallel and Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use S3 to store I/O files and SNS to distribute elaboration commands to a group of hosts working in parallel. Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SQS to distribute elaboration commands to a group of hosts working in parallel. Use Auto Scaling to dynamically size the group of hosts depending on the length of the SQS queue.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28403,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You require the ability to analyze a customer&rsquo;s clickstream data on a website to do the behavioral analysis. Your customer needs to know what sequence of pages and ads their customer clicked on. This data will be used in real-time to modify the page layouts as customers click through the site to increase stickiness and advertising click-through. Which option meets the requirements for capturing&nbsp;and analyzing this data?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B</p>\r\n\r\n<p>Whenever the question presents a scenario where the application needs to analyze real-time data such as clickstream (i.e.massive real-time data analysis), most of the time the best option is Amazon Kinesis. It is used to collect and process large&nbsp;<a href=\"https://aws.amazon.com/streaming-data/\" target=\"_blank\">streams</a>&nbsp;of data records in real-time.</p>\r\n\r\n<p>You&#39;ll create data-processing applications, known as&nbsp;<em>Amazon Kinesis Streams applications</em>. A typical Amazon Kinesis Streams application reads data from an&nbsp;<em>Amazon Kinesis stream</em>&nbsp;as data records. These applications can use the Amazon Kinesis Client Library, and they can run on Amazon EC2 instances. The processed records can be sent to dashboards, used to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services.</p>\r\n\r\n<p>The below diagrams from the AWS documentation shows how you can create custom streams in Amazon Kinesis.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_wxpg8i.png\" style=\"height:255px; width:700px\" /></p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_oztniz.png\" style=\"height:321px; width:700px\" /></p>\r\n\r\n<p>Refer to page 129 on the below link.</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-dg.pdf\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-dg.pdf</a></p>\r\n\r\n<p><strong>Application Name</strong></p>\r\n\r\n<p>The KCL requires an application name that is unique across your applications and across Amazon DynamoDB tables in the same Region. It uses the application name configuration value in the following ways: &bull; All workers associated with this application name are assumed to be working together on the same stream. These workers may be distributed on multiple instances. If you run an additional instance of the same application code, but with a different application name, the KCL treats the second instance as an entirely separate application that is also operating on the same stream. &bull; The KCL creates a DynamoDB table with the application name and uses the table to maintain state information (such as checkpoints and worker-shard mapping) for the application. Each application has its own DynamoDB table</p>\r\n\r\n<p>For more information on Kinesis, please visit the below link-</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/streams/latest/dev/introduction.html\" target=\"_blank\">http://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18520,
          "question_id": 28403,
          "answers": [
            {
              "choice": "<p>Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce.&nbsp;</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Write the click events directly to Amazon Redshift and then analyze with SQL.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Publish web clicks by session to an Amazon SQS queue, periodically drain these events to Amazon RDS, and analyze with SQL.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28404,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>An AWS customer runs a public blogging website. The site users upload two million blog entries a month. The average blog entry size is 200 KB. The access rate to blog entries drops to negligible 6 months after publication, and users rarely access a blog entry 1 year after publication. Additionally, blog entries have a high update rate during the first 3 months following publication. This drops to no updates after 6 months. The customer wants to use CloudFront to improve his user&rsquo;s load times. Which of the following recommendations would you make to the customer?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\r\n\r\n<p>CloudFront allows you to configure caching based on a URL path pattern when you create a new distribution. By partitioning the S3 bucket by the month, the blog post was created. You can control the CloudFront caching behavior of the distribution to optimize cost.</p>\r\n\r\n<p>This question is based on making the best use of CloudFront&#39;s Cache Behavior. You need to understand two things about CloudFront for such scenario: (1) CloudFront is a service that is designed to give geographically distributed users fast access to the content by maintaining the content in the cache that is maintained at multiple edge locations, and (2) using the cache-behavior of CloudFront, you can control the origin and path of the content, time to live (TTL) and control the user access using trusted signers.</p>\r\n\r\n<p>In this scenario, you need to control the content based on the time period at which the blog is published. i.e., when a blog is published, you need to cache the update for the first 3 months so that the users can quickly access it.&nbsp;After six months from the update, the content can be removed from the cache, as it is rarely accessed. Also, you need to make sure that CloudFront only accesses the content.</p>\r\n\r\n<ul>\r\n\t<li><strong>Option A is incorrect</strong> because maintaining two separate buckets will not improve the load time for the users.</li>\r\n\t<li><strong>Option B is incorrect</strong> as the location-wise distribution will not improve the load time for the users.</li>\r\n\t<li><strong>Option C is CORRECT </strong>because if&nbsp;(a) the content is only accessed by CloudFront, and (b) if the content is partitioned at the origin based on the month it was uploaded.&nbsp;You can control the cache behavior accordingly and keep only the latest updated content in the CloudFront cache so that it can be accessed with fast load-time,&nbsp;hence, improving the performance.&nbsp;For the contents that are over 6 months, you do not need to put them&nbsp;in the cache.</li>\r\n\t<li><strong>Option D is incorrect</strong>. The scenario states that the customer is running a public access blogging website. So there is no need to restrict viewer access.&nbsp;</li>\r\n</ul>\r\n\r\n<p>For more information on Cloudfront identity, please visit the below links-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesCacheBehavior\" target=\"_blank\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesCacheBehavior</a></li>\r\n\t<li><a href=\"http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\" target=\"_blank\">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 89468,
          "question_id": 28404,
          "answers": [
            {
              "question_id": "28404",
              "choice": "<p>Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to Cloud Front identity.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28404",
              "choice": "<p>Create a CloudFront distribution with &ldquo;US&rsquo;Europe price class for US/Europe users and a different CloudFront distribution with All Edge Locations&rsquo; for the remaining users.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28404",
              "choice": "<p>Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry&rsquo;s location in S3 according to the month it was uploaded to be used with CloudFront behaviors.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28404",
              "choice": "<p>Create a CloudFront distribution with Restrict Viewer Access Forward Query string set to true and minimum TTL of 0.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28405,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Your company is getting ready to make a major public announcement of a social media site on AWS. The website runs on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model.</p>\r\n\r\n<p>After comprehensive tests, you discover that there is read contention on RDS MySQL. Which of the following are&nbsp;the best approaches to meet these requirements (assuming&nbsp;we do not need to factor in the cost of development time)? Choose 2 options&nbsp;below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A and D</strong></p>\r\n\r\n<p>One of AWS pillars of a well-architected framework is cost optimization which incorporates &quot;Right Sizing&quot;. AWS defines right Sizing using the lowest cost resource that still meets the technical specifications of the specific workload. Options B and C do not meet those AWS standards.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because ElastiCache is an in-memory caching solution that reduces the load on the database and improves the read performance.</li>\r\n\t<li>Option B is INCORRECT because splitting the RDS into multiple databases increases both the read and write. Therefore wasting money on increased write capacity is not required. Also, Sharding increases the maintenance overhead as now multiple databases must be maintained and the application must be refactored to incorporate the additional connection strings.&nbsp;</li>\r\n\t<li>Option C is INCORRECT because this increases both read and write. Therefore wasting money on increased write capacity which is not required.</li>\r\n\t<li>Option D is CORRECT because Read Replicas are used to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Hence, improving the read performance.</li>\r\n</ul>\r\n\r\n<p>See more information on Read Replicas and ElastiCache below.</p>\r\n\r\n<p><strong>Read Replicas</strong></p>\r\n\r\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond a single DB Instance&#39;s capacity constraints for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.</p>\r\n\r\n<p>For more information on Read Replica&rsquo;s, please visit the below link:</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/rds/details/read-replicas/\" target=\"_blank\">https://aws.amazon.com/rds/details/read-replicas/</a></li>\r\n</ul>\r\n\r\n<p><strong>ElastiCache</strong></p>\r\n\r\n<p>Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in&nbsp;the cloud. The service improves web applications&#39; performance by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.&nbsp;</p>\r\n\r\n<p>For more information on Amazon ElastiCache, please visit the below link-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/elasticache/\" target=\"_blank\">https://aws.amazon.com/elasticache/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18522,
          "question_id": 28405,
          "answers": [
            {
              "choice": "<p>Deploy ElasticCache in-memory cache running in each availability zone.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Implement sharding to distribute the load to multiple RDS MySQL instances.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Increase the RDS MySQL Instance size and implement provisioned IOPS.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Add an RDS MySQL read replica in each availability zone.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28406,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A company runs a batch analysis with transactional reporting every hour on its main transactional DB running on an RDS MySQL instance to populate its central Data Warehouse running on Redshift. During the execution of the batch, their transactional applications are very slow. When the batch completes, they need to update the top management dashboard with the new data. The dashboard is produced by another system running on-premises that is currently started when a manually-sent email notifies that an update is required. The on-premises system cannot be modified because it is managed by another team.</p>\r\n\r\n<p>How would you optimize this scenario to solve performance issues and automate the process as much as possible?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - C</p>\r\n\r\n<p>There are two architectural considerations here. (1) you need to improve read performance by reducing the load on the RDS MySQL instance, and (2) automate the process of notifying to the on-premise system.</p>\r\n\r\n<p>When the scenario asks you to improve a DB instance&#39;s read performance, always look for options such as ElastiCache or Read Replicas. And when the question asks you to automate the notification process, always think of using SNS.&nbsp;</p>\r\n\r\n<p>Options A and B are incorrect because replacing RDS with Redshift may need many changes on the applications and are not required in this scenario.</p>\r\n\r\n<p>Option C is CORRECT because (a) it uses Read Replicas which improves the read performance, and (b) it uses SNS which automates the process of notifying the on-premise system to update the dashboard.</p>\r\n\r\n<p>Option D is incorrect because SQS is not a service to be used for sending the notification.</p>\r\n\r\n<p>For more information on Read Replica&rsquo;s, please visit the below link-</p>\r\n\r\n<p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\" target=\"_blank\">https://aws.amazon.com/rds/details/read-replicas/</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18523,
          "question_id": 28406,
          "answers": [
            {
              "choice": "<p>Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an RDS Read Replica for the batch analysis and SNS to notify the on-premises system to update the dashboard.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create an RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 53201,
      "topic_id": 367,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are implementing a URL whitelisting system for a company that wants to restrict outbound HTTPS connections to specific domains from their EC2-hosted applications. You deploy a single t2.micro EC2 instance running proxy software and configure it to accept traffic from all subnets and EC2 instances in the VPC. You configure the proxy to only pass through traffic to domains that you define in its whitelist configuration.</p>\r\n\r\n<p>You have a nightly maintenance window or 10 minutes where all instances fetch new software updates. Each update is different and has about 200MB in size. There are 500 instances In the VPC that routinely fetch updates. There is no issue for most of the days. However, you notice that some machines fail to download certain software updates within the maintenance window for a few days. The download URLs used for these updates are correctly listed in the proxy&rsquo;s whitelist configuration, and you can access them manually using a web browser on the instances. What might be happening?</p>\r\n\r\n<p>Choose an answer&nbsp;from the options below.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer: A&nbsp;</strong></p>\r\n\r\n<p>This scenario contains the following main points: (1) there is a single EC2 instance running proxy software that either acts as or connects to a NAT instance. The NAT instances are not AWS managed; they are user-managed; so, it may become the bottleneck, (2) there is a whitelist maintained so that limited outside access is given to the instances inside VPC, (3) the URLs in the whitelist are correctly maintained. Hence, whitelist is not an issue, (4) only some machines are having download problems with some updates. i.e. some updates are successful on some machines.</p>\r\n\r\n<p>This indicates that there is no setup issue, but most likely, it is the proxy instance that is a bottleneck and under-performing or inconsistently performing. As the proxy instance is not part of an auto-scaling group, its size must be definitely the issue.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT because due to the limited size of the proxy instance, its network throughput might not be sufficient to provide service to all the VPC instances (as only some of the instances are not able to download the updates).</li>\r\n\t<li>Option B is incorrect because limited storage on the proxy instance should not cause other instances of any problems in downloading the updates.</li>\r\n\t<li>Option C is incorrect because proxy instances are supposed to be in a public subnet, but the allocation of EIPs should not cause any issues for other instances in the VPC.</li>\r\n\t<li>Option D&nbsp;is incorrect because none of the instances would get the updates if this were the case. However, some of the instances could get the updates.&nbsp;So&nbsp;this cannot be the case.&nbsp;</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 43038,
          "question_id": 53201,
          "answers": [
            {
              "choice": "<p>You are running the proxy on an undersized EC2 instance type. So network throughput is not sufficient for all instances to download their updates in time.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>You have not allocated enough storage to the EC2 instance running the proxy. So the network buffer is filling up causing some requests to fail.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>You are running the proxy in a private subnet but&nbsp;have not allocated enough EIP&rsquo;s to support the needed network throughput through the Internet Gateway (IGW).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>The route table for the subnets containing the affected EC2 instances is not configured to direct network traffic for the software update locations to the proxy.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28408,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>To serve Web traffic for a popular product, your chief financial officer and the IT director have purchased 10 large Reserved Instances (RIs), evenly spread across two availability zones. Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular, and you need additional capacity. As a result, your company purchases two c5.2xlarge RI. You register the two c5.2xlarge instances with your ELB and quickly find while all the instances are at 100% of their capacity, the c5.2xlarge instances have a significant capacity that is unused. Which of the following is the most cost-effective solution that uses EC2 capacity most effectively?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; A</strong></p>\r\n\r\n<p>In this question, the problem is that the newly added c5.2xlarge instances are not fully utilized. This is happening because the load is spread evenly across all the instances. There is no logic to determine&nbsp;how much traffic is to be routed to which instance types.</p>\r\n\r\n<p>Hence, there is a need to add some logic where higher (more-weighted) traffic should be routed to c5.2xlarge instances and light-weighted to the other instances. Route 53&#39;s weighted routing policy does exactly like this, so you should look for this option.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Option A is Correct because it first creates separate ELBs, one each for the set of different instance types and uses Route 53&#39;s weighted routing policy such that a higher proportion of the load is routed to the ELB that has c5.2xlarge instances and a smaller proportion to the one with smaller instances.</p>\r\n\r\n<p>Option B is incorrect because shutting down c5.2xlarge instances will not effectively use the EC2 capacity. You have already paid for the instance so that you would lose money here.</p>\r\n\r\n<p>Option C is incorrect because latency-based routing may not always distribute heavy traffic to a large instance. You must use a weighted routing policy.</p>\r\n\r\n<p>Option D is incorrect because this option is not a good use of the existing capacity and would add to the cost.</p>\r\n\r\n<p>For more information on Route 53 weighted routing policy, please visit the URL below.</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\" target=\"_blank\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18525,
          "question_id": 28408,
          "answers": [
            {
              "choice": "<p>Use a separate ELB for each instance type and distribute the load to ELBs with Route53 Weighted Routing.<br />\r\n&nbsp;</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure AutoScaling group and Launch Configuration with ELB to add up to 10 more on-demand&nbsp;large instances when triggered by Cloudwatch shut off c5.2xiarge instances.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Route traffic to EC2 large and c5.2xlarge instances directly.&nbsp;Using Route 53 latency based routing and health checks shut off ELB.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure ELB with two c5.2xlarge Instances and use the on-demand AutoScaling group for up to two additional c5.2xlarge instances.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28409,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>A read-only news reporting site has a web application tier in EC2 and a database tier in RDS. The website receives large and unpredictable read traffic and automatically responds to these traffic fluctuations. Which of the following AWS services are the most suitable to meet the requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A</strong></p>\r\n\r\n<p>The scenario asks for 2 things: (1) a performance improvement solution for read-heavy web tier and database tier. Hint: Always see if any of the options contain caching solutions such as ElastiCache, CloudFront, or Read Replicas, and (2) whether to use stateless or stateful instances.</p>\r\n\r\n<p>Stateful instances are not suitable for distributed systems, as they retain the state of connection&nbsp;between client and web server. And&nbsp;the database remains engaged as long as the session is active. Hence, it increases the load on the server as well as the database. Stateless instances, however, are distributed and easy to scale in/scale-out. Hence, the stateless application tends to improve the performance of a distributed application.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li><strong>Option A is CORRECT </strong>because (a) it uses stateless instances, (b) the webserver uses ElastiCache for read operations, (c) it uses CloudWatch which monitors the fluctuations in the traffic and notifies to the auto-scaling group to scale in/scale-out accordingly, and (d) it uses read replicas for RDS to handle the read-heavy workload.</li>\r\n\t<li><strong>Option B is incorrect</strong> because (a) it uses stateful instances, and (b) it does not use any caching mechanism for web and application tier.</li>\r\n\t<li><strong>Option C is incorrect</strong> because (a) it uses stateful instances, (b) it does not use any caching mechanism for web and application tier, and (c) multi-AZ RDS does not improve read performance.</li>\r\n\t<li><strong>Option D is incorrect</strong> because multi-AZ RDS does not improve read performance.</li>\r\n</ul>\r\n\r\n<p>For more information on ElastiCache and Read Replicas, please refer to the following links-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/elasticache/\" target=\"_blank\">https://aws.amazon.com/elasticache/</a></li>\r\n\t<li><a href=\"https://aws.amazon.com/rds/details/read-replicas/\" target=\"_blank\">https://aws.amazon.com/rds/details/read-replicas/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 79601,
          "question_id": 28409,
          "answers": [
            {
              "question_id": "28409",
              "choice": "<p>Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS configured with read replicas. Add Amazon ElastiCache for RDS.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28409",
              "choice": "<p>Stateful instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with read replicas.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28409",
              "choice": "<p>Stateful instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with Multi-AZ.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28409",
              "choice": "<p>Stateless instances for the web application tier in an auto-scaling group monitored with CloudWatch. RDS with Multi-AZ.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28410,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>You are running a news website in the EU-west-1 region that updates every 15 minutes. The website has a worldwide audience. It uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS database. Static content resides on Amazon S3 and is distributed through Amazon CloudFront. Your Auto Scaling group is set to trigger a scale-up event at 60% CPU utilization.&nbsp;You use an Amazon RDS extra large DB instance with 10,000 Provisioned IOPS. Its CPU utilization is around 80%. Freeable memory is in the 2 GB range. Web analytics reports show that the average load time of your web pages is around 1.5 to 2 seconds, but your SEO consultant wants to bring down the average load time to under 0.5 seconds. Which of the following option&nbsp;would NOT&nbsp;help&nbsp;to improve page load times (performance) for your users?&nbsp;</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A</strong></p>\r\n\r\n<p>In this scenario, there are three major points of consideration.</p>\r\n\r\n<p>(1) news website updates every 15 minutes,</p>\r\n\r\n<p>(2) current average load time is high, and</p>\r\n\r\n<p>(3) the performance of the website should be improved (i.e., read performance needs improvement).</p>\r\n\r\n<p>When the questions ask for performance improvement&nbsp;solutions for read-heavy applications, always see if any of the options contain caching solutions such as ElastiCache, CloudFront, or Read Replicas.</p>\r\n\r\n<ul>\r\n\t<li><strong>Option A (why option A is correct)</strong>: Lowering scale up trigger of ASG <strong>will not help</strong> to improve the page load time of users</li>\r\n\t<li><strong>Option B is INCORRECT</strong>&nbsp;because using an&nbsp;ElastiCache for storing sessions as well as for frequent DB queries would reduce&nbsp;the load on the database. This would help to increase the read performance. Since the question is asking for a <strong>NOT</strong> recommended option, this option is INCORRECT.</li>\r\n\t<li><strong>Option C is incorrect</strong> because it uses CloudFront, a network of globally distributed &quot;edge-locations&quot; that caches the content and improves user experience.</li>\r\n\t<li><strong>Option D is incorrect</strong> because scaling up the RDS read replicas will help improve read performance.</li>\r\n</ul>\r\n\r\n<p><strong>References:</strong></p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/dynamic-whole-site-delivery-with-amazon-cloudfront/\" target=\"_blank\">https://aws.amazon.com/blogs/networking-and-content-delivery/dynamic-whole-site-delivery-with-amazon-cloudfront/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 84717,
          "question_id": 28410,
          "answers": [
            {
              "question_id": "28410",
              "choice": "<p>Lower the scale-up trigger of your Auto Scaling group to 30%, so it scales more aggressively.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28410",
              "choice": "<p>Add an Amazon ElastiCache caching layer to your application for storing sessions and frequent DB queries.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28410",
              "choice": "<p>Configure Amazon CloudFront dynamic content support to enable caching of re-usable content from your site.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28410",
              "choice": "<p>Configure read replicas for RDS Database Instance.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28411,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large real-estate brokerage is exploring the option of adding a cost-effective location-based alert to their existing mobile application. The application backend infrastructure currently runs on AWS. Users who opt for&nbsp;this service will receive alerts on their mobile devices regarding real-estate offers in proximity to their location. For the alerts to be relevant, delivery time needs to be in the low minute count. The existing mobile app has 5 million users across the US. Which one of the following architectural suggestions would you make to the customer?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\r\n\r\n<p>The scenario has the following architectural considerations: (1) the users should get notifications about the real estate in the area near to their location, (2) only subscribed users should get the notification, (3) the notification delivery should be fast, (4) the architecture should be scalable, and (5) it should be cost-effective.</p>\r\n\r\n<p>When the question has considerations for scalability, always think about DynamoDB as it is the most recommended database solution to handle the huge amount of data/records. For automated notifications, always think about SNS.&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because sending notifications via mobile earners/device providers as alerts is neither feasible nor cost-effective.&nbsp;</li>\r\n\t<li>Option B is incorrect because receiving location via Direct Connect and carrier connection is not cost-effective.&nbsp;It also does not deal with subscriptions. Sending notifications via mobile carriers as the alert is not cost-effective as well.</li>\r\n\t<li>Option C is CORRECT because (a) SQS is a highly scalable, cost-effective solution for carrying out utility tasks such as holding the location of millions of users, (b) it uses highly scalable DynamoDB, and (c) it uses the cost-effective AWS SNS Mobile Push service to send push notification messages directly to apps on mobile devices.</li>\r\n\t<li>Option D is incorrect because the AWS SNS Mobile Push service is used to send push notification messages to the mobile devices, not to get the mobile devices&#39; location.</li>\r\n</ul>\r\n\r\n<p>For more information on AWS SNS Mobile Push, please see the diagram and link given below:</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html\" target=\"_blank\">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></li>\r\n</ul>\r\n\r\n<p><strong>Note:</strong></p>\r\n\r\n<p>Option C says that the mobile application will send the device location to the processing EC2 instances using SQS. Then the instances would look at the DynamoDB database for offers relevant to the location. Then finally, SNS&nbsp;Mobile Push, which is part of SNS, will be used to send offers to the mobile application. So it leverages both SQS as well as SNS functionality for different parts of the architecture. This is the correct solution to this problem.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18528,
          "question_id": 28411,
          "answers": [
            {
              "choice": "<p>The mobile application will submit its location details to a web service endpoint using ELB and EC2 instances. DynamoDB will be used to store and retrieve the relevant offers from EC2 instances, which would then communicate with mobile carriers or device providers to push alerts back to the mobile application.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use AWS DirectConnect or VPN to establish connectivity with the mobile carrier.&nbsp;EC2 instances will receive the mobile application&#39;s location details through the mobile carrier. RDS will be used to store and retrieve the relevant offers. EC2 instances will communicate with the mobile carrier&nbsp;to push alerts back to the mobile application.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>SQS ( buffer storage ) would be used to capture the device location details sent from the Mobile application. EC2 instances will process the messages from the SQS queue and retrieve the relevant offers from DynamoDB. SNS Mobile Push will be used to send offers to the mobile application.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>The mobile application will send the device location&#39;s details using the SNS&nbsp;Mobile Push. EC2 instances will retrieve the relevant offers from DynamoDB. EC2 instances will communicate with the mobile carrier or the device provider&nbsp;to push alerts back to the mobile application.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28376,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A newspaper organization has an on-premises application that allows the public to search its back catalog and retrieve individual newspaper pages via a website written in Java. It also has a commercial search application nearing its end of life. They have scanned the old newspapers into JPEGs which is of a total size of 17TB and used Optical Character Recognition (OCR) to populate a commercial search product. The organization wants to migrate its archive to AWS and produce a cost-efficient, highly-available and durable architecture. Which of the below options is the most appropriate?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\n\n<p>This question presents the following scenarios: (1) type of storage that can store a large amount of data (17TB), (2)&nbsp;the architecture should be cost-effective, highly available, and durable.</p>\n\n<p>Tip: Whenever a storage service stores a large amount of data with low cost, high availability, and high durability, always think about using S3.&nbsp;</p>\n\n<p>Option A is incorrect because even though it uses S3, it uses the commercial search software at the end of its life.</p>\n\n<p>Option B is incorrect because striped EBS is not as durable of a solution as S3 and certainly not as cost-effective as S3. Also, it has maintenance overhead.</p>\n\n<p>Option C is CORRECT because (a) it uses S3 to store the cost-effective images, (b) for more efficiency, it uses CloudSearch for query processing, and (c) with an Auto Scaling group&nbsp;in multi-AZs, it achieves high availability.</p>\n\n<p>Option D is incorrect because it does not have high availability with a single AZ RDS instance.</p>\n\n<p>Option E is incorrect because (a) this configuration is not scalable, and (b) it does not mention any origin for the CloudFront distribution.</p>\n\n<p><strong>Amazon CloudSearch</strong></p>\n\n<p>With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don&#39;t need to become a search expert or worry about hardware provisioning, setup, and maintenance. With a few clicks in the&nbsp;<a href=\"https://aws.amazon.com/console/\">AWS Management Console</a>, you can create a search domain and upload the data that you want to make searchable. Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.</p>\n\n<p>You can easily change your search parameters, fine-tune search relevance, and apply new settings at any time. As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.</p>\n\n<p>For more information on AWS CloudSearch, please visit the below link-</p>\n\n<p><a href=\"https://aws.amazon.com/cloudsearch/\" target=\"_blank\">https://aws.amazon.com/cloudsearch/</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18493,
          "question_id": 28376,
          "answers": [
            {
              "choice": "<p>Use S3 to store and serve the scanned files. Install the commercial search application on EC2 Instances and configure it with auto-scaling and an Elastic Load Balancer.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Model the environment using CloudFormation. Use an EC2 instance running Apache webserver and an open-source search application, stripe multiple standard EBS volumes together to store the JPEGs and search index.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use S3 to store and serve the scanned files. Use CloudSearch for query processing, and use an Auto Scaling group&nbsp;to host the website across multiple availability zones.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use a single-AZ RDS MySQL instance to store the search index for the JPEG images and use an EC2 instance to serve the website and translate user queries into SQL.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use a CloudFront download distribution to serve the JPEGs to the end users. Install the current commercial search product, along with a Java Container for the website on EC2 instances and use Route53 with DNS round-robin.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28412,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a voting system for a popular TV show. Viewers watch the performances then visit the show&rsquo;s website to vote for their favorite performer. It is expected that the site will receive millions of visitors in a short period of time after the show is finished. The visitors will first login to the site using their Amazon.com credentials and then submit their votes. After the voting is completed, the page will display the vote totals. The company needs to build a site to handle the rapid influx of traffic while&nbsp;maintaining good performance.</p>\r\n\r\n<p>Which of the design patterns below should they use?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - D</strong></p>\r\n\r\n<p>This scenario has the following architectural considerations: (1) the application needs to be scalable to handle traffic coming from millions of users, (2) the application should handle the rapid influx of traffic, maintaining good performance.&nbsp;</p>\r\n\r\n<p>When the application needs to handle the data coming from millions of users, always think about using DynamoDB. Also, to provide the global users with high-performance content access, you need to consider CloudFront. You need to set the appropriate IAM Role for the front end/web servers to access DynamoDB tables.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because multi-AZ RDS is not a preferable solution compared to DynamoDB.</li>\r\n\t<li>Option B is incorrect because IAM roles should be used to provide permissions to write the DynamoDB table.</li>\r\n\t<li>Option C is incorrect because DynamoDB needs a high write capacity unit. With this option, DynamoDB may have a performance issue.</li>\r\n\t<li>Option D is CORRECT because (a) it is highly scalable, (b) creates appropriate IAM Role to access the DynamoDB database, and (c) more importantly uses SQS to hold the user data/votes such that the application does not consume read and write provisioned capacity of DynamoDB.&nbsp;</li>\r\n</ul>\r\n\r\n<p>As per the scenario, once the user completes the voting, the web page should display the total number of votes submitted online.<br />\r\nOption D also includes an Autoscaling group of EC2 instances to handle the traffic.<br />\r\nHence option D is optimal.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18529,
          "question_id": 28412,
          "answers": [
            {
              "choice": "<p>Use CloudFront and an Elastic Load balancer in front of an auto-scaled set of web servers. The web servers will first login with the Amazon service to authenticate the users, then process the users&#39; votes and store the votes into a multi-AZ Relational Database Service instance.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use CloudFront and the static website hosting feature of S3 with the Javascript SDK to call the login with Amazon service to authenticate the user. Configure IAM users to gain permissions to a DynamoDB table to store the users&#39; votes.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. The web servers will first login with Amazon service to authenticate the user. The web servers will process the users&#39; votes and store the votes into a DynamoDB table that has a large read capacity unit.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers. The web servers will first login with Amazon service to authenticate the user. The web servers will process the users&#39; votes and store the votes into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and update the &#39;vote totals&#39; into a DynamoDB table.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28413,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are developing a new mobile application and are considering storing user preferences in AWS. This would provide a more uniform cross-device experience to users using multiple mobile devices to access the application. The preference data for each user is estimated to be 50KB in size. Additionally, 5 million customers are expected to use the application regularly. The solution needs to be quick, highly available, scalable, and secure. How would you design a solution to meet the above requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; B</strong></p>\r\n\r\n<p>This scenario has the following architectural considerations:(1) the application should support millions of customers, so it should be scalable, (2) multiple mobile devices should be able to access the application, and (3) it should be cost-effective, highly available and secure.</p>\r\n\r\n<p>Tip: Whenever the application needs to (a) support millions of users and scalability is most important, always think about DynamoDB, and (b) give mobile apps access to AWS resource/service, always think about federated access using Web Identity Provider and &quot;AssumeRoleWithWebIdentity&quot; API.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB.</li>\r\n\t<li>Option B is CORRECT because (a) it uses DynamoDB for scalability.&nbsp;(b) It uses federated access using Web Identity Provider, and (c) uses Fine-Grained Access privileges for authenticating the access.</li>\r\n\t<li>Option C is incorrect because (a) RDS MySQL is not as scalable and cost-effective as DynamoDB, and (b) user management and access privilege system cannot be used for controlling access.</li>\r\n\t<li>Option D is incorrect because accessing the data via S3 would be slower compared to DynamoDB.</li>\r\n</ul>\r\n\r\n<p>For more information on DynamoDB, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/dynamodb/developer-resources/\" target=\"_blank\">https://aws.amazon.com/dynamodb/developer-resources/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18530,
          "question_id": 28413,
          "answers": [
            {
              "choice": "<p>Setup an RDS MySQL instance in 2 availability zones to store the user preference data. Deploy a public-facing application on a server in front of the database to manage security and access credentials.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>&nbsp;Set up a DynamoDB table with each user&#39;s item having the necessary attributes to hold the user preferences. The mobile application will query the user preferences directly from the DynamoDB table. Utilize STS. Web Identity Federation, and DynamoDB Fine-Grained Access Control to authenticate and authorize access.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Setup an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will query the user preferences from the read replicas. Leverage the MySQL user management and access privilege system to manage security and access credentials.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Store the user preference data in S3. Setup a DynamoDB table with an item for each user and an item attribute pointing to the user&rsquo;s S3 object. The mobile application will retrieve the S3 URL from DynamoDB and then access the S3 object directly by utilizing STS, Web identity Federation, and S3 ACLs to authenticate and authorize access.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28414,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": 0,
      "question_text": "<p>Your team has a Tomcat-based Java application you need to deploy into development, test, and production environments. After some research, you opt to use Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management. Your QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis. Similarly, other software teams in your organization want access to that same restored data via their EC2 instances in your VPC.<br />\r\nWhich&nbsp;of the following would be the optimal setup for persistence and security that meets the above requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - A</strong></p>\r\n\r\n<p>The main consideration in this question is only the EC2 instances in your VPC. You should be able to access RDS instances, and the setup should support persistence.<br />\r\nYou can connect to the external database from multiple environments, use database types that aren&#39;t supported with integrated databases, and perform blue/green deployments. As an alternative to using a decoupled database that Elastic Beanstalk created, you can also create a database instance outside of your Elastic Beanstalk environment.</p>\r\n\r\n<ul>\r\n\t<li><strong>Option A is CORRECT&nbsp;</strong>because the RDS instance can be persisted with Elastic Beanstalk based on the latest updates from AWS. See the references.</li>\r\n\t<li><strong>Option B is incorrect</strong> because you should always use the DNS endpoint of the RDS instance, not the IP address, as this option suggests.</li>\r\n\t<li><strong>Option C is incorrect </strong>because the RDS instance is optimal&nbsp;being a part of the Elastic Beanstalk environment and&nbsp;the security group is not configured correctly as it gives access to all the hosts in the application subnets.</li>\r\n\t<li><strong>Option D is incorrect</strong> because the security group is not configured correctly as it gives access to all the hosts in the application subnets.</li>\r\n</ul>\r\n\r\n<p><strong>Reference:</strong></p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/about-aws/whats-new/2021/10/aws-elastic-beanstalk-database-decoupling-elastic-beanstalk-environment/\">https://aws.amazon.com/about-aws/whats-new/2021/10/aws-elastic-beanstalk-database-decoupling-elastic-beanstalk-environment/</a></li>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 86762,
          "question_id": 28414,
          "answers": [
            {
              "question_id": "28414",
              "choice": "<p>Create your RDS instance as part of your Elastic Beanstalk environment and alter its security group to allow access to it from valid client machines in your application subnets.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28414",
              "choice": "<p>Create your RDS instance separately and add its IP address to your application&rsquo;s DB connection strings in your code. Alter its security group to allow access to it from hosts within your VPC&rsquo;s IP address block.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28414",
              "choice": "<p>Use the ElasticBeanstalk to deploy your application in various environments. Create your RDS instance separately, controlled by automation, and pass its DNS name to your app&#39;s DB connection string as an environment variable. After this, restore the sanitized copy of production data to the RDS instance. Create a security group to allow access to it from hosts in your application subnets.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28414",
              "choice": "<p>Create your RDS instance separately and pass its DNS name to your&nbsp;DB connection string as an environment variable. Alter its security group to allow access to it from hosts in your application subnets.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28415,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are looking to migrate your Development and Test environments to AWS. You have decided to use separate AWS accounts to host each environment. You plan to link each account bill to a Management&nbsp;AWS account using Consolidated Billing. To make sure that you keep within the budget, you would like to implement a way for administrators in the Management&nbsp;account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts. Identify which of the options will allow you to achieve this goal.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\n\n<p>The scenario here is asking you to give permissions to administrators in the Management account such that they can have access to stop, delete, and terminate the resources in two accounts: Dev and Test.</p>\n\n<p>Tip: Remember that you always create roles in the account whose resources are to be accessed. In this example, that would be Dev and Test. Then you create the users in the account who will be accessing the resources and give them that particular role. In this example, the Management account should create the users.</p>\n\n<p>Option A is incorrect because the Management account IAM user needs to assume roles from the Dev and Test accounts. The roles should have suitable permissions so that the Management account IAM user can access resources.</p>\n\n<p>Option B is incorrect because the cross-account role should be created in Dev and Test accounts, not in the Management account.</p>\n\n<p>Option C is CORRECT because (a) the cross-account role is created in Dev and Test accounts, and the users are created in the Management account given that role.</p>\n\n<p>Option D is incorrect because consolidated billing does not give access to resources in this fashion.</p>\n\n<p>For more information on cross-account access, please visit the below URL-</p>\n\n<ul>\n\t<li><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></li>\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18532,
          "question_id": 28415,
          "answers": [
            {
              "choice": "<p>Create IAM users in the Management account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant Management&nbsp;account access to the resources in the account by inheriting permissions from the&nbsp;Management&nbsp;account.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create IAM users and a cross-account role in the Management account that grants full Admin permissions to the Dev and Test accounts.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create IAM users in the Management account with the &quot;AssumeRole&quot; permissions. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant Management account access.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Link the accounts using Consolidated Billing. This will give IAM users in the Management account access to the resources in Dev and Test accounts.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28416,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your customer is willing to consolidate their log streams, access logs, application logs, security logs, etc. in one single system. Once consolidated, the customer wants to analyze these logs in real-time based on heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours? What is the best approach to meet your customer&rsquo;s requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Whenever the scenario - just like this one - wants to do real-time processing of a stream of data, always think about Amazon Kinesis. Also, remember that the records of the stream are available for 24 hours.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Option A is incorrect because SQS is not used for real-time processing of the stream of data.</p>\r\n\r\n<p>Option B is CORRECT because Amazon Kinesis is best suited for applying the real-time processing of the stream of data. Also, the records of the stream are available for 24 hours in Kinesis.</p>\r\n\r\n<p>Option C is incorrect because CloudTrail is not used to process the real-time data processing, and EMR is used for batch-processing.</p>\r\n\r\n<p>Option D is incorrect because setting autoscaling of EC2 instances is not cost-effective, and EMR is used for batch-processing.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>More information on Amazon Kinesis:</strong></p>\r\n\r\n<p>Amazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data and provide the ability to build custom streaming data applications for specialized needs.</p>\r\n\r\n<ul>\r\n\t<li>Use Amazon Kinesis Streams to collect and process large streams of data records in real-time.</li>\r\n\t<li>Use Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.</li>\r\n\t<li>Use Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.</li>\r\n</ul>\r\n\r\n<p>&nbsp;<img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_gr5jch.png\" style=\"height:321px; width:700px\" /></p>\r\n\r\n<p>For more information on Kinesis, please visit the below URL-</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/documentation/kinesis/\" target=\"_blank\">https://aws.amazon.com/documentation/kinesis/</a></li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18533,
          "question_id": 28416,
          "answers": [
            {
              "choice": "<p>Send all the log events to Amazon SQS. Setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Send all the log events to Amazon Kinesis. Develop a client process to apply heuristics to the logs.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure Amazon Cloud Trail to receive custom logs and&nbsp;use EMR to apply heuristics to the logs.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Setup Auto Scaling group of EC2 Syslog servers and&nbsp;store the logs S3 use EMR to apply heuristics on the logs.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 44742,
      "topic_id": 366,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A team is developing a feature that needs to recognize Celebrities. By using the App, clients can upload photos and search celebrities among the photos by clicking a button. Or they can upload a bunch of photos and search the times that a given celebrity has appeared. The team wants to run the App&nbsp;in AWS at a lower cost. Which option is the most efficient one to implement while still ensuring availability and stability?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; C</strong></p>\r\n\r\n<p><strong>Explanation</strong>:</p>\r\n\r\n<p>As the question asks for a lower cost while still ensuring availability and stability. Lambda should be considered first. Lambda can autoscale without manual operations. More importantly, it is very efficient to develop a web app without considering a lot about OS, patching, etc.</p>\r\n\r\n<p>Amazon Rekognition can recognize thousands of celebrities in a wide range of categories, such as entertainment and media, sports, business, and politics. With Amazon Rekognition, you can recognize celebrities in images and stored videos. You can also get additional information for recognized celebrities.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect: Because EC2 does not cost-efficient and brings in extra cost if the load is not stable. Lambda should be considered instead.</li>\r\n\t<li>Option B is incorrect: Because Rekognition API &ldquo;RecognizeCelebrities&rdquo; should be used in this case. For each celebrity recognized, &ldquo;RecognizeCelebrities&rdquo; returns a Celebrity object. The Celebrity object contains the celebrity name, ID, URL links to additional information, match confidence, and a ComparedFace object that you can use to locate the celebrity&#39;s face on the image. &ldquo;SearchFaces&rdquo; is incorrect as it is not used to search Celebrities.</li>\r\n\t<li>Option C is CORRECT: Because Lambda together with Rekognition can meet the need for low cost, availability, and stability. Refer to <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html\" target=\"_blank\">https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html</a> for this feature.</li>\r\n\t<li>Option D is incorrect: Because firstly, it has used EC2. Even if Autoscaling and ELB are used, it is not as efficient as Lambda. Secondly, API &ldquo;SearchFaces&rdquo; should not be used for the function of Celebrities Recognition.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34753,
          "question_id": 44742,
          "answers": [
            {
              "choice": "<p>Create the frontend and backend in a T2 medium EC2 instance to use its burstable capability. Call Rekognition API &ldquo;RecognizeCelebrities&rdquo; to fetch the information in a JSON format. Process the JSON result in the backend service and return the result to the frontend UI.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Develop the App&nbsp;in a serverless lambda to use Rekognition API &ldquo;SearchFaces&rdquo; to search a Celebrity. The input image can be base64-encoded bytes or an S3 object. After the API has been returned, present the result to clients.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use the AWS Rekognition service. Implement the App&nbsp;in a lambda to call Rekognition API &ldquo;RecognizeCelebrities&rdquo; to fetch the information required in a JSON format. Process the information in Lambda and return the result to end-users. Use S3 for clients to upload photos.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Implement the App&nbsp;in an M4 large EC2 instance&nbsp;with Autoscaling and Elastic Load Balancer. Build the application via a Cloudformation template. Use the App&nbsp;to call Rekognition API &ldquo;SearchFaces&rdquo; to get the information. Process the JSON result in the backend service and return the result to the frontend with a Cloudfront CDN.</p>\r\n\r\n<p>&nbsp;</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28418,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>You are running a successful multitier web application on AWS. Your marketing department has asked you to add a reporting tier to the application. The reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web application&rsquo;s database. You are currently running a Multi-AZ RDS MySQL instance for the database tier. You also have implemented ElastiCache as a database caching layer between the application tier and database tier. Select the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; C</strong></p>\n\n<p>In question is asking you to design a reporting layer with the least impact on the database. If the reporting queries are fired on the database instance, the database instance&#39;s performance would surely get impacted. Since querying for the report would be a read-heavy operation, the best solution is to create the read replicas of the database instance and query on them rather than on the database instance. This way, the existing database instance will have the least impact.</p>\n\n<p><strong>Option A is incorrect</strong> because sending the logs to S3 would add to the overhead on the database instance.</p>\n\n<p><strong>Option B is incorrect</strong> because you cannot access the standby instance.</p>\n\n<p><strong>Option C is CORRECT </strong>because it uses the Read Replicas of the database for the querying of reports.</p>\n\n<p><strong>Option D is incorrect</strong> because the querying on ElastiCache may not always give you the latest and entire data, as the cache may not always be up-to-date.</p>\n\n<p>For more information on Read Replica&rsquo;s, please visit the below URL-</p>\n\n<ul>\n\t<li><a href=\"https://aws.amazon.com/rds/details/read-replicas/\" target=\"_blank\">https://aws.amazon.com/rds/details/read-replicas/</a></li>\n\t<li><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></li>\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 69899,
          "question_id": 28418,
          "answers": [
            {
              "question_id": "28418",
              "choice": "<p>Continually send transaction logs from your master database to an S3 bucket and generate the reports from the S3 bucket using S3 byte-range requests.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28418",
              "choice": "<p>Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28418",
              "choice": "<p>Launch an RDS Read Replica connected to your Multi AZ master database and generate reports by querying the Read Replica.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28418",
              "choice": "<p>Generate the reports by querying the ElasliCache database caching tier.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28419,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A web company is looking to implement an intrusion detection and prevention system for their deployed VPC. This platform should have the ability to scale to thousands of instances running inside of the VPC. How should they architect their solution to achieve these goals?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - B</p>\r\n\r\n<p>This question asks you to design a scalable IDS/IPS solution (easily applicable to thousands of instances). Users can set up&nbsp;third party IDS/IPS solutions offered in AWS Marketplace.</p>\r\n\r\n<p>Option A is incorrect because AWS does not support promiscuous mode.</p>\r\n\r\n<p>Option B is CORRECT because there are various IDS/IPS solutions offered in AWS Marketplace that&nbsp;monitor networks and&nbsp;systems for malicious activity and provide layer of security for potential threats.</p>\r\n\r\n<p>Option C is incorrect.&nbsp;The incoming traffic should be passed through IDS/IPS before sending it to the servers.</p>\r\n\r\n<p>Option D is plausible, but (a) it is not a scalable solution, (b) it is only an IDS solution, not an IPS solution.</p>\r\n\r\n<p>Please check the following references:</p>\r\n\r\n<p><a href=\"https://aws.amazon.com/mp/scenarios/security/ids/\">https://aws.amazon.com/mp/scenarios/security/ids/</a></p>\r\n\r\n<p><a href=\"https://aws.amazon.com/marketplace/solutions/infrastructure-software/ids-ips\">https://aws.amazon.com/marketplace/solutions/infrastructure-software/ids-ips</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18536,
          "question_id": 28419,
          "answers": [
            {
              "choice": "<p>Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VPC.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create a IDS/IPS system from AWS Marketplace to monitor security events in the VPC network and stop threats once detected.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure servers running in the VPC using the host-based &lsquo;route&rsquo; commands to send all traffic through the platform to a scalable virtualized IDS/IPS.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28420,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load Balancer, an Auto-Scaling group of Java/Tomcat application-servers, and DynamoDB as a data store. The main web application best runs on m2 x large instances since it is highly memory-bound. Each new deployment requires the semi-automated creation and testing of a new AMI for the application servers which takes quite a while and is therefore only done once per week. Recently, a new chat feature has been implemented in Node.js and waits to be integrated into the architecture. The first tests show that the new component is CPU bound. Since the company has some experience with using Chef, they decided to streamline the deployment process and use AWS OpsWorks as an application lifecycle tool to simplify the application&#39;s management and reduce the deployment cycles. What configuration in AWS OpsWorks is required to manage the application integrated with the new chat module in the most cost-efficient and flexible way?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - B</strong></p>\r\n\r\n<p>The scenario here requires that you need to manage the application that is created with java, node.js, and DynamoDB using AWS OpsWorks. The deployment process should be streamlined, and the deployment cycles should be reduced.</p>\r\n\r\n<p>As the java and node.js have different resource requirements, it makes sense to deploy them on different layers. It would make maintenance easier as well.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because it would be a better solution to create&nbsp;two separate layers: one for Java and one for node.js.</li>\r\n\t<li>Option B is CORRECT because only one stack would be sufficient, and two layers (one for Java and one for node.js) would be required for handling separate requirements. One custom recipe for DynamoDB would be required.</li>\r\n\t<li>Option C is incorrect because two OpsWork stacks are unnecessary.</li>\r\n\t<li>Option D is incorrect because two OpsWork stacks are unnecessary.</li>\r\n</ul>\r\n\r\n<p><strong>More information on AWS OpsWork Stack</strong></p>\r\n\r\n<p>An AWS OpsWorks Stack defines your entire application&#39;s configuration: the load balancers, server software, database, etc. You control every part of the stack by building layers that define the software packages deployed to your instances and other configuration details such as Elastic IPs and security groups. You can also deploy your software onto layers by identifying the repository and optionally using Chef Recipes to automate everything Chef can do, such as creating directories and users, configuring databases, etc. You can use OpsWorks Stacks&rsquo; built-in automation to scale your application and automatically recover from instance failures. You can control who can view and manage the resources used by your application, including ssh access to the instances that your application uses.</p>\r\n\r\n<p>For more information on Ops work, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/opsworks/stacks/faqs/\" target=\"_blank\">https://aws.amazon.com/opsworks/stacks/faqs/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18537,
          "question_id": 28420,
          "answers": [
            {
              "choice": "<p>Create one AWS OpsWorks stack, create one AWS OpsWorks layer, and create one custom recipe.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create one AWS OpsWorks stack, create two AWS OpsWorks layers, and create one custom recipe.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, and create one custom recipe.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create two AWS OpsWorks stacks, create two AWS OpsWorks layers, and create two custom recipes.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28421,
      "topic_id": 366,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your firm has uploaded a large amount of aerial image data to S3. In the past, in your on-premises environment, you used a dedicated group of servers to process this data. You used Rabbit MQ &ndash; An open-source messaging system to get job information to the servers. Once processed, the data would go to the tape and be shipped offsite. Your manager told you to stay with the current design and leverage AWS archival storage and messaging services to minimize cost. Which of the following options is correct?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - B</p>\r\n\r\n<p>The most suggestive hint in this question is that it asks you to leverage AWS archival storage and messaging services. Hence, you should look for options Glacier and SQS.</p>\r\n\r\n<p>Option A is incorrect because (a) RRS is not an archival storage option, and (b) since auto-scaling is not mentioned, you cannot use CloudWatch alarms to terminate the idle EC2 instances.</p>\r\n\r\n<p>Option B is CORRECT because (a) it uses SQS to process the messages, (b) it uses Glacier as the archival storage solution - which is cost-optimized.</p>\r\n\r\n<p>Option C is incorrect because RRS is not an archival storage option; instead, use Glacier as it is a low-cost archival solution (cost lower than RRS).</p>\r\n\r\n<p>Option D is incorrect as SNS cannot be used to process the messages.&nbsp;It cannot replace the functionality that was getting provided by RabbitMQ.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18538,
          "question_id": 28421,
          "answers": [
            {
              "choice": "<p>Use SQS for passing job messages. Use Cloud Watch alarms to terminate EC2 worker instances when they become idle. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Setup Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Glacier.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Setup Auto-Scaled workers triggered by queue depth that use instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage (RRS).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use SNS to pass the job messages. Use Cloud Watch alarms to terminate worker instances when they become idle. Once data is processed, change the storage class of the S3 object to Glacier.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Cost Control"
    },
    {
      "question_id": 28377,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A corporate web application is deployed within an Amazon Virtual Private Cloud (VPC) and is connected to the corporate data center via an IPsec VPN. The application must authenticate against the on-premises LDAP server. After authentication, each logged-in user can only access an Amazon Simple Storage Service(S3) keyspace specific to that user. Which two approaches can satisfy these objectives?</p>\r\n\r\n<p>Choose 2 options from the below.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B and C</p>\r\n\r\n<p>There are two architectural considerations here: (1) The users must be authenticated via the on-premise LDAP server, and (2) each user should have access to S3 only.</p>\r\n\r\n<p>With this information, it is important to authenticate the users using LDAP, get the IAM Role name, get the temporary credentials from AWS STS, and access the S3 bucket using those credentials. And second, create an IAM Role that provides access to S3.</p>\r\n\r\n<p>Option A is incorrect because the users need to be authenticated using LDAP first, not AWS STS. Also, the temporary credentials to log into AWS are provided by AWS STS, not an identity broker.</p>\r\n\r\n<p>Option B is CORRECT because it follows the correct sequence. It authenticates users using LDAP, gets the security token from AWS STS, and then accesses the S3 bucket using the temporary credentials.</p>\r\n\r\n<p>Option C is CORRECT because it follows the correct sequence. It develops an identity broker that authenticates users against LDAP, gets the security token from AWS STS, and then accesses the S3 bucket using the IAM federated user credentials.</p>\r\n\r\n<p>Option D is incorrect because you cannot use the LDAP credentials to log in to IAM.</p>\r\n\r\n<p>An example diagram of how this works from the AWS documentation is given below.</p>\r\n\r\n<p>&nbsp;<img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_lwzqxu.png\" style=\"height:500px; width:500px\" /></p>\r\n\r\n<p>For more information on federated access, please visit the below link-</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\" target=\"_blank\">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18494,
          "question_id": 28377,
          "answers": [
            {
              "choice": "<p>Develop an identity broker that authenticates against AWS&nbsp;Security Token Service (STS) to assume an IAM role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials to access the appropriate S3 bucket.</p>\r\n\r\n<p>&nbsp;</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>The application authenticates against LDAP and retrieves the name of an IAM role associated with the user. The application then calls the AWS&nbsp;Security Token Service (STS) to assume that IAM role ( including the ARN ). The application then uses the temporary credentials to access the appropriate S3 bucket.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Develop an identity broker that authenticates against LDAP and then calls AWS&nbsp;Security Token Service (STS) to get IAM federated user credentials. The application then uses the temporary credentials to access the appropriate S3 bucket..</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>The application authenticates against LDAP and then calls the AWS Identity and Access&nbsp;Management (IAM) Security service to log in to IAM using the LDAP credentials. The&nbsp;application can then access the appropriate S3 bucket.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28422,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your company is planning to develop an application in which the front end is in .Net and the backend is in DynamoDB. It is expected that there would be an intermittent high load on the application. How could you ensure the application&#39;s scalability and cost-effectiveness to reduce the load on the DynamoDB database? Choose an answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\r\n\r\n<p>This question asks for an option that can be used to reduce the load on the DynamoDB database. The option has to be scalable.</p>\r\n\r\n<p>In such a scenario, the best option to use is SQS, because it is scalable and cost-efficient as well.</p>\r\n\r\n<p>Option A is incorrect because adding more databases will not reduce the load on the existing DynamoDB database. Also, this is not a cost-efficient solution.</p>\r\n\r\n<p>Option B is incorrect because increasing the write capacity is an expensive option.</p>\r\n\r\n<p>Option C is CORRECT because it uses SQS to assist in taking over the load from storing the data in DynamoDB, and it is scalable and cost-efficient.</p>\r\n\r\n<p>Option D is incorrect because the MultiAZ configuration is not going to help reduce the load. In fact, it will affect the performance as the records in DynamoDB would get replicated in multiple availability zones.</p>\r\n\r\n<p><strong>More information on SQS:</strong></p>\r\n\r\n<p>When the idea comes for scalability, then SQS is the best option. Normally DynamoDB is scalable, but since one is looking for a cost-effective solution, the messaging in SQS can help manage the situation mentioned in the question.</p>\r\n\r\n<p>Amazon Simple Queue Service (SQS) is a fully-managed message queuing service for reliably communicating among distributed software components and microservices - at any scale. Building applications from individual components that perform a discrete function improves scalability and reliability and is the best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.</p>\r\n\r\n<p>For more information on SQS, please refer to the below URL-<br />\r\n<a href=\"https://aws.amazon.com/sqs/\" target=\"_blank\">https://aws.amazon.com/sqs/</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18539,
          "question_id": 28422,
          "answers": [
            {
              "choice": "<p>Add more DynamoDB databases to handle the load.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Increase the write capacity of Dynamo DB to meet the peak loads.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use SQS to hold the database requests instead of overloading the DynamoDB database. Then have a service that asynchronously pulls the messages and writes them to DynamoDB.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Launch DynamoDB in Multi-AZ configuration with a global index to balance writes.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 45322,
      "topic_id": 364,
      "course_id": 0,
      "case_study_id": 0,
      "lab_id": 0,
      "question_text": "<p>A team has just received a task to build an application that needs to recognize faces in streaming videos. They will get the source videos from a third party that uses a container format (MKV).</p>\n\n<p>The APP should be able to quickly address faces through the video in real-time and save the output in a suitable manner for downstream to process. As recommended by the AWS Solutions Architect colleague, they would like to develop the service using AWS Rekognition. Which below options are needed to accomplish the task? (Select THREE)</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct&nbsp;Answer&nbsp;&ndash;&nbsp;B, C and&nbsp;F</strong></p>\n\n<p>For facial recognition in live videos, it is different from that in photos. Kinesis is required to meet the needs of the real-time process. Amazon Rekognition Video uses Amazon Kinesis Video Streams to receive and process a video stream. The analysis results are output from Amazon Rekognition Video to a Kinesis data stream and then read by your client application. Amazon Rekognition Video provides a stream processor (<a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html\" target=\"_blank\">CreateStreamProcessor</a>) that you can use to start and manage the analysis of streaming video.</p>\n\n<p>As a summary, the below 3 items are needed for Amazon Rekognition Video with streaming video.</p>\n\n<ul>\n\t<li>A Kinesis video stream for sending streaming video to Amazon Rekognition Video. For more information, see the&nbsp;<a href=\"https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/what-is-kinesis-video.html\" target=\"_blank\">Kinesis video stream</a>.</li>\n\t<li>An Amazon Rekognition Video stream processor to manage the analysis of the streaming video. For more information, see <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-starting-analysis.html\" target=\"_blank\">Starting Streaming Video Analysis</a>.</li>\n\t<li>A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream. For more information, see <a href=\"https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-consumers.html\" target=\"_blank\">Consumers for Amazon Kinesis Streams</a>.</li>\n</ul>\n\n<p>Option&nbsp;A is&nbsp;incorrect because the source videos should be put into the Kinesis video stream instead of S3. Afterwards, the Rekognition processor will pick up records in the Kinesis stream to process.</p>\n\n<p>Option&nbsp;B is&nbsp;CORRECT because it is the step to convert source data into the Kinesis video stream.</p>\n\n<p>Option&nbsp;C is&nbsp;CORRECT.&nbsp;A stream processor can be created by calling <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html\" target=\"_blank\">CreateStreamProcessor</a>. The request parameters include the Amazon Resource Names (ARNs) for the Kinesis video stream, the Kinesis data stream, and the identifier for the collection that&#39;s used to recognize faces in the streaming video. It also includes the name that you specify for the stream processor.</p>\n\n<p>Below is an example:</p>\n\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/03/20/ckeditor_9.png\" style=\"height:420px; width:1000px\" /></p>\n\n<p>Option&nbsp;D is&nbsp;incorrect because, for video processing, Rekognition API &ldquo;DetectFaces&rdquo; should not be used. &ldquo;DetectFaces&rdquo; is used to detects faces within an image that is provided as input. Instead, stream processor relevant APIs should be used.</p>\n\n<p>Option&nbsp;E is&nbsp;incorrect because the output from Rekognition should be stored in the Kinesis data stream. When the Rekognition stream processor is created, the Rekognition output (Kinesis Data Stream) is defined.</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Output&quot;: {<br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;KinesisDataStream&quot;: {<br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Arn&quot;: &quot;arn:aws:kinesis:us-east-1:nnnnnnnnnnnn:stream/outputData&quot;<br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</p>\n\n<p>&nbsp;</p>\n\n<p>Option&nbsp;F is&nbsp;CORRECT because it describes correctly how to consume the Kinesis data stream. You can use the Amazon Kinesis Data Streams Client Library to consume analysis results that are sent to the Amazon Kinesis Data Streams output stream.</p>\n\n<p>Details can be found in <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-kinesis-output.html\" target=\"_blank\">https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-kinesis-output.html</a>.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 52574,
          "question_id": 45322,
          "answers": [
            {
              "question_id": "45322",
              "choice": "<p>S3 buckets to store the source MKV videos for AWS Rekognition to process. S3 should be used in this case as it has provided an unlimited, highly available, and durable storing space. Make sure that the third party has the write access to S3 buckets.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "45322",
              "choice": "<p>A Kinesis video stream for sending streaming video to Amazon Rekognition Video. This can be done by using Kinesis &ldquo;PutMedia&rdquo; API in Java SDK. The PutMedia operation writes video data fragments into a Kinesis video stream that Amazon Rekognition Video consumes.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "45322",
              "choice": "<p>An Amazon Rekognition Video stream processor to manage the analysis of the streaming video. It can be used to start, stop, and manage stream processors according to needs.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "45322",
              "choice": "<p>Use EC2 or Lambda to call Rekognition API &ldquo;DetectFaces&rdquo; with the source videos saved in the S3 bucket. For each face detected, the operation returns face details. These details include a bounding box of the face, a confidence value, and a fixed set of attributes such as facial landmarks, etc.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "45322",
              "choice": "<p>After the APP has utilized Rekognition API to fetch the recognized faces from live videos, use S3 or RDS database to store the output from Rekognition. Another Lambda can be used to post-process the result and present it to UI.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "45322",
              "choice": "<p>A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream.&nbsp;The consumer can be autoscaled by running it on multiple EC2 instances under an Auto Scaling group.</p>",
              "feedback": "",
              "correct": true
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 44744,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A company has maintained a MySQL database on-premises and is considering migrating the database to AWS. They require the database instance in AWS to keep synchronizing with the on-premises one. Then after a while, the on-premises database will be decommissioned after the AWS RDS instance has been tested thoroughly by the QA team. Amazon Database Migration Service has been chosen to implement this migration. For this data migration task, which below options are necessary that you must perform? Select 3.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; A, D, E</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>To use the wizard to begin a database migration, select &ldquo;Getting started&rdquo; from the navigation pane on the AWS DMS console. Following the wizard process, you need to do the following.</p>\r\n\r\n<ul>\r\n\t<li>Allocate a replication instance that performs all the processes for the migration</li>\r\n\t<li>specify a source and a target database</li>\r\n\t<li>create a task or set of tasks to define what tables and replication processes you want to use.</li>\r\n</ul>\r\n\r\n<p>AWS DMS then creates your replication instance and performs the tasks on the data being migrated.</p>\r\n\r\n<p>Alternatively, you can create each of the components of an AWS DMS database migration by selecting the items from the navigation pane.</p>\r\n\r\n<p>Details on how to get started with AWS Database Migration Service can be found in</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_GettingStarted.html\" target=\"_blank\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_GettingStarted.html</a></p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT: Because the first task in migrating a database is to create a replication instance that has sufficient storage and processing power to perform the tasks you assign and migrate data from your source database to the target database.</li>\r\n\t<li>Option B is incorrect: For DMS, the replication instance should be created in the DMS console instead of the EC2 instance. An example is as below.</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_aws_q10.jpg\" style=\"height:449px; width:650px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option C is incorrect: Because for DMS, The source and target data stores can be on an Amazon Elastic Compute Cloud (Amazon EC2) instance, an Amazon Relational Database Service (Amazon RDS) DB instance, or an on-premises database.&nbsp;</li>\r\n\t<li>Option D is CORRECT: Because it is a required step to choose the source and target endpoints. The database engine should be &ldquo;MySQL&rdquo;. For example</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq10.1.png\" style=\"height:434px; width:750px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option&nbsp;E is&nbsp;CORRECT:&nbsp;Because the task is needed to specify what tables to migrate, how to map data using a target schema, and how to create new tables on the target database. In this case, the type of migration should be &ldquo;migrate existing data and replicate ongoing changes&rdquo;.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34755,
          "question_id": 44744,
          "answers": [
            {
              "choice": "<p>Allocate a replication instance in the Database Migration Service console that handles the migration processing. Ensure that the instance has a proper instance type with enough CPU and Memory resources to perform the migration task.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create a EC2 instance that is used as the processing instance for DMS with a proper instance type. Ensure the EC2 instance has a proper role to access RDS MySQL instance. In Database Migration Service console, configure the EC2 instance as the Replication instance.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Specify the source as the on-premises database, Select the database engine as &quot;MySQL&quot;, and choose the destination subnet. Database Migration Service will automatically create the destination endpoint.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Specify a source and a target database endpoint. Use the on-premises database as the source endpoint and an Amazon Relational Database Service (Amazon RDS) DB instance as the target endpoint. Select the database engine for both source and target as &ldquo;MySQL&rdquo;.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create a task or set of tasks to define what tables and replication processes to be used. For migration type, select &ldquo;migrate existing data and replicate ongoing changes&rdquo;.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44745,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A team has a website that provides the latest real estate news to subscribers. It has included a Java frontend, backend, and a MySQL database to store user information. They plan to migrate the MySQL database to PostgreSQL database to use some features that MySQL does not have. At the same time, the development lead has raised extra requirements.</p>\r\n\r\n<ol>\r\n\t<li>Only migrate users that have recent activities for the past 12 months. It is known that the MySQL table &ldquo;user_info&rdquo; has a column &ldquo;user_activity&rdquo; to track the activity date and time.</li>\r\n\t<li>For a table called &ldquo;latest news&rdquo;, two columns starting with &ldquo;test&rdquo; should be removed.</li>\r\n\t<li>Add a prefix to several tables.</li>\r\n</ol>\r\n\r\n<p>Can Amazon Data Migration Service accomplish this task?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; D</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>DMS can handle the task of migration from MySQL to PostgreSQL, as shown below.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq11.jpg\" style=\"height:531px; width:750px\" /></p>\r\n\r\n<p>The first task can be achieved by using a &quot;Selection Rule&quot;. The below diagram depicts using a filter for the &quot;Customers&quot; table on the column &quot;AgencyID&quot; where the values are between &quot;01&quot; and &quot;85&quot;</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq11.1.jpg\" style=\"height:682px; width:750px\" /></p>\r\n\r\n<p>The second task can be achieved by using the JSON template for &quot;Transformation Rules,&quot; as shown below.</p>\r\n\r\n<p>{<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; &quot;rules&quot;: [{<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-type&quot;: &quot;selection&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-id&quot;: &quot;1&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-name&quot;: &quot;1&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;object-locator&quot;: {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;schema-name&quot;: &quot;test&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;table-name&quot;: &quot;%&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-action&quot;: &quot;include&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; }, {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-type&quot;: &quot;transformation&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-id&quot;: &quot;2&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-name&quot;: &quot;2&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-action&quot;: &quot;remove-column&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-target&quot;: &quot;column&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;object-locator&quot;: {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;schema-name&quot;: &quot;test&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;table-name&quot;: &quot;<strong>latestnews</strong>&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;column-name&quot;: &quot;<strong>test%</strong>&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; }]<br />\r\n&nbsp;}</p>\r\n\r\n<p>The third task can also be achieved by using the JSON template for &quot;Transformation Rules&quot; as shown below.</p>\r\n\r\n<p>{<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; &quot;rules&quot;: [{<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-type&quot;: &quot;selection&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-id&quot;: &quot;1&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-name&quot;: &quot;1&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;object-locator&quot;: {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;schema-name&quot;: &quot;test&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;table-name&quot;: &quot;%&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-action&quot;: &quot;include&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; }, {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-type&quot;: &quot;transformation&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-id&quot;: &quot;2&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-name&quot;: &quot;2&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-action&quot;: &quot;add-prefix&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;rule-target&quot;: &quot;table&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;object-locator&quot;: {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;schema-name&quot;: &quot;test&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;table-name&quot;: &quot;%&quot;<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;value&quot;: <strong>&quot;DMS_&quot;</strong><br />\r\n&nbsp;&nbsp;&nbsp;&nbsp; }]<br />\r\n&nbsp;<br />\r\n}</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect. This option says DMS cannot handle migration. Indeed migration from MySQL to PostgreSQL is supported by DMS.</li>\r\n\t<li>Option B is incorrect. This option says tasks 2 and 3 cannot be done. In fact, tasks 2 and 3 can be achieved, as shown above.</li>\r\n\t<li>Option C is incorrect: Because task 3 can be achieved via &ldquo;Transformation Rules and Actions&rdquo;. A reference is in</li>\r\n</ul>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html\" target=\"_blank\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html</a>.</p>\r\n\r\n<ul>\r\n\t<li>Option D is&nbsp;CORRECT:&nbsp;Because DMS can perform this migration with all three tasks being achievable.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34756,
          "question_id": 44745,
          "answers": [
            {
              "choice": "<p>DMS cannot handle the task of migration from MySQL to PostgreSQL. When database engines need to be changed, use the AWS Schema Conversion Tool to convert existing database schema, including tables, indexes, and most application code, to the target platform.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column &ldquo;user_activity&rdquo; in table &ldquo;user_info&rdquo;. However, it cannot do tasks 2 and 3.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>DMS can handle the task of migration from MySQL to PostgreSQL. It can achieve task 1 by using the Selection Rule for column &ldquo;user_activity&rdquo; in table &ldquo;user_info&rdquo;. Task 2, can also be completed&nbsp;by using a &ldquo;Transformation Rules and Actions&rdquo; to remove the columns starting with &ldquo;test&rdquo;. However, for task 3, DMS cannot do that.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>DMS can handle the task of migration from MySQL to PostgreSQL. And all tasks can be achieved by specifying Table Selection and Transformations using JSON.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44746,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A data analysis engineer has an old on-premise database for his meteorology analysis for years. This database is growing too big and becoming less responsive. He prefers to migrate it to AWS DynamoDB, and he already has the mapping rules in place. However, he has been told that the database type is unsupported by AWS Database Migration Service. He can export the data to CSV format files from the old database. How can the data analysis engineer migrate the data to AWS DynamoDB successfully?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; C</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>In most cases, when someone is migrating to a new database, he has access to the source database and can use the database directly as a source. Sometimes, however, he might not have access to the source directly. In other cases, the source is really old or possibly unsupported. In these cases, if he can export the data in CSV format, he can still migrate or platform, the data. In this question, DMS does not support this database type. However, the CSV files can be used after being uploaded to S3. AWS Database Migration Services (AWS DMS) added support for using Amazon S3 as a source for database migration.</p>\r\n\r\n<p>If S3 is the source endpoint, an external table definition is required. An external table definition is a JSON document that describes how AWS DMS should interpret the data from Amazon S3. The maximum size of this document is 2 MB. If a source endpoint is created using the AWS DMS Management Console, a JSON file can be entered directly into the table mapping box such as:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq12.jpg\" style=\"height:352px; width:750px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect: Because the mapping rule should be put in the source endpoint configuration rather than the task settings if S3 is the source for DMS.</li>\r\n\t<li>Option B is incorrect: Because AWS Database Migration Service is suitable. Moreover, Data Pipeline does not deal with the table mappings. Data Pipeline is suitable for data backup instead of database migration.</li>\r\n\t<li>Option C is CORRECT: It correctly describes how to use S3 as the source endpoint and defines the mapping rules. Below is a piece of JSON mapping rule:</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq12.1.jpg\" style=\"height:570px; width:750px\" /></p>\r\n\r\n<p>A reference is in<a href=\"https://aws.amazon.com/blogs/database/migrate-delimited-files-from-amazon-s3-to-an-amazon-dynamodb-nosql-table-using-aws-database-migration-service-and-aws-cloudformation/\" target=\"_blank\"> https://aws.amazon.com/blogs/database/migrate-delimited-files-from-amazon-s3-to-an-amazon-dynamodb-nosql-table-using-aws-database-migration-service-and-aws-cloudformation/</a>.</p>\r\n\r\n<ul>\r\n\t<li>Option D is incorrect: Because DynamoDB cannot resolve the table mapping by uploading CSV files exported from databases. Data Migration Service can deal with this scenario properly.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34757,
          "question_id": 44746,
          "answers": [
            {
              "choice": "<p>Firstly, upload the CSV files to S3. Create an S3 source endpoint and DynamoDB target endpoint in AWS DMS. Create a migration task by referring to the source and target endpoints. Add the mapping rule in the task using a JSON format.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>AWS Database Migration Service is inappropriate for this task. Upload the CSV files to S3 and then use an AWS Data Pipeline to import the data from S3 to DynamoDB by an S3-to-DynamoDB template.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Upload the exported CSV files to S3 at first. Then create S3 source endpoint and DynamoDB target endpoint in AWS DMS console. When the S3 source endpoint is configured, add the table mapping rule with a JSON table structure.&nbsp;Create a Replication Task to move the data from the source endpoint to the target endpoint.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>AWS Database Migration Service is not needed for this scenario. The AWS DynamoDB supports the import of CSV Excel files directly by console and CLI. Create the DynamoDB table firstly with a proper schema and then import the CSV database data.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44747,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A large company starts to use AWS organizations with the consolidated billing feature&nbsp;to manage its separate departments. The AWS operation team has just created 3 OUs (organization units) with 2 AWS accounts each. To be compliant with company-wide security policy, CloudTrail is required for all AWS accounts which is already been set up. However, after some time, there are cases that users in certain OU have turned off the CloudTrail of their accounts. What is the best way for the AWS operation team to prevent this from happening again?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; A</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AWS Organizations has provided two feature sets.</p>\r\n\r\n<ul>\r\n\t<li>Consolidated billing &ndash; This feature set provides shared billing functionality but does not include the more advanced features of AWS Organizations.</li>\r\n\t<li>All features &ndash; The complete feature set that is available to AWS Organizations. It includes all the functionality of consolidated billing and advanced features that give you more control over your organization&#39;s accounts. For example, when all features are enabled, the master account of the organization has full control over what member accounts can do. The master account can apply SCPs to restrict the services and actions that users (including the root user) and roles in an account can access. It can prevent member accounts from leaving the organization.</li>\r\n</ul>\r\n\r\n<p>In this case, we should use &ldquo;All features&rdquo;. One thing to note is that the feature sets can be upgraded in flight. It does not need to delete/recreate the AWS Organizations.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT: Because SCP is suitable for limiting actions that AWS accounts in an Organization can do. Below is an example of a deny policy:</li>\r\n</ul>\r\n\r\n<p>{<br />\r\n&nbsp; &quot;Version&quot;: &quot;2012-10-17&quot;,<br />\r\n&nbsp; &quot;Statement&quot;: [<br />\r\n&nbsp;&nbsp;&nbsp; {<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Effect&quot;: &quot;Deny&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Action&quot;: &quot;cloudtrail:StopLogging&quot;,<br />\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Resource&quot;: &quot;*&quot;<br />\r\n&nbsp;&nbsp;&nbsp; }<br />\r\n&nbsp; ]<br />\r\n}</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>Option B is incorrect: Because it does not need to delete/recreate the AWS Organizations to upgrade feature sets.</li>\r\n\t<li>Option C is incorrect: Because although it can potentially work, it has lots of repeatable work and is not straightforward if compared with Option A.</li>\r\n\t<li>Option D is incorrect: Because it does not mention the upgrade of feature sets. Secondly, the allow policy is incorrect as this case only requires limiting CloudTrail deletion. Allow policy implicitly prevents everything except for several allow items.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34758,
          "question_id": 44747,
          "answers": [
            {
              "choice": "<p>Update the AWS Organizations feature sets to &ldquo;All features&rdquo; and then create a Service Control Policies (SCP) to Prevent Users from Disabling AWS CloudTrail. This can be achieved by a deny policy with cloudtrail:StopLogging denied.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>This can be achieved by Service Control Policies (SCP) in the &ldquo;All features&rdquo; set. The team needs to delete and recreate the AWS Organizations with &ldquo;All features&rdquo; enabled and then use a proper control policy to limit the operation of cloudtrail:StopLogging.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>In each AWS account in this organization, create an IAM policy to deny cloudtrail:StopLogging for all users including administrators.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use Service Control Policies (SCP) to prevent users from disabling AWS CloudTrail. This can be done by a allow policy that denies cloudtrail:StopLogging.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 44748,
      "topic_id": 363,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A big company has used AWS Organizations to manage its various AWS accounts by using several organization units. The organization master account is in charge of running the whole organization. One child AWS account belongs to the data analysis department. The company has recently made some organizational adjustments and needs to remove the data analysis department from the existing AWS Organizations. However, an error happened when the data analysis AWS administrator tried to leave the organization as a member account in the AWS console. Which below options are possible reasons for the failure? Select 2.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; A, C</strong></p>\r\n\r\n<p><strong>Explanation</strong>:</p>\r\n\r\n<p>According to <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_troubleshoot_general.html#troubleshoot_general_error-leaving-org\" target=\"_blank\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_troubleshoot_general.html#troubleshoot_general_error-leaving-org</a>, if an error happens when a member leaves an organization, two things need to be checked:</p>\r\n\r\n<ul>\r\n\t<li>You can remove a member account only after enabling IAM user access to billing in the member account.</li>\r\n\t<li>You can remove an account from your organization only if the account has the information required to operate as a standalone account.</li>\r\n</ul>\r\n\r\n<p>For the IAM user access to billing settings, log in to the AWS console and modify that in &ldquo;My account&rdquo; -&gt; &ldquo;IAM User and Role Access to Billing Information&rdquo;:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq2-2.jpg\" style=\"height:173px; width:850px\" /></p>\r\n\r\n<p>Other than this, there are some minimum IAM policy requirements for Leaving Organizations as a Member Account (<a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html#orgs_manage_accounts_leave-as-member\" target=\"_blank\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html#orgs_manage_accounts_leave-as-member</a>):</p>\r\n\r\n<ul>\r\n\t<li>organizations:DescribeOrganization (console only).</li>\r\n\t<li>organizations:LeaveOrganization &ndash; Note that the organization administrator can apply a policy to your account that removes this permission, preventing you from removing your account from the organization.</li>\r\n\t<li>If you sign in as an IAM user and the account is missing payment information, the IAM user must have the permissions aws-portal:ModifyBilling and aws-portal:ModifyPaymentMethods.</li>\r\n\t<li>Option A is CORRECT: Because this is a necessary step before leaving AWS Organizations.</li>\r\n\t<li>Option B is incorrect: Because there is no such limitation for overdue bills.</li>\r\n\t<li>Option C is CORRECT: Because in order to leave an AWS Organizations in console, organizations:DescribeOrganization and organizations:LeaveOrganization are required.</li>\r\n\t<li>Option D is incorrect: Because the member account can leave AWS Organizations as long as it meets the above requirements.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34759,
          "question_id": 44748,
          "answers": [
            {
              "choice": "<p>The member account was removed before the IAM user access to billing in the member account was enabled. This setting controls the access to Account Settings, Payment Methods, and Report pages.</p>\r\n\r\n<p>&nbsp;</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>The member account has bills that are already overdue for several days. All overdue bills need to be paid before the account is removed from the AWS Organizations.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>The IAM user of the member account does not have the permission of &ldquo;organizations:DescribeOrganization&rdquo; or &ldquo;organizations:LeaveOrganization&rdquo; so that it is blocked by IAM policy.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Member account cannot leave AWS Organizations by itself. Instead, the root account can remove member account if it has &ldquo;organizations:RemoveAccountFromOrganization&rdquo; permission.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28429,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>As an AWS Administrator, you have set up an ELB within a couple of Availability Zones. The ELB distributes application traffic to an Auto Scaling group with session affinity enabled. You notice that the traffic is not being evenly distributed across the AZ&rsquo;s. Which option may help to alleviate this issue? Choose an answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer &ndash; A</strong></p>\r\n\r\n<p>The traffic is not evenly distributed across the instances in multiple AZs. That means the traffic is going to only specific EC2 instances. This happens when either the instances that are not receiving the traffic are unhealthy or the instances receiving the traffic are holding onto the session.</p>\r\n\r\n<p>This scenario does not mention any unhealthy instances. So, it is most likely related to instances holding onto sessions. This means the ELB has sticky sessions enabled.</p>\r\n\r\n<p><strong>Option A is CORRECT</strong> because this situation occurs when ELB has sticky sessions or session affinity enabled.</p>\r\n\r\n<p><strong>Option B is incorrect</strong> because reducing the frequency of health checks will not force the even distribution of the traffic.</p>\r\n\r\n<p><strong>Option C is incorrect </strong>because if sticky sessions are enabled, increasing the number of instances in each AZ will not help receive the traffic. In fact, more instances will remain idle now.</p>\r\n\r\n<p><strong>Option D is incorrect</strong> because recreating ELB again will not resolve this issue.</p>\r\n\r\n<p><strong>More information on ELB Sticky Sessions:</strong></p>\r\n\r\n<p>The load balancer uses a special cookie to track the instance for each request to each listener. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the instance specified in the cookie. If there is no cookie, the load balancer chooses an instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that instance. The stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie.</p>\r\n\r\n<p>This could be a reason as to why the sessions are going to a certain AZ.</p>\r\n\r\n<p>For more information on ELB sticky sessions, please refer to the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html\" target=\"_blank\">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 80130,
          "question_id": 28429,
          "answers": [
            {
              "question_id": "28429",
              "choice": "<p>Disable sticky sessions on the ELB.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "28429",
              "choice": "<p>Reduce the frequency of the health checks.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28429",
              "choice": "<p>Increase the number of instances hosting the web application in each AZ.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "28429",
              "choice": "<p>Recreate the ELB again.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28430,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>In Cloudfront, what is the Origin Protocol policy that must be chosen to ensure that the communication with the origin is done either via HTTP or HTTPS?&nbsp;Choose an answer from the options below.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\r\n\r\n<p>It is clearly given in the AWS documentation that the Origin Protocol Policy should be set accordingly.</p>\r\n\r\n<p>Options A, B, and D are all incorrect because the answer is Match Viewer.</p>\r\n\r\n<p>Option C is CORRECT because if the Origin Protocol Policy is set to Match Viewer, the CloudFront communicates with the origin using HTTP or HTTPS, depending on the viewer&#39;s protocol.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/01/22/questions_htnkkl.png\" style=\"height:533px; width:1063px\" /></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>For more information on Cloudfront CDN, please see the below link-</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html\" target=\"_blank\">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html</a></p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18547,
          "question_id": 28430,
          "answers": [
            {
              "choice": "<p>HTTP</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>HTTPS</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Match Viewer</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>None of the above</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 44749,
      "topic_id": 364,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An IoT company needs to develop a product that can quickly count the number of persons in a given area. They have used wireless sensors and a Node.js backend in AWS EC2 (ap-southeast-2 region with 3 availability zones). As the data is very sensitive which will be analyzed by a third-party company, they need the backend to be highly available. The backend EC2 needs to connect to the internet to download patches. Other than that, for security reasons, EC2 should only open SSH port to a jump host. For the below descriptions, which one is the best?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; C</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>As in the scenario, high availability is a must. We should consider using public/private pairs in all three availability zones ap-southeast-2a, ap-southeast-2b, and ap-southeast-2c. Then create a NAT gateway in each public subnet as NAT gateway is within the scope of an availability zone. This can ensure that other NAT gateways can still server internet traffic even when an availability zone is out of service. In terms of the Bastion host, it should be put in a public subnet, and the EC2 security group should allow the SSH access (port 22) for the inbound traffic from the Bastion host security group.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect: Because NAT gateway should be created in each availability zone instead of meeting the need for high availability. And bastion host ARN id is incorrect as well. Bastion host security group should be used.</li>\r\n\t<li>Option B is incorrect: Because NAT gateway is better than NAT instance as NAT gateway is more reliable and easier to maintain. Moreover, the route should connect to the NAT gateway.</li>\r\n\t<li>Option C is CORRECT: Because it has used NAT gateway in three availability zones and Bastion host in a public subnet, which is the best way.</li>\r\n\t<li>Option D is incorrect: Because it should create three private subnets to make three public/private pairs. It can ensure that the service is not interrupted even when one availability zone is out of service. Moreover, NAT gateway is highly managed by AWS, and autoscaling is impossible to be added for that.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34760,
          "question_id": 44749,
          "answers": [
            {
              "choice": "<p>For each availability zone, create a public subnet and a private subnet. Create a NAT gateway in a single public subnet and for the route&nbsp;table in three private subnets, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the bastion host ARN id.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create a public subnet and a private subnet in all three availability zones in the ap-southeast-2 region. Create a NAT instance in each public subnet and for the route&nbsp;table in the&nbsp;private subnet, add a route from 0.0.0.0/0 to the public subnet. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the bastion host ARN id.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>For each availability zone, create a public subnet and a private subnet. Create a NAT gateway in each public subnet and for the route&nbsp;table in the&nbsp;private subnet, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the security group of the bastion host.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create a private subnet in ap-southeast-2a and three public subnets in ap-southeast-2a, ap-southeast-2b, and ap-southeast-2c. Create a NAT gateway using an autoscaling group in all three availability zones and for the route&nbsp;table in the private subnet, add a route from 0.0.0.0/0 to the NAT gateway. Add a bastion host in one public subnet and for EC2 instances, only open port 22 for the inbound traffic from the security group of the bastion host.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28378,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>You are designing a multi-platform,multi-port web application for AWS. The application will run on EC2 instances and will be accessed from PCs, tablets, and smartphones. Supported accessing platforms are Windows, MAC OS,&nbsp;IOS, and Android. Separate&nbsp;SSL certificate setups are required for different platform types. Which of the following describes the most cost-effective and performance efficient architecture setup?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B</p>\r\n\r\n<p>In this scenario, the main architectural considerations are (1) web application has EC2 instances running multiple platforms such as Android, iOS, etc., and (2) separate SSL certificate setups are required for different platforms.</p>\r\n\r\n<ul>\r\n\t<li>Options A is&nbsp;incorrect because it is not cost-effective to handle such hybrid architecture.</li>\r\n\t<li>Option B is correct. Originally, Application Load Balancers used to support only one certificate for a standard HTTPS listener (port 443). You had to use Wildcard or Multi-Domain (SAN) certificates to host multiple secure applications behind the same load balancer. The potential security risks with Wildcard certificates and the operational overhead of managing Multi-Domain certificates presented challenges.&nbsp;<strong>With SNI support you can associate multiple certificates with a listener and each secure application behind a load balancer can use its own certificate.&nbsp;</strong>&nbsp;You can use host conditions to define rules that forward requests to different target groups based on the hostname in the host header (also known as&nbsp;<em>host-based routing</em>). This enables you to support multiple domains using a single load balancer.</li>\r\n\t<li>Option C is incorrect because Classic Load Balancer cannot handle multiple SSL certificates.</li>\r\n\t<li>Option D is incorrect as it is not required since there is support for multiple TLS/SSL certificates on Application Load Balancers.&nbsp;It is not cost-efficient as one ALB can achieve the requirement.</li>\r\n\t<li>For more information on ELB, please visit the below URL-\r\n\t<ul>\r\n\t\t<li><a href=\"https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/\" target=\"_blank\">https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/</a></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18495,
          "question_id": 28378,
          "answers": [
            {
              "choice": "<p>Setup a hybrid architecture to handle session state, SSL certificates, on-premise resources, and EC2 Instances that run the web applications for different platform types in a VPC.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Set up an Application Load Balancer with Server Name Indicator (SNI) support, for handling the separate SSL certificates for each of the device platforms.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Set up two Classic Load Balancers. The first ELB handles SSL certificates for all platforms and the second ELB handles session stickiness for all platforms. For each ELB run separate EC2 instance groups to handle the web application&nbsp;for each the platforms.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Assign multiple ALBs to a group of EC2 instances running the common components of the web application. One ALB for each platform type, Session stickiness, and SSL termination.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28432,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You&#39;re consulting for a company that is migrating its legacy application ( built on TCP ) to the AWS cloud. In order to apply high availability, you&#39;ve decided to implement Elastic Load Balancer and Auto Scaling services to serve traffic to this legacy application.</p>\r\n\r\n<p>The legacy application is not a standard HTTP web application. But it is a custom application with custom codes that are run internally for the employees of the company you are consulting.</p>\r\n\r\n<p>The ports required to be open are port 80 and port 8080. Which listener configuration would you create? Choose an answer from the options below.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>The application in this scenario is a legacy-based application built on TCP and works on ports 80 and 8080. It requires that the traffic should be routed correctly.</p>\r\n\r\n<p>Option A is CORRECT because for the ELB to route the traffic correctly, it should be configured with ports TCP:80 and TCP 8080. For the backends as well, the ports that should be configured must be TCP:80 and&nbsp;TCP:8080.</p>\r\n\r\n<p>Options B, C, and D are all incorrect as both the ELB and instance protocol must be configured for ports TCP:80 and TCP:8080.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>More information on ELB</strong></p>\r\n\r\n<p>Since the application is a custom application and not a standard HTTP application.&nbsp;Hence you need to have the TCP ports open. Hence option A is the right option.</p>\r\n\r\n<p>Before you start using Elastic Load Balancing, you must configure one or more&nbsp;<em>listeners</em>&nbsp;for your Classic Load Balancer. A listener is a process that checks for connection requests. It is configured with a protocol and a port for front-end (client to load balancer) connections and a protocol and a port for back-end (load balancer to back-end instance) connections.</p>\r\n\r\n<p>Elastic Load Balancing supports the following protocols.</p>\r\n\r\n<ul>\r\n\t<li>HTTP</li>\r\n\t<li>HTTPS (secure HTTP)</li>\r\n\t<li>TCP</li>\r\n\t<li>SSL (secure TCP)</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>For more information on listener configuration for ELB, please see the below link.</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html\" target=\"_blank\">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html</a></p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18549,
          "question_id": 28432,
          "answers": [
            {
              "choice": "<p>Configure the load balancer with the following ports: TCP:80 and TCP:8080 and the instance protocol to TCP:80 and TCP:8080</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to HTTPs:80 and HTTPs:8080</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to TCP:80 and TCP:8080</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to HTTP:80 and HTTP:8080</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 44750,
      "topic_id": 367,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A software development team just finished phase 1 of a web service that provides NBA news to subscribers. The web service has used a dedicated VPC which has only IPv4 CIDR (10.0.0.0/16) with two public subnets and two private subnets. A NAT gateway is put into each public subnet for outbound internet traffic. The EC2 instances are put into private subnets with a route that connects all Internet-bound IPv4 traffic to the relevant NAT gateway. The product is getting more and more popular and needs IPv6 to support some new features. Which below options are required for the new support for IPv6? Select 3.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; B, C, F</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>Refer to <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html\" target=\"_blank\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html</a> for the details on how to migrate to IPv6 traffic. Please also note that this question asks for the necessary options. The below steps are required.</p>\r\n\r\n<ul>\r\n\t<li>Step 1: Associate an IPv6 CIDR Block with Your VPC and Subnets</li>\r\n\t<li>Step 2: Update Your Route Tables</li>\r\n\t<li>Step 3: Update Your Security Group Rules</li>\r\n\t<li>Step 4: Change Your Instance Type</li>\r\n\t<li>Step 5: Assign IPv6 Addresses to Your Instances</li>\r\n\t<li>Option A is incorrect: Because you do not need to delete and recreate VPC to support IPv6.</li>\r\n\t<li>Option B is CORRECT: Because that is a required step to add IPv6 CIDR in both VPC and subnets.</li>\r\n\t<li>Option C is CORRECT: Because the routing table needs to be modified to route the IPv6 traffic properly.</li>\r\n\t<li>Option D is incorrect: Because the NAT gateway is IPv4 only. For IPv6, an egress-only internet gateway should be used. Refer to<a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\" target=\"_blank\"> https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a>.</li>\r\n\t<li>Option E is incorrect: Same reason as option D. Also, you cannot add IPv6 IP range to the NAT gateway.</li>\r\n\t<li>Option F is CORRECT: Because EC2 instances need IPv6 addresses to route IPv6 traffic. This can be done in &ldquo;Actions, Networking, Manage IP Addresses&rdquo; and choose &ldquo;Assign new IP&rdquo; Under &ldquo;IPv6 Addresses&rdquo;.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34761,
          "question_id": 44750,
          "answers": [
            {
              "choice": "<p>Delete existing VPC and recreate a new VPC with both IPv4 and IPv6 CIDR. Create new public and private subnets with both IPv4 and IPv6 address ranges.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Associate an Amazon-provided IPv6 CIDR block with existing VPC and subnets. In the VPC and subnets console, choose &ldquo;Add IPv6 CIDR&rdquo;.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to an egress-only internet gateway.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Update the route tables to route the IPv6 traffic. For public subnets, create a route that routes all IPv6 traffic from the subnet to the internet gateway. For private subnets, create a route that routes all Internet-bound IPv6 traffic to NAT gateway.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Assign IPv6 addresses to NAT gateway which will be used to route the internet-bound IPv6 traffic from EC2 instances.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Assign IPv6 addresses to EC2 instances from the IPv6 address range that is allocated to the subnet.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28434,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You are responsible for a legacy web application whose server environment is approaching the end of life. You would like to migrate this application to AWS as quickly as possible since the application environment currently has the following limitations.</p>\r\n\r\n<p>The VM&rsquo;s single 10GB VMDK is almost full. The virtual network interface still uses the 10Mbps driver, which leaves your 100Mbps WAN connection completely underutilized. It is currently running on a highly customized Windows VM within a VMware environment.&nbsp;You do not have the installation media. How could you best migrate this application to AWS while meeting your business continuity requirements?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - B</p>\r\n\r\n<ul>\r\n\t<li>Option A is Incorrect because EC2 VM Import Connector is a facility that builds on top of VM Import. We need the VM import in place for the connector to work.</li>\r\n\t<li><a href=\"https://aws.amazon.com/blogs/aws/ec2-vm-import-connector/\" target=\"_blank\">https://aws.amazon.com/blogs/aws/ec2-vm-import-connector/</a></li>\r\n\t<li>Option B is correct&nbsp;because VM Import/Export offers several ways to import your virtual machine into Amazon EC2. VM Import/Export enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment.</li>\r\n\t<li>Option C is incorrect because the backup that is taken and stored on S3 may not be directly restored as an EC2 instance, and (b) it may not meet the RPO of 1 hour as this process will be slow for a large number of servers.</li>\r\n\t<li>Option D is incorrect because (a) it is applicable to only instance store-backed Windows instance and the data on the volumes other than the root device volume does not get preserved, and (b) this API is not applicable to the Windows instances that are backed by EBS volumes.</li>\r\n</ul>\r\n\r\n<ul>\r\n\t<li>For more information on EC2 VM Import/Export, please see the URL below.\r\n\t<ul>\r\n\t\t<li><a href=\"https://aws.amazon.com/ec2/vm-import/\">https://aws.amazon.com/ec2/vm-import/</a></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18551,
          "question_id": 28434,
          "answers": [
            {
              "choice": "<p>Use the EC2 VM Import Connector for vCenter to import the VM into EC2.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use VM Import/Export to import the VM as an Amazon Machine Image (AMI). Launch EC2 instances from the AMI.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use S3 to create a backup of the VM and restore the data into EC2.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use the EC2&rsquo;s bundle-instance API to import an image of the VM into EC2.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28435,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An administrator in your company has created a VPC with an IPv4 CIDR block 10.0.0.0/24. Now they want to add additional address space outside of the current VPC CIDR. Because&nbsp;there is a requirement to host more resources in that VPC. Which of the below requirement can be used to accomplish this? Choose an answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; B</p>\r\n\r\n<p>An existing CIDR for a VPC is not modifiable. However, you can add additional CIDR blocks, i.e., up to four secondary IPv4 CIDR blocks to an already existing VPC.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect because you can change the CIDR of VPC by adding up to 4 secondary IPv4 IP CIDRs to your VPC.</li>\r\n\t<li>Option B is CORRECT because you can expand your existing VPC by adding up to four secondary IPv4 IP ranges (CIDRs) to your VPC.</li>\r\n\t<li>Option C is incorrect because deleting the subnets is unnecessary.</li>\r\n\t<li>Option D is incorrect because this configuration would peer the VPC. It will not alter the existing VPC&rsquo;s CIDR.</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<ul>\r\n\t<li>For more information on VPC and its FAQs, please refer to the following links-\r\n\t<ul>\r\n\t\t<li><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/\" target=\"_blank\">https://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/</a></li>\r\n\t\t<li><a href=\"https://aws.amazon.com/vpc/faqs/\" target=\"_blank\">https://aws.amazon.com/vpc/faqs/</a></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18552,
          "question_id": 28435,
          "answers": [
            {
              "choice": "<p>You cannot change a VPC&#39;s size. Currently, to change the size of a VPC, you must terminate your existing VPC and create a new one.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VPC.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Delete all the subnets in the VPC and expand the VPC.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create a new VPC with a greater range and then connect the older VPC to the newer one.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 28436,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A company has a legacy-based software that needs to be transferred to the AWS cloud. The legacy-based software has a dependency on the license which is based on the MAC Address. What would be a possible solution to ensure that the legacy-based software will always work properly and not lose the MAC address at any point in time? Choose an answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; D</p>\r\n\r\n<p>Option A is incorrect because you cannot map a static IP address to a MAC address.</p>\r\n\r\n<p>Option B is incorrect because putting a license server in a private subnet would not resolve the dependency on the license based on a MAC address.</p>\r\n\r\n<p>Option C is incorrect because MAC addresses cannot be tied to subnets.</p>\r\n\r\n<p>Option D is CORRECT because you should use the Elastic Network Interface associated with a fixed MAC address. This will ensure that the legacy license-based software would always work and not lose the MAC address at any point in the future.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>For more information on Elastic Network Interfaces, please refer to the URL below.</p>\r\n\r\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\" target=\"_blank\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18553,
          "question_id": 28436,
          "answers": [
            {
              "choice": "<p>Ensure any EC2 Instance that you deploy has a static IP address mapped to the MAC address.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use a VPC with a private subnet for the license and a public subnet for the EC2.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use a VPC with a private subnet and configure the MAC address to be tied to that subnet.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use a VPC with instances having an elastic network interface attached that has a fixed MAC Address.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28437,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your customer has recently completed a Proof of Concept (PoC) and now wants you to recommend a database solutions in AWS. The PoC deployed 100 sensors to measure street noise and air quality for 3 months. During the PoC, your peak IOPS was 10, and the database received 30 GB worth of key-value sensor data per month. You are required to provide an architecture for 100,000 sensors initially and accommodate future scaling needs. The sensor data must be retained for a least two years to compare yearly improvements. Which AWS storage solution requires the least amount of change to the application?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Answer - A</strong></p>\r\n\r\n<p>The key point here is that the data is being sent key-value pairs from the sensors. Whenever you see key-value, you should immediately think of the Amazon no-SQL database DynamoDB. DynamoDB has nearly infinite capacity, sub-second latency, and designed to ingest large amounts of data.</p>\r\n\r\n<ul>\r\n\t<li>Option A is correct as DynamoDB is best suited to store and analysis key-value data.\r\n\t<ul>\r\n\t\t<li>Reference -&nbsp;<a href=\"https://aws.amazon.com/nosql/key-value/\" target=\"_blank\">https://aws.amazon.com/nosql/key-value/</a></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>Option B is incorrect as MySQL has a limit of 16 TB, and a no-SQL database is recommended for key-value pairs.\r\n\t<ul>\r\n\t\t<li>Reference -&nbsp;<a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.FileSize\" target=\"_blank\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.FileSize</a></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>Option C is incorrect as Kinesis is not a storage solution and only stores data for up to 7 days. (24 hours by default) The data from the sensors doesn&rsquo;t need to be analyzed in real-time or converted from a key-value pair. So Kinesis isn&rsquo;t needed.\r\n\t<ul>\r\n\t\t<li>Reference -&nbsp;<a href=\"https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\" target=\"_blank\">https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html</a></li>\r\n\t</ul>\r\n\t</li>\r\n\t<li>Option D is incorrect as it would require a change to the application.&nbsp;The PoC sent the data to a table in a database&nbsp;to use S3. We would need to change the application to add a log file to S3, as S3 is object storage. (.txt, .jpg, mp4, etc.). More importantly, S3 is not an ideal service for database storage. So it is not suitable for this case.\r\n\t<ul>\r\n\t\t<li>Reference -&nbsp;<a href=\"https://aws.amazon.com/s3/\" target=\"_blank\">https://aws.amazon.com/s3/</a></li>\r\n\t</ul>\r\n\t</li>\r\n</ul>\r\n\r\n<p>If you are interested in learning more about how to use DynamoDB for high-volume, time-series data. Here is a reference link on the Database Blog from 2019:&nbsp;</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/blogs/database/design-patterns-for-high-volume-time-series-data-in-amazon-dynamodb/\" target=\"_blank\">https://aws.amazon.com/blogs/database/design-patterns-for-high-volume-time-series-data-in-amazon-dynamodb/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18554,
          "question_id": 28437,
          "answers": [
            {
              "choice": "<p>DynamoDB</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>RDS MySQL</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Kinesis</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>S3</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 44751,
      "topic_id": 364,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)&ndash;aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access. Simple AD is a Microsoft Active Directory&ndash;compatible directory from AWS Directory Service powered by Samba 4. Which below scenarios are suitable for Simple AD? Choose 3.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; B, D, F</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>Simple AD is a subset of the features offered by AWS Managed Microsoft AD. It has included the ability to manage user accounts and group memberships, create and apply group policies, securely connect to Amazon EC2 instances, and provide Kerberos-based single sign-on (SSO).</p>\r\n\r\n<p>Simple AD offers many advantages according to<a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\" target=\"_blank\"> https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html</a>:</p>\r\n\r\n<ul>\r\n\t<li>Simple AD makes it easier to <a href=\"http://docs.aws.amazon.com/directoryservice/latest/admin-guide/join_a_directory.html\" target=\"_blank\">manage Amazon EC2 instances running Linux and Windows</a> and deploy Windows applications in the AWS Cloud.</li>\r\n\t<li>Many of the applications and tools that you use today that require Microsoft Active Directory support can be used with Simple AD.</li>\r\n\t<li>User accounts in Simple AD allow access to AWS applications such as Amazon WorkSpaces, Amazon WorkDocs, or Amazon WorkMail.</li>\r\n\t<li>You can manage AWS resources through IAM role-based access to the AWS Management Console.</li>\r\n\t<li>Daily automated snapshots enable point-in-time recovery.</li>\r\n\t<li>Option A is incorrect: Because Simple AD is not used to manage on-premise instances. It is a directory service in the AWS domain.</li>\r\n\t<li>Option B is CORRECT: Because Simple AD can provide directory service with lots of existing Active Directory tools and features supported.</li>\r\n\t<li>Option C is incorrect: Because this is suitable for AD Connector instead of Simple AD. Refer to <a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\" target=\"_blank\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</a>.</li>\r\n\t<li>Option D is CORRECT: Because Simple AD has the capability to backup with daily automated snapshots.</li>\r\n\t<li>Option E is incorrect: Because Simple AD is a basic directory service which is not suitable for large service. Also, note that Simple AD supports a maximum of 5000 users. Simple AD is available in two sizes.</li>\r\n</ul>\r\n\r\n<p>Small - Supports up to 500 users (approximately 2,000 objects including users, groups, and computers).</p>\r\n\r\n<p>Large - Supports up to 5,000 users (approximately 20,000 objects including users, groups, and computers).</p>\r\n\r\n<ul>\r\n\t<li>Option F is CORRECT: Because Simple AD provides basic directory service and is cheaper than &ldquo;AWS Directory Service for Microsoft Active Directory&rdquo;.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34762,
          "question_id": 44751,
          "answers": [
            {
              "choice": "<p>An operation management team needs to manage its Amazon EC2 instances and on-premises servers running Linux Ubuntu and Windows. Basic Active Directory features such as user accounts, group memberships are required.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>For a small project, a standalone directory in the cloud is needed, where the operators can create and manage user identities and manage access to applications. The operators want to use many familiar Active Directory&ndash;aware applications and tools that require basic Active Directory features.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>The development lead needs to set up a service that allows the on-premises users to log in to AWS applications and services with their Active Directory credentials.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>For the directory service in AWS, the security manager requires that it can be backed up via daily automated snapshots with point-in-time recovery enabled</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>A large company is considering to set up a new directory service in AWS that can support its existing 10000 users (approximately 30,000 objects including users, groups, and computers).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>With a limited budget, a startup company requires a directory service to be set up with basic Active Directory compatibility that supports Samba 4&ndash;compatible applications.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 28439,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your company has servers located in dozens of VPCs in different availability zones. These VPCs need to connect to each other. You need a service to establish the connections, simplify management and reduce operational costs. Which of the below options is best suited to achieve this requirement?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>Option A is CORRECT because Transit Gateway acts as a hub to connect different VPCs. It is a fully managed service and can achieve all the requirements.</p>\r\n\r\n<p>Option B is incorrect because you need to create lots of VPC Peering connections since there are dozens of VPCs. Unlike Transit Gateway, you have to manage all the connections.</p>\r\n\r\n<p>Option C is incorrect because, with Transit VPC, you need to manage and scale EC2 based software appliances. Please check <a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html</a> for the comparations between Transit Gateway and Transit VPC.</p>\r\n\r\n<p>Option D is incorrect because AWS PrivateLink is a service to establish private connections with AWS services. It is not a service to manage the connections among VPCs.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18556,
          "question_id": 28439,
          "answers": [
            {
              "choice": "<p>Set up AWS Transit Gateway to provide a hub for connecting VPCs.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Set up VPC Peering connections between each VPC.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Set up Transit VPC as a hub among the VPCs.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Establish AWS PrivateLink among different VPCs.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 44752,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Company ABC has used on-premises AD administrative tools to manage resources. The company wants to enable all employees to use on-premises credentials to sign in to the AWS Management Console to access and manage its various AWS resources (e.g., AWS Workspaces etc..) To achieve this, they have successfully created an AWS Microsoft AD directory connected to their on-premises AD via a forest trust relationship. Which below items are still required? Select 3.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; C, D, E</strong></p>\r\n\r\n<p>Using AWS Managed Microsoft AD directory, you can easily enable access to multiple AWS applications and services such as the AWS Management Console, Amazon WorkSpaces, and Amazon RDS for SQL Server. More importantly, your users can access the application or service with their existing AD credentials. Refer to <a href=\"https://aws.amazon.com/blogs/security/how-to-access-the-aws-management-console-using-aws-microsoft-ad-and-your-on-premises-credentials/\" target=\"_blank\">https://aws.amazon.com/blogs/security/how-to-access-the-aws-management-console-using-aws-microsoft-ad-and-your-on-premises-credentials/</a><a href=\"https://aws.amazon.com/blogs/security/how-to-access-the-aws-management-console-using-aws-microsoft-ad-and-your-on-premises-credentials/\" target=\"_blank\"> </a>on the details on how to setup AWS Managed Microsoft AD directory and manage administrative permissions to AWS resources.</p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<ul>\r\n\t<li>Option A is INCORRECT because Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and is not used to provide access to AWS instances for on-premise users.</li>\r\n\t<li>Option B is incorrect: Because NACL ( Network Access Control List ) belongs to the scope of subnets. In this case, the security group of the AWS Microsoft AD directory should be taken care of.</li>\r\n\t<li>Option C is CORRECT because we need to access all services like the AWS Workspaces, AWS WorkMail, AWS Console, and RDS. The access to the management console can be enabled as given below.</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq2-6.jpg\" style=\"height:398px; width:800px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option D is&nbsp;CORRECT:&nbsp;Because this enables AWS Management Console access and provides a URL that can be used to connect to the console. The URL is generated by appending &ldquo;/console&rdquo; to the end of the access URL generated in Option C.</li>\r\n\t<li>Option E is&nbsp;CORRECT:&nbsp;Because this critical step is used to control which AWS resources on-premises users and groups can access from the AWS Management Console.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34763,
          "question_id": 44752,
          "answers": [
            {
              "choice": "<p>Configure AWS System Manager to provide access to AWS instances for on-premise users.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Ensure the NACL of the AWS Microsoft AD directory allows the outbound traffic to on-premise AD tools.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>In AWS apps &amp; services section for the AWS Microsoft AD directory, enable all services. This is used to activate the access for services include Amazon WorkSpaces, Amazon WorkMail, RDS SQL Server, and AWS Management Console.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Enable AWS Management Console access for the AWS Microsoft AD directory and get the URL that can be used to connect to the console. For example, the AWS Management Console URL is &ldquo;https://example-corp.awsapps.com/console&rdquo;.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Assign on-premises users and groups to IAM roles.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44753,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An IOS developer is creating a simple notes APP that includes functions such as notes creation, retrieval, and deletion for an authenticated user.&nbsp; The mobile application also requires sign-up and sign-in functionality. An API exposes the notes service through API Gateway. The developer would like to implement authorization in the API to identify the authenticated user and perform operations in the context of that user, such as Create Note and Delete Note. Which of the following should be used to achieve this requirement by AWS Cognito?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; D</strong></p>\n\n<p><strong>Explanation:</strong></p>\n\n<p>Amazon Cognito has key components of user pools and identity pools, which can be used in several scenarios. User pools are user directories that provide sign-up and sign-in options. And Identity pools provide AWS credentials to grant your users access to other AWS services.</p>\n\n<p>In this question, the App needs Cognito to implement the function of authorization for the API in the API gateway. It requires both the User pools and Identity Pools. In the first step, your app user signs in through a user pool and receives user pool tokens after successful authentication.&nbsp;Next, your app exchanges the user pool tokens for AWS credentials through an identity pool.</p>\n\n<ul>\n\t<li>Option A is INCORRECT: Because it is User Pool, not Identity Pool which provides ID and Access Token.</li>\n\t<li>Option B is INCORRECT: Because Cognito has used User Pool as a central place to manage user access instead of IAM roles. Moreover, user.id is not used to authorize users.</li>\n\t<li>Option C is INCORRECT because User Pool should be used for authentication and the Identity Pool should be used for authorization.</li>\n\t<li>Option D is CORRECT because both the identity and the user pool are connected. Identity pool is used to exchange tokens for AWS credentials.</li>\n</ul>\n\n<p>Please refer to page 1 on the below link-</p>\n\n<ul>\n\t<li><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-dg.pdf\" target=\"_blank\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-dg.pdf</a></li>\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34764,
          "question_id": 44753,
          "answers": [
            {
              "choice": "<p>In Amazon Cognito, create an Identity pool used to return an ID and Access Token to the app for the authenticated user if the user logins successfully. The Access Token can then be used to authorize API invocations through API Gateway.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>In Amazon Cognito, create users with proper IAM roles. Ensure the roles have proper policies to access API resources. Return the user.id if the user signs in successfully. The user.id can be used to invoke API for the notes service.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>In Amazon Cognito, create an Identity&nbsp;Pool and User&nbsp;Pool. Connect the Identity Pool with the User Pool. First, authenticate with the identity pool to get the token, then exchange the token with the user pool to get temporary credentials. These credentials&nbsp;can be used to invoke API for the notes service.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>In Amazon Cognito, create a User Pool and Identity Pool. Connect the Identity Pool with the User Pool. First, authenticate with the user pool to get the token, then exchange the token with the identity pool to get temporary credentials. These credentials&nbsp;can be used to invoke API for the notes service.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28379,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your company has an on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst In web traffic due to a company announcement. Over the coming days, you are expecting similar announcements to drive similar unpredictable bursts. You&nbsp;are looking for ways to quickly improve your infrastructure&#39;s ability to handle unexpected traffic increases.</p>\r\n\r\n<p>The application currently consists of a 2 tier web tier consisting of a load balancer and several Linux Apache web servers and a database tier that hosts a Linux server hosting a MySQL database.</p>\r\n\r\n<p>Which of the below scenario will provide full site functionality while helping to improve the ability of your application in the short timeframe required?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>In this scenario, the major points of consideration are: (1) your application may get unpredictable bursts of traffic, (b) you need to improve the current infrastructure in the shortest period possible, and (3) your web servers are on-premises.</p>\r\n\r\n<p>Since the time period in hand is short, instead of migrating the app to AWS, you need to consider different ways to improve the performance without modifying the existing infrastructure.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Option A is CORRECT because (a) CloudFront is AWS&rsquo;s highly scalable, highly available content delivery service, where it can perform excellently even in case of a sudden unpredictable burst of traffic, (b) the only change you need to make is making the on-premises load balancer as the custom origin of the CloudFront distribution.&nbsp;</p>\r\n\r\n<p>Option B is incorrect because you are supposed to improve the current situation in the shortest time possible. Migrating to AWS would be more time consuming than simply setting up the CloudFront distribution.</p>\r\n\r\n<p>Option C is incorrect because you cannot host dynamic websites on the S3 bucket. Also, this option provides insufficient infrastructure to set up options.</p>\r\n\r\n<p>Option D is incorrect. It is now possible to use Application Load Balancers through IP Address to On-Premises and AWS Resources. However, option D is incorrect as it is not covering the database tier which is an essential part of this 2 tier architecture.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><br />\r\n<strong>More information on CloudFront:</strong></p>\r\n\r\n<p>You can have CloudFront sit in front of your on-premise web environment via a custom origin. This would protect against unexpected bursts in traffic by letting CloudFront handle the traffic from the cache, thus removing some of the load from the on-premise web servers.<br />\r\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Like other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long-term&nbsp;commitments or minimum fees. With CloudFront, your files are delivered to end-users using a global network of edge locations.<br />\r\nIf you have dynamic content, then it is best to have the TTL set to 0.</p>\r\n\r\n<p>For more information on CloudFront, please visit the below URL-<br />\r\n<a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\">https://aws.amazon.com/cloudfront/</a><br />\r\n&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18496,
          "question_id": 28379,
          "answers": [
            {
              "choice": "<p>Offload traffic from an on-premises environment by setting up a CloudFront distribution and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behavior, and select a TTL that objects should exist in the cache.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Migrate to AWS. Use VM import &lsquo;Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic. Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an S3 bucket and configure it tor website hosting. Migrate your DNS to Route53 using zone import and leverage Route53 DNS failover to failover to the S3 hosted website.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an AMI which can be used to launch web servers in EC2. Create an Auto Scaling group which uses the AMI&rsquo;s to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28442,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>Your application provides data transformation services. Files containing data to be transformed are first uploaded to Amazon S3 and then transformed by a fleet of spot EC2 instances. Files submitted by your premium customers must be transformed with the highest priority. How should you implement such a system?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - C</p>\r\n\r\n<p>Option A is incorrect because using DynamoDB tables will be a very expensive solution compared to using the SQS queue(s).</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Option B is incorrect because the transformation instances are spot instances that may not be up and running all the time; there are chances that they will be terminated.</p>\r\n\r\n<p>Option C is CORRECT because (a) it decouples the components of a distributed application, so the application is not impacted due to using spot instances, (b) it is a much cheaper option compared to using DynamoDB tables, and more importantly, (b) it maintains a separate queue for the high priority messages which can be processed before the default priority queue.</p>\r\n\r\n<p>Option D is incorrect because the transformation instances cannot poll high-priority messages first.&nbsp;They just poll and can determine priority only after receiving the messages.</p>\r\n\r\n<p><strong>More information about implementing the priority queue via SQS:</strong></p>\r\n\r\n<p><a href=\"http://awsmedia.s3.amazonaws.com/pdf/queues.pdf\" target=\"_blank\">http://awsmedia.s3.amazonaws.com/pdf/queues.pdf</a></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18559,
          "question_id": 28442,
          "answers": [
            {
              "choice": "<p>Use a DynamoDB table with an attribute defining the priority level. Transformation instances will scan the table for tasks, sorting the results by priority level.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use Route 53 latency based-routing to send high priority tasks to the closest transformation instances.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use two SQS queues, one for high priority messages, the other for default priority. Transformation instances first poll the high priority queue; if there is no message, they poll the default priority queue.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use a single SQS queue. Each message contains the priority level. Transformation instances poll high-priority messages first.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28443,
      "topic_id": 363,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>There is a requirement to split a VPC with a CIDR block of 10.0.0.0/24 into two subnets, each of which should give 128&nbsp;IP addresses (including the 5 fixed IPs reserved by AWS). Can this be done and if so, how will the allocation of the IP addresses be configured? Choose the correct answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; C</p>\r\n\r\n<p>This is clearly given in the AWS documentation.</p>\r\n\r\n<p>&quot; For example, if you create a VPC with CIDR block 10.0.0.0/24, it supports 256 IP addresses. You can break this CIDR block into two subnets, each supporting 128&nbsp;IP addresses. One subnet uses CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other uses CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255)&quot;.</p>\r\n\r\n<p>For more information on VPC and subnets, please see the below link-</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\" target=\"_blank\">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></p>\r\n\r\n<p>To get the IP addresses and subnets, please see the below link-</p>\r\n\r\n<p><a href=\"http://www.subnet-calculator.com/cidr.php\">http://www.subnet-calculator.com/cidr.php</a></p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/09/28/ckeditor_69_2.png\" style=\"height:900px; width:1600px\" /></p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/09/28/ckeditor_69_1.png\" style=\"height:900px; width:1600px\" /></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18560,
          "question_id": 28443,
          "answers": [
            {
              "choice": "<p>One subnet will use CIDR block 10.0.0.0/127 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/255 (for addresses 10.0.0.128 - 10.0.0.255).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.1.0/25 (for addresses 10.0.1.0 - 10.0.1.127).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255).</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>This is not possible.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for Organizational Complexity"
    },
    {
      "question_id": 44754,
      "topic_id": 367,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A mobile App developer just made an App in both IOS and Android with a feature to count step numbers. He has used AWS Cognito to authorize users to provide access to the AWS DynamoDB table. The App uses the DynamoDB table to store user subscriber data and many steps. Now the developer also needs Cognito to integrate with Google to provide federated authentication for the mobile application users so that the user does not need to remember extra login access. &nbsp;What should the developer do to authenticate and authorize the users with suitable permissions for the IOS and Android App? (Select TWO.)</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer &ndash;&nbsp;A and C</p>\n\n<p>Explanation:</p>\n\n<p>One common use case for Amazon Cognito is to access AWS Services with an Identity Pool. For the Identity pool itself, it can include:</p>\n\n<ul>\n\t<li>Users in an Amazon Cognito identity&nbsp;pool.</li>\n\t<li>Users who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider.</li>\n\t<li>Users authenticated via your own existing authentication process.</li>\n</ul>\n\n<p>Option A is CORRECT&nbsp;because Identity pool can be used to set up the federated identities through third-party identity providers such as Google.</p>\n\n<p>Option B is&nbsp;incorrect:&nbsp;Because Google federated identities work for both Android and IOS. Refer to&nbsp;<a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/google.html\" target=\"_blank\">https://docs.aws.amazon.com/cognito/latest/developerguide/google.html</a> on the details.</p>\n\n<p>Option C is CORRECT because the User Pool is where the federated identity would be set-up and the Identity Pool is where permissions would be granted.&nbsp;Please also check <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\">https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/</a> for the differences between User pools and Identify pools. User pools are for authentication (identify verification), while Identity pools are for authorization (access control).</p>\n\n<p>Option D is incorrect:&nbsp;Same reason as Option B.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34765,
          "question_id": 44754,
          "answers": [
            {
              "choice": "<p>Amazon Cognito Identity pools (federated identities) support user authorization through federated identity providers&mdash;including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Only Android works for federated identities if Google access is required for AWS Cognito. This can be done by configuring Cognito identity pools with a Google Client ID.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Amazon Cognito User pools support user authentication through federated identity providers&mdash;including Amazon, Facebook, Google, and SAML identity providers. The developer just needs to set up the federated identities for Google access in the Cognito User Pool.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Only IOS (Objective-C and Swift) works for federated identities if Google access is required for AWS Cognito. This can be done by configuration Cognito identity pools with a Google Client ID. Google federated access does not work for the android app.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 45323,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A software engineer is developing a web App using Sinatra, which is a free and open-source software web application library and domain-specific language written in Ruby. He wants to develop that in AWS Elastic Beanstalk. However, it seems that Sinatra is not a supported built-in platform that can&nbsp;be used. He does not want to use docker either for this. Can he create a customized environment for the App in Elastic Beanstalk?&nbsp;Choose 2 options.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer: B and C</strong></p>\r\n\r\n<p>Elastic Beanstalk supports custom platforms now. A custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances.</p>\r\n\r\n<p>The custom image can be built from one of the supported operating systems of Ubuntu, RHEL, or Amazon Linux. In order to simplify the creation of these specialized Elastic Beanstalk platforms, machine images are now created using the Packer tool.</p>\r\n\r\n<p>Refer to the technical blog <a href=\"https://aws.amazon.com/blogs/aws/launch-aws-elastic-beanstalk-launches-support-for-custom-platforms/\" target=\"_blank\">https://aws.amazon.com/blogs/aws/launch-aws-elastic-beanstalk-launches-support-for-custom-platforms/</a> on how to set up a customer platform.</p>\r\n\r\n<p>After setting up a customer platform, choose the new platform while creating a new environment.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/03/20/ckeditor_9.1_26_23.png\" style=\"height:426px; width:1000px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect.&nbsp;Because the customer platform is supported. Also, please note that Docker is another option that can work as well. However, this case disallows Docker.</li>\r\n\t<li>Option B is Correct.&nbsp;</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/06/12/ckeditor_custom_ami.png\" style=\"height:470px; width:824px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option C is&nbsp;CORRECT.&nbsp;Because it is a proper way to create a custom platform for Sinatra.</li>\r\n\t<li>Option D&nbsp;is incorrect.&nbsp;Because creating a customer platform uses Packer rather than Docker as a tool to build up the AMI. In order to create the custom platform, the developer should start with a Packer template. A sample of a basic folder structure for building a platform definition archive looks as follows.</li>\r\n</ul>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/03/20/ckeditor_9.2.png\" style=\"height:221px; width:1000px\" /></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 35330,
          "question_id": 45323,
          "answers": [
            {
              "choice": "<p>No. He cannot create the Sinatra app in Elastic Beanstalk as only the Platforms published and maintained by AWS Elastic Beanstalk are supported. He should use the Docker platform.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Yes, He can build a custom AMI in advance with Sinatra installed and configured on it. Create an ElasticBeanStalk environment and then modify the EC2 image ID with the customized AMI.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Yes. He can create his own platforms on Ubuntu, Red Hat Enterprise, or Amazon Linux and customize his instances with Sinatra. A Packer template will be used. When creating an Elastic Beanstalk environment, select the customized platform.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Yes. He can create his own platforms on Amazon Linux and customize his instances using docker. A Dockerfile will be used together with other scripts. When creating an Elastic Beanstalk environment, select the customized docker platform.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 44756,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": 0,
      "lab_id": null,
      "question_text": "<p>One full-stack engineer is developing an application to process images and videos from a highly scaled website. The incoming web traffic is very unbalanced and unstable during the day. When the web traffic rises up, it may require a maximum of 10 m4.xlarge EC2 instances. However, the requirement may drop down to 1 m4.xlarge instance. The processing for the application takes time, resulting in the delay of web application response. He prefers to deploy the App in Elastic Beanstalk. Which of the following&nbsp;options should the engineer choose?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; B</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<ul>\r\n\t<li><strong>Option A is INCORRECT</strong> because the traffic is very unbalanced.&nbsp;Since it is a &quot;long-running process&quot; an Elastic Beanstalk Worker environment is preferred over the Elastic Beanstalk Web environment.</li>\r\n\t<li><strong>Option B is CORRECT </strong>because it is a &quot;long-running process&quot; an Elastic Beanstalk Worker environment is preferred here. Also, SQS has the following capabilities: Separating queries from the long-running tasks. Also, we can &quot;park&quot; the video and let CPUs process from the queue because we cannot wait on the video to be done before sending a response to the user and for building backlogs (Dead Letter Queues for later processing)</li>\r\n\t<li><strong>Option C is INCORRECT</strong> because it should be a long-time running task instead of a short-time running task.</li>\r\n\t<li><strong>Option D is INCORRECT</strong> because an Elastic Beanstalk Worker environment is preferred over the Elastic Beanstalk Web environment. And SQS is preferred based on the requirement, auto-scaling can&#39;t only be enough&nbsp;to process images and videos from a highly scaled website.</li>\r\n</ul>\r\n\r\n<p><strong>References:</strong></p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html\" target=\"_blank\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html</a></li>\r\n\t<li><a href=\"https://www.ideaminetech.com/blog/deploy-high-available-application-using-elastic-beanstalk/\" target=\"_blank\">https://www.ideaminetech.com/blog/deploy-high-available-application-using-elastic-beanstalk/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 84453,
          "question_id": 44756,
          "answers": [
            {
              "question_id": "44756",
              "choice": "<p>Deploy the application in one Elastic Beanstalk web server environment. Make sure that the environment uses a load-balancing, autoscaling configuration. Therefore, it can auto-scale depending on the traffic level.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "44756",
              "choice": "<p>Deploy the application in one Elastic Beanstalk worker environment. Create an SQS queue and an SQS daemon in EC2 to handling with the queue messages. Make sure that the environment uses a load-balancing and autoscaling configuration.</p>",
              "feedback": "",
              "correct": true
            },
            {
              "question_id": "44756",
              "choice": "<p>Create one Elastic Beanstalk worker environment and deploy the application in the new environment. Make sure that the application performs the short-running task in response to the POST from SQS daemon. Uses an auto-scaling configuration for the environment.</p>",
              "feedback": "",
              "correct": false
            },
            {
              "question_id": "44756",
              "choice": "<p>Create an Elastic Beanstalk web server environment and add the application bundle to it. Make sure that the environment uses an autoscaling configuration with a maximum at 10 and a minimum at 1.</p>",
              "feedback": "",
              "correct": false
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 44757,
      "topic_id": 365,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>A big company has a service to process gigantic clickstream data sets which are often the result of holiday shopping traffic on a retail website, or sudden dramatic growth on the data network of a media or social networking site. It is becoming more and more complex to analyze these clickstream datasets for its on-premise infrastructure. As the sample data set keeps growing, fewer applications are available to provide a timely response. The service is using a Hadoop cluster with Cascading. How can they migrate the applications to AWS in the best way?&nbsp;</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; D</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>The application needs to process data timely; therefore Kinesis stream should be considered first. After a click event happens, a message should be put into the Kinesis stream in real-time. Moreover, Cascading is a proven, highly extensible application development framework for building massively parallelized data applications on EMR. By using EMR, the application does not need to change a lot for the migration. Refer to <a href=\"https://aws.amazon.com/blogs/big-data/integrating-amazon-kinesis-amazon-s3-and-amazon-redshift-with-cascading-on-amazon-emr/\" target=\"_blank\">https://aws.amazon.com/blogs/big-data/integrating-amazon-kinesis-amazon-s3-and-amazon-redshift-with-cascading-on-amazon-emr/</a>.</p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect: Because the RDS database&#39;s output is improper as RDS does not scale well when the traffic is high. Redshift is much more appropriate.</li>\r\n\t<li>Option B is incorrect: AWS Lambda can potentially work for Hadoop. However, EMR provides native support for Hadoop. Also, RDS is incorrect.</li>\r\n\t<li>Option C is incorrect: Because EC2 is not suitable for Hadoop processing if compared with EMR. This question asks for the best option. So EMR should be chosen.</li>\r\n\t<li>Option D is CORRECT: Because the EMR cluster with Cascading can process the data from Kinesis stream in real-time, and Redshift is also a proper place to store the output data.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34768,
          "question_id": 44757,
          "answers": [
            {
              "choice": "<p>Put the source data to S3 and migrate the processing service to an AWS EMR Hadoop cluster with Cascading. Enable EMR to read and query data from S3 buckets directly. Write the output to the RDS database.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Put the source data to a Kinesis stream and migrate the processing service to AWS lambda to utilize its scaling feature. Enable lambda to read and query data from the Kinesis stream directly. Write the output to the RDS database.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Put the source data to an S3 bucket and migrate the processing service to AWS EC2 with auto-scaling. Ensure that the auto-scaling configuration has a proper maximum and&nbsp;minimum number of instances. Monitor the performance in the Cloudwatch dashboard. Write the output to the DynamoDB table for downstream to process.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Put the source data to a Kinesis stream and migrate the processing service to an AWS EMR cluster with Cascading. Enable EMR to read and query data from Kinesis streams directly. Write the output to Redshift.</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44758,
      "topic_id": 364,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A development team has got a new assignment to maintain and process data in several DynamoDb tables and S3 buckets. They need to perform sophisticated queries and operations across DynamoDB and S3. For example, they would export rarely used data from DynamoDB to Amazon S3 to reduce the storage costs while preserving low latency access required for high-velocity data. The team has rich Hadoop and SQL knowledge. What is the best way to accomplish the assignment?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct&nbsp;Answer&nbsp;&ndash;&nbsp;B</strong></p>\r\n\r\n<p>For the scenarios that Hadoop or Hive is mentioned, the first AWS service to think about should be EMR as it uses a SQL-based engine for Hadoop called Hive. One thing to note is that outside data sources are referenced in the Hive cluster by creating an EXTERNAL TABLE such as:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/21/ckeditor_awsq2-12.jpg\" style=\"height:193px; width:750px\" /></p>\r\n\r\n<p>Details on using EMR to efficiently export DynamoDB tables to S3 or import S3 data into DynamoDB are in <a href=\"https://aws.amazon.com/articles/using-dynamodb-with-amazon-elastic-mapreduce/?tag=articles%23keywords%23elastic-mapreduce\" target=\"_blank\">https://aws.amazon.com/articles/using-dynamodb-with-amazon-elastic-mapreduce/?tag=articles%23keywords%23elastic-mapreduce</a>.</p>\r\n\r\n<ul>\r\n\t<li>Option&nbsp;A is&nbsp;incorrect:&nbsp;Because although this option may work, it is more complicated as it needs to maintain the EC2 cluster, install Hadoop, etc.</li>\r\n\t<li>Option&nbsp;B is&nbsp;CORRECT:&nbsp;Because by using an EMR cluster, the team can use its Hadoop and SQL experience while EMR takes care of the infrastructure setup.</li>\r\n\t<li>Option&nbsp;C is&nbsp;incorrect:&nbsp;Because EMR is better than lambda as EMR provides native Hadoop support.</li>\r\n\t<li>Option&nbsp;D is&nbsp;incorrect:&nbsp;Because data pipelines cannot perform complicated SQL commands and unsuitable for this task.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34769,
          "question_id": 44758,
          "answers": [
            {
              "choice": "<p>Use an Elastic Beanstalk environment to set up a compute-optimized EC2 instance so that the instance has better performance for SQL commands to query tables or export/import data.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use an EMR cluster as it uses Hadoop Hive which is a SQL-based engine. Create external tables in EMR for DynamoDB table and S3 buckets. Use a SQL-based command to export/import data.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use a CloudFormation template to utilize a lambda to run SQL commands. Make sure the lambda has enough memory allocated as SQL commands consume high memory resources</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Set up several data pipelines to automatically move data from DynamoDB to S3 if the data has met user-defined conditions such as the items are older than a year. Modify data pipeline configurations when needed.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 44759,
      "topic_id": 365,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A middle-sized company is planning to migrate its on-premises servers to AWS. At the moment, they have used various licenses, including windows operating system server, SQL Server, IBM Db2, SAP ERP, etc. After migration, the existing licenses should continue to work in EC2. The IT administrators prefer to use a centralized place to control and manage the licenses to prevent potential non-compliant license usages. For example, SQL Server Standard&#39;s license only allows 50 vCPUs, which means a rule is needed to limit the number of SQL Servers in EC2. Which option is correct for the IT administrators to use?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct&nbsp;Answer&nbsp;&ndash;&nbsp;B</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>AWS License Manager is a central place to manage licenses in AWS EC2 and on-premises instances. It contains 3 parts to use:</p>\r\n\r\n<ul>\r\n\t<li>Define licensing rules.</li>\r\n\t<li>Enforce licensing rules.</li>\r\n\t<li>Track usage.</li>\r\n</ul>\r\n\r\n<p>AWS License Manager currently integrates with Amazon EC2, allowing you to track licenses for default (shared-tenancy) EC2 instances, Dedicated Instances, Dedicated Hosts, Spot Instances, and Spot Fleet, and Auto Scaling groups. Refer to <a href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html\" target=\"_blank\">https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html</a>.</p>\r\n\r\n<ul>\r\n\t<li>Option&nbsp;A is&nbsp;incorrect.&nbsp;Because AWS License Manager manages the BYOL licenses. Although AWS System Manager can work together with AWS License Manager to manage licenses for on-premises servers and non-AWS public clouds, it is not the central place to provide license management.</li>\r\n\t<li>Option&nbsp;B is&nbsp;CORRECT:&nbsp;Because AWS License Manager can define licensing rules, track license usage, and enforce controls on license use to reduce the risk of license overages.</li>\r\n\t<li>Option&nbsp;C is&nbsp;incorrect:&nbsp;Because the AWS License manager should be considered first for licensing management.</li>\r\n\t<li>Option&nbsp;D is&nbsp;incorrect:&nbsp;Because AWS License Manager can manage non-Microsoft licenses.</li>\r\n</ul>\r\n\r\n<p>According to <a href=\"https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html\" target=\"_blank\">https://docs.aws.amazon.com/license-manager/latest/userguide/license-manager.html</a>, license Manager tracks various software products from Microsoft, IBM, SAP, Oracle, and other vendors.</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34770,
          "question_id": 44759,
          "answers": [
            {
              "choice": "<p>Create license rules in AWS System Manager for all BYOL licenses. Use the rules to make sure that there are no non-compliant activities. Link the rules when EC2 AMI is created. System Manager console has provided license usage status.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Define license rules in AWS License Manager for the required licenses. Enforce the license rules in EC2 and track usage in the AWS License Manager console.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use a license management blueprint to create a dedicated Lambda to control license usage. Lambda outputs the usage status to Cloudwatch Metrics which can be used by the administrators to track the status.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Define and enforce license rules in AWS License Manager for the Microsoft relevant licenses such as windows, SQL Server as only Microsoft licenses are supported. For the other licenses such as IBM Db2, track the license usage in AWS System Manager.</p>\r\n\r\n<p>&nbsp;</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 44760,
      "topic_id": 364,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An outsourcing company is working on a government project. Security is very important to the success of the application. The application is developed mainly in EC2 with several application load balancers. CloudFront and Route53 are also configured. The major concern is that it should be able to be protected against DDoS attacks. The company decides to activate the AWS Shield Advanced feature. To this effect, it has hired an external consultant to &#39;educate&#39; its employees on the same. For the below options, which ones help the company to understand the AWS Shield Advanced plan? Select 3.</p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "2",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; A, D, E</strong></p>\r\n\r\n<p><strong>Explanation</strong>:</p>\r\n\r\n<p>AWS Shield has two plans - AWS Shield Standard and AWS Shield Advanced.</p>\r\n\r\n<p><strong>AWS Shield Standard:</strong></p>\r\n\r\n<p>AWS Shield Standard activates automatically at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your applications.</p>\r\n\r\n<p><strong>AWS Shield Advanced:</strong></p>\r\n\r\n<p>For higher levels of protection against attacks. It has a subscription fee which is $ 3000 per month.</p>\r\n\r\n<ul>\r\n\t<li>Option A is CORRECT.&nbsp;Because Elastic Load Balancing (ELB), Amazon CloudFront, Amazon Route 53 are all covered by AWS Shield Advanced.</li>\r\n\t<li>Option B is incorrect.&nbsp;Because AWS Shield Advanced has a subscription commitment of 1 year with a base monthly fee of 3000$.</li>\r\n\t<li>Option C is incorrect.&nbsp;Because Route 53 is covered by AWS Shield Advanced.</li>\r\n\t<li>Option D is CORRECT.&nbsp;Because 24*7 support by the DDoS Response team is a key feature of the advanced plan.</li>\r\n\t<li>Option E is CORRECT.&nbsp;Because AWS Shield Advanced integrates with AWS CloudWatch and provides relevant reports.</li>\r\n\t<li>Option F is incorrect.&nbsp;Because AWS Shield is not within AWS WAF. Please note that both of them help protect the AWS resources. AWS WAF is a web application firewall service, while AWS Shield provides expanded DDoS attack protection for the AWS resources.</li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34771,
          "question_id": 44760,
          "answers": [
            {
              "choice": "<p>AWS Shield Advanced plan is able to protect application load balancers, CloudFront and Route53 from DDoS attacks.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>AWS Shield Advanced plan does not have a monthly base charge. The company only needs to pay the data transfer fee. Other than that, AWS WAF includes no additional cost.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Route 53 is not covered by AWS Shield Advanced plan. However, Route 53 is able to be protected under AWS WAF. A dedicated rule in WAF should be customized.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>24*7 support by the DDoS Response team. Critical and urgent priority cases can be answered quickly by DDoS experts. Custom mitigations during attacks are also available.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Real-time notification of attacks is available via Amazon CloudWatch. Historical attack reports are also provided.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>AWS Shield is a sub-feature within AWS WAF. AWS Shield Advanced can be activated in AWS WAF console, which also provides the near real-time metrics and packet captures for attack forensics.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Design for New Solutions"
    },
    {
      "question_id": 44761,
      "topic_id": 367,
      "course_id": 0,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>An Artificial Intelligence startup company has used lots of EC2 instances. Some instances use the SQL Server database, while others use Oracle. As the data needs to be kept secure, regular snapshots are required. They want SQL Server EBS volume to take a snapshot every 12 hours. However, for Oracle, it only needs a snapshot every day. Which option below is the best one that the company should choose?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p><strong>Correct Answer &ndash; D</strong></p>\r\n\r\n<p><strong>Explanation:</strong></p>\r\n\r\n<p>Amazon Data Lifecycle Manager (Amazon DLM) should be considered if automating snapshot management is required (<a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\" target=\"_blank\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a>).&nbsp; One thing to note is that the DLM policy has used Tags to choose EBS volumes.</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/22/ckeditor_awsq3-3.jpg\" style=\"height:275px; width:750px\" /></p>\r\n\r\n<ul>\r\n\t<li>Option A is incorrect.&nbsp;Because although this may work, it is not a straightforward solution as DLM.</li>\r\n\t<li>Option B is incorrect.&nbsp;Because prefix in the name is incorrect. Tags (name and value) are used to choose EBS volumes.</li>\r\n\t<li>Option C is incorrect.&nbsp;Because it brings extra cost and is not as easy as DLM. The question asks for the solution without extra cost.</li>\r\n\t<li>Option D is CORRECT.&nbsp;Because two management policies in DLM can meet the needs.\r\n\t<p>For example:</p>\r\n\t</li>\r\n</ul>\r\n\r\n<p><strong>For SQL Server EBS volume</strong>:</p>\r\n\r\n<p><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/22/ckeditor_awsq3-3.1.jpg\" style=\"height:179px; width:750px\" /></p>\r\n\r\n<p><strong>For Oracle EBS volume:</strong></p>\r\n\r\n<p><strong><img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2019/02/22/ckeditor_awsq3-3.2.jpg\" style=\"height:177px; width:750px\" /></strong></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 34772,
          "question_id": 44761,
          "answers": [
            {
              "choice": "<p>Use a free third-party tool such as Clive to Manage EC2 instance lifecycle. It can design various backup policies for EC2 EBS volumes. Add a 12 hours backup policy to SQL Server EBS volumes and a 24 hours backup policy to Oracle EBS volumes.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Add a prefix to the name of both SQL Server and Oracle EBS volumes. In the AWS Data Lifecycle Management console, create two management policies based on the name prefix. For example, add a 12 hours backup schedule to EBS volumes with a name starting with &ldquo;SQL&rdquo; and add a 24 hours backup schedule to EBS volumes with a name starting with &ldquo;oracle&rdquo;.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create a dedicated Lambda function to differentiate EC2 EBS volumes and take snapshots. Set up Cloudwatch Events Rules to call the lambda so that the function runs every 12 hours for SQL Server and 24 hours for Oracle.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Add different tags for SQL Server and Oracle EBS volumes. In the AWS Data Lifecycle Management console, create two management policies based on the tags. Add a 12 hours schedule to SQL Server lifecycle policy and a 24 hours schedule to Oracle lifecycle policy</p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Continuous Improvement for Existing Solutions"
    },
    {
      "question_id": 28380,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Your company produces customer commissioned one-of-a-kind high-graphical skiing helmets, combining high-fashion with custom technical enhancements. The current manufacturing process is data-rich and complex. It includes assessments to ensure that the custom electronics and materials used to assemble the helmets are of the highest standards.</p>\r\n\r\n<p>The current assessments are&nbsp;a mixture of human and automated checks. You have been tasked with adding a new set of assessments&nbsp;to model the custom electronics&#39; failure modes using GPUs across a cluster of servers with low latency networking.&nbsp;</p>\r\n\r\n<p>What architecture would allow you to automate the existing process through a hybrid approach while&nbsp;ensuring&nbsp;the architecture can support future iterations?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - B</p>\r\n\r\n<p>Tip: Whenever the scenario in the question mentions high graphical processing servers with low latency networking, always think about using G3&nbsp;instances. And, when there are tasks involving human intervention, always think about using SWF.</p>\r\n\r\n<p>Option A is incorrect because AWS Data Pipeline cannot work in a hybrid approach where some of the tasks involve human actions.</p>\r\n\r\n<p>Option B is CORRECT because (a) it uses G3&nbsp;instances which are specialized for high graphical processing of data with low latency networking, and (b) SWF supports workflows involving human interactions along with AWS services.</p>\r\n\r\n<p>Option C is incorrect because it uses C3 instances used for situations where compute optimization is required. In this scenario, you should be using G3&nbsp;instances.</p>\r\n\r\n<p>Option D is incorrect because (a) AWS Data Pipeline cannot work in a hybrid approach where some of the tasks involve human actions, and (b) it uses C3 instances used for situations where compute optimization is required. In this scenario, you should be using G3&nbsp;instances.</p>\r\n\r\n<p><strong>More information on G3&nbsp;instances:</strong></p>\r\n\r\n<p>Using G3&nbsp;instances is preferred. Hence options C and D are wrong.</p>\r\n\r\n<p>&nbsp;<img alt=\"\" src=\"https://s3.amazonaws.com/media.whizlabs.com/learn/2018/12/19/ckeditor_csap-pt1-78.png\" style=\"height:640px; width:607px\" /></p>\r\n\r\n<p>For more information on Instances types, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/ec2/instance-types/\" target=\"_blank\">https://aws.amazon.com/ec2/instance-types/</a></li>\r\n</ul>\r\n\r\n<p>Since there is an element of human intervention, SWF can be used for this purpose.</p>\r\n\r\n<p>For more information on SWF, please visit the below URL-</p>\r\n\r\n<ul>\r\n\t<li><a href=\"https://aws.amazon.com/swf/\" target=\"_blank\">https://aws.amazon.com/swf/</a></li>\r\n</ul>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18497,
          "question_id": 28380,
          "answers": [
            {
              "choice": "<p>Use AWS Data Pipeline to manage the movement of data,&nbsp;metadata, and assessments. Use an auto-scaling group of G3&nbsp;instances in a placement group.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and&nbsp;metadata. Use an auto-scaling group of G3&nbsp;instances in a placement group.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Use Amazon Simple Workflow (SWF) to manage assessments, movement of data, and&nbsp;metadata. Use an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization).</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use AWS Data Pipeline to manage movement of data, metadata, and assessments. Use an auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization).</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28452,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>There is a requirement for a company to transfer large amounts of data between AWS and an on-premise location. There is an additional requirement for low latency and high consistency traffic to AWS. Out of these given requirements, how would you design a hybrid architecture? Choose the correct answer from the below options.</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer &ndash; A</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Tip: Whenever the scenario in the question requires the use of low latency transfer of data between AWS/VPC and on-premise servers/database, always think about provisioning AWS Direct Connect.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>Option A is CORRECT because Direct Connect creates a dedicated connection between AWS and on-premises server for low latency secured data transfer.</p>\r\n\r\n<p>Option B is incorrect because setting up VPN connectivity has higher&nbsp;maintenance overhead compared to Direct Connect. Also, Direct Connect provides a dedicated network connection bypassing the internet. Hence it is more secure.</p>\r\n\r\n<p>Option C is incorrect because setting up the IPSec tunnel has setup and maintenance overhead. Also, the IPSec tunnel does not guarantee the end-to-end security of the data as it uses the internet.</p>\r\n\r\n<p>Option D is incorrect as Direct Connect is the most suited option for this scenario.</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p><strong>More information on AWS Direct Connect:</strong></p>\r\n\r\n<p>AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p>\r\n\r\n<p>For more information on AWS direct connect, just browse to the below URL-</p>\r\n\r\n<p><a href=\"https://aws.amazon.com/directconnect/\" target=\"_blank\">https://aws.amazon.com/directconnect/</a></p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18569,
          "question_id": 28452,
          "answers": [
            {
              "choice": "<p>Provision a Direct Connect connection to an AWS region using a Direct Connect partner.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Create a VPN tunnel for private connectivity which increases network consistency and reduces latency.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Create an IPSec tunnel for private connectivity which increases network consistency and reduces latency.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>This is not possible.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    },
    {
      "question_id": 28381,
      "topic_id": 365,
      "course_id": 168,
      "case_study_id": null,
      "lab_id": null,
      "question_text": "<p>You&rsquo;re running an application on-premises due to its dependency on non-x86 hardware and want to use AWS for data backup. Your backup application is only able to write to POSIX-compatible block-based storage. You have 624TB of data and would like to mount it as a single folder on your file server. Users must be able to access portions of this data while the backups are taking place. What backup solution would be most appropriate for this use case?</p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Answer - A</p>\r\n\r\n<p>Gateway-Cached volumes can support volumes of 1,024TB in size, whereas Gateway-stored volume supports volumes of 512 TB size.</p>\r\n\r\n<p>Option A is CORRECT because (a)&nbsp;it supports volumes of up to 1,024TB in size, and (b) the frequently accessed data is store on the on-premise server while the entire data is backed up over AWS.</p>\r\n\r\n<p>Option B is incorrect because S3 is not ideal for POSIX compliant data.</p>\r\n\r\n<p>Option C is incorrect because the data stored in Amazon Glacier is not available immediately. Retrieval jobs typically require 3&ndash;5 hours to complete; so, if you need immediate access to your data as mentioned in the question, this may not be the ideal choice.</p>\r\n\r\n<p>Option D is incorrect because gateway stored volumes can only store only 512TB worth of data.</p>\r\n\r\n<p>For more information on all of the options for storage, please refer to the below link-</p>\r\n\r\n<p><a href=\"http://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits\" target=\"_blank\">http://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits</a></p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>\r\n\r\n<p>&nbsp;</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 18498,
          "question_id": 28381,
          "answers": [
            {
              "choice": "<p>Use Storage Gateway and configure it to use Gateway Cached volumes.</p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>Configure your backup software to use S3 as the target for your data backups.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Configure your backup software to use Glacier as the target for your data backups.</p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>Use Storage Gateway and configure it to use Gateway Stored volumes.</p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "Migration Planning"
    }
  ]
}