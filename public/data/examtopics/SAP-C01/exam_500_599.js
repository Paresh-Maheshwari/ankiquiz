var ExamTopic_500_599 = {
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": "#500",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company must deploy multiple independent instances of an application. The front-end application is internet accessible. However, corporate policy stipulates that the backends are to be isolated from each other and the internet, yet accessible from a centralized administration server. The application setup should be automated to minimize the opportunity for mistakes as new instances are deployed.<br>Which option meets the requirements and MINIMIZES costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#500",
          "answers": [
            {
              "choice": "<p>A. Use an AWS CloudFormation template to create identical IAM roles for each region. Use AWS CloudFormation StackSets to deploy each application instance by using parameters to customize for each instance, and use security groups to isolate each instance while permitting access to the central server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create each instance of the application IAM roles and resources in separate accounts by using AWS CloudFormation StackSets. Include a VPN connection to the VPN gateway of the central administration server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Duplicate the application IAM roles and resources in separate accounts by using a single AWS CloudFormation template. Include VPC peering to connect the VPC of each application instance to a central VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the parameters of the AWS CloudFormation template to customize the deployment into separate accounts. Include a NAT gateway to allow communication back to the central administration server.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 42695,
          "date": "Tue 28 Sep 2021 11:36",
          "username": "dumma",
          "content": "Option A is wrong as security groups cannot be used across region. Also security groups may provide an opportunity for mistakes. Inter-region data transfer charges are higher as compared to Intra-region transfer charges. Correct answer is C",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 62793,
          "date": "Sat 02 Oct 2021 02:42",
          "username": "youqGanfengRVivekRocketeer",
          "content": "should be \\\"C\\\" ? IAM role is global so A is incorrect.IAM role is global however, not cross accountAnswer A sates \\\"IAM roles for each region\\\", that is wrongIt says \\\"identical IAM roles for each region\\\". Hence I can create role-region1 and role-region2 with the same policies. I think A is correct for the least cost.",
          "upvote_count": "10211",
          "selected_answers": ""
        },
        {
          "id": 175422,
          "date": "Tue 19 Oct 2021 09:21",
          "username": "GanfengRVivekRocketeer",
          "content": "IAM role is global however, not cross accountAnswer A sates \\\"IAM roles for each region\\\", that is wrongIt says \\\"identical IAM roles for each region\\\". Hence I can create role-region1 and role-region2 with the same policies. I think A is correct for the least cost.",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 546173,
          "date": "Sun 13 Feb 2022 02:01",
          "username": "RVivekRocketeer",
          "content": "Answer A sates \\\"IAM roles for each region\\\", that is wrongIt says \\\"identical IAM roles for each region\\\". Hence I can create role-region1 and role-region2 with the same policies. I think A is correct for the least cost.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 670997,
          "date": "Fri 16 Sep 2022 19:16",
          "username": "Rocketeer",
          "content": "It says \\\"identical IAM roles for each region\\\". Hence I can create role-region1 and role-region2 with the same policies. I think A is correct for the least cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 689875,
          "date": "Sun 09 Oct 2022 04:16",
          "username": "Jonfernz",
          "content": "What a terribly worded question.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 671207,
          "date": "Sat 17 Sep 2022 04:30",
          "username": "Dionenonly",
          "content": "the answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 671002,
          "date": "Fri 16 Sep 2022 19:19",
          "username": "Rocketeer",
          "content": "A<br>It says \\\"identical IAM roles for each region\\\". Hence I can create role-region1 and role-region2 with the same policies.<br>Security groups can be used to isolate the instances<br>A is much cheaper compared to C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 565682,
          "date": "Fri 11 Mar 2022 18:25",
          "username": "asfsdfsdf",
          "content": "The questions asks to minimize the cost and provide access to a central server.<br>Lets eliminate:<br>A - will not create any real connection outside the VPC - No IGW is allowed and no VPN/DC/Peering so...<br>B- Can work correctly however VPN connection is paid by the hour per connection + Egress data<br>D - cant be used as no IGW is allowed so NAT GW cant be created<br>So answer is:<br>C - Can work if the administration instance is in AWS - peering is free only data out is payed<br>If the question stated the admin server must be on premise then B was the answer regardless the cost",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 496578,
          "date": "Wed 08 Dec 2021 07:14",
          "username": "cldy",
          "content": "C.  Duplicate the application IAM roles and resources in separate accounts by using a single AWS CloudFormation template. Include VPC peering to connect the VPC of each application instance to a central VPC. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494007,
          "date": "Sat 04 Dec 2021 23:54",
          "username": "AzureDP900",
          "content": "I agree with C as correct",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 483778,
          "date": "Mon 22 Nov 2021 02:26",
          "username": "acloudguru",
          "content": "The issue with option A is that IAM roles are global and not regional.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 448738,
          "date": "Sat 06 Nov 2021 19:02",
          "username": "network_zeal",
          "content": "The issue with option A is that IAM roles are global and not regional. <br> But then C also is not convincing. Firstly, VPC peering will add to cost. Also if applications were already in different accounts, unless some inter account networking was already in place(and this is not stated anywhere), they were anyway isolated from each others.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437896,
          "date": "Thu 04 Nov 2021 09:17",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435673,
          "date": "Thu 04 Nov 2021 01:48",
          "username": "denccc",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410476,
          "date": "Tue 02 Nov 2021 10:25",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 379036,
          "date": "Mon 01 Nov 2021 19:54",
          "username": "Amitv2706",
          "content": "For A.  <br>Seems slightly unclear but is this statement not enough to clarify that central server inward access is allowed through security groups \\\"and use security groups to isolate each instance while permitting access to the central server. \\\"<br>Seems its doing two parts :<br>1- Isolation instances<br>2- at the same time allowing central server",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 346254,
          "date": "Mon 01 Nov 2021 16:41",
          "username": "blackgamer",
          "content": "I will go with C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345935,
          "date": "Mon 01 Nov 2021 14:55",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 291355,
          "date": "Mon 01 Nov 2021 07:46",
          "username": "Kian1",
          "content": "will go with C",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#501",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A group of Amazon EC2 instances have been configured as a high performance computing (HPC) cluster. The instances are running in a placement group, and are able to communicate with each other at network speeds of up to 20 Gbps.<br>The cluster needs to communicate with a control EC2 instance outside of the placement group. The control instance has the same instance type and AMI as the other instances, and is configured with a public IP address.<br>How can the Solutions Architect improve the network speeds between the control instance and the instances in the placement group?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#501",
          "answers": [
            {
              "choice": "<p>A. Terminate the control instance and relaunch it in the placement group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Ensure that the instances are communicating using their private IP addresses.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Ensure that the control instance is using an Elastic Network Adapter.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Move the control instance inside the placement group.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 17790,
          "date": "Fri 24 Sep 2021 07:33",
          "username": "TechGuruAWS2020Frank1LCC92",
          "content": "D : A instance can me moved to placement groupYESbut you would need to terminate it first ad reluch it in the placement grp, so I think the answer is AThe instance does not need to be terminated. It just needs to be stopped. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html \\\"Changing the Placement Group for an Instance\\\"updated to -> \\\"Change the placement group for an instance\\\"",
          "upvote_count": "222301",
          "selected_answers": ""
        },
        {
          "id": 17905,
          "date": "Fri 24 Sep 2021 09:32",
          "username": "AWS2020Frank1LCC92",
          "content": "YESbut you would need to terminate it first ad reluch it in the placement grp, so I think the answer is AThe instance does not need to be terminated. It just needs to be stopped. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html \\\"Changing the Placement Group for an Instance\\\"updated to -> \\\"Change the placement group for an instance\\\"",
          "upvote_count": "2301",
          "selected_answers": ""
        },
        {
          "id": 23914,
          "date": "Fri 24 Sep 2021 18:18",
          "username": "Frank1LCC92",
          "content": "The instance does not need to be terminated. It just needs to be stopped. See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html \\\"Changing the Placement Group for an Instance\\\"updated to -> \\\"Change the placement group for an instance\\\"",
          "upvote_count": "301",
          "selected_answers": ""
        },
        {
          "id": 357628,
          "date": "Mon 01 Nov 2021 12:02",
          "username": "LCC92",
          "content": "updated to -> \\\"Change the placement group for an instance\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 11736,
          "date": "Tue 21 Sep 2021 02:13",
          "username": "donathonPhatUtpaljamjam2020UpsetUserHari008biki1985",
          "content": "A<br>You cannot move an existing instance into a placement group. Instances within a placement group generally have both a public and a private IP address. Communications across a public IP address are limited to 5Gbps or less. Low-latency, high-throughput communications between placement group members can only occur across the private interfaces, using the private IP addresses. These communications can be either 10Gbps or 20Gbps, and are limited by the instance's network configuration.<br>https://awsinsider.net/articles/2017/06/12/ec2-placement-groups.aspxI think Donathon is right.Not at all. B is right optionB<br>The cluster needs to communicate with a control EC2 instance outside of the placement group.No Buddy, You can move existing instances to placement group. Even you can move an instance from one placement group to another, or remove an instance from a placementgroup.<br><br>https://acloud.guru/forums/aws-certified-solutions-architect-associate/discussion/-LBVlYuS1HKudeoD52ur/You%20can%20move%20an%20existing%20instance%20to%20a%20placement%20groupThe question clearly says the placement group should communicate with instance outside placement groupYou can move instances outside the placement group to within the placement group. For that, you have to stop the instance and then make it part of the instance group. Start instance once again.",
          "upvote_count": "13126614",
          "selected_answers": ""
        },
        {
          "id": 173451,
          "date": "Thu 14 Oct 2021 16:41",
          "username": "PhatUtpal",
          "content": "I think Donathon is right.Not at all. B is right option",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 177569,
          "date": "Fri 15 Oct 2021 14:15",
          "username": "Utpal",
          "content": "Not at all. B is right option",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 129618,
          "date": "Tue 05 Oct 2021 14:01",
          "username": "jamjam2020",
          "content": "B<br>The cluster needs to communicate with a control EC2 instance outside of the placement group.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 285540,
          "date": "Wed 27 Oct 2021 21:09",
          "username": "UpsetUserHari008",
          "content": "No Buddy, You can move existing instances to placement group. Even you can move an instance from one placement group to another, or remove an instance from a placementgroup.<br><br>https://acloud.guru/forums/aws-certified-solutions-architect-associate/discussion/-LBVlYuS1HKudeoD52ur/You%20can%20move%20an%20existing%20instance%20to%20a%20placement%20groupThe question clearly says the placement group should communicate with instance outside placement group",
          "upvote_count": "61",
          "selected_answers": ""
        },
        {
          "id": 563339,
          "date": "Tue 08 Mar 2022 16:11",
          "username": "Hari008",
          "content": "The question clearly says the placement group should communicate with instance outside placement group",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 335992,
          "date": "Sat 30 Oct 2021 22:28",
          "username": "biki1985",
          "content": "You can move instances outside the placement group to within the placement group. For that, you have to stop the instance and then make it part of the instance group. Start instance once again.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 720110,
          "date": "Thu 17 Nov 2022 02:08",
          "username": "myco",
          "content": "Answer id D clearly mention in AWS docs only missing piece in answer is stopping the instance",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714954,
          "date": "Thu 10 Nov 2022 06:56",
          "username": "janvandermerwer",
          "content": "D - Looks to be the best option<br><br>To move an instance to a placement group using the AWS CLI<br><br>Stop the instance using the stop-instances command.<br><br>Use the modify-instance-placement command and specify the name of the placement group to which to move the instance.<br><br>aws ec2 modify-instance-placement --instance-id i-0123a456700123456 --group-name MySpreadGroup<br>Start the instance using the start-instances command.https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#change-instance-placement-group\\",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 695242,
          "date": "Sat 15 Oct 2022 09:05",
          "username": "Dionenonly",
          "content": "I think it's A. <br>Instance can't be moved live.<br>I also sow this question in udemy and the answer is the same.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 671208,
          "date": "Sat 17 Sep 2022 04:35",
          "username": "Dionenonly",
          "content": "D.  Yes you can't move existing instance to placement group without a down time. But question did not mention there should not be disruption so D is okay.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 637700,
          "date": "Wed 27 Jul 2022 02:40",
          "username": "hilft",
          "content": "D.  directly from jon bonso's exam",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 579841,
          "date": "Sat 02 Apr 2022 14:48",
          "username": "Ni_yot",
          "content": "the answer is D. Although like many have said the instance needs to be stopped first",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 570638,
          "date": "Fri 18 Mar 2022 17:20",
          "username": "Ricky53",
          "content": "Answer is \\\"A\\\": Terminate and relaunch",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 565698,
          "date": "Fri 11 Mar 2022 18:43",
          "username": "asfsdfsdfasfsdfsdf",
          "content": "To optimize performance on the control instance need to move it into the placement group.<br>For elimination:<br>A - The question does not state if its the same region, I would assume it is - if so, no need to terminate the instance just to stop and modify it and re-start it.<br>B- It can help but i assume the instances are already communication with a private since the cluster instances dont have one.<br>C - It would help but cant reach 20GBs<br>so answer is<br>D - The best way is to migrate the instance inside the cluster placement group - stop it - run the modify CLI command and start it.<br>https://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-placement.html<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#change-instance-placement-groupIn Addition to the above, <br>Network traffic to the internet and over an AWS Direct Connect connection to on-premises resources is limited to 5 Gbps. <br>Instances that are not within a cluster placement group can use up to 5 Gbps for single-flow traffic.<br>So at max using public / private IPs we can get 5GBs this will for sure eliminate B and C. <br>The correct answer is D if all instances are in the same Region/AZ.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 565707,
          "date": "Fri 11 Mar 2022 18:55",
          "username": "asfsdfsdf",
          "content": "In Addition to the above, <br>Network traffic to the internet and over an AWS Direct Connect connection to on-premises resources is limited to 5 Gbps. <br>Instances that are not within a cluster placement group can use up to 5 Gbps for single-flow traffic.<br>So at max using public / private IPs we can get 5GBs this will for sure eliminate B and C. <br>The correct answer is D if all instances are in the same Region/AZ.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 563044,
          "date": "Tue 08 Mar 2022 07:06",
          "username": "Alexey79",
          "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#change-instance-placement-group<br><br>Why NOT A:<br>“<br>Before you move or remove the instance, the instance must be in the stopped state.<br>“<br>Termination in AWS has different results than Stop.<br>“<br>The key difference between stopping and terminating an instance is that the attached bootable EBS volume will not be deleted. The data on your EBS volume will remain after stopping while all information on the local (ephemeral) hard drive will be lost as usual.<br>“",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 549900,
          "date": "Fri 18 Feb 2022 03:12",
          "username": "jyrajan69",
          "content": "D.  Based on the following:<br>To move an instance to a placement group using the AWS CLI<br>Stop the instance using the stop-instances command.<br>Use the modify-instance-placement command and specify the name of the placement group to which to move the instance. ...<br>Start the instance using the start-instances command",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 525206,
          "date": "Sun 16 Jan 2022 20:28",
          "username": "cannottellname",
          "content": "How can we enhance performance?<br>A.  Terminate the control instance and relaunch it in the placement group. - Stopping helps changing the placement group. Hence, not needed.<br>B.  Ensure that the instances are communicating using their private IP addresses. - There is no Throughput level difference between them<br>C.  Ensure that the control instance is using an Elastic Network Adapter. - It is only helpful is instance is in cluster placement<br>D.  Move the control instance inside the placement group. - Seems best option. But we need to stop & move - this is not mentioned and so, I will go with A.  They have also given hint that AMI is same & hence, termination & recreation seem to help.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 524922,
          "date": "Sun 16 Jan 2022 13:07",
          "username": "RVivek",
          "content": "D.  Here though put is the issue. So no need to chnage IP. We have to change the placement group<br>A is incorrect , because we can stop the instance and move placement group. No need to terminate",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 524541,
          "date": "Sun 16 Jan 2022 02:32",
          "username": "Polu",
          "content": "D - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#:~:text=Before%20you%20move%20or%20remove%20the%20instance%2C%20the%20instance%20must%20be%20in%20the%20stopped%20state.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499292,
          "date": "Sat 11 Dec 2021 10:54",
          "username": "cldy",
          "content": "D.  Move the control instance inside the placement group.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496818,
          "date": "Wed 08 Dec 2021 13:43",
          "username": "wem",
          "content": "B<br>\\\"The cluster must communicate with an EC2 instance that is not a member of the placement group\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#502",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect has created an AWS CloudFormation template for a three-tier application that contains an Auto Scaling group of Amazon EC2 instances running a custom AMI.<br>The Solutions Architect wants to ensure that future updates to the custom AMI can be deployed to a running stack by first updating the template to refer to the new<br>AMI, and then invoking UpdateStack to replace the EC2 instances with instances launched from the new AMI.<br>How can updates to the AMI be deployed to meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#502",
          "answers": [
            {
              "choice": "<p>A. Create a change set for a new version of the template, view the changes to the running EC2 instances to ensure that the AMI is correctly updated, and then execute the change set.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Edit the AWS::AutoScaling::LaunchConfiguration resource in the template, changing its DeletionPolicy to Replace.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Edit the AWS::AutoScaling::AutoScalingGroup resource in the template, inserting an UpdatePolicy attribute.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new stack from the updated template. Once it is successfully deployed, modify the DNS records to point to the new stack and delete the old stack.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 29402,
          "date": "Sat 25 Sep 2021 08:24",
          "username": "9Ow30Stec1980Stec1980DashL",
          "content": "C<br><br>Quoting<br>\\\"If you want to update existing instances when you update the LaunchConfiguration resource, you must specify an UpdatePolicy attribute for the Auto Scaling group. \\\"<br><br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-launchconfig.htmlThat's for the AWS::AutoScaling::LaunchConfiguration resource though, not the AWS::AutoScaling::AutoScalingGroup resource, which is what answer C suggests...Actually, yes this suggests you can \\\"add an UpdatePolicy attribute to your Auto Scaling group to perform rolling updates\\\"<br><br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.htmlWhen you update the launch template or launch configuration for an Auto Scaling group, this update action does not deploy any change across the running Amazon EC2 instances in the Auto Scaling group. All new instances will get the updated configuration, but existing instances continue to run with the configuration that they were originally launched with. This works the same way as any other Auto Scaling group.<br><br>You can add an UpdatePolicy attribute to your stack to perform rolling updates (or replace the group) when a change has been made to the group.",
          "upvote_count": "28322",
          "selected_answers": ""
        },
        {
          "id": 160060,
          "date": "Mon 11 Oct 2021 00:54",
          "username": "Stec1980",
          "content": "That's for the AWS::AutoScaling::LaunchConfiguration resource though, not the AWS::AutoScaling::AutoScalingGroup resource, which is what answer C suggests...",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 160079,
          "date": "Tue 12 Oct 2021 19:03",
          "username": "Stec1980",
          "content": "Actually, yes this suggests you can \\\"add an UpdatePolicy attribute to your Auto Scaling group to perform rolling updates\\\"<br><br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 400998,
          "date": "Sat 30 Oct 2021 23:14",
          "username": "DashL",
          "content": "When you update the launch template or launch configuration for an Auto Scaling group, this update action does not deploy any change across the running Amazon EC2 instances in the Auto Scaling group. All new instances will get the updated configuration, but existing instances continue to run with the configuration that they were originally launched with. This works the same way as any other Auto Scaling group.<br><br>You can add an UpdatePolicy attribute to your stack to perform rolling updates (or replace the group) when a change has been made to the group.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 10510,
          "date": "Tue 21 Sep 2021 20:22",
          "username": "dpvnmesimonyuTiredDadviet1991TiredDad",
          "content": "A.  Ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.htmlThis link says change set will change the stack, not the template.\\\"When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template\\\" - to change a stack, you change its templateAws is C. <br>Without inserting an UpdatePolicy attribute to AWS::AutoScaling::LaunchConfiguration, execute the change setwill only create new LaunchConfiguration and existing instances are not affected.Option A says - Create a change set for a new version of the template, as part of that you would include UpdatePolicy attribute to the resource \\\"AWS::AutoScaling::AutoScalingGroup\\\" (and not to LaunchConfiguration)",
          "upvote_count": "164123",
          "selected_answers": ""
        },
        {
          "id": 35772,
          "date": "Sat 25 Sep 2021 20:45",
          "username": "simonyuTiredDad",
          "content": "This link says change set will change the stack, not the template.\\\"When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template\\\" - to change a stack, you change its template",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 414983,
          "date": "Sun 31 Oct 2021 16:03",
          "username": "TiredDad",
          "content": "\\\"When you use CloudFormation, you manage related resources as a single unit called a stack. You create, update, and delete a collection of resources by creating, updating, and deleting stacks. All the resources in a stack are defined by the stack's CloudFormation template\\\" - to change a stack, you change its template",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 365085,
          "date": "Mon 25 Oct 2021 12:47",
          "username": "viet1991TiredDad",
          "content": "Aws is C. <br>Without inserting an UpdatePolicy attribute to AWS::AutoScaling::LaunchConfiguration, execute the change setwill only create new LaunchConfiguration and existing instances are not affected.Option A says - Create a change set for a new version of the template, as part of that you would include UpdatePolicy attribute to the resource \\\"AWS::AutoScaling::AutoScalingGroup\\\" (and not to LaunchConfiguration)",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 414984,
          "date": "Sun 31 Oct 2021 21:15",
          "username": "TiredDad",
          "content": "Option A says - Create a change set for a new version of the template, as part of that you would include UpdatePolicy attribute to the resource \\\"AWS::AutoScaling::AutoScalingGroup\\\" (and not to LaunchConfiguration)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 626465,
          "date": "Sun 03 Jul 2022 09:33",
          "username": "aandc",
          "content": "vote for C <br>\\\"To update existing instances when you update the AWS::AutoScaling::LaunchConfiguration resource, you can specify an UpdatePolicy attribute for the group. \\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 580871,
          "date": "Mon 04 Apr 2022 19:54",
          "username": "roka_ua",
          "content": "Vote C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 532765,
          "date": "Wed 26 Jan 2022 10:56",
          "username": "shotty1",
          "content": "I think the \\\"best\\\" answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 504138,
          "date": "Sat 18 Dec 2021 10:48",
          "username": "tkanmani76tkanmani76",
          "content": "Answer A - Refer to Section \\\"Update the AMI on an Amazon EC2 instance\\\" in https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/updating.stacks.walkthrough.html.C is right - Refer Remarks section https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 523460,
          "date": "Fri 14 Jan 2022 10:55",
          "username": "tkanmani76",
          "content": "C is right - Refer Remarks section https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494585,
          "date": "Sun 05 Dec 2021 20:19",
          "username": "AzureDP900",
          "content": "When you update the launch configuration for an Auto Scaling group, CloudFormation deletes that resource and creates a new launch configuration with the updated properties and a new name. Existing instances are not affected. To update existing instances when you update the AWS::AutoScaling::LaunchConfiguration resource, Answer C is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413946,
          "date": "Sun 31 Oct 2021 11:54",
          "username": "DerekKeyTiredDad",
          "content": "\\\"UpdateStack to replace the EC2 instances with instances launched from the new AMI\\\"<br>A wrong - will only apply to EC2 instances managed separately<br>B wrong<br>C correct - have to use UpdatePolicy attribute on Auto Scaling group<br>D wrongYour statement \\\"A wrong - will only apply to EC2 instances managed separately\\\" is incorrect. Option A says \\\"Create a change set for a new version of the template\\\" and the template in this case corresponds to the stack which includes AutoScalingGroup",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 414988,
          "date": "Tue 02 Nov 2021 21:39",
          "username": "TiredDad",
          "content": "Your statement \\\"A wrong - will only apply to EC2 instances managed separately\\\" is incorrect. Option A says \\\"Create a change set for a new version of the template\\\" and the template in this case corresponds to the stack which includes AutoScalingGroup",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410488,
          "date": "Sun 31 Oct 2021 02:18",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 397668,
          "date": "Fri 29 Oct 2021 14:14",
          "username": "Pb55",
          "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html<br>C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 367572,
          "date": "Tue 26 Oct 2021 23:17",
          "username": "ss160700",
          "content": "B: - reason as below <br>{<br>\\\"Resources\\\": {<br>\\\"LaunchConfig\\\": {<br>\\\"Type\\\": \\\"AWS::AutoScaling::LaunchConfiguration\\\",<br>\\\"Properties\\\": {<br>\\\"KeyName\\\": {<br>\\\"Ref\\\": \\\"KeyName\\\"<br>},<br>\\\"ImageId\\\": {<br>\\\"Fn::FindInMap\\\": [<br>\\\"AWSRegionArch2AMI\\\",<br>{<br>\\\"Ref\\\": \\\"AWS::Region\\\"<br>},<br>{<br>\\\"Fn::FindInMap\\\": [<br>\\\"AWSInstanceType2Arch\\\",",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 366867,
          "date": "Tue 26 Oct 2021 08:29",
          "username": "ss160700",
          "content": "A - it us the changes",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346131,
          "date": "Fri 22 Oct 2021 23:16",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 296799,
          "date": "Fri 22 Oct 2021 09:12",
          "username": "cnethers",
          "content": "The best answer is A because:<br>- creating a changeset is the accepted method for making changes to a stack.<br>- it includes all the changes in the template that are required <br>- changes that would be required would not just be updating the ASG UpdatePolicy attribute, it would also require updating the AMI ID,<br>it would be worth parameterizing the AMI ID so that it's just a parameter change in the future. Launch Config would need replacing because a new AMI is being used and launch configs can't be updated,<br>they are replaced and then deleted once you have successfully deployed an \\\"in-service\\\" instance so that roll-back can occur.<br><br>There is more to this question than first considered. Don't rush to the first answer the makes sense, sometimes there is a better answer. B and C and not wrong but they are not the best answer. D could be done but really why would you do that?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 296150,
          "date": "Wed 20 Oct 2021 18:56",
          "username": "gparkTiredDad",
          "content": "C.  <br>===<br>A.  is not enough. Please, reference below<br>https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-group-rolling-updates/UpdatePolicy has to be included in the template, so although not mentioned explicitly, option A covers that",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 414989,
          "date": "Wed 03 Nov 2021 20:44",
          "username": "TiredDad",
          "content": "UpdatePolicy has to be included in the template, so although not mentioned explicitly, option A covers that",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294145,
          "date": "Wed 20 Oct 2021 06:45",
          "username": "kievTiredDad",
          "content": "Change set is to help you view changes before they are made in CF.  To introduce new AMIthen you have to look at the launch auto scaling group and update it and so I would go with C. Option A includes executing the change set as well. You don't need to separately look at \\\"the launch auto scaling group and update it\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 414991,
          "date": "Fri 05 Nov 2021 12:22",
          "username": "TiredDad",
          "content": "Option A includes executing the change set as well. You don't need to separately look at \\\"the launch auto scaling group and update it\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291367,
          "date": "Wed 20 Oct 2021 03:53",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#503",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing a multi-account structure that has 10 existing accounts. The design must meet the following requirements:<br>✑ Consolidate all accounts into one organization.<br>✑ Allow full access to the Amazon EC2 service from the master account and the secondary accounts.<br>✑ Minimize the effort required to add additional secondary accounts.<br>Which combination of steps should be included in the solution? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#503",
          "answers": [
            {
              "choice": "<p>A. Create an organization from the master account. Send invitations to the secondary accounts from the master account. Accept the invitations and create an OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an organization from the master account. Send a join request to the master account from each secondary account. Accept the requests and create an OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPC peering connection between the master account and the secondary accounts. Accept the request for the VPC peering connection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a service control policy (SCP) that enables full EC2 access, and attach the policy to the OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a full EC2 access policy and map the policy to a role in each account. Trust every other account to assume the role.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11737,
          "date": "Tue 21 Sep 2021 06:40",
          "username": "donathonMobidicsarah_t",
          "content": "AD<br>B\\E: This is not minimizing the effort.<br>C: You don’t need VPC peering unless the EC2 needs access across the accounts.SCP only reduce access, never give more. So D is clearly not a choice.SCP can deny or allow.",
          "upvote_count": "2734",
          "selected_answers": ""
        },
        {
          "id": 55546,
          "date": "Wed 29 Sep 2021 13:49",
          "username": "Mobidicsarah_t",
          "content": "SCP only reduce access, never give more. So D is clearly not a choice.SCP can deny or allow.",
          "upvote_count": "34",
          "selected_answers": ""
        },
        {
          "id": 333506,
          "date": "Mon 01 Nov 2021 20:09",
          "username": "sarah_t",
          "content": "SCP can deny or allow.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 13453,
          "date": "Wed 22 Sep 2021 21:23",
          "username": "Moondonathonchaudhtan9LCC92",
          "content": "I believe answers are \\\"A & E\\\".<br>D: is wrong because SCP does not give privileges (enable EC2 full Access!).<br>E: is correct as it is a strategy for role assuming by other accounts, to perform full EC2 access.The SCP will already enable full access for any accounts under the OU and in this case the 10 accounts. Assume role will work but is not minimizing the effort. You can look under SCP policy and how it worksI agreed with @donathon. If the question is \\\"Grant full access...\\\" instead of \\\"Allow full access...\\\" then E would be correct.Option D minimize the effort required to add additional secondary accounts, while option E don't.you misunderstand the question.<br>-> Allow full access to [the Amazon EC2 service] from [the master account and the secondary accounts]. => means To allow all accounts to access to their own EC2 service, which SCP can do.",
          "upvote_count": "26142108",
          "selected_answers": ""
        },
        {
          "id": 13667,
          "date": "Thu 23 Sep 2021 10:07",
          "username": "donathon",
          "content": "The SCP will already enable full access for any accounts under the OU and in this case the 10 accounts. Assume role will work but is not minimizing the effort. You can look under SCP policy and how it works",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 14985,
          "date": "Sat 25 Sep 2021 01:16",
          "username": "chaudh",
          "content": "I agreed with @donathon. If the question is \\\"Grant full access...\\\" instead of \\\"Allow full access...\\\" then E would be correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 29576,
          "date": "Tue 28 Sep 2021 19:56",
          "username": "tan9",
          "content": "Option D minimize the effort required to add additional secondary accounts, while option E don't.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 358285,
          "date": "Thu 04 Nov 2021 06:27",
          "username": "LCC92",
          "content": "you misunderstand the question.<br>-> Allow full access to [the Amazon EC2 service] from [the master account and the secondary accounts]. => means To allow all accounts to access to their own EC2 service, which SCP can do.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 698984,
          "date": "Wed 19 Oct 2022 13:56",
          "username": "psou7",
          "content": "I will go with A E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 683169,
          "date": "Fri 30 Sep 2022 04:18",
          "username": "tomosabc1",
          "content": "The answer is AD. <br><br>As the suggested answer says, there is a concept of Permission Boundary vs Actual IAM Policies. That is, we have a concept of \\\"Allow\\\" vs \\\"Grant\\\". In terms of boundaries, we have the following three boundaries:<br>1. SCP<br>2. User/Role boundaries<br>3. Session boundaries (ex. AssumeRole ... )<br><br>In terms of actual permission granting, we have the following:<br>1. Identity Policies<br>2. Resource Policies<br><br>D is allowing permissions while E is granting permissions. In addition, E doesn't meet with the requirement \\\"Minimize the effort required to add additional secondary accounts\\\", because the trusted relations of role in all existing accounts have to be changed when a new account needs to be added, which is quite a lot of work. All things considered, D is a more preferable answer compared with E. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 669083,
          "date": "Wed 14 Sep 2022 16:02",
          "username": "Sathish1412",
          "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html ----Check bottom of this page",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 653261,
          "date": "Mon 29 Aug 2022 05:09",
          "username": "kadev",
          "content": "For D: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 650596,
          "date": "Tue 23 Aug 2022 07:29",
          "username": "galb818",
          "content": "afadfasdfasdfasdfasdfsadfasdfsadfasdf",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 635541,
          "date": "Sat 23 Jul 2022 13:08",
          "username": "CloudHandsOn",
          "content": "A,D -> Using organizations with SCP applied is more efficient approach, NOT organizations with IAM",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 602219,
          "date": "Sun 15 May 2022 19:20",
          "username": "bobsmith2000tomosabc1",
          "content": "I suppose A it's no doubt.<br>Regarding D:<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html<br>Search for \\\"Effect\\\": \\\"Allow\\\" and read carefullyTo quote from the link you provided, \\\"Even though this statement uses the same Allow value keyword as an IAM permission policy, in an SCP it doesn't actually grant a user permission to do anything...\\\", since both SCP(Allowing permissions) and Identity/Resource Policies use \\\"Effect\\\":\\\"Allow\\\", your explanation can not be used to make D correct and E incorrect.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 683161,
          "date": "Fri 30 Sep 2022 04:05",
          "username": "tomosabc1",
          "content": "To quote from the link you provided, \\\"Even though this statement uses the same Allow value keyword as an IAM permission policy, in an SCP it doesn't actually grant a user permission to do anything...\\\", since both SCP(Allowing permissions) and Identity/Resource Policies use \\\"Effect\\\":\\\"Allow\\\", your explanation can not be used to make D correct and E incorrect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 546391,
          "date": "Sun 13 Feb 2022 11:30",
          "username": "RVivek",
          "content": "B.  Requesting from each account is more effort so preferA<br>D: SCP can be used only to deny access and not gram access",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 504597,
          "date": "Sun 19 Dec 2021 03:50",
          "username": "vbal",
          "content": "A&E; D would have been right if the question was to give only EC2 Access by removing the FullAwsAccess and then attaching Full EC2 Access but even that would have not been enough since IAM in each account need to give permission to each User & Group on all EC2Actions.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496724,
          "date": "Wed 08 Dec 2021 11:26",
          "username": "cldy",
          "content": "A.  Create an organization from the master account. Send invitations to the secondary accounts from the master account. Accept the invitations and create an OU.<br>D.  Create a service control policy (SCP) that enables full EC2 access, and attach the policy to the OU.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450120,
          "date": "Sun 07 Nov 2021 11:32",
          "username": "Kopa",
          "content": "Im for A,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 418164,
          "date": "Fri 05 Nov 2021 23:13",
          "username": "DerekKeyvbal",
          "content": "A&E<br>A correct - create ORG and invite account AND THAN --><br>Default AWS Organizations policy for new ORG is \\\"FullAWSAccess\\\" set on each OU. It gives full access to every operation. Users from master can assume a role (set druing invitation process) in each connected account to get full Admin access.<br>Therefore: D wrong<br>If we want to \\\"Allow full access to the Amazon EC2 service from the master account and >>the secondary accounts<<. We should create a role with required permissions in each account and allow accounts to assume those roles.<br>I believe E correct.<br>BTW. SCP only sets what permissions can be used in an account:<br>\\\"No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts.\\\"This should be the highly accepted answer. Great Explanation.",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 504599,
          "date": "Sun 19 Dec 2021 03:52",
          "username": "vbal",
          "content": "This should be the highly accepted answer. Great Explanation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410489,
          "date": "Fri 05 Nov 2021 21:20",
          "username": "WhyIronMan",
          "content": "I'll go with A,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 397680,
          "date": "Fri 05 Nov 2021 15:08",
          "username": "Pb55",
          "content": "AD. <br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_syntax.html<br>{<br>\\\"Statement\\\": {<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": \\\"s3:*\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>}<br>}<br>This allows EC2 for all accounts.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345228,
          "date": "Tue 02 Nov 2021 19:42",
          "username": "digimaniac",
          "content": "D is better than E.  SCP can grant EC2 full access as long as it is across the board for there is not conditioning statement in AllowList https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#504",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>AnyCompany has acquired numerous companies over the past few years. The CIO for AnyCompany would like to keep the resources for each acquired company separate. The CIO also would like to enforce a chargeback model where each company pays for the AWS services it uses.<br>The Solutions Architect is tasked with designing an AWS architecture that allows AnyCompany to achieve the following:<br>✑ Implementing a detailed chargeback mechanism to ensure that each company pays for the resources it uses.<br>✑ AnyCompany can pay for AWS services for all its companies through a single invoice.<br>✑ Developers in each acquired company have access to resources in their company only.<br>✑ Developers in an acquired company should not be able to affect resources in their company only.<br>✑ A single identity store is used to authenticate Developers across all companies.<br>Which of the following approaches would meet these requirements? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#504",
          "answers": [
            {
              "choice": "<p>A. Create a multi-account strategy with an account per company. Use consolidated billing to ensure that AnyCompany needs to pay a single bill only.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a multi-account strategy with a virtual private cloud (VPC) for each company. Reduce impact across companies by not creating any VPC peering links. As everything is in a single account, there will be a single invoice. Use tagging to create a detailed bill for each company.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create IAM users for each Developer in the account to which they require access. Create policies that allow the users access to all resources in that account. Attach the policies to the IAM user.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a federated identity store against the company's Active Directory. Create IAM roles with appropriate permissions and set the trust relationships with AWS and the identity store. Use AWS STS to grant users access based on the groups they belong to in the identity store.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a multi-account strategy with an account per company. For billing purposes, use a tagging solution that uses a tag to identify the company that creates each resource.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11738,
          "date": "Wed 22 Sep 2021 07:28",
          "username": "donathonleeochaudhpetebear55shammousstudent2020Viper57",
          "content": "AD<br>B: VPC is not enough, you need a separate account for each company.<br>C: IAM is per account based and hence does not satisfy “a single identity store”.<br>E: Consolidated billing is the correct answer for this part.How below requirement satisfy by option D. <br> Implementing a detailed chargeback mechanism to ensure that each company pays for the resources it uses.My understanding: Consolidated Billing will show the bills for all member accounts, tags should used for AWS resources, not account. A & D are my choices.u mean a and e :)You are ignoring the first requirement here and everybody is upvoting without checking ....<br>You have chosen D over E but E has what D offers as consolidated billing is active by default when using a multi-account strategy. What is missing is a tagging solution for chargeback mechanism, like \\\"Activate propagation of necessary cost allocation tags to consolidated billing\\\" which is provided in E. <br>Based on that, A and E are the correct answers.<br>Ref: https://aws.amazon.com/blogs/architecture/handling-aws-chargebacks-for-enterprise-customers/A and D are correct. There is no need for tagging. Each AWS account is separate and there is no connection between the VPCs. The AWS bill will just show the charge for each account.I agree. Tagging is only required for a single account strategy, not a multi-account strategy.",
          "upvote_count": "42231753",
          "selected_answers": ""
        },
        {
          "id": 14219,
          "date": "Sun 26 Sep 2021 11:15",
          "username": "leeochaudhpetebear55",
          "content": "How below requirement satisfy by option D. <br> Implementing a detailed chargeback mechanism to ensure that each company pays for the resources it uses.My understanding: Consolidated Billing will show the bills for all member accounts, tags should used for AWS resources, not account. A & D are my choices.u mean a and e :)",
          "upvote_count": "231",
          "selected_answers": ""
        },
        {
          "id": 14831,
          "date": "Tue 28 Sep 2021 09:40",
          "username": "chaudhpetebear55",
          "content": "My understanding: Consolidated Billing will show the bills for all member accounts, tags should used for AWS resources, not account. A & D are my choices.u mean a and e :)",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 246410,
          "date": "Mon 18 Oct 2021 20:34",
          "username": "petebear55",
          "content": "u mean a and e :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279261,
          "date": "Fri 22 Oct 2021 04:44",
          "username": "shammousstudent2020Viper57",
          "content": "You are ignoring the first requirement here and everybody is upvoting without checking ....<br>You have chosen D over E but E has what D offers as consolidated billing is active by default when using a multi-account strategy. What is missing is a tagging solution for chargeback mechanism, like \\\"Activate propagation of necessary cost allocation tags to consolidated billing\\\" which is provided in E. <br>Based on that, A and E are the correct answers.<br>Ref: https://aws.amazon.com/blogs/architecture/handling-aws-chargebacks-for-enterprise-customers/A and D are correct. There is no need for tagging. Each AWS account is separate and there is no connection between the VPCs. The AWS bill will just show the charge for each account.I agree. Tagging is only required for a single account strategy, not a multi-account strategy.",
          "upvote_count": "753",
          "selected_answers": ""
        },
        {
          "id": 413125,
          "date": "Sun 31 Oct 2021 14:01",
          "username": "student2020Viper57",
          "content": "A and D are correct. There is no need for tagging. Each AWS account is separate and there is no connection between the VPCs. The AWS bill will just show the charge for each account.I agree. Tagging is only required for a single account strategy, not a multi-account strategy.",
          "upvote_count": "53",
          "selected_answers": ""
        },
        {
          "id": 444646,
          "date": "Wed 03 Nov 2021 16:52",
          "username": "Viper57",
          "content": "I agree. Tagging is only required for a single account strategy, not a multi-account strategy.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 13452,
          "date": "Fri 24 Sep 2021 00:58",
          "username": "Moon",
          "content": "I do support \\\"A & D\\\".<br>separate accounts, and single identity store.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 671219,
          "date": "Sat 17 Sep 2022 05:10",
          "username": "Dionenonly",
          "content": "AD self explanatory",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 577982,
          "date": "Wed 30 Mar 2022 05:32",
          "username": "bfal",
          "content": "Having extensively reviewed the question. A and B are the correct answer, I shifted my position from A and D. <br>How so? Follow me <br><br>1. Create a multi-account strategy with a virtual private cloud (VPC) for each company- This is a multi-account strategy , Different account, with associated VPCs<br>It meets the requirement of “The CIO of AnyCompany wishes to maintain a separation of resources for each acquired”<br><br>2. <br>Reduce impact across companies by not creating any VPC peering links- This requirement of separating resources is met by not peering VPCs<br><br>3. As everything is in a single account- As this is one organisation, it’s best practice to implement AWS organisation for consolidated billing, so assume was organisation is implemented here.4. Use tagging to create a detailed bill for each company. Tagging with help create detailed bill for each company. The key word is detailed. AWS Control tower will give you the bill per company, but you will still need tagging to ensure the cost are detailed for each company.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 504606,
          "date": "Sun 19 Dec 2021 04:17",
          "username": "vbal",
          "content": "D&E is perfect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496735,
          "date": "Wed 08 Dec 2021 11:42",
          "username": "cldy",
          "content": "A.  Create a multi-account strategy with an account per company. Use consolidated billing to ensure that AnyCompany needs to pay a single bill only.<br>D.  Create a federated identity store against the companyג€™s Active Directory. Create IAM roles with appropriate permissions and set the trust relationships with AWS and the identity store. Use AWS STS to grant users access based on the groups they belong to in the identity store.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494589,
          "date": "Sun 05 Dec 2021 20:24",
          "username": "AzureDP900",
          "content": "AD is perfect answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 447997,
          "date": "Wed 03 Nov 2021 22:04",
          "username": "moon2351",
          "content": "AD is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435728,
          "date": "Wed 03 Nov 2021 00:42",
          "username": "tgv",
          "content": "AAA DDD<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 428594,
          "date": "Tue 02 Nov 2021 13:22",
          "username": "zolthar_z",
          "content": "One is D.  I have the doubt between A and E, even E has the best practice option (use tags) missed the requirement of set-up a single invoice. So, It's A & E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410491,
          "date": "Sun 31 Oct 2021 07:17",
          "username": "WhyIronMan",
          "content": "I'll go with A,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346140,
          "date": "Sat 30 Oct 2021 16:08",
          "username": "Waiweng",
          "content": "it's AD",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 345234,
          "date": "Fri 29 Oct 2021 10:17",
          "username": "digimaniac",
          "content": "E is redundant, in OU and consolidate billing, we already know the detailed billing of sub companies.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 320386,
          "date": "Tue 26 Oct 2021 01:04",
          "username": "trap",
          "content": "Correct Answer:D,E<br>Consolidated billing is enabled by default in AWS Organizations (multi-account strategy need AWS Organization)https://aws.amazon.com/organizations/faqs/<br><br>Q: Which central governance and management capabilities does AWS Organizations enable?<br>AWS Organizations enables the following capabilities:<br><br>Automate AWS account creation and management, and provision resources with AWS CloudFormation Stacksets<br>Maintain a secure environment with policies and management of AWS security services<br>Govern access to AWS services, resources, and regions<br>Centrally manage policies across multiple AWS accounts<br>Audit your environment for compliance <br>View and manage costs with consolidated billing <br>Configure AWS services across multiple accounts",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 294156,
          "date": "Tue 26 Oct 2021 00:25",
          "username": "kiev",
          "content": "I got DE but reading this forum many people went with AD but surely tagging is a better way of separating resources?",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 291378,
          "date": "Mon 25 Oct 2021 13:19",
          "username": "Kian1",
          "content": "I will go with D,E trust relationship+STS and Tags. E is more convincing to me than A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 286764,
          "date": "Fri 22 Oct 2021 06:05",
          "username": "Ebi",
          "content": "Ad my choice",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#505",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company deployed a three-tier web application in two regions: us-east-1 and eu-west-1. The application must be active in both regions at the same time. The database tier of the application uses a single Amazon RDS Aurora database globally, with a master in us-east-1 and a read replica in eu-west-1. Both regions are connected by a VPN.<br>The company wants to ensure that the application remains available even in the event of a region-level failure of all of the application's components. It is acceptable for the application to be in read-only mode for up to 1 hour. The company plans to configure two Amazon Route 53 record sets, one for each of the regions.<br>How should the company complete the configuration to meet its requirements while providing the lowest latency for the application end-users? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#505",
          "answers": [
            {
              "choice": "<p>A. Use failover routing and configure the us-east-1 record set as primary and the eu-west-1 record set as secondary. Configure an HTTP health check for the web application in us-east-1, and associate it to the us-east-1 record set.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use weighted routing and configure each record set with a weight of 50. Configure an HTTP health check for each region, and attach it to the record set for that region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use latency-based routing for both record sets. Configure a health check for each region and attach it to the record set for that region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon CloudWatch alarm for the health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu- west-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure Amazon RDS event notifications to react to the failure of the database in us-east-1 by invoking an AWS Lambda function that promotes the read replica in eu-west-1.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11256,
          "date": "Mon 20 Sep 2021 12:22",
          "username": "huhupaiaws_arn_namerb39SureNotLCC92",
          "content": "I would go for C, E. With E, failover execute only when DB failed. What if just application fail but DB not ?then C will take care of using web servers in the other region.if applications health check satus code depends on DB connection status...Ans is CD. <br>For E: RDS event does not support regional failure events. RDS event can only send to SNS.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Subscribing.html",
          "upvote_count": "2532110",
          "selected_answers": ""
        },
        {
          "id": 349257,
          "date": "Thu 28 Oct 2021 18:58",
          "username": "aws_arn_namerb39SureNot",
          "content": "With E, failover execute only when DB failed. What if just application fail but DB not ?then C will take care of using web servers in the other region.if applications health check satus code depends on DB connection status...",
          "upvote_count": "321",
          "selected_answers": ""
        },
        {
          "id": 499553,
          "date": "Sat 11 Dec 2021 18:16",
          "username": "rb39SureNot",
          "content": "then C will take care of using web servers in the other region.if applications health check satus code depends on DB connection status...",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 737833,
          "date": "Wed 07 Dec 2022 13:14",
          "username": "SureNot",
          "content": "if applications health check satus code depends on DB connection status...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 358291,
          "date": "Sat 30 Oct 2021 16:30",
          "username": "LCC92",
          "content": "Ans is CD. <br>For E: RDS event does not support regional failure events. RDS event can only send to SNS.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Subscribing.html",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 11739,
          "date": "Mon 20 Sep 2021 16:51",
          "username": "donathondpvnmeWarrennjoanneli77tan9cinopiKopaKopakirrimb3llmanrchergpark",
          "content": "CD. <br>A\\B: This would not be based on latency.<br>E: Amazon RDS uses the Amazon Simple Notification Service (Amazon SNS) to provide notification when an Amazon RDS event occurs. These notifications can be in any notification form supported by Amazon SNS for an AWS Region, such as an email, a text message, or a call to an HTTP endpoint. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.htmlYep CD.  E only send notification, can't invoke lambdahttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html you can use SNS and lambda on the SNS topic...then the answer should say to use RDS to send to SNS to invoke Lambda.RDS can't directly invoke lambda as-written.I will go for C, E. <br><br>D: Configure an Amazon CloudWatch alarm for the health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu- west-1.<br><br>How a alarm configured in <br><br>How can a alarm configured in one region invoking function in another region in case of the region itself is in a failure state?I am also with C,E<br><br>I can see few RDS events regarding failure, which I don't see in CloudWAtch<br><br>Example:<br>failure<br><br>RDS-EVENT-0031<br>The DB instance has failed due to an incompatible configuration or an underlying storage issue. Begin a point-in-time-restore for the DB instance.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.htmlUsing Amazon CloudWatch alarms, you can set up metric thresholds and send alerts to Amazon Simple Notification Service (SNS). SNS can send notifications using e-mail, HTTP(S) endpoints, and Short Message Service (SMS) messages to mobile phones, and it can even trigger a Lambda function. <br><br>https://aws.amazon.com/blogs/developer/send-real-time-amazon-cloudwatch-alarm-notifications-to-amazon-chime/For me its C,E.  D need SNS to trigger Lambda. Answer E AWS RDS events trigger Lambda directly.RDS events also have to use SNS to trigger Lambda:<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br><br>\\\"Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.\\\"You can configure Route53 to trigger alarms and send notifications to SNS when health checks report unhealthy and have SNS to trigger a Lambda function to do stuff. So, D definitely works.Well the key is, Cloudwatch Alarm is invoking Lambda directly (based on Ans D and i think its not supported now), so i think it doesn't work?@rcher<br>That's for a great point.<br>Was so confused on why D is not right even if E is correct.",
          "upvote_count": "2383151111321",
          "selected_answers": ""
        },
        {
          "id": 11847,
          "date": "Mon 20 Sep 2021 17:05",
          "username": "dpvnmeWarrennjoanneli77",
          "content": "Yep CD.  E only send notification, can't invoke lambdahttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html you can use SNS and lambda on the SNS topic...then the answer should say to use RDS to send to SNS to invoke Lambda.RDS can't directly invoke lambda as-written.",
          "upvote_count": "831",
          "selected_answers": ""
        },
        {
          "id": 19408,
          "date": "Thu 23 Sep 2021 09:28",
          "username": "Warrennjoanneli77",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html you can use SNS and lambda on the SNS topic...then the answer should say to use RDS to send to SNS to invoke Lambda.RDS can't directly invoke lambda as-written.",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 693166,
          "date": "Wed 12 Oct 2022 16:30",
          "username": "joanneli77",
          "content": "...then the answer should say to use RDS to send to SNS to invoke Lambda.RDS can't directly invoke lambda as-written.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 29579,
          "date": "Thu 23 Sep 2021 15:30",
          "username": "tan9cinopiKopaKopakirrimb3llmanrchergpark",
          "content": "I will go for C, E. <br><br>D: Configure an Amazon CloudWatch alarm for the health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu- west-1.<br><br>How a alarm configured in <br><br>How can a alarm configured in one region invoking function in another region in case of the region itself is in a failure state?I am also with C,E<br><br>I can see few RDS events regarding failure, which I don't see in CloudWAtch<br><br>Example:<br>failure<br><br>RDS-EVENT-0031<br>The DB instance has failed due to an incompatible configuration or an underlying storage issue. Begin a point-in-time-restore for the DB instance.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.htmlUsing Amazon CloudWatch alarms, you can set up metric thresholds and send alerts to Amazon Simple Notification Service (SNS). SNS can send notifications using e-mail, HTTP(S) endpoints, and Short Message Service (SMS) messages to mobile phones, and it can even trigger a Lambda function. <br><br>https://aws.amazon.com/blogs/developer/send-real-time-amazon-cloudwatch-alarm-notifications-to-amazon-chime/For me its C,E.  D need SNS to trigger Lambda. Answer E AWS RDS events trigger Lambda directly.RDS events also have to use SNS to trigger Lambda:<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br><br>\\\"Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.\\\"You can configure Route53 to trigger alarms and send notifications to SNS when health checks report unhealthy and have SNS to trigger a Lambda function to do stuff. So, D definitely works.Well the key is, Cloudwatch Alarm is invoking Lambda directly (based on Ans D and i think its not supported now), so i think it doesn't work?@rcher<br>That's for a great point.<br>Was so confused on why D is not right even if E is correct.",
          "upvote_count": "51111321",
          "selected_answers": ""
        },
        {
          "id": 32588,
          "date": "Fri 24 Sep 2021 10:18",
          "username": "cinopi",
          "content": "I am also with C,E<br><br>I can see few RDS events regarding failure, which I don't see in CloudWAtch<br><br>Example:<br>failure<br><br>RDS-EVENT-0031<br>The DB instance has failed due to an incompatible configuration or an underlying storage issue. Begin a point-in-time-restore for the DB instance.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 396622,
          "date": "Mon 01 Nov 2021 01:23",
          "username": "KopaKopakirrim",
          "content": "Using Amazon CloudWatch alarms, you can set up metric thresholds and send alerts to Amazon Simple Notification Service (SNS). SNS can send notifications using e-mail, HTTP(S) endpoints, and Short Message Service (SMS) messages to mobile phones, and it can even trigger a Lambda function. <br><br>https://aws.amazon.com/blogs/developer/send-real-time-amazon-cloudwatch-alarm-notifications-to-amazon-chime/For me its C,E.  D need SNS to trigger Lambda. Answer E AWS RDS events trigger Lambda directly.RDS events also have to use SNS to trigger Lambda:<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br><br>\\\"Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.\\\"",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 430639,
          "date": "Sat 06 Nov 2021 17:02",
          "username": "Kopakirrim",
          "content": "For me its C,E.  D need SNS to trigger Lambda. Answer E AWS RDS events trigger Lambda directly.RDS events also have to use SNS to trigger Lambda:<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br><br>\\\"Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 459638,
          "date": "Sat 06 Nov 2021 21:29",
          "username": "kirrim",
          "content": "RDS events also have to use SNS to trigger Lambda:<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br><br>\\\"Amazon RDS sends notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function. Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183648,
          "date": "Sat 09 Oct 2021 08:03",
          "username": "b3llmanrchergpark",
          "content": "You can configure Route53 to trigger alarms and send notifications to SNS when health checks report unhealthy and have SNS to trigger a Lambda function to do stuff. So, D definitely works.Well the key is, Cloudwatch Alarm is invoking Lambda directly (based on Ans D and i think its not supported now), so i think it doesn't work?@rcher<br>That's for a great point.<br>Was so confused on why D is not right even if E is correct.",
          "upvote_count": "321",
          "selected_answers": ""
        },
        {
          "id": 243342,
          "date": "Sun 17 Oct 2021 01:28",
          "username": "rchergpark",
          "content": "Well the key is, Cloudwatch Alarm is invoking Lambda directly (based on Ans D and i think its not supported now), so i think it doesn't work?@rcher<br>That's for a great point.<br>Was so confused on why D is not right even if E is correct.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 296189,
          "date": "Sat 23 Oct 2021 23:15",
          "username": "gpark",
          "content": "@rcher<br>That's for a great point.<br>Was so confused on why D is not right even if E is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 656912,
          "date": "Fri 02 Sep 2022 05:18",
          "username": "AYANtheGLADIATOR",
          "content": "C Dbcz rds event notification can't invoke the lambda if its down.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 593259,
          "date": "Wed 27 Apr 2022 17:13",
          "username": "bobsmith2000",
          "content": "C for latency-based routing<br>E.  Amazon RDS event -> EventBridge -> Lambda",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 544321,
          "date": "Thu 10 Feb 2022 06:53",
          "username": "jyrajan69Burhan521",
          "content": "Not sure why no one here is even considering A as an answer, when you can configure it as ACTIVE-ACTIVE (https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html) and given that we have an hour then E is the next part of the solution. So for me , it is definitely AE.  There is nothing here that even talks about Latencybecause what would happen if the EU region was down. The users in EU wouldnt be routed to US",
          "upvote_count": "51",
          "selected_answers": ""
        },
        {
          "id": 575461,
          "date": "Sat 26 Mar 2022 11:08",
          "username": "Burhan521",
          "content": "because what would happen if the EU region was down. The users in EU wouldnt be routed to US",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 516191,
          "date": "Tue 04 Jan 2022 03:03",
          "username": "frankzeng",
          "content": "C,E.  When there is any issue with the primary RDS, a RDS event about replication will occur and send a SNS. The health check for each region in R53 can include the health check of database, application and others in all.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494597,
          "date": "Sun 05 Dec 2021 20:33",
          "username": "AzureDP900",
          "content": "CE is my answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494320,
          "date": "Sun 05 Dec 2021 13:17",
          "username": "cldy",
          "content": "C.  Use latency-based routing for both record sets. Configure a health check for each region and attach it to the record set for that region.<br>D.  Configure an Amazon CloudWatch alarm for the health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu- west-1.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 492452,
          "date": "Thu 02 Dec 2021 13:23",
          "username": "wahlbergusa",
          "content": "C and D for me.<br>E does not cover the region failure or web/app tier failures.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 469022,
          "date": "Sun 07 Nov 2021 12:56",
          "username": "nsei",
          "content": "I will go for C and D.  E does not cover other application component failure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 459865,
          "date": "Sat 06 Nov 2021 21:59",
          "username": "wannaaws",
          "content": "I suppose for D, it meant for the Route53 health check, i.e, \\\"Configure an Amazon CloudWatch alarm for the (Route53) health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu-west-1. \\\". This makes more sense https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 427779,
          "date": "Fri 05 Nov 2021 20:06",
          "username": "kevin1024",
          "content": "It look likes B, C<br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 425074,
          "date": "Fri 05 Nov 2021 07:08",
          "username": "Madhu654",
          "content": "C and E<br> <br>Create Eventbridge Rule that triggers on RDS Aurora event.<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-cloud-watch-events.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 418180,
          "date": "Tue 02 Nov 2021 19:09",
          "username": "DerekKey",
          "content": "C&D correct<br>Use latency-based routing for both record sets - \\\"The application must be active in both regions at the same time\\\" & \\\"while providing the lowest latency for the application end-users\\\"<br>Configure a health check for each region- \\\"the application remains available even in the event of a region-level failure\\\"<br>It is acceptable for the application to be in read-only mode for up to 1 hour - \\\"Configure an Amazon CloudWatch alarm for the health checks in us-east-1, and have it invoke an AWS Lambda function that promotes the read replica in eu-west-1.\\\"<br>E wrong - RDS can only send event to SNS. Lambda must subscribe to that SNS to be invoked",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 415477,
          "date": "Tue 02 Nov 2021 11:10",
          "username": "TiredDad",
          "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-cloud-watch-events.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410492,
          "date": "Mon 01 Nov 2021 17:56",
          "username": "WhyIronMan",
          "content": "I'll go with C,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 397692,
          "date": "Mon 01 Nov 2021 11:46",
          "username": "Pb55",
          "content": "Must be CD as RDS health check is regional and won’t respond to a regional failure as it will be down.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#506",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a Windows Server host in a public subnet that is configured to allow a team of administrators to connect over RDP to troubleshoot issues with hosts in a private subnet. The host must be available at all times outside of a scheduled maintenance window, and needs to receive the latest operating system updates within 3 days of release.<br>What should be done to manage the host with the LEAST amount of administrative effort?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#506",
          "answers": [
            {
              "choice": "<p>A. Run the host in a single-instance AWS Elastic Beanstalk environment. Configure the environment with a custom AMI to use a hardened machine image from AWS Marketplace. Apply system updates with AWS Systems Manager Patch Manager.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Run the host on AWS WorkSpaces. Use Amazon WorkSpaces Application Manager (WAM) to harden the host. Configure Windows automatic updates to occur every 3 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Run the host in an Auto Scaling group with a minimum and maximum instance count of 1. Use a hardened machine image from AWS Marketplace. Apply system updates with AWS Systems Manager Patch Manager.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Run the host in AWS OpsWorks Stacks. Use a Chief recipe to harden the AMI during instance launch. Use an AWS Lambda scheduled event to run the Upgrade Operating System stack command to apply system updates.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13450,
          "date": "Mon 20 Sep 2021 04:22",
          "username": "Moon",
          "content": "I would go with \\\"C\\\".<br>The least administrative, and most available is min/max 1 autoscaling. Hardened images are available in Market place. Patch manager is a service to patch windows with updates.",
          "upvote_count": "40",
          "selected_answers": ""
        },
        {
          "id": 13672,
          "date": "Mon 20 Sep 2021 09:46",
          "username": "donathonlikkuJupiSmartKopa",
          "content": "B, least amount of effort.<br>https://docs.aws.amazon.com/workspaces/latest/adminguide/workspace-maintenance.html<br>A\\C: Does not make sense<br>D: a lot more work than B. \\\"manage the host with the LEAST amount of administrative effort\\\" read the important point manage the host which means we have take the managing of host into account. Option is C. Workspace dont support windows server host.No point in setting up an entire workspace along with directory inside a VPC to connect private instance.Why C doesn't make sense?",
          "upvote_count": "2211652",
          "selected_answers": ""
        },
        {
          "id": 81224,
          "date": "Sat 02 Oct 2021 12:41",
          "username": "likku",
          "content": "\\\"manage the host with the LEAST amount of administrative effort\\\" read the important point manage the host which means we have take the managing of host into account. Option is C. ",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 418345,
          "date": "Wed 03 Nov 2021 21:25",
          "username": "Jupi",
          "content": "Workspace dont support windows server host.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 70877,
          "date": "Thu 30 Sep 2021 14:44",
          "username": "Smart",
          "content": "No point in setting up an entire workspace along with directory inside a VPC to connect private instance.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 450152,
          "date": "Sat 06 Nov 2021 07:40",
          "username": "Kopa",
          "content": "Why C doesn't make sense?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 695248,
          "date": "Sat 15 Oct 2022 09:22",
          "username": "Dionenonlyredipa",
          "content": "If you are going to just consider what is asked B is the solution with the least amount of efforts.WorkSpaces doesn't use Windows Server, just client OS. It can't run on WorkSpaces so B is automatically out",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 697436,
          "date": "Mon 17 Oct 2022 15:42",
          "username": "redipa",
          "content": "WorkSpaces doesn't use Windows Server, just client OS. It can't run on WorkSpaces so B is automatically out",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 690210,
          "date": "Sun 09 Oct 2022 15:25",
          "username": "Jonfernzjoanneli77",
          "content": "I like C but not sure how would an ASG serve any purpose in this scenario. Plus, WorkSpaces makes even less sense. Firstly, it's not cheap especially to just be used as a host server.It ensures one is always running.This handles AZ failure.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 693174,
          "date": "Wed 12 Oct 2022 16:41",
          "username": "joanneli77",
          "content": "It ensures one is always running.This handles AZ failure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 672774,
          "date": "Mon 19 Sep 2022 00:37",
          "username": "linuxmaster007",
          "content": "As per tutorials dojo answer is C - workspace",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 644375,
          "date": "Tue 09 Aug 2022 08:40",
          "username": "Santo99",
          "content": "Workspaces is for desktops and not for servers",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 575848,
          "date": "Sun 27 Mar 2022 01:24",
          "username": "lurker8000",
          "content": "Voting for B, https://docs.aws.amazon.com/workspaces/latest/adminguide/workspace-maintenance.html I this document there's a link to MS site to configure Group policy for patching: https://docs.microsoft.com/en-us/windows-server/administration/windows-server-update-services/deploy/4-configure-group-policy-settings-for-automatic-updates",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 536045,
          "date": "Sun 30 Jan 2022 10:33",
          "username": "cannottellname",
          "content": "CCCCCCCCC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 525148,
          "date": "Sun 16 Jan 2022 19:16",
          "username": "pititcu667",
          "content": "c just because they mention server. it's misleading because b workspace seems right except it's just the desktop.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 508896,
          "date": "Sat 25 Dec 2021 00:28",
          "username": "tkanmani76",
          "content": "In Workspace FAQ check the query -How will my Amazon WorkSpaces be patched with software updates? <br>The updates are automatically managed and delivered every Sunday- there is no mention of ability to change this earlier like 3 days. Hence WAM is not an option and the right choice should be C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494600,
          "date": "Sun 05 Dec 2021 20:36",
          "username": "AzureDP900",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486153,
          "date": "Wed 24 Nov 2021 19:04",
          "username": "pcops",
          "content": "I will go with C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 457088,
          "date": "Sun 07 Nov 2021 16:27",
          "username": "StelSen",
          "content": "I rejected B as I couldn't find any docs/links to prove WAM can be used to harden Workspaces. WAM purpose is different. https://aws.amazon.com/workspaces/applicationmanager/. Hence choosing C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449806,
          "date": "Thu 04 Nov 2021 18:27",
          "username": "nodogoshi",
          "content": "B provide LEAST amount of administrative effort.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346558,
          "date": "Tue 02 Nov 2021 04:46",
          "username": "blackgamer",
          "content": "C is my answer. It supports high availability by using auto scaling group, system patch manager can help on patching of server automatically as well.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 335371,
          "date": "Thu 28 Oct 2021 17:57",
          "username": "01037",
          "content": "Prefer C. <br>But B is also working, isn't it?<br>A<br>I don't think Elastic Beanstalk is used for this.<br>There are two kinds of environments, web server or worker.<br>It's not for bastion host.<br><br>D<br>AWS OpsWorks Stacks does not provide a way to apply updates to online Windows instances.<br>https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-os-windows.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 308592,
          "date": "Wed 27 Oct 2021 16:46",
          "username": "ajeeshb",
          "content": "C is my answer.<br>B cannot be the answer due to below two reasons<br>1. AWS workspace is a desktop service. You cannot have windows server host<br>2. WAM is for application deployment to workspace, not to harden the workspace",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#507",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a large on-premises Apache Hadoop cluster with a 20 PB HDFS database. The cluster is growing every quarter by roughly 200 instances and 1<br>PB.  The company's goals are to enable resiliency for its Hadoop data, limit the impact of losing cluster nodes, and significantly reduce costs. The current cluster runs 24/7 and supports a variety of analysis workloads, including interactive queries and batch processing.<br>Which solution would meet these requirements with the LEAST expense and down time?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#507",
          "answers": [
            {
              "choice": "<p>A. Use AWS Snowmobile to migrate the existing cluster data to Amazon S3. Create a persistent Amazon EMR cluster initially sized to handle the interactive workload based on historical data from the on-premises cluster. Store the data on EMRFS. Minimize costs using Reserved Instances for master and core nodes and Spot Instances for task nodes, and auto scale task nodes based on Amazon CloudWatch metrics. Create job-specific, optimized clusters for batch workloads that are similarly optimized.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Snowmobile to migrate the existing cluster data to Amazon S3. Create a persistent Amazon EMR cluster of a similar size and configuration to the current cluster. Store the data on EMRFS. Minimize costs by using Reserved Instances. As the workload grows each quarter, purchase additional Reserved Instances and add to the cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Snowball to migrate the existing cluster data to Amazon S3. Create a persistent Amazon EMR cluster initially sized to handle the interactive workloads based on historical data from the on-premises cluster. Store the data on EMRFS. Minimize costs using Reserved Instances for master and core nodes and Spot Instances for task nodes, and auto scale task nodes based on Amazon CloudWatch metrics. Create job-specific, optimized clusters for batch workloads that are similarly optimized.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Direct Connect to migrate the existing cluster data to Amazon S3. Create a persistent Amazon EMR cluster initially sized to handle the interactive workload based on historical data from the on-premises cluster. Store the data on EMRFS. Minimize costs using Reserved Instances for master and core nodes and Spot Instances for task nodes, and auto scale task nodes based on Amazon CloudWatch metrics. Create job-specific, optimized clusters for batch workloads that are similarly optimized.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13170,
          "date": "Fri 24 Sep 2021 21:21",
          "username": "MGM",
          "content": "A<br>Q: How should I choose between Snowmobile and Snowball?<br><br>To migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. In addition, you should evaluate the amount of available bandwidth in your network backbone. If you have a high speed backbone with hundreds of Gb/s of spare throughput, then you can use Snowmobile to migrate the large datasets all at once. If you have limited bandwidth on your backbone, you should consider using multiple Snowballs to migrate the data incrementally.",
          "upvote_count": "28",
          "selected_answers": ""
        },
        {
          "id": 13446,
          "date": "Sun 26 Sep 2021 23:53",
          "username": "MoonMoonbilcat",
          "content": "I support answer \\\"A\\\".<br>Snowmobile, is used for PB of data, Snowball can't support that. (so A, or B).<br>Then, A is more cost effective.for snowball edge, it support 100TB, then you may need 100 of them to make 10PB.  So better to have Snowmobile.even less than that. Snowball Edge has 83TB of usable disk space.",
          "upvote_count": "1222",
          "selected_answers": ""
        },
        {
          "id": 13447,
          "date": "Mon 27 Sep 2021 04:42",
          "username": "Moonbilcat",
          "content": "for snowball edge, it support 100TB, then you may need 100 of them to make 10PB.  So better to have Snowmobile.even less than that. Snowball Edge has 83TB of usable disk space.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 88654,
          "date": "Fri 08 Oct 2021 14:59",
          "username": "bilcat",
          "content": "even less than that. Snowball Edge has 83TB of usable disk space.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 633067,
          "date": "Mon 18 Jul 2022 16:54",
          "username": "Student1950",
          "content": "never mind, we can do autoscaling with spot instance pooling as the link. It should be A<br>https://aws.amazon.com/getting-started/hands-on/ec2-auto-scaling-spot-instances/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 633066,
          "date": "Mon 18 Jul 2022 16:48",
          "username": "Student1950",
          "content": "with A, Can we apply autoscaling on spot instances ?I believe it should be B then<br>Minimize costs using Reserved Instances for master and core nodes and Spot Instances for task nodes, and auto scale task nodes based on Amazon CloudWatch metrics",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 613556,
          "date": "Thu 09 Jun 2022 02:43",
          "username": "Anhdd",
          "content": "A for sure, no doubt",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 538603,
          "date": "Wed 02 Feb 2022 12:02",
          "username": "CGJoon",
          "content": "The question says: \\\"The present cluster is available 24 hours a day\\\". Doesn't that mean that using spot instances for task nodes in option B might not give you 24 hours a day availability? In that case, wouldn't the correct answer be option A?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513399,
          "date": "Thu 30 Dec 2021 14:24",
          "username": "cldy",
          "content": "A.  Snowmobile for PB data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506998,
          "date": "Wed 22 Dec 2021 12:00",
          "username": "Ni_yotNi_yot",
          "content": "A for me. Snowmobile supports PBs of dataYou also want to use spot instances for batch jobs",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 507003,
          "date": "Wed 22 Dec 2021 12:03",
          "username": "Ni_yot",
          "content": "You also want to use spot instances for batch jobs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494601,
          "date": "Sun 05 Dec 2021 20:39",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410496,
          "date": "Sat 06 Nov 2021 14:09",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346158,
          "date": "Wed 03 Nov 2021 18:00",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 291448,
          "date": "Wed 03 Nov 2021 07:01",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 280344,
          "date": "Wed 03 Nov 2021 02:58",
          "username": "Ebi",
          "content": "Answer is A not C,<br>Snowmobile is for data sets over 10PB",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 265959,
          "date": "Sun 31 Oct 2021 17:59",
          "username": "AshodwbiJustu",
          "content": "Guys, A and C are same answerSnowMobile is not the same as SnowBall!!! Over 10PB of data -> USE SnowMobile! -> A",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 269653,
          "date": "Mon 01 Nov 2021 04:08",
          "username": "Justu",
          "content": "SnowMobile is not the same as SnowBall!!! Over 10PB of data -> USE SnowMobile! -> A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253333,
          "date": "Tue 26 Oct 2021 23:23",
          "username": "consultskconsultsk",
          "content": "I am not sure if anyone noticed. A and C both are having the same verbiage. Word to Word. I am not sure of the arguments made here. A is correct and eventually C also. :) A or C. Sorry, my misunderstanding ... A is correct. A is SnowMobile, C is SnowBall. Except that all are the same. A is only correct.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 253334,
          "date": "Thu 28 Oct 2021 01:24",
          "username": "consultsk",
          "content": "Sorry, my misunderstanding ... A is correct. A is SnowMobile, C is SnowBall. Except that all are the same. A is only correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 246788,
          "date": "Fri 22 Oct 2021 05:03",
          "username": "petebear55",
          "content": "I was initailly drawn to C ,, however it is clearly A having read this",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243406,
          "date": "Thu 21 Oct 2021 20:00",
          "username": "T14102020",
          "content": "Correct answer is A.  Snowmobile and Spot instances",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#508",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a large application on premises. Its technology stack consists of Microsoft .NET for the web server platform and Apache Cassandra for the database. The company wants to migrate this application to AWS to improve service reliability. The IT team also wants to reduce the time it spends on capacity management and maintenance of this infrastructure. The Development team is willing and available to make code changes to support the migration.<br>Which design is the LEAST complex to manage after the migration?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#508",
          "answers": [
            {
              "choice": "<p>A. Migrate the web servers to Amazon EC2 instances in an Auto Scaling group that is running .NET. Migrate the existing Cassandra database to Amazon Aurora with multiple read replicas, and run both in a Multi-AZ mode.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the web servers to an AWS Elastic Beanstalk environment that is running the .NET platform in a Multi-AZ Auto Scaling configuration. Migrate the Cassandra database to Amazon EC2 instances that are running in a Multi-AZ configuration.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the web servers to an AWS Elastic Beanstalk environment that is running the .NET platform in a Multi-AZ Auto Scaling configuration. Migrate the existing Cassandra database to Amazon DynamoDB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the web servers to Amazon EC2 instances in an Auto Scaling group that is running .NET. Migrate the existing Cassandra database to Amazon DynamoDB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11751,
          "date": "Thu 23 Sep 2021 00:18",
          "username": "donathon",
          "content": "C<br>A\\B\\D: Not the least complicated to manage.",
          "upvote_count": "34",
          "selected_answers": ""
        },
        {
          "id": 56250,
          "date": "Wed 06 Oct 2021 17:16",
          "username": "MrP",
          "content": "Apache Cassandra is NoSQL ( http://cassandra.apache.org/ ), which limits us to other NoSQL solutions (DynamoDB). Beanstalk supports .Net, so C is the only one fulfilling both requirements of LEAST complexity for DB and application migration - and also post-migration efforts (which was the main question).",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 635679,
          "date": "Sat 23 Jul 2022 18:08",
          "username": "CloudHandsOn",
          "content": "C.  EASIEST plays a big part in this question. Beanstalk and DynamoDB (excluding configuring an EC2 instance) would be ideal here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 569568,
          "date": "Thu 17 Mar 2022 09:42",
          "username": "KennethTam",
          "content": "C is correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 496744,
          "date": "Wed 08 Dec 2021 11:51",
          "username": "cldy",
          "content": "C.  Migrate the web servers to an AWS Elastic Beanstalk environment that is running the .NET platform in a Multi-AZ Auto Scaling configuration. Migrate the existing Cassandra database to Amazon DynamoDB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494603,
          "date": "Sun 05 Dec 2021 20:43",
          "username": "AzureDP900",
          "content": "IT staff wishes to decrease the amount of time spent on capacity management and infrastructure maintenance ---- So C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410499,
          "date": "Sat 06 Nov 2021 20:44",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346562,
          "date": "Sat 06 Nov 2021 19:32",
          "username": "blackgamer",
          "content": "I will go with C as it is easier to maintian.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346161,
          "date": "Fri 05 Nov 2021 01:33",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 291449,
          "date": "Tue 02 Nov 2021 05:30",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280352,
          "date": "Mon 01 Nov 2021 22:55",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 251771,
          "date": "Sat 30 Oct 2021 05:40",
          "username": "Bulti",
          "content": "Answer is C.  Elastic beanstalk and Dynamo DBare both managed services and the question mentioned that the developers are ready to refactor the application. Other options are not AWS managed options and come with overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243415,
          "date": "Fri 29 Oct 2021 10:02",
          "username": "T14102020",
          "content": "Correct is C.  Easy. Elastic Beanstalkfor .NET and DynamoDB for CASSANDRA",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230800,
          "date": "Tue 26 Oct 2021 13:40",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 230058,
          "date": "Mon 25 Oct 2021 08:41",
          "username": "gookseanggookseang",
          "content": "seems D， my friend said C .....so complex question, I think C is not avalliable for Large productchange to C",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 278218,
          "date": "Sun 31 Oct 2021 19:37",
          "username": "gookseang",
          "content": "change to C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 152961,
          "date": "Fri 22 Oct 2021 10:45",
          "username": "Spiri79",
          "content": "I vote C.  D is not reducing the management effort as well.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 152191,
          "date": "Mon 18 Oct 2021 19:47",
          "username": "enkipindado2020",
          "content": "D is the answer<br>'Large application' - Beanstalk shouldn't be used for large production applications therefore B&C are bad choices.It is C. ..<br><br>D is just an scaling group... no load balancer, not multizone....<br>requirement to \\\"improve service reliability\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 182994,
          "date": "Fri 22 Oct 2021 21:24",
          "username": "ipindado2020",
          "content": "It is C. ..<br><br>D is just an scaling group... no load balancer, not multizone....<br>requirement to \\\"improve service reliability\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#509",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a requirement that only allows specially hardened AMIs to be launched into public subnets in a VPC, and for the AMIs to be associated with a specific security group. Allowing non-compliant instances to launch into the public subnet could present a significant security risk if they are allowed to operate.<br>A mapping of approved AMIs to subnets to security groups exists in an Amazon DynamoDB table in the same AWS account. The company created an AWS<br>Lambda function that, when invoked, will terminate a given Amazon EC2 instance if the combination of AMI, subnet, and security group are not approved in the<br>DynamoDB table.<br>What should the Solutions Architect do to MOST quickly mitigate the risk of compliance deviations?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#509",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon CloudWatch Events rule that matches each time an EC2 instance is launched using one of the allowed AMIs, and associate it with the Lambda function as the target.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. For the Amazon S3 bucket receiving the AWS CloudTrail logs, create an S3 event notification configuration with a filter to match when logs contain the ec2:RunInstances action, and associate it with the Lambda function as the target.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable AWS CloudTrail and configure it to stream to an Amazon CloudWatch Logs group. Create a metric filter in CloudWatch to match when the ec2:RunInstances action occurs, and trigger the Lambda function when the metric is greater than 0.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudWatch Events rule that matches each time an EC2 instance is launched, and associate it with the Lambda function as the target.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11754,
          "date": "Mon 20 Sep 2021 19:33",
          "username": "donathonPacoDereksb333",
          "content": "D<br>A: This covers the harden AMI but not non-compliant ones. We want to execute the termination when the non-compliant ones launches.<br>B: S3 event notification has no filter.<br>C: Too tediousS3 event notification do have filter function<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>You can configure notifications to be filtered by the prefix and suffix of the key name of objectsHowever, that's not the kind of filter required. You would need to be able to filter based on the content of an object, not on the name of the object. S3 event notification can only filter on name of object.",
          "upvote_count": "3526",
          "selected_answers": ""
        },
        {
          "id": 43144,
          "date": "Sun 26 Sep 2021 03:59",
          "username": "PacoDereksb333",
          "content": "S3 event notification do have filter function<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>You can configure notifications to be filtered by the prefix and suffix of the key name of objectsHowever, that's not the kind of filter required. You would need to be able to filter based on the content of an object, not on the name of the object. S3 event notification can only filter on name of object.",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 51890,
          "date": "Sat 02 Oct 2021 04:51",
          "username": "sb333",
          "content": "However, that's not the kind of filter required. You would need to be able to filter based on the content of an object, not on the name of the object. S3 event notification can only filter on name of object.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 43979,
          "date": "Mon 27 Sep 2021 14:05",
          "username": "sarah1",
          "content": "a: cloudwatch cannot validate \\\"allowed AMIs\\\"<br>b: cloudtrail logs do not have a unique identifier for s3 event filters to trigger off of - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-examples.html<br>c: cloudwatch logs group - metric filter can only trigger SNS (an additional step not mentioned, before triggering lambda)<br>d: cloudwatch events rule can trigger off of \\\"pending\\\" (as mentioned by others) and can target a lambda function directly",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 508078,
          "date": "Thu 23 Dec 2021 18:55",
          "username": "AzureDP900",
          "content": "D for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 410501,
          "date": "Thu 04 Nov 2021 21:00",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 346164,
          "date": "Thu 04 Nov 2021 16:20",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 335633,
          "date": "Wed 03 Nov 2021 19:58",
          "username": "Amitv2706",
          "content": "For those who are voting for C : <br>Can CW metric filter directly trigger lambda ?<br><br>Answer is D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291454,
          "date": "Tue 02 Nov 2021 00:49",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280353,
          "date": "Sat 30 Oct 2021 14:50",
          "username": "Ebi",
          "content": "D is the answer",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 251791,
          "date": "Fri 29 Oct 2021 23:42",
          "username": "Bulti",
          "content": "correct answer is D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243423,
          "date": "Fri 29 Oct 2021 19:35",
          "username": "T14102020",
          "content": "Correct answer is D.  CloudWatch and Lambda for all instances",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230812,
          "date": "Thu 28 Oct 2021 20:54",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230070,
          "date": "Thu 28 Oct 2021 18:52",
          "username": "gookseang",
          "content": "seems D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230067,
          "date": "Wed 27 Oct 2021 21:51",
          "username": "gookseang",
          "content": "seems D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 188035,
          "date": "Wed 27 Oct 2021 07:31",
          "username": "sam422",
          "content": "I go with D after reading this document https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 188032,
          "date": "Tue 26 Oct 2021 23:23",
          "username": "sam422",
          "content": "I go with D after reading this document https://d1.awsstatic.com/whitepapers/aws-building-ami-factory-process-using-ec2-ssm-marketplace-and-service-catalog.pdf",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 151670,
          "date": "Sun 24 Oct 2021 01:48",
          "username": "fullaws",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 136071,
          "date": "Sat 23 Oct 2021 06:33",
          "username": "infPhathilft",
          "content": "Answer: C<br>A - incorrect - need to trigger when any EC2 instance is launched<br>B - incorrect - does not inspect the logs, thus can't filter on appropriate events such as starting of an instance. Instead send S3 to CloudWatch and use an Event Rule<br>C - incorrect - should work but akin to using a sledgehammer to crack a nut. Also it should have mentioned CloudWatch Alarm to trigger SNS then Lambda, however what details of the instance are being passed to Lambda from CloudWatch? the notification includesthe \\\"metric\\\", its value, time, etc - not the AMI, EC2 Instance ID, subnet, etc. Or should have said use Cloudwatch Event Rules, like the next answer does.<br>D - correct - takes 2 minutes to configure via the console - Event Rule <- [ [ EC2 Events <- \\\"AWS API Call via CloudTrail\\\" <- RunInstances] -> Lambda]. Nothing stopping us using RunInstances as the trigger. Remember CloudTrail is enabled by default and \\\"AWS API Call via CloudTrail\\\" is an AWS managed event source - simpleI think you mean Dgood good",
          "upvote_count": "631",
          "selected_answers": ""
        },
        {
          "id": 173469,
          "date": "Mon 25 Oct 2021 04:40",
          "username": "Phat",
          "content": "I think you mean D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 635821,
          "date": "Sun 24 Jul 2022 02:42",
          "username": "hilft",
          "content": "good good",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#510",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must migrate an existing on-premises web application with 70 TB of static files supporting a public open-data initiative. The Architect wants to upgrade to the latest version of the host operating system as part of the migration effort.<br>Which is the FASTEST and MOST cost-effective way to perform the migration?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#510",
          "answers": [
            {
              "choice": "<p>A. Run a physical-to-virtual conversion on the application server. Transfer the server image over the internet, and transfer the static data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Run a physical-to-virtual conversion on the application server. Transfer the server image over AWS Direct Connect, and transfer the static data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Re-platform the server to Amazon EC2, and use AWS Snowball to transfer the static data to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Re-platform the server by using the AWS Server Migration Service to move the code and data to a new Amazon EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11756,
          "date": "Sat 25 Sep 2021 06:36",
          "username": "donathonDashL",
          "content": "C<br>A: This will be too slow.<br>B: Direct connect takes too long to provision.<br>C: Because the question did not state what is the bandwidth of the company, using Snowball to transfer 70TB make sense.<br>D: While this is possible, we do not know If the server is physical or virtual and SMS just migrate it does not upgrade. Wherelse in C you can immediately select the best AMI to start and rely on Snowball to transfer the data.Agree with Ans C. <br>But all the reasons provided here doesn't sound right.<br>The key part of the question is: \\\"Which is the FASTEST and MOST cost-effective way to perform the migration?\\\"<br>A: First of all, you cannot convert a physical server to a virtual server. You have to create a new VM and migrate the applications and data. Because the question did not state what is the bandwidth, it is difficult to calculate how long it will take to transfer data.<br>B: Reasons same as A plus it will take significant amount of time to provision DX<br>C: Seems to be the most reasonable solution.<br>D: SMS cannot migrate a physical server. On top of that Because the question did not state what is the bandwidth, it is difficult to calculate how long it will take to transfer data.",
          "upvote_count": "353",
          "selected_answers": ""
        },
        {
          "id": 401353,
          "date": "Sat 30 Oct 2021 10:45",
          "username": "DashL",
          "content": "Agree with Ans C. <br>But all the reasons provided here doesn't sound right.<br>The key part of the question is: \\\"Which is the FASTEST and MOST cost-effective way to perform the migration?\\\"<br>A: First of all, you cannot convert a physical server to a virtual server. You have to create a new VM and migrate the applications and data. Because the question did not state what is the bandwidth, it is difficult to calculate how long it will take to transfer data.<br>B: Reasons same as A plus it will take significant amount of time to provision DX<br>C: Seems to be the most reasonable solution.<br>D: SMS cannot migrate a physical server. On top of that Because the question did not state what is the bandwidth, it is difficult to calculate how long it will take to transfer data.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 13440,
          "date": "Tue 28 Sep 2021 03:30",
          "username": "Moonsarah_t",
          "content": "I do support answer \\\"C\\\".<br>The snowball is the FASTEST option for transfer (next business day for delivery).Migration via Snowball takes about a week.",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 333526,
          "date": "Tue 26 Oct 2021 17:48",
          "username": "sarah_t",
          "content": "Migration via Snowball takes about a week.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 693183,
          "date": "Wed 12 Oct 2022 16:55",
          "username": "joanneli77",
          "content": "Without knowing local bandwidth, you can't determine whether snowball is faster or not.The problem is with the question.70TB may take a long time to transfer, but it may not.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 671248,
          "date": "Sat 17 Sep 2022 06:35",
          "username": "Dionenonly",
          "content": "c is the most feasible answer here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 534244,
          "date": "Fri 28 Jan 2022 02:15",
          "username": "jj22222",
          "content": "C looks right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499207,
          "date": "Sat 11 Dec 2021 09:39",
          "username": "cldy",
          "content": "C.  Re-platform the server to Amazon EC2, and use AWS Snowball to transfer the static data to Amazon S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491510,
          "date": "Wed 01 Dec 2021 11:02",
          "username": "AzureDP900",
          "content": "Internet speed is not provided, direct connect is expensive. So my answer is C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490284,
          "date": "Tue 30 Nov 2021 01:17",
          "username": "acloudguru",
          "content": "C it is a easy one hope I can have it in my exam",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 459665,
          "date": "Thu 04 Nov 2021 03:45",
          "username": "kirrim",
          "content": "Don't see this one mentioned, so just calling it out... another reason D is invalid is because it's storing the static file data in EBS, not in S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410502,
          "date": "Sat 30 Oct 2021 21:27",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346166,
          "date": "Sat 30 Oct 2021 08:09",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 280356,
          "date": "Mon 25 Oct 2021 07:32",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 255504,
          "date": "Sun 24 Oct 2021 03:24",
          "username": "01037",
          "content": "C. <br>B is not only expensive, but also provision of DX takes time.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 251792,
          "date": "Sun 24 Oct 2021 01:57",
          "username": "Bulti",
          "content": "Answer is C.  Using Snowball is going to be faster than Direct Connect. Other options are either not feasibleor too slow.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243427,
          "date": "Sat 23 Oct 2021 08:36",
          "username": "T14102020",
          "content": "Correct is C.  Snowball and Re-platform",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230813,
          "date": "Thu 21 Oct 2021 15:00",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 230074,
          "date": "Thu 21 Oct 2021 12:40",
          "username": "gookseang",
          "content": "seems C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#511",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that generates a weather forecast that is updated every 15 minutes with an output resolution of 1 billion unique positions, each approximately 20 bytes in size (20 Gigabytes per forecast). Every hour, the forecast data is globally accessed approximately 5 million times (1,400 requests per second), and up to 10 times more during weather events. The forecast data is overwritten every update. Users of the current weather forecast application expect responses to queries to be returned in less than two seconds for each request.<br>Which design meets the required request rate and response time?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#511",
          "answers": [
            {
              "choice": "<p>A. Store forecast locations in an Amazon ES cluster. Use an Amazon CloudFront distribution targeting an Amazon API Gateway endpoint with AWS Lambda functions responding to queries as the origin. Enable API caching on the API Gateway stage with a cache-control timeout set for 15 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store forecast locations in an Amazon EFS volume. Create an Amazon CloudFront distribution that targets an Elastic Load Balancing group of an Auto Scaling fleet of Amazon EC2 instances that have mounted the Amazon EFS volume. Set the cache-control timeout for 15 minutes in the CloudFront distribution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store forecast locations in an Amazon ES cluster. Use an Amazon CloudFront distribution targeting an API Gateway endpoint with AWS Lambda functions responding to queries as the origin. Create an Amazon Lambda@Edge function that caches the data locally at edge locations for 15 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store forecast locations in Amazon S3 as individual objects. Create an Amazon CloudFront distribution targeting an Elastic Load Balancing group of an Auto Scaling fleet of EC2 instances, querying the origin of the S3 object. Set the cache-control timeout for 15 minutes in the CloudFront distribution.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13678,
          "date": "Tue 21 Sep 2021 17:52",
          "username": "donathonHaykhayIbranthovichuhupaiFrank1",
          "content": "I have new insight after doing this question for 2nd time.<br>B<br>A: Cache control should be done at the Cloudfront not API Stage.<br>B: EFS has better performance than S3. The data size is only 20GB so this seems suitable.<br>C: Lambda@Edge does not cache data. Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world.<br>D: Why have the EC2 in the middle when CloudFront can set S3 as the origin?it is absolutely not true that API caching isnt possible https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.htmlI'm still not sure to be honest, I still prefer D over B. 20GB is per forecast and forecast is updated every 15 minutes, use EFS is more expensive.Cache control should be done at API gateway level. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
          "upvote_count": "171223",
          "selected_answers": ""
        },
        {
          "id": 715531,
          "date": "Thu 10 Nov 2022 21:04",
          "username": "Haykhay",
          "content": "it is absolutely not true that API caching isnt possible https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 14559,
          "date": "Tue 21 Sep 2021 18:42",
          "username": "Ibranthovic",
          "content": "I'm still not sure to be honest, I still prefer D over B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 22173,
          "date": "Tue 21 Sep 2021 21:21",
          "username": "huhupai",
          "content": "20GB is per forecast and forecast is updated every 15 minutes, use EFS is more expensive.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 23973,
          "date": "Wed 22 Sep 2021 21:20",
          "username": "Frank1",
          "content": "Cache control should be done at API gateway level. https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 12205,
          "date": "Mon 20 Sep 2021 02:16",
          "username": "donathonsarah1tiana528",
          "content": "D<br>Amazon EC2, Elastic Load Balancing, Amazon S3 buckets configured as website endpoints, or your own web server (HTTP). These are the only origin that you can define for CloudFront.<br>EFS also has lower limits then S3 which make it less suitable for this case which may have 14k request per second.<br>You can control how long your files stay in a CloudFront cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. Increasing the duration means your users get better performance because your files are more likely to be served directly from the edge cache. A longer duration also reduces the load on your origin.<br>To change the cache duration for an individual file, you can configure your origin to add a Cache-Control max-age or Cache-Control s-maxage directive, or an Expires header field to the file.cloudfront can target APIgateway (and most other dns origins):<br>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-cloudfront-distribution/Not D.  D says `Store forecast locations in Amazon S3 as individual objects`, the question says `15-minute weather prediction with a resolution of 1 billion distinct locations`. Uploading so many small objects to s3 every 15 minutes seems very ineffective. EFS is much more efficient.",
          "upvote_count": "1311",
          "selected_answers": ""
        },
        {
          "id": 43983,
          "date": "Sun 26 Sep 2021 00:53",
          "username": "sarah1",
          "content": "cloudfront can target APIgateway (and most other dns origins):<br>https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-cloudfront-distribution/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 492371,
          "date": "Thu 02 Dec 2021 11:36",
          "username": "tiana528",
          "content": "Not D.  D says `Store forecast locations in Amazon S3 as individual objects`, the question says `15-minute weather prediction with a resolution of 1 billion distinct locations`. Uploading so many small objects to s3 every 15 minutes seems very ineffective. EFS is much more efficient.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 697940,
          "date": "Tue 18 Oct 2022 07:39",
          "username": "Vizz5585",
          "content": "The answer is B. <br>Lambdas have concurrency limits <br>S3 has minimum storage limits",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 684059,
          "date": "Sat 01 Oct 2022 09:23",
          "username": "tomosabc1tomosabc1",
          "content": "I think the answer should be D.  The following is what I consolidated after reading the analysis from all other comments.<br>The question seems to be inspired from the actual case study of the weather company. All their data are stored in S3. https://aws.amazon.com/solutions/case-studies/the-weather-company/<br><br>A(wrong): Cache-control is not available for API Gateway, for which it is TTL.B(wrong): EFS limits :<br>1 read = 1 Operation<br>1 Write = 5 Operations.<br>EFS suports 35000 read operations limit only if you are just READING and not WRITING anything.<br>EFS has 7000 Write Operations limit limiting only if you are just WRITING and not READING anything<br>So EFS cannot handle 1 billlion files ( each 20 bytes) write requests in 15mins.<br><br>C(wrong): Maximum RPS for API Gateway is 10,000requests/s, for lambda it is 1,000requests/s. They can't meet with the requirements of maximum 14,000+ requests/s during whether events. In addition, Lambda@Edge is not used to cache data at edge locations for the specific time. <br>https://aws.amazon.com/blogs/networking-and-content-delivery/lambdaedge-design-best-practices/",
          "upvote_count": "21",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 684060,
          "date": "Sat 01 Oct 2022 09:24",
          "username": "tomosabc1",
          "content": "B(wrong): EFS limits :<br>1 read = 1 Operation<br>1 Write = 5 Operations.<br>EFS suports 35000 read operations limit only if you are just READING and not WRITING anything.<br>EFS has 7000 Write Operations limit limiting only if you are just WRITING and not READING anything<br>So EFS cannot handle 1 billlion files ( each 20 bytes) write requests in 15mins.<br><br>C(wrong): Maximum RPS for API Gateway is 10,000requests/s, for lambda it is 1,000requests/s. They can't meet with the requirements of maximum 14,000+ requests/s during whether events. In addition, Lambda@Edge is not used to cache data at edge locations for the specific time. <br>https://aws.amazon.com/blogs/networking-and-content-delivery/lambdaedge-design-best-practices/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 672792,
          "date": "Mon 19 Sep 2022 01:22",
          "username": "linuxmaster007",
          "content": "Answer is B.  Lambda can only handle 10,000 request. Also B is the answer per dojo tutorials.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 645523,
          "date": "Thu 11 Aug 2022 16:37",
          "username": "Sumit_Kumar",
          "content": "Amazon S3 automatically scales to high request rates. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned prefix.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496667,
          "date": "Wed 08 Dec 2021 09:44",
          "username": "cldy",
          "content": "B.  Store forecast locations in an Amazon EFS volume. Create an Amazon CloudFront distribution that targets an Elastic Load Balancing group of an Auto Scaling fleet of Amazon EC2 instances that have mounted the Amazon EFS volume. Set the cache-control timeout for 15 minutes in the CloudFront distribution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491519,
          "date": "Wed 01 Dec 2021 11:10",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 469259,
          "date": "Sun 07 Nov 2021 06:08",
          "username": "Kopa",
          "content": "Im going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 457113,
          "date": "Sun 07 Nov 2021 04:35",
          "username": "StelSen",
          "content": "The \\\"cache-control timeout is possible in CloudFront only. API Gateway is time to live. Lambda@Edge don't have cache-control timeout option. This left with only either B or D is right. Now, both B&D uses EC2/ASG. But From EC2, accessing EFS is faster than accessing S3. https://dzone.com/articles/confused-by-aws-storage-options-s3-ebs-amp-efs-explained. So I chose \\\"B\\\".",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 456262,
          "date": "Sat 06 Nov 2021 23:04",
          "username": "student22",
          "content": "B <br>---<br>B vs D - EFS better than S3 to query many small files frequently. <br>A & D - API gateway will throttle at 10k rpm by default.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 435383,
          "date": "Sat 06 Nov 2021 14:46",
          "username": "blackgamer",
          "content": "Yes, this is B.  Lambda is out because of concurrent limit and response time, s3 is out because of update frequency.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 411112,
          "date": "Sat 06 Nov 2021 08:07",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 401415,
          "date": "Fri 05 Nov 2021 07:52",
          "username": "DashLDashL",
          "content": "I have a question - home someone can provide some insight. I did a lot of search, but couldn't find an answer.<br>As per the document: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html, the Default quota for Lambda concurrent executions is 1000, but can be increased to \\\"Tens of thousands\\\". So, why everybody is stuck on the figure of \\\"1000\\\" for lambda?Also, nowhere in the question it says that the solution has to be implemented immediately. So, there will be enough time to create a quota increase request before implementing the solution.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 401417,
          "date": "Fri 05 Nov 2021 19:14",
          "username": "DashL",
          "content": "Also, nowhere in the question it says that the solution has to be implemented immediately. So, there will be enough time to create a quota increase request before implementing the solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 401413,
          "date": "Fri 05 Nov 2021 04:56",
          "username": "DashL",
          "content": "I have a question - home someone can provide some insight. I did a lot of search, but couldn't find an answer.<br>As per the document: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html, the Default quota for Lambda concurrent executions is 1000, but can be increased to \\\"Tens of thousands\\\". So, why everybody is stuck on the figure of \\\"1000\\\" for lambda??",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 383969,
          "date": "Fri 05 Nov 2021 01:06",
          "username": "SheldonHofstadter",
          "content": "According to this https://aws.amazon.com/solutions/case-studies/the-weather-company/.. they built it with S3 and they state The platform is robust enough to handle between 10 and 15 billion transactions each day at 100,000 to 150,000 per second, depending on the weather.”",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 346950,
          "date": "Thu 04 Nov 2021 22:34",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#512",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.<br>How can the company prevent users from accidentally deleting data in this way?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#512",
          "answers": [
            {
              "choice": "<p>A. Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure a stack policy that disallows the deletion of RDS and EBS resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify IAM policies to deny deleting RDS and EBS resources that are tagged with an ג€aws:cloudformation:stack-nameג€ tag.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Config rules to prevent deleting RDS and EBS resources.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12206,
          "date": "Thu 23 Sep 2021 22:35",
          "username": "donathon",
          "content": "A<br>With the DeletionPolicy attribute you can preserve or (in some cases) backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify Retain for that resource. You can use retain for any resource. For example, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue to use or modify those resources after you delete their stacks.<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 11264,
          "date": "Sun 19 Sep 2021 18:36",
          "username": "huhupai",
          "content": "I would go for A. ",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 715595,
          "date": "Thu 10 Nov 2022 23:32",
          "username": "Aum",
          "content": "why not B?Stack policy can also disallows deletion of resources within the stack. \\\"A\\\" just said to configure \\\"DeletionPolicy\\\" it didn't actually say to prevent deletion. You can actually set it to snapshot before delete, etc.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495819,
          "date": "Tue 07 Dec 2021 10:22",
          "username": "cldy",
          "content": "A.  Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494613,
          "date": "Sun 05 Dec 2021 20:57",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448094,
          "date": "Sat 06 Nov 2021 17:33",
          "username": "moon2351",
          "content": "The answer is definitely A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411113,
          "date": "Fri 05 Nov 2021 07:58",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346912,
          "date": "Wed 03 Nov 2021 16:13",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280366,
          "date": "Wed 27 Oct 2021 16:30",
          "username": "Ebi",
          "content": "A is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 251862,
          "date": "Sun 24 Oct 2021 11:05",
          "username": "Bulti",
          "content": "Answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243019,
          "date": "Fri 22 Oct 2021 10:46",
          "username": "T14102020",
          "content": "Correct is A.  CloudFormation",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230821,
          "date": "Sat 16 Oct 2021 05:26",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230079,
          "date": "Fri 15 Oct 2021 16:34",
          "username": "gookseang",
          "content": "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183004,
          "date": "Thu 14 Oct 2021 08:38",
          "username": "ipindado2020",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 151781,
          "date": "Sat 09 Oct 2021 13:35",
          "username": "fullaws",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133124,
          "date": "Sat 09 Oct 2021 02:39",
          "username": "NikkyDicky",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 64042,
          "date": "Fri 08 Oct 2021 19:15",
          "username": "paulwang",
          "content": "A. <br>No doubt.<br>Just FYI, <br>You can prevent a stack from being accidentally deleted by enabling termination protection on the stack. That protect the whole stack from delete.<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#513",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to migrate an application from on-premises to AWS. The application currently uses an Oracle database and the company can tolerate a brief downtime of 1 hour when performing the switch to the new infrastructure. As part of the migration, the database engine will be changed to MySQL. A<br>Solutions Architect needs to determine which AWS services can be used to perform the migration while minimizing the amount of work and time required.<br>Which of the following will meet the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#513",
          "answers": [
            {
              "choice": "<p>A. Use AWS SCT to generate the schema scripts and apply them on the target prior to migration. Use AWS DMS to analyze the current schema and provide a recommendation for the optimal database engine. Then, use AWS DMS to migrate to the recommended engine. Use AWS SCT to identify what embedded SQL code in the application can be converted and what has to be done manually.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS SCT to generate the schema scripts and apply them on the target prior to migration. Use AWS DMS to begin moving data from the on-premises database to AWS. After the initial copy, continue to use AWS DMS to keep the databases in sync until cutting over to the new database. Use AWS SCT to identify what embedded SQL code in the application can be converted and what has to be done manually.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS DMS to help identify the best target deployment between installing the database engine on Amazon EC2 directly or moving to Amazon RDS. Then, use AWS DMS to migrate to the platform. Use AWS Application Discovery Service to identify what embedded SQL code in the application can be converted and what has to be done manually.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS DMS to begin moving data from the on-premises database to AWS. After the initial copy, continue to use AWS DMS to keep the databases in sync until cutting over to the new database. Use AWS Application Discovery Service to identify what embedded SQL code in the application can be converted and what has to be done manually.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13643,
          "date": "Fri 01 Oct 2021 00:22",
          "username": "donathonsam422",
          "content": "B<br>A: Need to minimize work and time required. MySQL has already been chosen why do we need to provide recommendation?<br>C\\D: SCT needs to be used.To move an instance to a placement group using the AWS CLI<br><br>Stop the instance using the stop-instances command.<br><br>Use the modify-instance-placement command and specify the name of the placement group to which to move the instance.<br><br>aws ec2 modify-instance-placement --instance-id i-0123a456700123456 --group-name MySpreadGroup<br>Start the instance using the start-instances command.",
          "upvote_count": "221",
          "selected_answers": ""
        },
        {
          "id": 185986,
          "date": "Fri 08 Oct 2021 00:32",
          "username": "sam422",
          "content": "To move an instance to a placement group using the AWS CLI<br><br>Stop the instance using the stop-instances command.<br><br>Use the modify-instance-placement command and specify the name of the placement group to which to move the instance.<br><br>aws ec2 modify-instance-placement --instance-id i-0123a456700123456 --group-name MySpreadGroup<br>Start the instance using the start-instances command.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 11069,
          "date": "Wed 22 Sep 2021 13:10",
          "username": "huhupaidpvnme",
          "content": "AWS Application Discovery Service can't identify what embedded SQL code in the application, SCT can scan application source code for embedded SQL statements and convert them as part of a database schema conversion project. So I think the correct answer is B. Bbbbbbb",
          "upvote_count": "125",
          "selected_answers": ""
        },
        {
          "id": 11878,
          "date": "Wed 22 Sep 2021 16:08",
          "username": "dpvnme",
          "content": "Bbbbbbb",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 497386,
          "date": "Thu 09 Dec 2021 07:26",
          "username": "cldy",
          "content": "B.  Use AWS SCT to generate the schema scripts and apply them on the target prior to migration. Use AWS DMS to begin moving data from the on-premises database to AWS. After the initial copy, continue to use AWS DMS to keep the databases in sync until cutting over to the new database. Use AWS SCT to identify what embedded SQL code in the application can be converted and what has to be done manually.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494614,
          "date": "Sun 05 Dec 2021 20:58",
          "username": "AzureDP900",
          "content": "I agree with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411115,
          "date": "Sun 07 Nov 2021 05:48",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346917,
          "date": "Fri 05 Nov 2021 11:40",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 336694,
          "date": "Wed 03 Nov 2021 21:45",
          "username": "Kelvin",
          "content": "BBBBBBBBBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291679,
          "date": "Sat 30 Oct 2021 19:42",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279862,
          "date": "Fri 22 Oct 2021 17:34",
          "username": "Ebi",
          "content": "Answer is B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 267024,
          "date": "Fri 22 Oct 2021 04:14",
          "username": "rkbala",
          "content": "B is the correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 251869,
          "date": "Tue 19 Oct 2021 06:29",
          "username": "Bulti",
          "content": "Answer is B not A as it doesn't seem to meet the 1 hour downtime req",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 247496,
          "date": "Sun 17 Oct 2021 17:03",
          "username": "petebear55",
          "content": "This is a typical example of how aws tries to trip u up in the exam .. using terms like SCT instead of the full The AWS Schema Conversion Tool (AWS SCT) which helps convert your existing database schema from one database engine to another. You can convert from a relational OLTP schema or any supported data warehouse OLAP schema to Amazon RDS (for example, Amazon Aurora MySQL or Amazon Aurora PostgreSQL, among others).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243457,
          "date": "Sat 16 Oct 2021 09:07",
          "username": "T14102020",
          "content": "Correct is B.  SCT + without DMS to analyze",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 238648,
          "date": "Sat 16 Oct 2021 07:27",
          "username": "MeepMeep",
          "content": "BBBBBBBBBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230823,
          "date": "Wed 13 Oct 2021 03:30",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230084,
          "date": "Mon 11 Oct 2021 01:52",
          "username": "gookseang",
          "content": "seems B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 198499,
          "date": "Mon 11 Oct 2021 01:10",
          "username": "Paitan",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#514",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS to run an internet-facing production application written in Node.js. The Development team is responsible for pushing new versions of their software directly to production. The application software is updated multiple times a day. The team needs guidance from a Solutions Architect to help them deploy the software to the production fleet quickly and with the least amount of disruption to the service.<br>Which option meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#514",
          "answers": [
            {
              "choice": "<p>A. Prepackage the software into an AMI and then use Auto Scaling to deploy the production fleet. For software changes, update the AMI and allow Auto Scaling to automatically push the new AMI to production.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS CodeDeploy to push the prepackaged AMI to production. For software changes, reconfigure CodeDeploy with new AMI identification to push the new AMI to the production fleet.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Elastic Beanstalk to host the production application. For software changes, upload the new application version to Elastic Beanstalk to push this to the production fleet using a blue/green deployment method.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the base AMI through Auto Scaling and bootstrap the software using user data. For software changes, SSH to each of the instances and replace the software with the new version.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13431,
          "date": "Wed 22 Sep 2021 16:04",
          "username": "Moon",
          "content": "\\\"C\\\",<br>ou can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 13642,
          "date": "Fri 24 Sep 2021 17:53",
          "username": "donathon",
          "content": "C<br>A\\D: Not feasible<br>B: CodeDeploy does not push AMI.",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 671264,
          "date": "Sat 17 Sep 2022 07:00",
          "username": "Dionenonly",
          "content": "C has the least amount of efforts",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 647728,
          "date": "Tue 16 Aug 2022 17:25",
          "username": "Ni_yot",
          "content": "yep C for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 561155,
          "date": "Sat 05 Mar 2022 04:29",
          "username": "pal40sg",
          "content": "push this to the production fleet using a blue/green deployment method",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 552958,
          "date": "Mon 21 Feb 2022 16:31",
          "username": "pititcu667",
          "content": "c seems to make sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 494618,
          "date": "Sun 05 Dec 2021 21:01",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411118,
          "date": "Sat 06 Nov 2021 23:15",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346919,
          "date": "Thu 04 Nov 2021 15:53",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346677,
          "date": "Thu 04 Nov 2021 15:26",
          "username": "blackgamer",
          "content": "C is the correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291682,
          "date": "Thu 04 Nov 2021 11:39",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279876,
          "date": "Mon 01 Nov 2021 09:32",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 279686,
          "date": "Thu 28 Oct 2021 15:43",
          "username": "Firststack",
          "content": "Answer is C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 251870,
          "date": "Sat 16 Oct 2021 00:45",
          "username": "Bulti",
          "content": "Answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243460,
          "date": "Fri 15 Oct 2021 18:43",
          "username": "T14102020",
          "content": "Correct is C. Elastic Beanstalk",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230826,
          "date": "Wed 13 Oct 2021 00:57",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230090,
          "date": "Mon 11 Oct 2021 20:00",
          "username": "gookseang",
          "content": "seems c",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#515",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.<br>The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.<br>Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#515",
          "answers": [
            {
              "choice": "<p>A. Reconfigure Amazon EFS to enable maximum I/O.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13646,
          "date": "Wed 22 Sep 2021 23:30",
          "username": "donathonleeoIbranthovicshammousahmedghanemPacoDerekbobsmith2000",
          "content": "C<br>A: Issue seems to be latency and load related. EFS does not solve the issue since the issue lies with EC2.<br>B: Risky as an EC2 instance failure could corrupt the data.<br>D: Origin cannot point to ALB (either S3, EC2 or HTTP based)?C looks more relevant ,but we can add ALB as CF origin.<br>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/We can add ALB as Cloudfront origin, then whynot D ?<br>Why to use S3 when the data are already in EFS. and go to migration between S3 and EFS.<br>I think the right answer is DBecause S3 is more cost-effective.the deployment should be cost-effective and scalable <br>as u know EFS 10 time more expensive than S3orgin can be ELB, the point of D is the data to be served resided on EFS, point to ELB is uselessOrigins:<br>Using an Amazon S3 bucket<br>Using a MediaStore container or a MediaPackage channel<br>Using an Application Load Balancer<br>Using a Lambda function URL<br>Using Amazon EC2 (or another custom origin)<br>Using CloudFront origin groups<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html",
          "upvote_count": "196311221",
          "selected_answers": ""
        },
        {
          "id": 14237,
          "date": "Fri 24 Sep 2021 17:59",
          "username": "leeoIbranthovicshammousahmedghanem",
          "content": "C looks more relevant ,but we can add ALB as CF origin.<br>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/We can add ALB as Cloudfront origin, then whynot D ?<br>Why to use S3 when the data are already in EFS. and go to migration between S3 and EFS.<br>I think the right answer is DBecause S3 is more cost-effective.the deployment should be cost-effective and scalable <br>as u know EFS 10 time more expensive than S3",
          "upvote_count": "63112",
          "selected_answers": ""
        },
        {
          "id": 14563,
          "date": "Fri 24 Sep 2021 23:49",
          "username": "Ibranthovicshammousahmedghanem",
          "content": "We can add ALB as Cloudfront origin, then whynot D ?<br>Why to use S3 when the data are already in EFS. and go to migration between S3 and EFS.<br>I think the right answer is DBecause S3 is more cost-effective.the deployment should be cost-effective and scalable <br>as u know EFS 10 time more expensive than S3",
          "upvote_count": "3112",
          "selected_answers": ""
        },
        {
          "id": 279177,
          "date": "Thu 21 Oct 2021 03:04",
          "username": "shammous",
          "content": "Because S3 is more cost-effective.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 16529,
          "date": "Wed 29 Sep 2021 00:44",
          "username": "ahmedghanem",
          "content": "the deployment should be cost-effective and scalable <br>as u know EFS 10 time more expensive than S3",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 43317,
          "date": "Fri 01 Oct 2021 02:52",
          "username": "PacoDerek",
          "content": "orgin can be ELB, the point of D is the data to be served resided on EFS, point to ELB is useless",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 597403,
          "date": "Thu 05 May 2022 19:22",
          "username": "bobsmith2000",
          "content": "Origins:<br>Using an Amazon S3 bucket<br>Using a MediaStore container or a MediaPackage channel<br>Using an Application Load Balancer<br>Using a Lambda function URL<br>Using Amazon EC2 (or another custom origin)<br>Using CloudFront origin groups<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 45608,
          "date": "Fri 01 Oct 2021 07:09",
          "username": "ashp",
          "content": "C. <br>S3 Cheaper compare to EFS<br>CF woks better with S3<br>Using ALB which makes easy to point to Video file when needed",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 494915,
          "date": "Mon 06 Dec 2021 06:18",
          "username": "cldy",
          "content": "C.  Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494621,
          "date": "Sun 05 Dec 2021 21:04",
          "username": "AzureDP900",
          "content": "C is right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411119,
          "date": "Thu 04 Nov 2021 22:56",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 377097,
          "date": "Wed 03 Nov 2021 17:55",
          "username": "pradhyumna",
          "content": "I would go with D.  Option C may only solve the buffering issue with videos and there would be additional changes required for the app to provide links to s3. The question also points at users having issues in reaching the site which can't be solved with option C, so an overall caching solution like option D would really help.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346923,
          "date": "Wed 03 Nov 2021 15:55",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291686,
          "date": "Sun 31 Oct 2021 09:56",
          "username": "Kian1",
          "content": "defo going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279893,
          "date": "Sat 23 Oct 2021 10:00",
          "username": "Ebilechuk",
          "content": "C doesn't seem to be correct, moving videos to S3 does not mean that architecture won't have EFS anymore, all other contents are still in EFS,<br>I go with D, ALB of course can be the origin for CFN<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesDomainNameC does not say the EFS is going to be removed, just say to move videos from EFS to S3. <br><br>I'll go with C",
          "upvote_count": "24",
          "selected_answers": ""
        },
        {
          "id": 288186,
          "date": "Sun 24 Oct 2021 06:14",
          "username": "lechuk",
          "content": "C does not say the EFS is going to be removed, just say to move videos from EFS to S3. <br><br>I'll go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 279688,
          "date": "Sat 23 Oct 2021 07:50",
          "username": "Firststack",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251871,
          "date": "Wed 20 Oct 2021 16:54",
          "username": "Bulti",
          "content": "C is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 247512,
          "date": "Wed 20 Oct 2021 05:14",
          "username": "petebear55",
          "content": "D IS TOO COMPLEX AND DOES NOT MEET THE PARAMETERS OF THE QUESTION ... ANSWER IS CWHICH IS BEST PRACTICE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243463,
          "date": "Mon 18 Oct 2021 20:02",
          "username": "T14102020",
          "content": "Correct is C.  S3 + CloudFront",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230828,
          "date": "Mon 18 Oct 2021 02:47",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230091,
          "date": "Sun 17 Oct 2021 14:04",
          "username": "gookseang",
          "content": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 199549,
          "date": "Fri 15 Oct 2021 07:02",
          "username": "SMS",
          "content": "A.  Reconfigure Amazon EFS to enable maximum I/O is the right answer.https://docs.aws.amazon.com/efs/latest/ug/performance.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 198504,
          "date": "Wed 13 Oct 2021 20:36",
          "username": "Paitan",
          "content": "Option C",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#516",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs its containerized batch jobs on Amazon ECS. The jobs are scheduled by submitting a container image, a task definition, and the relevant data to an Amazon S3 bucket. Container images may be unique per job. Running the jobs as quickly as possible is of utmost importance, so submitting job artifacts to the<br>S3 bucket triggers the job to run immediately. Sometimes there may be no jobs running at all. However, jobs of any size can be submitted with no prior warning to the IT Operations team. Job definitions include CPU and memory resource requirements.<br>What solution will allow the batch jobs to complete as quickly as possible after being scheduled?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#516",
          "answers": [
            {
              "choice": "<p>A. Schedule the jobs on an Amazon ECS cluster using the Amazon EC2 launch type. Use Service Auto Scaling to increase or decrease the number of running tasks to suit the number of running jobs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Schedule the jobs directly on EC2 instances. Use Reserved Instances for the baseline minimum load, and use On-Demand Instances in an Auto Scaling group to scale up the platform based on demand.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Schedule the jobs on an Amazon ECS cluster using the Fargate launch type. Use Service Auto Scaling to increase or decrease the number of running tasks to suit the number of running jobs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Schedule the jobs on an Amazon ECS cluster using the Fargate launch type. Use Spot Instances in an Auto Scaling group to scale the platform based on demand. Use Service Auto Scaling to increase or decrease the number of running tasks to suit the number of running jobs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13647,
          "date": "Sun 03 Oct 2021 04:34",
          "username": "donathon",
          "content": "C<br>A: EC2 Launch type you have to determine the EC2 instance beforehand and scaling up down is not that fast as Fargate.<br>B: This is not feasible as it may not be fast enough and it’s not managed.<br>D: You cannot use Spot instance because it is not guaranteed.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 10466,
          "date": "Tue 21 Sep 2021 08:52",
          "username": "dpvnme",
          "content": "I would go with C in this situation",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 495948,
          "date": "Tue 07 Dec 2021 13:13",
          "username": "cldy",
          "content": "C.  Schedule the jobs on an Amazon ECS cluster using the Fargate launch type. Use Service Auto Scaling to increase or decrease the number of running tasks to suit the number of running jobs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494622,
          "date": "Sun 05 Dec 2021 21:05",
          "username": "AzureDP900",
          "content": "I go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411120,
          "date": "Mon 01 Nov 2021 16:55",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346933,
          "date": "Mon 01 Nov 2021 06:15",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 294348,
          "date": "Mon 25 Oct 2021 04:48",
          "username": "kiev",
          "content": "Fargate. C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291689,
          "date": "Tue 19 Oct 2021 02:07",
          "username": "Kian1",
          "content": "ofc going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279898,
          "date": "Sun 17 Oct 2021 08:49",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263777,
          "date": "Sun 17 Oct 2021 01:40",
          "username": "cox1960",
          "content": "very strange answers since a \\\"service\\\" in ECS is not used for batches.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251872,
          "date": "Sat 16 Oct 2021 21:40",
          "username": "Bulti",
          "content": "Answer is C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 247520,
          "date": "Wed 13 Oct 2021 22:44",
          "username": "petebear55",
          "content": "C THIS TOPIC IS COVERED QUITE WELL IN THE LATEST ADDED QUESTIONS ON WIZZ LABS ... REMEMBER ITS FARGATE ONLY IF YOU WANT AWS TO MANAGE IT .. SO BE CAREFUL HERE IN THE EXAM,,,,, ANSWER C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243466,
          "date": "Wed 13 Oct 2021 10:09",
          "username": "T14102020",
          "content": "Correct is C.  Fargate + without Spot",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230829,
          "date": "Wed 13 Oct 2021 01:45",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230094,
          "date": "Tue 12 Oct 2021 00:20",
          "username": "gookseang",
          "content": "CCCCCCCCCCCCCCCCCCCC",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 151793,
          "date": "Mon 11 Oct 2021 12:54",
          "username": "fullaws",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 133147,
          "date": "Sat 09 Oct 2021 07:52",
          "username": "NikkyDicky",
          "content": "C most likley",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#517",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company receives clickstream data files to Amazon S3 every five minutes. A Python script runs as a cron job once a day on an Amazon EC2 instance to process each file and load it into a database hosted on Amazon RDS. The cron job takes 15 to 30 minutes to process 24 hours of data. The data consumers ask for the data be available as soon as possible.<br>Which solution would accomplish the desired outcome?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#517",
          "answers": [
            {
              "choice": "<p>A. Increase the size of the instance to speed up processing and update the schedule to run once an hour.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Convert the cron job to an AWS Lambda function and trigger this new function using a cron job on an EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Convert the cron job to an AWS Lambda function and schedule it to run once an hour using Amazon CloudWatch Events.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Lambda function that runs when a file is delivered to Amazon S3 using S3 event notifications.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 10565,
          "date": "Mon 20 Sep 2021 01:22",
          "username": "donathondonathonPacoDerekpixepePacoDerekLunchTimeshammous",
          "content": "D<br>A: Will not help.<br>B: Not feasible since it is based on a schedule not event.<br>C: This is still based on schedule.https://docs.aws.amazon.com/lambda/latest/dg/with-s3.htmlC<br>as s3 event may lost.cloudwatch event is more reliable. using rate expression to trigger Lambda is okC (hourly schedule) is INCORRECTas requirement is \\\"The data consumers ask for the data be available as soon as possible.\\\"<br><br>D is correcthttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.htmlD is the correct answer.<br>If S3 event notifications are configured property, this will not be an issue, as explained in the following link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-configure-s3-event-notification/D is the answer, but I disagree with you regarding CW Event being \\\"still based on schedule\\\". It can do the same job as S3 events by instantaneously triggering a lambda function based on a write event on S3, but the fact that it should be \\\"as soon as possible\\\" make S3 events service a better choice as it will perform quicker than CW Event.",
          "upvote_count": "27211213",
          "selected_answers": ""
        },
        {
          "id": 13648,
          "date": "Tue 28 Sep 2021 05:30",
          "username": "donathonPacoDerekpixepePacoDerekLunchTime",
          "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.htmlC<br>as s3 event may lost.cloudwatch event is more reliable. using rate expression to trigger Lambda is okC (hourly schedule) is INCORRECTas requirement is \\\"The data consumers ask for the data be available as soon as possible.\\\"<br><br>D is correcthttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.htmlD is the correct answer.<br>If S3 event notifications are configured property, this will not be an issue, as explained in the following link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-configure-s3-event-notification/",
          "upvote_count": "21121",
          "selected_answers": ""
        },
        {
          "id": 43328,
          "date": "Tue 28 Sep 2021 09:15",
          "username": "PacoDerekpixepePacoDerekLunchTime",
          "content": "C<br>as s3 event may lost.cloudwatch event is more reliable. using rate expression to trigger Lambda is okC (hourly schedule) is INCORRECTas requirement is \\\"The data consumers ask for the data be available as soon as possible.\\\"<br><br>D is correcthttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.htmlD is the correct answer.<br>If S3 event notifications are configured property, this will not be an issue, as explained in the following link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-configure-s3-event-notification/",
          "upvote_count": "1121",
          "selected_answers": ""
        },
        {
          "id": 659286,
          "date": "Sun 04 Sep 2022 14:51",
          "username": "pixepe",
          "content": "C (hourly schedule) is INCORRECTas requirement is \\\"The data consumers ask for the data be available as soon as possible.\\\"<br><br>D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 43330,
          "date": "Wed 29 Sep 2021 18:34",
          "username": "PacoDerekLunchTime",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.htmlD is the correct answer.<br>If S3 event notifications are configured property, this will not be an issue, as explained in the following link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-configure-s3-event-notification/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 123665,
          "date": "Mon 11 Oct 2021 03:33",
          "username": "LunchTime",
          "content": "D is the correct answer.<br>If S3 event notifications are configured property, this will not be an issue, as explained in the following link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-configure-s3-event-notification/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279193,
          "date": "Fri 29 Oct 2021 17:17",
          "username": "shammous",
          "content": "D is the answer, but I disagree with you regarding CW Event being \\\"still based on schedule\\\". It can do the same job as S3 events by instantaneously triggering a lambda function based on a write event on S3, but the fact that it should be \\\"as soon as possible\\\" make S3 events service a better choice as it will perform quicker than CW Event.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 11948,
          "date": "Thu 23 Sep 2021 05:49",
          "username": "uopspop",
          "content": "prefer D, too.<br>C is still hour-based, which is not \\\"as soon as possible\\\".",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 607633,
          "date": "Thu 26 May 2022 14:19",
          "username": "Racinely",
          "content": "Explanation<br>https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494625,
          "date": "Sun 05 Dec 2021 21:08",
          "username": "AzureDP900",
          "content": "D is fine with me. Amazon S3 using S3 event notifications",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494209,
          "date": "Sun 05 Dec 2021 11:06",
          "username": "cldy",
          "content": "D.  Create an AWS Lambda function that runs when a file is delivered to Amazon S3 using S3 event notifications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448104,
          "date": "Thu 04 Nov 2021 04:45",
          "username": "moon2351",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411123,
          "date": "Thu 04 Nov 2021 03:48",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348215,
          "date": "Wed 03 Nov 2021 05:23",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291691,
          "date": "Tue 02 Nov 2021 11:19",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280231,
          "date": "Sun 31 Oct 2021 22:28",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 251873,
          "date": "Tue 26 Oct 2021 23:24",
          "username": "Bulti",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243469,
          "date": "Tue 19 Oct 2021 12:22",
          "username": "T14102020",
          "content": "Correct is D. Lambda runs when a file is delivered",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230830,
          "date": "Tue 19 Oct 2021 01:59",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230098,
          "date": "Sun 17 Oct 2021 07:46",
          "username": "gookseang",
          "content": "DDDDDDDDDDDDDD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 197438,
          "date": "Sat 16 Oct 2021 16:21",
          "username": "Paitan",
          "content": "Definitely D.  Since we are calling Lambda based on S3 notifications, 15 minutes limit of Lambda is not an issue here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 151800,
          "date": "Sat 16 Oct 2021 07:18",
          "username": "fullaws",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 147776,
          "date": "Wed 13 Oct 2021 09:35",
          "username": "3parusrkhksoma",
          "content": "Has to be A - trick question since lambda Max run time is 15 minutes? https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/The cron job takes 15 to 30 minutes to process 24 hours of data. We are not doing this once a day..we are doing it based on the s3 notification.",
          "upvote_count": "37",
          "selected_answers": ""
        },
        {
          "id": 148155,
          "date": "Thu 14 Oct 2021 07:34",
          "username": "khksoma",
          "content": "The cron job takes 15 to 30 minutes to process 24 hours of data. We are not doing this once a day..we are doing it based on the s3 notification.",
          "upvote_count": "7",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#518",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company that is new to AWS reports it has exhausted its service limits across several accounts that are on the Basic Support plan. The company would like to prevent this from happening in the future.<br>What is the MOST efficient way of monitoring and managing all service limits in the company's accounts?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#518",
          "answers": [
            {
              "choice": "<p>A. Use Amazon CloudWatch and AWS Lambda to periodically calculate the limits across all linked accounts using AWS Trusted Advisor, provide notifications using Amazon SNS if the limits are close to exceeding the threshold.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Reach out to AWS Support to proactively increase the limits across all accounts. That way, the customer avoids creating and managing infrastructure just to raise the service limits.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon CloudWatch and AWS Lambda to periodically calculate the limits across all linked accounts using AWS Trusted Advisor, programmatically increase the limits that are close to exceeding the threshold.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon CloudWatch and AWS Lambda to periodically calculate the limits across all linked accounts using AWS Trusted Advisor, and use Amazon SNS for notifications if a limit is close to exceeding the threshold. Ensure that the accounts are using the AWS Business Support plan at a minimum.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 736933,
          "date": "Tue 06 Dec 2022 16:09",
          "username": "SureNot",
          "content": "D - https://aws.amazon.com/ru/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 716758,
          "date": "Sat 12 Nov 2022 15:48",
          "username": "et22s",
          "content": "This solution requires a paid plan (Business at the minimum).<br>\\\"The stack uses AWS Support APIs which are not available under the free developer plan. For more information, refer to https://aws.amazon.com/premiumsupport/plans\\\"<br><br>Source: https://docs.aws.amazon.com/solutions/latest/quota-monitor-for-aws/plan-your-deployment.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 714965,
          "date": "Thu 10 Nov 2022 07:24",
          "username": "janvandermerwer",
          "content": "D - You'll need business support plan for most functionality.<br>- In fact, with \\\"basic\\\" support, you may have trouble even getting a ticket logged!<br>Ran into this scenario recently.<br>Sure - basic checks are available in the trusted advisor console - However, you can't report on them using cloudwatch etc",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 670156,
          "date": "Thu 15 Sep 2022 19:28",
          "username": "dcdcdc3",
          "content": "D<br>https://docs.aws.amazon.com/solutions/latest/quota-monitor-on-aws/welcome.html<br>\\\"To use this solution, each account must have a Business- or Enterprise-level AWS Support plan in order to gain access to the Trusted Advisor service quota checks.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 660503,
          "date": "Mon 05 Sep 2022 21:36",
          "username": "epomatti",
          "content": "It's D.  Here is the proof you need:<br><br>https://aws.amazon.com/solutions/implementations/quota-monitor/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 626409,
          "date": "Sun 03 Jul 2022 06:49",
          "username": "nm4unm4u",
          "content": "AWS Documentation says below. <br>If you have a Basic or Developer Support plan, you can use the Trusted Advisor console to access all checks in the Service Limits category and six checks in the Security category.<br><br>If you have a Business, Enterprise On-Ramp, or Enterprise Support plan, you can use the Trusted Advisor console and the AWS Support API to access all Trusted Advisor checks.<br><br>Based on this, the Correct answer should be A. Changing to D.  <br>Reason explained below. It's very tricky question. <br>Yes, we basic support, you can use trusted advisor console to access all checks in service limit. However, for the given usecase, we need to have the service/quota monitoring automatically using lambda which will use aws trusted advisor via api calls. So correct answer should be D.  <br>Reference: https://docs.aws.amazon.com/solutions/latest/quota-monitor-on-aws/welcome.html",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 626414,
          "date": "Sun 03 Jul 2022 07:02",
          "username": "nm4u",
          "content": "Changing to D.  <br>Reason explained below. It's very tricky question. <br>Yes, we basic support, you can use trusted advisor console to access all checks in service limit. However, for the given usecase, we need to have the service/quota monitoring automatically using lambda which will use aws trusted advisor via api calls. So correct answer should be D.  <br>Reference: https://docs.aws.amazon.com/solutions/latest/quota-monitor-on-aws/welcome.html",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 570309,
          "date": "Fri 18 Mar 2022 08:38",
          "username": "KengLuser0001bobsmith2000",
          "content": "D as trust advisor API is not available for basic plan.<br>https://docs.aws.amazon.com/awssupport/latest/user/Welcome.htmlA is right https://aws.amazon.com/premiumsupport/plans/<br>D is wronghttps://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html<br>There are no mentions of any restrictions",
          "upvote_count": "311",
          "selected_answers": ""
        },
        {
          "id": 602082,
          "date": "Sun 15 May 2022 14:34",
          "username": "user0001",
          "content": "A is right https://aws.amazon.com/premiumsupport/plans/<br>D is wrong",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 611332,
          "date": "Sat 04 Jun 2022 07:13",
          "username": "bobsmith2000",
          "content": "https://docs.aws.amazon.com/awssupport/latest/user/trustedadvisor.html<br>There are no mentions of any restrictions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 564928,
          "date": "Thu 10 Mar 2022 18:59",
          "username": "Sonujunkowassb",
          "content": "https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html<br><br>\\\" If you have a Basic or Developer Support plan, you can use the Trusted Advisor console to access all checks in the Service Limits category and six checks in the Security category.\\\"Trusted Advisor CONSOLE",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 691034,
          "date": "Mon 10 Oct 2022 13:01",
          "username": "wassb",
          "content": "Trusted Advisor CONSOLE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 557374,
          "date": "Sun 27 Feb 2022 15:33",
          "username": "Alexey79",
          "content": "https://docs.aws.amazon.com/solutions/latest/limit-monitor/deployment.html<br>Prerequisites<br>To use this solution, each account must have a Business- or Enterprise-level AWS Support plan in order to gain access to the Trusted Advisor Service Limits checks.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 538046,
          "date": "Tue 01 Feb 2022 17:35",
          "username": "padel",
          "content": "Why not A instead of D ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 536572,
          "date": "Mon 31 Jan 2022 01:34",
          "username": "Bigbearcn",
          "content": "It's D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 522623,
          "date": "Thu 13 Jan 2022 05:14",
          "username": "tkanmani76",
          "content": "Should be D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 518425,
          "date": "Thu 06 Jan 2022 18:26",
          "username": "wahlbergusa",
          "content": "From \\\"Trusted Advisor\\\" page : \\\"AWS Basic Support and AWS Developer Support customers can access core security checks and all checks for service quotas.\\\"<br><br>I think it should be A.  (sample solution : https://aws.amazon.com/solutions/implementations/limit-monitor/)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 516939,
          "date": "Tue 04 Jan 2022 20:28",
          "username": "AndySH",
          "content": "Answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#519",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company's Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.<br>The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.<br>Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost- efficient? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#519",
          "answers": [
            {
              "choice": "<p>A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13296,
          "date": "Mon 27 Sep 2021 16:27",
          "username": "MoonIbranthovicMultiAZDerekKeyAWSum1",
          "content": "I will go with \\\"C & E\\\".<br>A: 6TB will not resolve the issue permanently.<br>B: the issue in the question is writing issue. So why we need read replica!<br>C: Kinesis is always best for IoT and load.<br>D: does not make sense.<br>E: the question does not say anything about keeping the same DB architect! Dynamo is so scalable, for indefinitely solution.I was thinking about B, but i read your command, and i agree with you.<br>It's C and EActually B is better than E.  Dynamo will add quite some cost, which is not according to the requirement.<br>Generally Aurora will perform better on writes than MySQL. Offloading the read queries (which are nto problematic, but still add load on the IO subsystem) to read replica will leave more room for your writes.<br>AMD Aurora is very scalable, just like Dynamo, though lacking the single-millisecond response timesYou are wrong. The problem is related to writing information to DB.  This Aurora implementation will have one WRITER. The problem will persist.Thanks for pointing out why E is valid. I didn't think of it in that way",
          "upvote_count": "344231",
          "selected_answers": ""
        },
        {
          "id": 14566,
          "date": "Thu 30 Sep 2021 03:16",
          "username": "IbranthovicMultiAZDerekKey",
          "content": "I was thinking about B, but i read your command, and i agree with you.<br>It's C and EActually B is better than E.  Dynamo will add quite some cost, which is not according to the requirement.<br>Generally Aurora will perform better on writes than MySQL. Offloading the read queries (which are nto problematic, but still add load on the IO subsystem) to read replica will leave more room for your writes.<br>AMD Aurora is very scalable, just like Dynamo, though lacking the single-millisecond response timesYou are wrong. The problem is related to writing information to DB.  This Aurora implementation will have one WRITER. The problem will persist.",
          "upvote_count": "423",
          "selected_answers": ""
        },
        {
          "id": 143321,
          "date": "Fri 15 Oct 2021 14:50",
          "username": "MultiAZDerekKey",
          "content": "Actually B is better than E.  Dynamo will add quite some cost, which is not according to the requirement.<br>Generally Aurora will perform better on writes than MySQL. Offloading the read queries (which are nto problematic, but still add load on the IO subsystem) to read replica will leave more room for your writes.<br>AMD Aurora is very scalable, just like Dynamo, though lacking the single-millisecond response timesYou are wrong. The problem is related to writing information to DB.  This Aurora implementation will have one WRITER. The problem will persist.",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 425116,
          "date": "Fri 05 Nov 2021 20:32",
          "username": "DerekKey",
          "content": "You are wrong. The problem is related to writing information to DB.  This Aurora implementation will have one WRITER. The problem will persist.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 449667,
          "date": "Sun 07 Nov 2021 00:51",
          "username": "AWSum1",
          "content": "Thanks for pointing out why E is valid. I didn't think of it in that way",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 10469,
          "date": "Sun 19 Sep 2021 20:03",
          "username": "dpvnme",
          "content": "C&E would be my choice",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 693196,
          "date": "Wed 12 Oct 2022 17:20",
          "username": "joanneli77",
          "content": "I'm not sure how you can assume DynamoDB is appropriate for the data if it has already been deployed to RDS.Are we assuming intelligence, or assuming stupidity of prior engineers?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 627165,
          "date": "Tue 05 Jul 2022 02:48",
          "username": "KiraguJohn",
          "content": "If you change RDS to Dynamo DB will you also not be required to make some changes on Nodejs code as well?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497588,
          "date": "Thu 09 Dec 2021 11:16",
          "username": "cldy",
          "content": "C.  Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data<br>E.  Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494628,
          "date": "Sun 05 Dec 2021 21:14",
          "username": "AzureDP900",
          "content": "C & E is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411125,
          "date": "Wed 03 Nov 2021 11:38",
          "username": "WhyIronMan",
          "content": "I'll go with C,E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346943,
          "date": "Wed 03 Nov 2021 04:27",
          "username": "Waiweng",
          "content": "It's C&E",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 316495,
          "date": "Tue 02 Nov 2021 10:49",
          "username": "awsexamprep47",
          "content": "Going with C&E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291695,
          "date": "Tue 02 Nov 2021 05:36",
          "username": "Kian1",
          "content": "going with CE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 282810,
          "date": "Fri 29 Oct 2021 00:46",
          "username": "Trap_D0_rEbi",
          "content": "CD<br>\\\"The API Servers are constantly overloaded\\\" only one answer addresses the API limit issue, and that's D.  D must be one of the answers. As someone else stated, Kinesis makes the most sense here. Kinesis will address the high write latency with Lambda to do processing/transforms and more API servers will address the API bottleneck.Dude you need to change the way you think. If you want to add more instances how many you will add to fix the load issue permanently? Also is it cost efficient to add more servers???!",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 286744,
          "date": "Sun 31 Oct 2021 01:49",
          "username": "Ebi",
          "content": "Dude you need to change the way you think. If you want to add more instances how many you will add to fix the load issue permanently? Also is it cost efficient to add more servers???!",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 280237,
          "date": "Wed 27 Oct 2021 10:00",
          "username": "Ebi",
          "content": "I go with CE",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 279692,
          "date": "Tue 26 Oct 2021 07:52",
          "username": "Firststack",
          "content": "C & E for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251888,
          "date": "Tue 26 Oct 2021 02:03",
          "username": "Bulti",
          "content": "C & E is the right answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 247539,
          "date": "Sun 24 Oct 2021 23:32",
          "username": "petebear55",
          "content": "YES B WILL NOT BE 'cost efficient' c and E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243478,
          "date": "Sun 24 Oct 2021 10:45",
          "username": "T14102020",
          "content": "Correct is CE.  Kinesis + DynamoDB",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230837,
          "date": "Thu 21 Oct 2021 14:56",
          "username": "jackdryan",
          "content": "I'll go with C,E",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#520",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing a system that will collect and store data from 2,000 internet-connected sensors. Each sensor produces 1 KB of data every second. The data must be available for analysis within a few seconds of it being sent to the system and stored for analysis indefinitely.<br>Which is the MOST cost-effective solution for collecting and storing the data?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#520",
          "answers": [
            {
              "choice": "<p>A. Put each record in Amazon Kinesis Data Streams. Use an AWS Lambda function to write each record to an object in Amazon S3 with a prefix that organizes the records by hour and hashes the record's key. Analyze recent data from Kinesis Data Streams and historical data from Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Put each record in Amazon Kinesis Data Streams. Set up Amazon Kinesis Data Firehouse to read records from the stream and group them into objects in Amazon S3. Analyze recent data from Kinesis Data Streams and historical data from Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Put each record into an Amazon DynamoDB table. Analyze the recent data by querying the table. Use an AWS Lambda function connected to a DynamoDB stream to group records together, write them into objects in Amazon S3, and then delete the record from the DynamoDB table. Analyze recent data from the DynamoDB table and historical data from Amazon S3<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Put each record into an object in Amazon S3 with a prefix what organizes the records by hour and hashes the record's key. Use S3 lifecycle management to transition objects to S3 infrequent access storage to reduce storage costs. Analyze recent and historical data by accessing the data in Amazon S3<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13294,
          "date": "Fri 24 Sep 2021 10:08",
          "username": "MoonByrneydijesim222",
          "content": "I prefer \\\"B\\\" for scalability and cost-effectiveness..<br>Even I like A, for grouping by prefixes. But that is not a requirement in the question. Plus answer B is saying \\\"group them into objects in amazon S3\\\". So it has some sort of classification for the streams in groups...maybe per second!!<br>So, my preference is B. Custom prefixes are possible with Firehose (option B):<br>https://aws.amazon.com/blogs/big-data/amazon-kinesis-data-firehose-custom-prefixes-for-amazon-s3-objects/cost for s3 puts : 24 hr/day* 60 min/hr* 60 sec/min * 2000 req/sec* 0.005e-3 usd/req = 864 $/day<br>cost for kinesis data streams: <br>required shards (2) : 24 hr/day * 0.015 $/hr/shard * 2 shards = 0.72 $/day<br>puts (1kb is 1 payload unit):24 hr/day* 60 min/hr* 60 sec/min * 2000 req/sec * 0.014e-6 $/req = 2.42 $/day<br><br>huge cost difference..",
          "upvote_count": "2516",
          "selected_answers": ""
        },
        {
          "id": 712807,
          "date": "Mon 07 Nov 2022 05:40",
          "username": "Byrney",
          "content": "Custom prefixes are possible with Firehose (option B):<br>https://aws.amazon.com/blogs/big-data/amazon-kinesis-data-firehose-custom-prefixes-for-amazon-s3-objects/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 341601,
          "date": "Fri 05 Nov 2021 10:43",
          "username": "dijesim222",
          "content": "cost for s3 puts : 24 hr/day* 60 min/hr* 60 sec/min * 2000 req/sec* 0.005e-3 usd/req = 864 $/day<br>cost for kinesis data streams: <br>required shards (2) : 24 hr/day * 0.015 $/hr/shard * 2 shards = 0.72 $/day<br>puts (1kb is 1 payload unit):24 hr/day* 60 min/hr* 60 sec/min * 2000 req/sec * 0.014e-6 $/req = 2.42 $/day<br><br>huge cost difference..",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 11146,
          "date": "Thu 23 Sep 2021 03:26",
          "username": "huhupaidpvnmejar0d",
          "content": "I prefer B. on second read, i'll go with B tooB is better than A , since its about cost-effective solution.<br>Choosing A involves lambda - that only adds cost to the equation.",
          "upvote_count": "1231",
          "selected_answers": ""
        },
        {
          "id": 11881,
          "date": "Thu 23 Sep 2021 10:22",
          "username": "dpvnmejar0d",
          "content": "on second read, i'll go with B tooB is better than A , since its about cost-effective solution.<br>Choosing A involves lambda - that only adds cost to the equation.",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 212604,
          "date": "Fri 22 Oct 2021 13:11",
          "username": "jar0d",
          "content": "B is better than A , since its about cost-effective solution.<br>Choosing A involves lambda - that only adds cost to the equation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 660507,
          "date": "Mon 05 Sep 2022 21:42",
          "username": "epomatti",
          "content": "B - Firehose is cheaper than lambda. There is no requirement stated for manipulating the data, hence no requirement for Lambda making A incorrect.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 654381,
          "date": "Tue 30 Aug 2022 21:45",
          "username": "Rocketeer",
          "content": "B is more practical. I can buffer, group and write data to S3 every 60 secs. I do not want to write a file to S3 every seconds using the lambda.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 603185,
          "date": "Wed 18 May 2022 08:53",
          "username": "bobsmith2000",
          "content": "B no-brainer.<br>Data Streams for injecting data and realtime processing, Fire Hose for buffering and storing in S3.<br>Classic.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 557878,
          "date": "Mon 28 Feb 2022 06:26",
          "username": "Alexey79user0001",
          "content": "“Within a few seconds of being submitted to the system, the data must be accessible for processing”<br>Kinesis Firehouse buffer time minimum 60 sec.<br>Real-time solution is required, which replaces Firehouse by Lamda.<br>https://aws.amazon.com/kinesis/B is right answer as you can useAmazon Kinesis Data Streams for recent data and s3 for historical ,",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 602093,
          "date": "Sun 15 May 2022 15:04",
          "username": "user0001",
          "content": "B is right answer as you can useAmazon Kinesis Data Streams for recent data and s3 for historical ,",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497375,
          "date": "Thu 09 Dec 2021 07:12",
          "username": "cldy",
          "content": "B.  Put each record in Amazon Kinesis Data Streams. Set up Amazon Kinesis Data Firehouse to read records from the stream and group them into objects in Amazon S3. Analyze recent data from Kinesis Data Streams and historical data from Amazon S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494630,
          "date": "Sun 05 Dec 2021 21:18",
          "username": "AzureDP900",
          "content": "B is my choice",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484674,
          "date": "Tue 23 Nov 2021 02:11",
          "username": "acloudguru",
          "content": "B is the right answer. Kinesis Data stream with Kinesis Forehouse reading and buffering from it to write to S3 is a standard ingestion pattern for ingesting IoT data.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 411128,
          "date": "Sun 07 Nov 2021 10:36",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346944,
          "date": "Sat 06 Nov 2021 22:22",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346708,
          "date": "Fri 05 Nov 2021 11:40",
          "username": "blackgamer",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 316497,
          "date": "Thu 04 Nov 2021 12:51",
          "username": "awsexamprep47",
          "content": "B is the answer<br>Perfect use case for Kinesis Data Stream,Kinesis Firehose & S-3 combination",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291704,
          "date": "Sun 31 Oct 2021 05:33",
          "username": "Kian1",
          "content": "it is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 288695,
          "date": "Thu 28 Oct 2021 04:41",
          "username": "lechuklechuk",
          "content": "A.  <br>Firehose has ~60 seconds latencyNevermind, It's B",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 288698,
          "date": "Sun 31 Oct 2021 00:18",
          "username": "lechuk",
          "content": "Nevermind, It's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280239,
          "date": "Wed 27 Oct 2021 13:28",
          "username": "Ebi",
          "content": "Definitely B",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 251890,
          "date": "Tue 26 Oct 2021 23:28",
          "username": "Bulti",
          "content": "B is the right answer. Kinesis Data stream with Kinesis Forehouse reading and buffering from it to write to S3 is a standard ingestion pattern for ingesting IoT data.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#521",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An auction website enables users to bid on collectible items. The auction rules require that each bid is processed only once and in the order it was received. The current implementation is based on a fleet of Amazon EC2 web servers that write bid records into Amazon Kinesis Data Streams. A single t2.large instance has a cron job that runs the bid processor, which reads incoming bids from Kinesis Data Streams and processes each bid. The auction site is growing in popularity, but users are complaining that some bids are not registering.<br>Troubleshooting indicates that the bid processor is too slow during peak demand hours, sometimes crashes while processing, and occasionally loses track of which records is being processed.<br>What changes should make the bid processing more reliable?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#521",
          "answers": [
            {
              "choice": "<p>A. Refactor the web application to use the Amazon Kinesis Producer Library (KPL) when posting bids to Kinesis Data Streams. Refactor the bid processor to flag each record in Kinesis Data Streams as being unread, processing, and processed. At the start of each bid processing run, scan Kinesis Data Streams for unprocessed records.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Refactor the web application to post each incoming bid to an Amazon SNS topic in place of Kinesis Data Streams. Configure the SNS topic to trigger an AWS Lambda function that processes each bid as soon as a user submits it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Refactor the web application to post each incoming bid to an Amazon SQS FIFO queue in place of Kinesis Data Streams. Refactor the bid processor to continuously the SQS queue. Place the bid processing EC2 instance in an Auto Scaling group with a minimum and a maximum size of 1.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Switch the EC2 instance type from t2.large to a larger general compute instance type. Put the bid processor EC2 instances in an Auto Scaling group that scales out the number of EC2 instances running the bid processor, based on the IncomingRecords metric in Kinesis Data Streams.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13292,
          "date": "Tue 21 Sep 2021 05:20",
          "username": "MoonAShahine2101037Kelvindijesim222chicagomassageseekerdijesim222Alvindo",
          "content": "I prefer \\\"C\\\".<br>FIFO is better in this case compared to Kinesis, as it guarantee the order of the bid.<br>Min Max 1, is okay as the SQS will hold the queue in case of failure of the instance, till it come back again.\\\"Troubleshooting indicates that the bid processor is too slow during peak demand hours\\\".. C will not solve this problem.Yes.<br>But the question is \\\"What changes should make the bid processing more reliable?\\\", only about reliability, not speed.Yes, C.  Only SQS works. More than one EC2 instances in an auto scaling DOESN'T WORK as they are to serve one queue in FIFO.exactly. bids cannot processed in parallel, which rules out D completely, C is the only sensible answer leftSQS is not suitable for Real time bidding. Also SQL FIFO has can scale only to a max 300 messages and 3000 messages (in batch). C for sure doesn't fit the solutionquestion mentions cron job hence no real time or near real time (cron job's max resolution 1 second)kinesis data streams consumes data in the order they are stored which is basically going to be fifo https://aws.amazon.com/kinesis/data-streams/faqs/",
          "upvote_count": "261523221",
          "selected_answers": ""
        },
        {
          "id": 105180,
          "date": "Mon 04 Oct 2021 07:52",
          "username": "AShahine2101037",
          "content": "\\\"Troubleshooting indicates that the bid processor is too slow during peak demand hours\\\".. C will not solve this problem.Yes.<br>But the question is \\\"What changes should make the bid processing more reliable?\\\", only about reliability, not speed.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 259181,
          "date": "Tue 19 Oct 2021 04:21",
          "username": "01037",
          "content": "Yes.<br>But the question is \\\"What changes should make the bid processing more reliable?\\\", only about reliability, not speed.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 336763,
          "date": "Thu 28 Oct 2021 01:09",
          "username": "Kelvindijesim222",
          "content": "Yes, C.  Only SQS works. More than one EC2 instances in an auto scaling DOESN'T WORK as they are to serve one queue in FIFO.exactly. bids cannot processed in parallel, which rules out D completely, C is the only sensible answer left",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 341391,
          "date": "Fri 29 Oct 2021 14:19",
          "username": "dijesim222",
          "content": "exactly. bids cannot processed in parallel, which rules out D completely, C is the only sensible answer left",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 124518,
          "date": "Sat 09 Oct 2021 12:53",
          "username": "chicagomassageseekerdijesim222",
          "content": "SQS is not suitable for Real time bidding. Also SQL FIFO has can scale only to a max 300 messages and 3000 messages (in batch). C for sure doesn't fit the solutionquestion mentions cron job hence no real time or near real time (cron job's max resolution 1 second)",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 341392,
          "date": "Sat 30 Oct 2021 18:47",
          "username": "dijesim222",
          "content": "question mentions cron job hence no real time or near real time (cron job's max resolution 1 second)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 561363,
          "date": "Sat 05 Mar 2022 11:28",
          "username": "Alvindo",
          "content": "kinesis data streams consumes data in the order they are stored which is basically going to be fifo https://aws.amazon.com/kinesis/data-streams/faqs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 44449,
          "date": "Fri 24 Sep 2021 08:39",
          "username": "dummafreelyfly84Smart",
          "content": "Correct answer is D as a single consumer is not able to keep up the bids, multiple<br>consumers can be used with Auto Scaling based on the incoming records metricagree with D. <br>https://d0.awsstatic.com/whitepapers/Building_a_Real_Time_Bidding_Platform_on_AWS_v1_Final.pdfThis one refers to Real-Time Bidding which I don't think is the scenario here. <br><br>Check this out: https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/",
          "upvote_count": "2184",
          "selected_answers": ""
        },
        {
          "id": 46807,
          "date": "Sat 25 Sep 2021 07:27",
          "username": "freelyfly84Smart",
          "content": "agree with D. <br>https://d0.awsstatic.com/whitepapers/Building_a_Real_Time_Bidding_Platform_on_AWS_v1_Final.pdfThis one refers to Real-Time Bidding which I don't think is the scenario here. <br><br>Check this out: https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/",
          "upvote_count": "84",
          "selected_answers": ""
        },
        {
          "id": 84783,
          "date": "Tue 28 Sep 2021 20:00",
          "username": "Smart",
          "content": "This one refers to Real-Time Bidding which I don't think is the scenario here. <br><br>Check this out: https://aws.amazon.com/blogs/compute/solving-complex-ordering-challenges-with-amazon-sqs-fifo-queues/",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 736944,
          "date": "Tue 06 Dec 2022 16:26",
          "username": "SureNot",
          "content": "C is the best option, but why do we have \\\"Auto Scaling group with a minimum and a maximum size of 1\\\"...",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 713214,
          "date": "Mon 07 Nov 2022 18:44",
          "username": "AjayPrajapati",
          "content": "C is correct<br>kinesis and SNS can have duplicate. App need to handle the duplicate.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 687615,
          "date": "Thu 06 Oct 2022 10:32",
          "username": "JohnPi",
          "content": "Ordering is guaranteed on a shard level of kinesis data streams, but not across all stream. hard to choose",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 684156,
          "date": "Sat 01 Oct 2022 13:07",
          "username": "tomosabc1tomosabc1",
          "content": "The answer is C. <br><br>Because the auction website already used Kinesis Data Stream, but still its bid processor \\\"sometimes crashes while processing, and occasionally loses track of which records is being processed\\\", the question is asking us to make the bid processing more reliable, rather than faster.<br>As for option D, neither \\\"switch to a larger instance type\\\" nor \\\"adding more EC2 instances within an Auto Scaling group\\\" are able to solve aforementioned reliability issue.The answer is C.  I voted for a wrong answer remissly.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 684157,
          "date": "Sat 01 Oct 2022 13:09",
          "username": "tomosabc1",
          "content": "The answer is C.  I voted for a wrong answer remissly.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 654450,
          "date": "Tue 30 Aug 2022 23:56",
          "username": "Rocketeer",
          "content": "I prefer D.  There can be multiple items being auctioned. With kinesis data streams I can get the bids for different items in different shards in order. With FIFO, they will all be going through a single queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 650854,
          "date": "Tue 23 Aug 2022 16:22",
          "username": "Ni_yot",
          "content": "Ans = C. SQS FIFO was made for this.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 638625,
          "date": "Thu 28 Jul 2022 12:13",
          "username": "Enigmaaaaaa",
          "content": "This is between C and D. <br>For D - We need to make sure ordering is in in-place and process it once -Kinesis can do ordering but cant avoid duplicates especially with an ASG, see below link:<br>https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html<br>For C - with FIFO we have dedup already set and ordering- ASG is making sure 1 instance is alive - although its better to scale it according to queue size since max batch processing is 3000 messages.<br>Since the question asks about reliability and not throughput or speed - C will make sure all bids are processed in-order and only once. D Will process in order and quickly but with duplicates.<br>So C is the only valid answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 625604,
          "date": "Fri 01 Jul 2022 08:33",
          "username": "TechX",
          "content": "Answer: C<br>Explanation:<br>A\\B: Not feasible<br>C: FIFO is better in this case compared to Kinesis, as it guarantee the order of the bid. Min Max 1, is okay as the SQS will hold the queue in case of failure of the instance, till it come back again. <br>D: Still it does not solve the ordering issue.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 597758,
          "date": "Fri 06 May 2022 16:18",
          "username": "tartarus23",
          "content": "C.  SQS then Kinesis decouples the architecture and business flow to ensure that all bids are getting sent almost real time.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 596096,
          "date": "Mon 02 May 2022 17:16",
          "username": "aloha123",
          "content": "My problem with D is that it didn't state how the processing of bids is coordinated among the EC2s.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 578129,
          "date": "Wed 30 Mar 2022 09:55",
          "username": "jj22222",
          "content": "C looks right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 500462,
          "date": "Mon 13 Dec 2021 09:24",
          "username": "KiraguJohn",
          "content": "Kinesis just like SQS FIFO provides ordering of records. The only difference is that Kinesis is near real time.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 457137,
          "date": "Sun 07 Nov 2021 17:01",
          "username": "StelSen",
          "content": "People tends to choose Option-C, because of this \\\"The auction rules require that each bid is processed only once and in the order it was received.\\\". But the real problem statement is different. i.e. Slow, Missing etc., All because of consumer side. Option-C stick to 1 instance. No use. Option-D resolves problem. AWS Kinesis Data Stream also taking care of ORDER. Refer https://aws.amazon.com/kinesis/data-streams/faqs/ (Look for word ORDER)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 450654,
          "date": "Sun 07 Nov 2021 13:19",
          "username": "AWSum1",
          "content": "\\\"Troubleshooting indicates that the bid processor is too slow during peak demand hours, sometimes crashes while processing, and occasionally loses track of which records is being processed.<br>What changes should make the bid processing more reliable?\\\"<br><br>Crashes while processing = Needs to be replaced asap to continue processing the bids <br><br>Occasionally loses track = Only happens sometimes not ALL the time<br><br>From troubleshooting , the problem is the BID PROCESSOR <br><br>Then, what changes to make it more RELIABLE = continued service due to crashes and slow processing<br><br>Answer is D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 425202,
          "date": "Sat 06 Nov 2021 07:49",
          "username": "DerekKey",
          "content": "A is wrong - how the bid processor can flag a record in Kinesis Data Stream? It can only read data<br>B is wrong - would be OK if the answer would mention SNS FIFO<br>D is wrong - we don't know a number of shards - we can't have two consumers for the same shard",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#522",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A bank is re-architecting its mainframe-based credit card approval processing application to a cloud-native application on the AWS cloud.<br>The new application will receive up to 1,000 requests per second at peak load. There are multiple steps to each transaction, and each step must receive the result of the previous step. The entire request must return an authorization response within less than 2 seconds with zero data loss. Every request must receive a response. The solution must be Payment Card Industry Data Security Standard (PCI DSS)-compliant.<br>Which option will meet all of the bank's objectives with the LEAST complexity and LOWEST cost while also meeting compliance requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#522",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon API Gateway to process inbound requests using a single AWS Lambda task that performs multiple steps and returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Application Load Balancer with an Amazon ECS cluster on Amazon EC2 Dedicated Instances in a target group to process incoming requests. Use Auto Scaling to scale the cluster out/in based on average CPU utilization. Deploy a web service that processes all of the approval steps and returns a JSON object with the approval status.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application on Amazon EC2 on Dedicated Instances. Use an Elastic Load Balancer in front of a farm of application servers in an Auto Scaling group to handle incoming requests. Scale out/in based on a custom Amazon CloudWatch metric for the number of inbound requests per second after measuring the capacity of a single instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon API Gateway to process inbound requests using a series of AWS Lambda processes, each with an Amazon SQS input queue. As each step completes, it writes its result to the next step's queue. The final step returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 10453,
          "date": "Mon 20 Sep 2021 10:58",
          "username": "dpvnmeheanyYamchisindra",
          "content": "Seems like D would be a better choiceonly problem is that :A Lambda function may run for up to 15 minutes (this is called the Lambda timeout), meaning Lambda is not suited to long-running processes ' .According to the question, the app 'will receive up to 1,000 requests per second at peak load' which means the lambda function will need to keep running.So, both A and D don't look right. that leaves only C. https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html <br>\\\"Allocate tasks to multiple worker nodes: process a high number of credit card validation requests.\\\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html <br>Incline to D",
          "upvote_count": "27151",
          "selected_answers": ""
        },
        {
          "id": 692487,
          "date": "Wed 12 Oct 2022 01:27",
          "username": "heany",
          "content": "only problem is that :A Lambda function may run for up to 15 minutes (this is called the Lambda timeout), meaning Lambda is not suited to long-running processes ' .According to the question, the app 'will receive up to 1,000 requests per second at peak load' which means the lambda function will need to keep running.So, both A and D don't look right. that leaves only C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 581844,
          "date": "Wed 06 Apr 2022 15:37",
          "username": "Yamchisindra",
          "content": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html <br>\\\"Allocate tasks to multiple worker nodes: process a high number of credit card validation requests.\\\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html <br>Incline to D",
          "upvote_count": "51",
          "selected_answers": ""
        },
        {
          "id": 710914,
          "date": "Fri 04 Nov 2022 06:04",
          "username": "sindra",
          "content": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html <br>Incline to D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13657,
          "date": "Fri 24 Sep 2021 02:01",
          "username": "donathonPacoDerekashppetebear55Amitv2706",
          "content": "A<br>A: The process must complete within 2 seconds. This sounds like what Lambda can do.<br>B\\C: Not the most cost efficient compared to Lambda. Scaling may also not be fast enough.<br>D: SQS may have data loss due to DLQ? SQS also does not process in order so this may be another problem unless you use FIFO.SQS guarantee at-least-One deliveryThe entire request must return an authorization response within less than 2 seconds with zero data loss.a will cause too much bottle neckif SQS is used we still have an option to keep storing the message as failed lambda will not delete it ideally.<br><br>But with A - Where that message/reqwill go if lambda fails ?<br><br>Seems D is better choice",
          "upvote_count": "152123",
          "selected_answers": ""
        },
        {
          "id": 43527,
          "date": "Sun 26 Sep 2021 11:55",
          "username": "PacoDerekashp",
          "content": "SQS guarantee at-least-One deliveryThe entire request must return an authorization response within less than 2 seconds with zero data loss.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 46227,
          "date": "Sun 26 Sep 2021 15:48",
          "username": "ashp",
          "content": "The entire request must return an authorization response within less than 2 seconds with zero data loss.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 247557,
          "date": "Sat 23 Oct 2021 22:08",
          "username": "petebear55",
          "content": "a will cause too much bottle neck",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 380550,
          "date": "Fri 05 Nov 2021 13:24",
          "username": "Amitv2706",
          "content": "if SQS is used we still have an option to keep storing the message as failed lambda will not delete it ideally.<br><br>But with A - Where that message/reqwill go if lambda fails ?<br><br>Seems D is better choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 733886,
          "date": "Fri 02 Dec 2022 16:50",
          "username": "timmysixstrings",
          "content": "EC2 is too slow to scale, so that rules out B/C.  <br>The question emphasis is on LEAST complexity and LOWEST cost, so I think A the best option. having individual lambdas for each step will increase the complexity and having SQS in between each will increase both complexity and cost",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 713221,
          "date": "Mon 07 Nov 2022 19:02",
          "username": "AjayPrajapati",
          "content": "EC2 scale in/out is slow. SQS can be slow tooand it is asych.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 673468,
          "date": "Mon 19 Sep 2022 18:29",
          "username": "Rocketeertomosabc1",
          "content": "A for me.<br>Using SQS makes it asynch. How do you respond back to the API call ?<br>Also multiple lambdas and SQS will be slower than option A.  Step function is a better option.\\\"Using SQS makes it asynch. How do you respond back to the API call ?\\\"<br>A good point! This definitely rules option D out.",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 684167,
          "date": "Sat 01 Oct 2022 13:33",
          "username": "tomosabc1",
          "content": "\\\"Using SQS makes it asynch. How do you respond back to the API call ?\\\"<br>A good point! This definitely rules option D out.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 670211,
          "date": "Thu 15 Sep 2022 20:37",
          "username": "dcdcdc3",
          "content": "D is preferred as it is more robust, with cloud native services etc, but 1000 requests per second with API GW may make this much more expensive, maybe this is why C is proposed",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 637709,
          "date": "Wed 27 Jul 2022 02:57",
          "username": "hilft",
          "content": "D is the only choice here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 607128,
          "date": "Wed 25 May 2022 11:01",
          "username": "bobsmith2000",
          "content": "No data loss, full compliance to pci",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 560059,
          "date": "Thu 03 Mar 2022 13:52",
          "username": "pal40sgczarno",
          "content": "D.  gp3 can't set IOPSnot only you specifically selected C, then you also went on to comment D. .. and reference to the previous question.<br>Other than that I think it is D. .. APIGW + Lambda + SQS",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 576082,
          "date": "Sun 27 Mar 2022 10:55",
          "username": "czarno",
          "content": "not only you specifically selected C, then you also went on to comment D. .. and reference to the previous question.<br>Other than that I think it is D. .. APIGW + Lambda + SQS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513353,
          "date": "Thu 30 Dec 2021 13:34",
          "username": "cldy",
          "content": "D is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499330,
          "date": "Sat 11 Dec 2021 11:32",
          "username": "cldy",
          "content": "D.  Create an Amazon API Gateway to process inbound requests using a series of AWS Lambda processes, each with an Amazon SQS input queue. As each step completes, it writes its result to the next stepג€™s queue. The final step returns a JSON object with the approval status. Open a support case to increase the limit for the number of concurrent Lambdas to allow room for bursts of activity due to the new application.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490413,
          "date": "Tue 30 Nov 2021 05:52",
          "username": "acloudguru",
          "content": "D, as SQS is also PCI DSS compliance <br>https://aws.amazon.com/compliance/services-in-scope/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 450657,
          "date": "Sun 07 Nov 2021 08:20",
          "username": "AWSum1",
          "content": "A <br><br>My simple understanding:<br>Multiple Lambda functions for each step can add up to 300ms/step",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 441676,
          "date": "Sat 06 Nov 2021 20:13",
          "username": "student22",
          "content": "A<br><br>Why not D? The question is asking for theleast complex working solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435669,
          "date": "Sat 06 Nov 2021 08:21",
          "username": "mustafa1p",
          "content": "It should be 'C' since emphasis is on compliance with PCI-DSS as long as the standard allows the app to be on shared tenants.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 431202,
          "date": "Fri 05 Nov 2021 23:51",
          "username": "Kopa",
          "content": "i think A is more complex because of all functions are included in one lambda, also it does not offer low cost as it will run all the time, while several lambda they will process the function once and pass to the other lambda. Im more on D then A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 425210,
          "date": "Fri 05 Nov 2021 21:45",
          "username": "DerekKeyDerekKeySalmariaz",
          "content": "D is wrong - the answer is not mentioning step functionshttps://d1.awsstatic.com/whitepapers/compliance/pci-dss-compliance-on-aws.pdfAWS SQS is PCI complaint<br>https://aws.amazon.com/compliance/services-in-scope/",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 425211,
          "date": "Fri 05 Nov 2021 21:58",
          "username": "DerekKeySalmariaz",
          "content": "https://d1.awsstatic.com/whitepapers/compliance/pci-dss-compliance-on-aws.pdfAWS SQS is PCI complaint<br>https://aws.amazon.com/compliance/services-in-scope/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 467665,
          "date": "Sun 07 Nov 2021 10:01",
          "username": "Salmariaz",
          "content": "AWS SQS is PCI complaint<br>https://aws.amazon.com/compliance/services-in-scope/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#523",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is migrating a 10 TB PostgreSQL database to Amazon RDS for PostgreSQL. The company's internet link is 50 MB with a VPN in the<br>Amazon VPC, and the Solutions Architect needs to migrate the data and synchronize the changes before the cutover. The cutover must take place within an 8-day period.<br>What is the LEAST complex method of migrating the database securely and reliably?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#523",
          "answers": [
            {
              "choice": "<p>A. Order an AWS Snowball device and copy the database using the AWS DMS. When the database is available in Amazon S3, use AWS DMS to load it to Amazon RDS, and configure a job to synchronize changes before the cutover.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS DMS job to continuously replicate the data from on premises to AWS. Cutover to Amazon RDS after the data is synchronized.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Order an AWS Snowball device and copy a database dump to the device. After the data has been copied to Amazon S3, import it to the Amazon RDS instance. Set up log shipping over a VPN to synchronize changes before the cutover.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Order an AWS Snowball device and copy the database by using the AWS Schema Conversion Tool. When the data is available in Amazon S3, use AWS DMS to load it to Amazon RDS, and configure a job to synchronize changes before the cutover.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11485,
          "date": "Sun 19 Sep 2021 22:50",
          "username": "donathon94xychenfuten0326redipamostafasookar",
          "content": "Answer is A. <br>B: Not possible. Because transferring 10TB over 50Mbps will take 17 days at least.<br>C: Use DMS to copy not database dump.<br>D: You don’t need SCT since there is no need for conversion.<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.htmlB: It's 10TB over 50MB. .. not 50MbpsQuestion clearly states \\\"50 megabits per second\\\" which is Mbps. MBps is mega bytes per second.<br><br>Scary how people are testing for a cert like this, and don't even know the differenceBut the question does say \\\"MB\\\":<br>\\\"The company's internet link is 50 MB with a VPN in the<br>Amazon VPC. ..\\\"A is right <br>Megabits per second (Mbps) so it will take 17 days IF it is Megabytes per second (MBps) so B it will work",
          "upvote_count": "2511411",
          "selected_answers": ""
        },
        {
          "id": 111161,
          "date": "Sun 10 Oct 2021 19:09",
          "username": "94xychenfuten0326redipa",
          "content": "B: It's 10TB over 50MB. .. not 50MbpsQuestion clearly states \\\"50 megabits per second\\\" which is Mbps. MBps is mega bytes per second.<br><br>Scary how people are testing for a cert like this, and don't even know the differenceBut the question does say \\\"MB\\\":<br>\\\"The company's internet link is 50 MB with a VPN in the<br>Amazon VPC. ..\\\"",
          "upvote_count": "1141",
          "selected_answers": ""
        },
        {
          "id": 553412,
          "date": "Tue 22 Feb 2022 04:34",
          "username": "futen0326redipa",
          "content": "Question clearly states \\\"50 megabits per second\\\" which is Mbps. MBps is mega bytes per second.<br><br>Scary how people are testing for a cert like this, and don't even know the differenceBut the question does say \\\"MB\\\":<br>\\\"The company's internet link is 50 MB with a VPN in the<br>Amazon VPC. ..\\\"",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 697470,
          "date": "Mon 17 Oct 2022 16:36",
          "username": "redipa",
          "content": "But the question does say \\\"MB\\\":<br>\\\"The company's internet link is 50 MB with a VPN in the<br>Amazon VPC. ..\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 587055,
          "date": "Sun 17 Apr 2022 06:58",
          "username": "mostafasookar",
          "content": "A is right <br>Megabits per second (Mbps) so it will take 17 days IF it is Megabytes per second (MBps) so B it will work",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 29999,
          "date": "Mon 27 Sep 2021 11:25",
          "username": "9Ow30cinopi9Ow30syscaothirstylion9Ow30cinopirobsonchirara",
          "content": "B<br>Using the calculator here http://www.calctool.org/CALC/prof/computing/transfer_time it will take 2 days to transfer 10TB over 50MB line.It will take 17.6606 days using that tool, NOT 2 daysI checked again, the speed is 50 MB not 50 Mb so it will take 2. Please try again.it's 50 Mb, not 50MB, if you check one more time.Its Mbps (megabits) so 17 days.The company's internet link is 50 MB with a VPNAhh, it's 2.20758 days. Correct. <br>So answer is BMB is not the same as Mb<br>B is for Byte<br>b is for bit",
          "upvote_count": "153216252",
          "selected_answers": ""
        },
        {
          "id": 30718,
          "date": "Mon 27 Sep 2021 18:26",
          "username": "cinopi9Ow30syscaothirstylion9Ow30cinopi",
          "content": "It will take 17.6606 days using that tool, NOT 2 daysI checked again, the speed is 50 MB not 50 Mb so it will take 2. Please try again.it's 50 Mb, not 50MB, if you check one more time.Its Mbps (megabits) so 17 days.The company's internet link is 50 MB with a VPNAhh, it's 2.20758 days. Correct. <br>So answer is B",
          "upvote_count": "321625",
          "selected_answers": ""
        },
        {
          "id": 31759,
          "date": "Wed 29 Sep 2021 22:33",
          "username": "9Ow30syscaothirstylion",
          "content": "I checked again, the speed is 50 MB not 50 Mb so it will take 2. Please try again.it's 50 Mb, not 50MB, if you check one more time.Its Mbps (megabits) so 17 days.",
          "upvote_count": "216",
          "selected_answers": ""
        },
        {
          "id": 568537,
          "date": "Tue 15 Mar 2022 18:08",
          "username": "syscao",
          "content": "it's 50 Mb, not 50MB, if you check one more time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 479132,
          "date": "Tue 16 Nov 2021 03:41",
          "username": "thirstylion",
          "content": "Its Mbps (megabits) so 17 days.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 31760,
          "date": "Thu 30 Sep 2021 13:56",
          "username": "9Ow30cinopi",
          "content": "The company's internet link is 50 MB with a VPNAhh, it's 2.20758 days. Correct. <br>So answer is B",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 32567,
          "date": "Fri 01 Oct 2021 14:17",
          "username": "cinopi",
          "content": "Ahh, it's 2.20758 days. Correct. <br>So answer is B",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 622558,
          "date": "Sun 26 Jun 2022 15:00",
          "username": "robsonchirara",
          "content": "MB is not the same as Mb<br>B is for Byte<br>b is for bit",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 720072,
          "date": "Thu 17 Nov 2022 00:05",
          "username": "desertlotus1211",
          "content": "Internet is measured in bit per second NOT Bytes per second.Don't know why the question would say MegaBytes...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714368,
          "date": "Wed 09 Nov 2022 08:51",
          "username": "whuzzup",
          "content": "A.  Snowball copies existing data + additional synchronization",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 713245,
          "date": "Mon 07 Nov 2022 19:45",
          "username": "AjayPrajapati",
          "content": "A is right. Snowball can combine with DMS and S3 for faster migration. SCT is not required because it is like to like DB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 708093,
          "date": "Mon 31 Oct 2022 01:26",
          "username": "mnizamu",
          "content": "It says very clearly that \\\"The company's internet link is 50 MB with a VPN.\\\"Therefore, transferring 10TB of data over a 50 MB link will take 2 days 7 hours 33 mins 20 sec. Therefore, the answer should be B.  Calculation:8 bits/1 bytex50 Megabytes/s=400 Megabits/s",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 703995,
          "date": "Tue 25 Oct 2022 17:42",
          "username": "kharakbeer",
          "content": "The internet speed is calculated by bitpersecond and NOT Bytepersecond. B is wrong the answer is A as you don't need SCT when migrating from on-prem postgreSQL to AWS PostgreSQL",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 698176,
          "date": "Tue 18 Oct 2022 13:38",
          "username": "Vizz5585",
          "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 693208,
          "date": "Wed 12 Oct 2022 17:32",
          "username": "joanneli77",
          "content": "50 MB is not 50 mb.The author wrote the question wrong.One is 800% different from the other.Remember networks are measured in 'b'its not 'B'ytes, even though almost all other data is capital B. In either case, data transfer will take too long.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 687633,
          "date": "Thu 06 Oct 2022 11:12",
          "username": "JohnPi",
          "content": "When you're using an Edge device, the data migration process has the following stages:<br><br>-You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.<br>-You ship the Edge device or devices back to AWS.<br>-After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.<br>-AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 687426,
          "date": "Thu 06 Oct 2022 05:50",
          "username": "caveman712",
          "content": "https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.Process.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 684334,
          "date": "Sat 01 Oct 2022 18:03",
          "username": "tomosabc1Cal88",
          "content": "The answer is B. <br>We can figure out the answer by ruling out the wrong ones.<br>A,D(wrong): Neither AWS DMS nor AWS Schema Conversion Tool can be used to copying on premise DB to Showball device.<br>https://aws.amazon.com/dms/schema-conversion-tool/<br>C(wrong): Log Shipping is for SQL server on EC2, rather than AWS RDS for PostgreSQL. <br>https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/ec2-log-shipping.html<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.FeatureNonSupportYou are wrong , DMS can be used with snowball to migrate databases from on-premise to AWS<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 707934,
          "date": "Sun 30 Oct 2022 18:11",
          "username": "Cal88",
          "content": "You are wrong , DMS can be used with snowball to migrate databases from on-premise to AWS<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 670237,
          "date": "Thu 15 Sep 2022 20:59",
          "username": "dcdcdc3Cal88",
          "content": "MB means nothing really. If it is MBps then B is correct, If it is Mbps D is correct (Use SCT):<br>https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_DMSIntegration.html<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.htmlWhy do you need schema conversion if you are migrating to the same DB<br>I think answer A is correct Snowball + DMS",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 707936,
          "date": "Sun 30 Oct 2022 18:12",
          "username": "Cal88",
          "content": "Why do you need schema conversion if you are migrating to the same DB<br>I think answer A is correct Snowball + DMS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 648759,
          "date": "Fri 19 Aug 2022 08:05",
          "username": "jerrykid",
          "content": "C is correct. DMS now support S3<br>The only way to transfer data into Snowball is dump data and copy by Snowball agent, not DMS or SCT. Both DMS and SCT is service and hosted in AWS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 625648,
          "date": "Fri 01 Jul 2022 10:20",
          "username": "she1989",
          "content": "Answer is D: https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Source.PostgreSQL.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 614262,
          "date": "Fri 10 Jun 2022 02:06",
          "username": "azurehunter",
          "content": "Answer is D<br>A is wrong because DMS is cloud service and it cannot extract database offline. It requires VPN or DX for data migration and replication. So, the only solution is to use SCT to extract the data and schemas prior to copy to Snowball.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 553437,
          "date": "Tue 22 Feb 2022 05:13",
          "username": "johnnsmith",
          "content": "D is the correct answer. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_LargeDBs.htmlWhen you're using an Edge device, the data migration process has the following stages:<br><br>You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device.<br>You ship the Edge device or devices back to AWS.<br>After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.<br>AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#524",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The Solutions Architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.<br>What should be done next to complete the update?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#524",
          "answers": [
            {
              "choice": "<p>A. Redirect to the new environment using Amazon Route 53<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Select the Swap Environment URLs option<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace the Auto Scaling launch configuration<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the DNS records to point to the green environment<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 10420,
          "date": "Wed 22 Sep 2021 16:27",
          "username": "awsec2",
          "content": "b.https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 496788,
          "date": "Wed 08 Dec 2021 12:59",
          "username": "cldy",
          "content": "B.  Select the Swap Environment URLs option",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494640,
          "date": "Sun 05 Dec 2021 21:31",
          "username": "AzureDP900",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411142,
          "date": "Tue 02 Nov 2021 09:13",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 371360,
          "date": "Sun 31 Oct 2021 17:12",
          "username": "zolthar_z",
          "content": "The Answer is B, https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 348238,
          "date": "Fri 29 Oct 2021 09:33",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 316508,
          "date": "Sat 23 Oct 2021 12:57",
          "username": "awsexamprep47",
          "content": "B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291725,
          "date": "Fri 22 Oct 2021 01:31",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279659,
          "date": "Tue 19 Oct 2021 18:17",
          "username": "Ebi",
          "content": "B is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 279098,
          "date": "Tue 19 Oct 2021 15:44",
          "username": "tipzzzkirrim",
          "content": "answer is D : <br>https://medium.com/@kumargaurav1247/blue-green-deployment-introduction-68b01d471ddeThat's the method for changing the CNAME entry if you have full control over the environment.In EB you can't change the CNAME record yourself, EB controls that.You have to tell EB what environment it should point the CNAME record to.That is done by changing the FQDN (technically not a URL, but AWS still calls it a URL) for the environment:<br><br>https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 459705,
          "date": "Sun 07 Nov 2021 10:14",
          "username": "kirrim",
          "content": "That's the method for changing the CNAME entry if you have full control over the environment.In EB you can't change the CNAME record yourself, EB controls that.You have to tell EB what environment it should point the CNAME record to.That is done by changing the FQDN (technically not a URL, but AWS still calls it a URL) for the environment:<br><br>https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 252274,
          "date": "Tue 19 Oct 2021 15:20",
          "username": "Bulti",
          "content": "Answer is B.  You need to swap Environment URLs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 245013,
          "date": "Tue 19 Oct 2021 11:28",
          "username": "spring21",
          "content": "B: When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments—blue and green—increases availability and reduces risk. In this Quick Start architecture, the blue environment is the production environment that normally handles live traffic. The CI/CD pipeline architecture creates a clone (green) of the live Elastic Beanstalk environment (blue). The pipeline then swaps the URLs between the two environments.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243542,
          "date": "Tue 19 Oct 2021 08:47",
          "username": "T14102020",
          "content": "Correct is B.  Swap Environment URLs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230852,
          "date": "Mon 18 Oct 2021 16:10",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 152161,
          "date": "Fri 15 Oct 2021 23:51",
          "username": "AWSKrishPhatpetebear55",
          "content": "D: Please note it is B/G deployment and once Updating DNS is saffice. Wondering SWAP would do traffic in 2 directions once new env is ready that is not needed,B is correct.despite everyone disagreeing with us i'm inclined to go a,long with u",
          "upvote_count": "311",
          "selected_answers": ""
        },
        {
          "id": 177455,
          "date": "Sat 16 Oct 2021 21:26",
          "username": "Phat",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 247593,
          "date": "Tue 19 Oct 2021 14:50",
          "username": "petebear55",
          "content": "despite everyone disagreeing with us i'm inclined to go a,long with u",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 151917,
          "date": "Tue 12 Oct 2021 12:24",
          "username": "fullaws",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133201,
          "date": "Sun 10 Oct 2021 23:46",
          "username": "NikkyDicky",
          "content": "B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#525",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a legacy application running on servers on premises. To increase the application's reliability, the company wants to gain actionable insights using application logs. A Solutions Architect has been given following requirements for the solution:<br>✑ Aggregate logs using AWS.<br>✑ Automate log analysis for errors.<br>✑ Notify the Operations team when errors go beyond a specified threshold.<br>What solution meets the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#525",
          "answers": [
            {
              "choice": "<p>A. Install Amazon Kinesis Agent on servers, send logs to Amazon Kinesis Data Streams and use Amazon Kinesis Data Analytics to identify errors, create an Amazon CloudWatch alarm to notify the Operations team of errors<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Install an AWS X-Ray agent on servers, send logs to AWS Lambda and analyze them to identify errors, use Amazon CloudWatch Events to notify the Operations team of errors.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Install Logstash on servers, send logs to Amazon S3 and use Amazon Athena to identify errors, use sendmail to notify the Operations team of errors.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Install the Amazon CloudWatch agent on servers, send logs to Amazon CloudWatch Logs and use metric filters to identify errors, create a CloudWatch alarm to notify the Operations team of errors.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11496,
          "date": "Mon 20 Sep 2021 22:29",
          "username": "donathonAWS2020LunchTimeAWSPro24AWSPro24AWSPro24JAWS1600",
          "content": "D<br>A: Amazon Kinesis Data Analytics used for data analytics.<br>B: Cannot be implemented on premise.<br>C: Athena is servers SQL based query system. Should use SNS instead of sendmail.<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.htmlI think the Answer is A.  D may work if we only need to identify the error not analyze itand you would need to put error filter such as 400x, 500x. What happens if we have error tat is not specified in metric filterThis is obviously a close call between options A and D.  Both appear to fulfill the requirements. However, AWS2020 makes a great point – what if the error is not specified in the error filter? Consequently, option D may not report on some errors. Also, slides 17 and 18 in the following AWS presentation would lead me to believe “A” is the answer they are looking for on the exam. https://www.slideshare.net/AmazonWebServices/realtime-application-monitoring-with-amazon-kinesis-and-amazon-cloudwatch-aws-online-tech-talksI agree with A. It says \\\"Log Analysis\\\" not \\\"Log Analytics\\\"CloudWatch Logs can do the job.https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.htmlSorry I meant D. Also the CloudWatch landing page includes the term \\\"actionable insights\\\" <br> https://aws.amazon.com/cloudwatch/<br><br>I don't see any reason why A would not work but it seems like overkill for just error counting.Interesting.It seems that CloudWatch Logs is even built on top of Kinesis https://forums.aws.amazon.com/thread.jspa?threadID=157966Here is what A is missing. Collecting the logs from on-prem servers and sending to kinesis. Option A does not provide solution for that piece.",
          "upvote_count": "22341232",
          "selected_answers": ""
        },
        {
          "id": 18211,
          "date": "Thu 23 Sep 2021 00:36",
          "username": "AWS2020LunchTime",
          "content": "I think the Answer is A.  D may work if we only need to identify the error not analyze itand you would need to put error filter such as 400x, 500x. What happens if we have error tat is not specified in metric filterThis is obviously a close call between options A and D.  Both appear to fulfill the requirements. However, AWS2020 makes a great point – what if the error is not specified in the error filter? Consequently, option D may not report on some errors. Also, slides 17 and 18 in the following AWS presentation would lead me to believe “A” is the answer they are looking for on the exam. https://www.slideshare.net/AmazonWebServices/realtime-application-monitoring-with-amazon-kinesis-and-amazon-cloudwatch-aws-online-tech-talks",
          "upvote_count": "34",
          "selected_answers": ""
        },
        {
          "id": 125070,
          "date": "Wed 13 Oct 2021 07:21",
          "username": "LunchTime",
          "content": "This is obviously a close call between options A and D.  Both appear to fulfill the requirements. However, AWS2020 makes a great point – what if the error is not specified in the error filter? Consequently, option D may not report on some errors. Also, slides 17 and 18 in the following AWS presentation would lead me to believe “A” is the answer they are looking for on the exam. https://www.slideshare.net/AmazonWebServices/realtime-application-monitoring-with-amazon-kinesis-and-amazon-cloudwatch-aws-online-tech-talks",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 42689,
          "date": "Sat 25 Sep 2021 09:05",
          "username": "AWSPro24AWSPro24AWSPro24JAWS1600",
          "content": "I agree with A. It says \\\"Log Analysis\\\" not \\\"Log Analytics\\\"CloudWatch Logs can do the job.https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.htmlSorry I meant D. Also the CloudWatch landing page includes the term \\\"actionable insights\\\" <br> https://aws.amazon.com/cloudwatch/<br><br>I don't see any reason why A would not work but it seems like overkill for just error counting.Interesting.It seems that CloudWatch Logs is even built on top of Kinesis https://forums.aws.amazon.com/thread.jspa?threadID=157966Here is what A is missing. Collecting the logs from on-prem servers and sending to kinesis. Option A does not provide solution for that piece.",
          "upvote_count": "1232",
          "selected_answers": ""
        },
        {
          "id": 42691,
          "date": "Sat 25 Sep 2021 13:40",
          "username": "AWSPro24AWSPro24JAWS1600",
          "content": "Sorry I meant D. Also the CloudWatch landing page includes the term \\\"actionable insights\\\" <br> https://aws.amazon.com/cloudwatch/<br><br>I don't see any reason why A would not work but it seems like overkill for just error counting.Interesting.It seems that CloudWatch Logs is even built on top of Kinesis https://forums.aws.amazon.com/thread.jspa?threadID=157966Here is what A is missing. Collecting the logs from on-prem servers and sending to kinesis. Option A does not provide solution for that piece.",
          "upvote_count": "232",
          "selected_answers": ""
        },
        {
          "id": 42694,
          "date": "Sat 25 Sep 2021 17:27",
          "username": "AWSPro24",
          "content": "Interesting.It seems that CloudWatch Logs is even built on top of Kinesis https://forums.aws.amazon.com/thread.jspa?threadID=157966",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 98572,
          "date": "Sat 09 Oct 2021 00:58",
          "username": "JAWS1600",
          "content": "Here is what A is missing. Collecting the logs from on-prem servers and sending to kinesis. Option A does not provide solution for that piece.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 13285,
          "date": "Mon 20 Sep 2021 23:46",
          "username": "MoonSD13Student1950",
          "content": "I would for with A. <br>https://docs.aws.amazon.com/kinesis-agent-windows/latest/userguide/what-is-kinesis-agent-windows.html<br>https://medium.com/@khandelwal12nidhi/build-log-analytic-solution-on-aws-cc62a70057b2Kinesis agent can not forward logs to cloudwatch, how the cloudwatch alarm will be triggered? option A is missing this partKinesis agent can forward logs to AWS cloudwatch as per info at this blog<br>https://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/",
          "upvote_count": "1441",
          "selected_answers": ""
        },
        {
          "id": 330391,
          "date": "Tue 02 Nov 2021 01:35",
          "username": "SD13Student1950",
          "content": "Kinesis agent can not forward logs to cloudwatch, how the cloudwatch alarm will be triggered? option A is missing this partKinesis agent can forward logs to AWS cloudwatch as per info at this blog<br>https://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 540621,
          "date": "Fri 04 Feb 2022 20:58",
          "username": "Student1950",
          "content": "Kinesis agent can forward logs to AWS cloudwatch as per info at this blog<br>https://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 733914,
          "date": "Fri 02 Dec 2022 17:35",
          "username": "timmysixstrings",
          "content": "I think the answer is D, even though A would work too. <br>A is a more complex and Kinesis Data Analytics is built for Analytics. while it might be possible to repurpose this for automated log analysis, it's not ideal. CloudWatch Metrics is purpose built for automated log analysis.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 713258,
          "date": "Mon 07 Nov 2022 20:00",
          "username": "AjayPrajapati",
          "content": "A - its a legacy app and log format might not be something that cloud watch would detect things as \\\"error\\\". A, you have flexibility to define more logic and do more \\\"Automation\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 640783,
          "date": "Mon 01 Aug 2022 19:31",
          "username": "ibrahimsow",
          "content": "The answer is D:<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 544641,
          "date": "Thu 10 Feb 2022 16:03",
          "username": "RVivek",
          "content": "D. <br>The key is \\\"Notify the Operations team when errors go beyond a specified threshold\\\" <br>That says \\\" metric filters \\\" and a defined threshold",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 540620,
          "date": "Fri 04 Feb 2022 20:56",
          "username": "Student1950",
          "content": "As per the following AWS blog from 2018, correct answer seems to be A<br>https://aws.amazon.com/blogs/big-data/collect-parse-transform-and-stream-windows-events-logs-and-metrics-using-amazon-kinesis-agent-for-microsoft-windows/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494642,
          "date": "Sun 05 Dec 2021 21:33",
          "username": "AzureDP900",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494162,
          "date": "Sun 05 Dec 2021 09:31",
          "username": "cldy",
          "content": "D.  Install the Amazon CloudWatch agent on servers, send logs to Amazon CloudWatch Logs and use metric filters to identify errors, create a CloudWatch alarm to notify the Operations team of errors.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491482,
          "date": "Wed 01 Dec 2021 10:36",
          "username": "AzureDP900",
          "content": "D is right, there is no need of kinesis data analytics for this .",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 436464,
          "date": "Thu 04 Nov 2021 22:21",
          "username": "denccc",
          "content": "I go for D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 421625,
          "date": "Wed 03 Nov 2021 21:10",
          "username": "blackgamer",
          "content": "I think A is better suited than D.  <br>Cloudwatch agent can only install on Amazon Linux 2. Since this is legacy applicaion, I am assuming they are on different OS. Kinesis agent can install Redhat Linux 7 , so it is more reasonable here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411143,
          "date": "Wed 03 Nov 2021 03:58",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 348243,
          "date": "Tue 02 Nov 2021 06:32",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 316510,
          "date": "Mon 01 Nov 2021 07:25",
          "username": "awsexamprep47",
          "content": "D is the answer.<br>CW Agent can be installed on On-Prem servers",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291729,
          "date": "Sun 31 Oct 2021 04:01",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279667,
          "date": "Sat 30 Oct 2021 11:40",
          "username": "EbiEbiRocketeer",
          "content": "D is the answerA is not the answer, first of all we don't need real-time, Kinesis is a very good use case of real time log analysis, second we don't need Kinesis Analytics, only automation is required is capturing errors which can be done using CW metric, lastly Kinesis agent although can be installed on-premise server but has more limited OS support compared to CW agent, so for a legacy app is not a good choice:<br><br>for Kinesis agent: \\\"Your operating system must be either Amazon Linux AMI with version 2015.09 or later, or Red Hat Enterprise Linux version 7 or later.\\\"<br>https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html#download-install<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.htmlPart of the requirement is to do aggregations which can be done in kinesis data analytics. Hence I am leaning towards A. ",
          "upvote_count": "581",
          "selected_answers": ""
        },
        {
          "id": 286697,
          "date": "Sat 30 Oct 2021 12:16",
          "username": "EbiRocketeer",
          "content": "A is not the answer, first of all we don't need real-time, Kinesis is a very good use case of real time log analysis, second we don't need Kinesis Analytics, only automation is required is capturing errors which can be done using CW metric, lastly Kinesis agent although can be installed on-premise server but has more limited OS support compared to CW agent, so for a legacy app is not a good choice:<br><br>for Kinesis agent: \\\"Your operating system must be either Amazon Linux AMI with version 2015.09 or later, or Red Hat Enterprise Linux version 7 or later.\\\"<br>https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html#download-install<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.htmlPart of the requirement is to do aggregations which can be done in kinesis data analytics. Hence I am leaning towards A. ",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 654510,
          "date": "Wed 31 Aug 2022 01:19",
          "username": "Rocketeer",
          "content": "Part of the requirement is to do aggregations which can be done in kinesis data analytics. Hence I am leaning towards A. ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#526",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>What combination of steps could a Solutions Architect take to protect a web workload running on Amazon EC2 from DDoS and application layer attacks? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#526",
          "answers": [
            {
              "choice": "<p>A. Put the EC2 instances behind a Network Load Balancer and configure AWS WAF on it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the DNS to Amazon Route 53 and use AWS Shield.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Put the EC2 instances in an Auto Scaling group and configure AWS WAF on it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create and use an Amazon CloudFront distribution and configure AWS WAF on it.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create and use an internet gateway in the VPC and use AWS Shield.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 348250,
          "date": "Mon 25 Oct 2021 02:09",
          "username": "Waiweng",
          "content": "B and D",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 327273,
          "date": "Mon 11 Oct 2021 19:12",
          "username": "CarisB",
          "content": "I go with B and D<br><br>\\\"AWS Shield Standard automatically protects your Amazon Route 53 Hosted Zones from infrastructure layer DDoS attacks\\\"<br>https://aws.amazon.com/shield/?nc1=h_ls&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc<br><br>\\\"AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync.\\\"<br>https://aws.amazon.com/waf/faqs/",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 715096,
          "date": "Thu 10 Nov 2022 10:36",
          "username": "janvandermerwer",
          "content": "Most likely answers:<br>B and D<br>A - NLB Doesn't support WAF<br>C - ASG direct doesn't support waf - needs an ALB/Cloudfront in front<br><br>E - Shield is included by default anyway",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 698181,
          "date": "Tue 18 Oct 2022 13:45",
          "username": "Vizz5585",
          "content": "B and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 627753,
          "date": "Wed 06 Jul 2022 08:54",
          "username": "TechX",
          "content": "100% BD",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 619606,
          "date": "Tue 21 Jun 2022 07:36",
          "username": "KiraguJohn",
          "content": "I have a problem with D because we have not been told whether the web content is static or dynamic. Can we use Cloudfront on a dynamic web content?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 597743,
          "date": "Fri 06 May 2022 15:32",
          "username": "tartarus23",
          "content": "B.  Route 53 and AWS Shields helps in mitigating the flood of DDoS attacks<br>D.  Cloudfront and WAF also aid in preventing DDoS attacks",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 580858,
          "date": "Mon 04 Apr 2022 19:16",
          "username": "roka_ua",
          "content": "Vote BD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 532773,
          "date": "Wed 26 Jan 2022 11:09",
          "username": "shotty1",
          "content": "it is BD",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 494643,
          "date": "Sun 05 Dec 2021 21:34",
          "username": "AzureDP900",
          "content": "B and D is the answer",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 450312,
          "date": "Sat 06 Nov 2021 04:28",
          "username": "tonikus",
          "content": "how on Earth could E got marked as an answer?<br>It's B and D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411145,
          "date": "Tue 02 Nov 2021 16:37",
          "username": "WhyIronMan",
          "content": "I'll go with B, D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 380566,
          "date": "Wed 27 Oct 2021 10:26",
          "username": "Amitv2706",
          "content": "B and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346738,
          "date": "Tue 19 Oct 2021 12:40",
          "username": "blackgamer",
          "content": "B and D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 317728,
          "date": "Mon 11 Oct 2021 06:33",
          "username": "nitinz",
          "content": "B and D is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 316515,
          "date": "Mon 04 Oct 2021 23:52",
          "username": "awsexamprep47",
          "content": "B&D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 316205,
          "date": "Mon 27 Sep 2021 18:24",
          "username": "JJu",
          "content": "i go with B, D. <br><br>-\tAWS Shield<br>\tAmazon CloudFront distributions<br>\tAmazon Route 53 hosted zones<br>\tAWS Global Accelerator accelerators<br>\tApplication load balancers<br>\tElastic Load Balancing (ELB) load balancers<br>\tAmazon Elastic Compute Cloud (Amazon EC2) Elastic IP addresses<br>-\tAWS WAF<br>\tAmazon CloudFront<br>\tAmazon API Gateway REST API<br>\tApplication Load Balancer <br>\tAWS AppSync GraphQL APIi refer this link :<br>https://docs.aws.amazon.com/ko_kr/waf/latest/developerguide/waf-chapter.html <br>https://docs.aws.amazon.com/ko_kr/waf/latest/developerguide/shield-chapter.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#527",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A photo-sharing and publishing company receives 10,000 to 150,000 images daily. The company receives the images from multiple suppliers and users registered with the service. The company is moving to AWS and wants to enrich the existing metadata by adding data using Amazon Rekognition.<br>The following is an example of the additional data:<br><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0034700001.png\" class=\"in-exam-image\"><br>As part of the cloud migration program, the company uploaded existing image data to Amazon S3 and told users to upload images directly to Amazon S3.<br>What should the Solutions Architect do to support these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#527",
          "answers": [
            {
              "choice": "<p>A. Trigger AWS Lambda based on an S3 event notification to create additional metadata using Amazon Rekognition. Use Amazon DynamoDB to store the metadata and Amazon ES to create an index. Use a web front-end to provide search capabilities backed by Amazon ES.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Kinesis to stream data based on an S3 event. Use an application running in Amazon EC2 to extract metadata from the images. Then store the data on Amazon DynamoDB and Amazon CloudSearch and create an index. Use a web front-end with search capabilities backed by CloudSearch.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Start an Amazon SQS queue based on S3 event notifications. Then have Amazon SQS send the metadata information to Amazon DynamoDB.  An application running on Amazon EC2 extracts data from Amazon Rekognition using the API and adds data to DynamoDB and Amazon ES. Use a web front-end to provide search capabilities backed by Amazon ES.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Trigger AWS Lambda based on an S3 event notification to create additional metadata using Amazon Rekognition. Use Amazon RDS MySQL Multi-AZ to store the metadata information and use Lambda to create an index. Use a web front-end with search capabilities backed by Lambda.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 317729,
          "date": "Sun 03 Oct 2021 06:42",
          "username": "nitinz",
          "content": "A is answer",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 698190,
          "date": "Tue 18 Oct 2022 13:49",
          "username": "Vizz5585",
          "content": "A is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 693221,
          "date": "Wed 12 Oct 2022 17:45",
          "username": "joanneli77",
          "content": "I may need to search based on more than one metadata field.DynamoDB searches can't do every field.RDS.I literally had this use case IRL.D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 686564,
          "date": "Wed 05 Oct 2022 06:31",
          "username": "aqiao",
          "content": "why need ES, why not search from ddb directly?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 660651,
          "date": "Tue 06 Sep 2022 03:03",
          "username": "epomatti",
          "content": "A - It makes no sense to use a relational database for this use case.<br><br>Not sure why ES, it should be CloudSearch??",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 595436,
          "date": "Sun 01 May 2022 06:56",
          "username": "pankajrawat",
          "content": "A is answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 515263,
          "date": "Sun 02 Jan 2022 23:52",
          "username": "Buggie",
          "content": "A it is",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 511172,
          "date": "Tue 28 Dec 2021 14:58",
          "username": "RVivek",
          "content": "A is correct<br>https://github.com/aws-samples/lambda-refarch-imagerecognition",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495093,
          "date": "Mon 06 Dec 2021 12:53",
          "username": "cldy",
          "content": "A.  Trigger AWS Lambda based on an S3 event notification to create additional metadata using Amazon Rekognition. Use Amazon DynamoDB to store the metadata and Amazon ES to create an index. Use a web front-end to provide search capabilities backed by Amazon ES.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494644,
          "date": "Sun 05 Dec 2021 21:37",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 491491,
          "date": "Wed 01 Dec 2021 10:45",
          "username": "AzureDP900",
          "content": "A is more cost effective and no need of streams!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 488845,
          "date": "Sun 28 Nov 2021 06:07",
          "username": "backfringe",
          "content": "I'd go with A<br>DynamoDB and Rekognition",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 487045,
          "date": "Fri 26 Nov 2021 03:22",
          "username": "RVD",
          "content": "A is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 484916,
          "date": "Tue 23 Nov 2021 10:40",
          "username": "backfringe",
          "content": "I go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 482933,
          "date": "Sun 21 Nov 2021 01:10",
          "username": "acloudguru",
          "content": "refer to the blog.https://aws.amazon.com/blogs/machine-learning/find-distinct-people-in-a-video-with-amazon-rekognition/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 450787,
          "date": "Thu 28 Oct 2021 19:30",
          "username": "Kopa",
          "content": "Im going for A, Dynamo DB and Amazon Rekognition makes the difference.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411147,
          "date": "Wed 27 Oct 2021 16:44",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#528",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is redesigning an image-viewing and messaging platform to be delivered as SaaS. Currently, there is a farm of virtual desktop infrastructure<br>(VDI) that runs a desktop image-viewing application and a desktop messaging application. Both applications use a shared database to manage user accounts and sharing. Users log in from a web portal that launches the applications and streams the view of the application on the user's machine. The Development Operations team wants to move away from using VDI and wants to rewrite the application.<br>What is the MOST cost-effective architecture that offers both security and ease of management?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#528",
          "answers": [
            {
              "choice": "<p>A. Run a website from an Amazon S3 bucket with a separate S3 bucket for images and messaging data. Call AWS Lambda functions from embedded JavaScript to manage the dynamic content, and use Amazon Cognito for user and sharing management.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Run a website from Amazon EC2 Linux servers, storing the images in Amazon S3, and use Amazon Cognito for user accounts and sharing. Create AWS CloudFormation templates to launch the application by using EC2 user data to install and configure the application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Run a website as an AWS Elastic Beanstalk application, storing the images in Amazon S3, and using an Amazon RDS database for user accounts and sharing. Create AWS CloudFormation templates to launch the application and perform blue/green deployments.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Run a website from an Amazon S3 bucket that authorizes Amazon AppStream to stream applications for a combined image viewer and messenger that stores images in Amazon S3. Have the website use an Amazon RDS database for user accounts and sharing.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 42658,
          "date": "Tue 28 Sep 2021 17:52",
          "username": "AWSPro24Smart",
          "content": "I believe the answer should be A. There are examples of filling in the dynamic elements of S3 websites with Lambda. <br><br>https://aws.amazon.com/blogs/architecture/create-dynamic-contact-forms-for-s3-static-websites-using-aws-lambda-amazon-api-gateway-and-amazon-ses/<br>https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/<br><br>I feel the words \\\"wants to rewrite the application\\\" are key.They aren't looking to move the same code to AppStreah which is App streaming, similar to VDI but scoped at the App level. <br><br>B - EC2 will be more expensive and \\\"EC2 user data\\\" is just silly and wrong<br>C - RDS isn't the best choice for a user store and there is no blue/green requirement<br>D - Don't believe AppStream can be launched from S3.Too Dynamic.Might be possible with Lambda.Can \\\"rewrite the app\\\" means switching from VDI to App Streaming?",
          "upvote_count": "182",
          "selected_answers": ""
        },
        {
          "id": 71525,
          "date": "Fri 01 Oct 2021 06:43",
          "username": "Smart",
          "content": "Can \\\"rewrite the app\\\" means switching from VDI to App Streaming?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 137732,
          "date": "Thu 14 Oct 2021 09:02",
          "username": "inftimmysixstrings",
          "content": "Answer: A<br>A - correct - solution will work and with low cost and management. No infrastructure to manage.<br>B - incorrect - cost of running and managing infrastructure expensive - not easy to maintain<br>C - incorrect - cost of running and managing infrastructure expensive - blue/green more so which requires the database to be external to the environment or data will be lost.<br>D - incorrect - RDS for authentication/authorisation to provide secure access to S3? possible? plus cost of running infrastructure, and AppStream is the same tech as the current streaming solution<br><br>Light reading<br>https://stackoverflow.com/questions/49782492/cognito-user-authorization-to-access-an-s3-object<br>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.htmlI agree the answer is A.  But per your explanation about D, using RDS auth to secure access to S3 is possible. Once authenticated the backend can provide S3-signed URLs. The bucket resource policy could then restrict access to the EC2 instance role",
          "upvote_count": "91",
          "selected_answers": ""
        },
        {
          "id": 733940,
          "date": "Fri 02 Dec 2022 18:21",
          "username": "timmysixstrings",
          "content": "I agree the answer is A.  But per your explanation about D, using RDS auth to secure access to S3 is possible. Once authenticated the backend can provide S3-signed URLs. The bucket resource policy could then restrict access to the EC2 instance role",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716785,
          "date": "Sat 12 Nov 2022 16:33",
          "username": "et22s",
          "content": "Ans: A<br>You can Invoke a Lambda function from a browser using the SDK for JavaScript. <br><br>https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/cross_LambdaForBrowser_javascript_topic.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 715382,
          "date": "Thu 10 Nov 2022 17:25",
          "username": "MarianKowalskiExam",
          "content": "Definitely A as it is the simplest one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 707707,
          "date": "Sun 30 Oct 2022 09:40",
          "username": "nsvijay04b1",
          "content": "A) Java script trigger lambda, S3 is cost effective, cognito for auth<br>https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/using-lambda-functions.html<br>B,C) costly and others explained already<br>D) App stream support user pool /sso/federated users not RDS, not cheap although pasy as u go, ques wants to refactor app from desktop not migrate to another desktop steaming solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704029,
          "date": "Tue 25 Oct 2022 18:31",
          "username": "kharakbeer",
          "content": "A is right. Easy question ya 3azeezy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 693227,
          "date": "Wed 12 Oct 2022 17:50",
          "username": "joanneli77",
          "content": "A has no database of record - where is the data?I went with D since it has a DB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 684840,
          "date": "Sun 02 Oct 2022 15:41",
          "username": "tomosabc1",
          "content": "A(wrong): AWS Lambda function cannot be called by embedded JavaScript directly, API Gateway is required, which is not mentioned by the option.<br>B/C(wrong): These two options involve the use of EC2(EC2 is in used even in the case of Elastic Beanstalk), not cost effective, compared with D. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 684838,
          "date": "Sun 02 Oct 2022 15:40",
          "username": "tomosabc1tomosabc1tomosabc1",
          "content": "A(wrong): AWS Lambda function cannot be called by embedded JavaScript directly, API Gateway is required, which is not mentioned by the option.<br>B/C(wrong): These two options involve the use of EC2(EC2 is in used even in the case of Elastic Beanstalk), not cost effective, compared with D. D(correct): AppStream 2.0 manages the AWS resources required to host and run your applications, scales automatically, and provides access to your users on demand...With AppStream 2.0, you can easily add your existing desktop applications to AWS and enable your users to instantly stream them(*** ease of management ***)...Your applications run on AWS compute resources, and data is never stored on users' devices, which means they always get a high performance, secure experience((*** secure ***)). Unlike traditional on-premises solutions for desktop application streaming, AppStream 2.0 offers pay-as-you-go pricing, with no upfront investment and no infrastructure to maintain(*** Cost effective ***). You can scale instantly and globally, ensuring that your users always have the best possible experience.<br>https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.htmlSomeone might argue that, as the question mentioned, the development operation team wants to move away from using VDI...I doubt whether moving away from VDI means the same as moving away from AppStream 2.0.",
          "upvote_count": "211",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 684839,
          "date": "Sun 02 Oct 2022 15:40",
          "username": "tomosabc1",
          "content": "D(correct): AppStream 2.0 manages the AWS resources required to host and run your applications, scales automatically, and provides access to your users on demand...With AppStream 2.0, you can easily add your existing desktop applications to AWS and enable your users to instantly stream them(*** ease of management ***)...Your applications run on AWS compute resources, and data is never stored on users' devices, which means they always get a high performance, secure experience((*** secure ***)). Unlike traditional on-premises solutions for desktop application streaming, AppStream 2.0 offers pay-as-you-go pricing, with no upfront investment and no infrastructure to maintain(*** Cost effective ***). You can scale instantly and globally, ensuring that your users always have the best possible experience.<br>https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 684842,
          "date": "Sun 02 Oct 2022 15:42",
          "username": "tomosabc1",
          "content": "Someone might argue that, as the question mentioned, the development operation team wants to move away from using VDI...I doubt whether moving away from VDI means the same as moving away from AppStream 2.0.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 671839,
          "date": "Sun 18 Sep 2022 00:50",
          "username": "Dionenonly",
          "content": "A is the answer for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 508200,
          "date": "Fri 24 Dec 2021 01:16",
          "username": "vbalvbal",
          "content": "A is the right Answer. Cognito Identity Pool would help run Lambda using AWS SDK for Javascript.https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/using-lambda-functions.html",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 508206,
          "date": "Fri 24 Dec 2021 01:27",
          "username": "vbal",
          "content": "https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/using-lambda-functions.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497799,
          "date": "Thu 09 Dec 2021 15:32",
          "username": "cldy",
          "content": "A.  Run a website from an Amazon S3 bucket with a separate S3 bucket for images and messaging data. Call AWS Lambda functions from embedded JavaScript to manage the dynamic content, and use Amazon Cognito for user and sharing management.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491497,
          "date": "Wed 01 Dec 2021 10:50",
          "username": "AzureDP900",
          "content": "A is right because they want to discontinue VDI solutions.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450672,
          "date": "Sun 07 Nov 2021 13:23",
          "username": "AWSum1",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450547,
          "date": "Wed 03 Nov 2021 12:35",
          "username": "Bigbearcn",
          "content": "Option A is wrong. JavaScript is run on client-side and cannot load Lambda without api gateway. They don't mention api gateway anywhere.<br>I prefer option D.  Even though it's not perfect. Considering the question require \\\"offers both security and ease of management\\\", It matches AppStream better than others.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449239,
          "date": "Sun 31 Oct 2021 23:58",
          "username": "38745",
          "content": "A.  <br>Cognito as a keyword narrows down to A/B.  CloudFormation is not the case so not B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435731,
          "date": "Sun 31 Oct 2021 21:21",
          "username": "tgv",
          "content": "AAA<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#529",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company would like to implement a serverless application by using Amazon API Gateway, AWS Lambda, and Amazon DynamoDB.  They deployed a proof of concept and stated that the average response time is greater than what their upstream services can accept. Amazon CloudWatch metrics did not indicate any issues with DynamoDB but showed that some Lambda functions were hitting their timeout.<br>Which of the following actions should the Solutions Architect consider to improve performance? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#529",
          "answers": [
            {
              "choice": "<p>A. Configure the AWS Lambda function to reuse containers to avoid unnecessary startup time.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Increase the amount of memory and adjust the timeout on the Lambda function. Complete performance testing to identify the ideal memory and timeout configuration for the Lambda function.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon ElastiCache cluster running Memcached, and configure the Lambda function for VPC integration with access to the Amazon ElastiCache cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable API cache on the appropriate stage in Amazon API Gateway, and override the TTL for individual methods that require a lower TTL than the entire stage.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Increase the amount of CPU, and adjust the timeout on the Lambda function. Complete performance testing to identify the ideal CPU and timeout configuration for the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13644,
          "date": "Tue 21 Sep 2021 01:11",
          "username": "donathon",
          "content": "BD<br>https://lumigo.io/blog/aws-lambda-timeout-best-practices/<br>A: While this will improve the situation, it may not be enough.<br>B: Memory – The amount of memory available to the function during execution. Choose an amount between 128 MB and 3,008 MB in 64 MB increments. Lambda allocates CPU power linearly in proportion to the amount of memory configured. At 1,792 MB, a function has the equivalent of 1 full vCPU (one vCPU-second of credits per second).<br>All calls made to AWS Lambda must complete execution within 900 seconds. The default timeout is 3 seconds, but you can set the timeout to any value between 1 and 900 seconds.<br>C: The problem is not with the DB. <br>D: AWS API Gateway has a max timeout of 29 seconds for all integration types, which includes Lambda as well. It means that any API call coming through API Gateway cannot exceed 29 seconds. It makes sense for most of the APIs except for few high computational ones.<br>E: Increase the memory not CPU.",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 626389,
          "date": "Sun 03 Jul 2022 05:10",
          "username": "aandc",
          "content": "You cannot config container being reused or not",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 548339,
          "date": "Wed 16 Feb 2022 07:36",
          "username": "cannottellname",
          "content": "A.  https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/ (Remember, you can’t depend on a container being reused, since it’s Lambda’s prerogative to create a new one instead.)<br><br>B.  Increase Memory is good option. (https://lumigo.io/learn/aws-lambda-timeout-best-practices/)<br><br>C.  No DynamoDB<br><br>D.  Sounds good to have less load on Lambda. Caching always gives things faster and better, lesser computation for Lambda. (https://lumigo.io/learn/aws-lambda-timeout-best-practices/)<br><br>E.  Not possible. Increase Memory to Increase CPU.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 526294,
          "date": "Tue 18 Jan 2022 05:12",
          "username": "tkanmani76tkanmani76",
          "content": "A and B are right - as they help optimize and improve Lambda performance.Changing to B and D. ",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 547742,
          "date": "Tue 15 Feb 2022 13:22",
          "username": "tkanmani76",
          "content": "Changing to B and D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491503,
          "date": "Wed 01 Dec 2021 10:55",
          "username": "AzureDP900",
          "content": "Before even looking answers I decided to go with B,D . It is most appropriate.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 470051,
          "date": "Sun 07 Nov 2021 17:53",
          "username": "nsei",
          "content": "B & D are the answers",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 436297,
          "date": "Fri 05 Nov 2021 13:59",
          "username": "wakame",
          "content": "A B is correct!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435131,
          "date": "Fri 05 Nov 2021 11:14",
          "username": "kyoneyam",
          "content": "https://lumigo.io/blog/aws-lambda-timeout-best-practices/<br>A: While this will improve the situation, it may not be enough.<br>B: Memory – The amount of memory available to the function during execution. Choose an amount between 128 MB and 3,008 MB in 64 MB increments. Lambda allocates CPU power linearly in proportion to the amount of memory configured. At 1,792 MB, a function has the equivalent of 1 full vCPU (one vCPU-second of credits per second).<br>All calls made to AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411155,
          "date": "Thu 04 Nov 2021 09:59",
          "username": "WhyIronMan",
          "content": "I'll go with B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348266,
          "date": "Thu 04 Nov 2021 06:01",
          "username": "Waiweng",
          "content": "it's B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 330410,
          "date": "Wed 03 Nov 2021 07:42",
          "username": "SD13wakame",
          "content": "A B looks correct. This question is asking to reduce execution time. D will only help if caching is applicable, not always.Agree with you !<br>API Cache is a feature that improves request latency.<br>But, If there is no cache, call Lambda.<br>Even if API Cache reduces calls to your Lambda, it often doesn't reduce the processing time of Lambda function.<br><br>On the other hand, A is correct. It is also mentioned in best practices.<br>https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 435163,
          "date": "Fri 05 Nov 2021 12:51",
          "username": "wakame",
          "content": "Agree with you !<br>API Cache is a feature that improves request latency.<br>But, If there is no cache, call Lambda.<br>Even if API Cache reduces calls to your Lambda, it often doesn't reduce the processing time of Lambda function.<br><br>On the other hand, A is correct. It is also mentioned in best practices.<br>https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291733,
          "date": "Sat 30 Oct 2021 22:15",
          "username": "Kian1",
          "content": "will go with BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290027,
          "date": "Mon 25 Oct 2021 18:05",
          "username": "ujizane",
          "content": "B and D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290026,
          "date": "Mon 25 Oct 2021 12:47",
          "username": "ujizane",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279777,
          "date": "Mon 25 Oct 2021 03:57",
          "username": "Ebi",
          "content": "I go with BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 252300,
          "date": "Mon 25 Oct 2021 03:22",
          "username": "Bulti",
          "content": "Answer is B and D. <br>A- incorrect because there is no configuration in Lambda to reuse the same sandbox/contain<br>B- is correct because when memory size increases, the total time decreases. It means AWS keeps its promise and gives proportional CPU to your function.<br>C:- there is no need to use ElasticCache as the problem is not related to caching data from DB. <br>D- This makes sense as it will increase performance and put less load on Lambda function.<br>E- You need to increase memory and not CPU.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 249474,
          "date": "Wed 20 Oct 2021 09:14",
          "username": "petebear55",
          "content": "B AND D ... BUT ANOTHER EXAMPLE OF SHI** AMAZON TYPE QUESTION PERSECUTING US !!! .. A WOULD BE CORRECT https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html BUT IT MENTIONS containersNOT WHAT IS SPECIFIED IN THE LINK ... SO A AND D FOR ME",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#530",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large, important documents within the application with the following requirements:<br>✑ The data must be highly durable and available.<br>✑ The data must always be encrypted at rest and in transit.<br>✑ The encryption key must be managed by the company and rotated periodically.<br>Which of the following solutions should the Solutions Architect recommend?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#530",
          "answers": [
            {
              "choice": "<p>A. Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon DynamoDB with SSL to connect to DynamoDB.  Use an AWS KMS key to encrypt DynamoDB objects at rest.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 10548,
          "date": "Tue 21 Sep 2021 23:43",
          "username": "donathon",
          "content": "B.  As Storage Gateway is not a managed service",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 9326,
          "date": "Mon 20 Sep 2021 23:28",
          "username": "dpvnme",
          "content": "B would be my choice",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 623468,
          "date": "Mon 27 Jun 2022 21:41",
          "username": "kangtamo",
          "content": "Agree with B: S3 HTTPS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 497646,
          "date": "Thu 09 Dec 2021 11:59",
          "username": "cldy",
          "content": "B.  Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 491505,
          "date": "Wed 01 Dec 2021 10:57",
          "username": "AzureDP900",
          "content": "I will pick B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450676,
          "date": "Sat 06 Nov 2021 23:41",
          "username": "AWSum1",
          "content": "B.  \\\"Highly durable and available\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411156,
          "date": "Sat 06 Nov 2021 12:06",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 396358,
          "date": "Sat 06 Nov 2021 05:22",
          "username": "KittuCheeku",
          "content": "Definitely B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348267,
          "date": "Fri 05 Nov 2021 02:21",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346764,
          "date": "Thu 04 Nov 2021 22:05",
          "username": "blackgamer",
          "content": "B for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 330021,
          "date": "Thu 04 Nov 2021 16:33",
          "username": "KnightVictor",
          "content": "Should be B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 321771,
          "date": "Wed 03 Nov 2021 23:04",
          "username": "alisyech",
          "content": "i go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 316578,
          "date": "Tue 02 Nov 2021 17:09",
          "username": "awsexamprep47",
          "content": "B is the answer<br>All the encryption requirements are satisfied using S-3 bucket policy",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 299741,
          "date": "Mon 01 Nov 2021 11:44",
          "username": "kiev",
          "content": "B for me. In fact I don't even worry to read when a question talks about storage that's fully managed and cost effective, I just for S3",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 291736,
          "date": "Mon 01 Nov 2021 00:57",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290025,
          "date": "Sun 31 Oct 2021 15:52",
          "username": "ujizane",
          "content": "B is collect",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279780,
          "date": "Sun 31 Oct 2021 15:20",
          "username": "Ebi",
          "content": "Answer is B for sure",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#531",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing a highly available and reliable solution for a cluster of Amazon EC2 instances.<br>The Solutions Architect must ensure that any EC2 instance within the cluster recovers automatically after a system failure. The solution must ensure that the recovered instance maintains the same IP address.<br>How can these requirements be met?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#531",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Lambda script to restart any EC2 instances that shut down unexpectedly.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Auto Scaling group for each EC2 instance that has a minimum and maximum size of 1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new t2.micro instance to monitor the cluster instances. Configure the t2.micro instance to issue an aws ec2 reboot-instances command upon failure.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudWatch alarm for the StatusCheckFailed_System metric, and then configure an EC2 action to recover the instance.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 15991,
          "date": "Fri 24 Sep 2021 18:54",
          "username": "DJTau",
          "content": "Answer = D<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 515873,
          "date": "Mon 03 Jan 2022 16:30",
          "username": "Ni_yot",
          "content": "Yep Dfor me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514345,
          "date": "Sat 01 Jan 2022 05:03",
          "username": "cldy",
          "content": "D correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494654,
          "date": "Sun 05 Dec 2021 21:47",
          "username": "AzureDP900",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411522,
          "date": "Fri 05 Nov 2021 16:36",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 348536,
          "date": "Thu 28 Oct 2021 00:29",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291738,
          "date": "Wed 27 Oct 2021 15:56",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279786,
          "date": "Mon 25 Oct 2021 15:03",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 252303,
          "date": "Mon 25 Oct 2021 05:04",
          "username": "Bulti",
          "content": "Answer is D based on the ability to configure the recover action on a CloudWatch event.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243563,
          "date": "Sun 24 Oct 2021 22:04",
          "username": "T14102020",
          "content": "Correct is D.  Recover with CloudWatch alarm for the StatusCheckFailed_System",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230875,
          "date": "Thu 21 Oct 2021 16:41",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230165,
          "date": "Tue 19 Oct 2021 09:06",
          "username": "gookseang",
          "content": "DDDDDDDDDDDDDDDDDDD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 151972,
          "date": "Fri 15 Oct 2021 05:44",
          "username": "fullaws",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 133217,
          "date": "Thu 14 Oct 2021 12:18",
          "username": "NikkyDicky",
          "content": "D for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 102040,
          "date": "Mon 11 Oct 2021 10:44",
          "username": "jv1",
          "content": "D<br>https://n2ws.com/blog/aws-disaster-recovery/how-aws-instance-auto-recovery-works-and-the-data-corruption-challenge",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 30759,
          "date": "Sat 25 Sep 2021 23:55",
          "username": "dojoqianhaopowersparkf1",
          "content": "What about retaining IP?“A recovered instance is identical to the original instance, including the instance ID, private IP addreSs”If your instance has a public IPv4 address, it retains the public IPv4 address after recovery.<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "upvote_count": "124",
          "selected_answers": ""
        },
        {
          "id": 94219,
          "date": "Sat 02 Oct 2021 03:10",
          "username": "qianhaopower",
          "content": "“A recovered instance is identical to the original instance, including the instance ID, private IP addreSs”",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 37677,
          "date": "Fri 01 Oct 2021 19:14",
          "username": "sparkf1",
          "content": "If your instance has a public IPv4 address, it retains the public IPv4 address after recovery.<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#532",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a<br>Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.<br>Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.<br>Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#532",
          "answers": [
            {
              "choice": "<p>A. Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 325476,
          "date": "Wed 13 Oct 2021 02:21",
          "username": "kalyan_krishna742020epomatti",
          "content": "BE. .<br>\\\"A: Since the issue lies with query response (read) it is cheaper and faster to use ElastiCache which is in memory.<br>B\\C: Unlike a Classic Load Balancer or a Network Load Balancer, an Application Load Balancer can't have transport layer (layer 4) TCP or SSL/TLS listeners. It supports only HTTP and HTTPS listeners. Additionally, it can't use backend authentication to authenticate HTTPS connections between the load balancer and backend instances.<br>D: Should not recover the RDS instance.\\\"Adding replicas is more of a \\\"remediation\\\" (as stated in the question) than adding a cache, which will require architectural changes.<br><br>Honestly it is annoying how badly these questions are written.<br><br>Problem is that \\\"single reader endpoint\\\" is a feature of Aurora, not RDS MySQL.<br><br>So probably A is incorrect.",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 661546,
          "date": "Tue 06 Sep 2022 20:05",
          "username": "epomatti",
          "content": "Adding replicas is more of a \\\"remediation\\\" (as stated in the question) than adding a cache, which will require architectural changes.<br><br>Honestly it is annoying how badly these questions are written.<br><br>Problem is that \\\"single reader endpoint\\\" is a feature of Aurora, not RDS MySQL.<br><br>So probably A is incorrect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 401297,
          "date": "Sun 24 Oct 2021 03:36",
          "username": "qurrenWhyIronMan",
          "content": "BE<br>The problem in A is that \\\"reader endpoint\\\" is for Aurora, not RDS!Also, configuring the same single reader endpoint will result the same problem, since the read replica will be overloaded",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 411526,
          "date": "Sun 31 Oct 2021 13:28",
          "username": "WhyIronMan",
          "content": "Also, configuring the same single reader endpoint will result the same problem, since the read replica will be overloaded",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 648742,
          "date": "Fri 19 Aug 2022 07:22",
          "username": "Kyperos",
          "content": "Between B & C, application can be failed but TCP Port (443) still UP so If using TCP healthcheck, it will not accurate. We must HTTP Healthcheck to get HTTP Ressponse to enhance healthcheck capability.<br>--> B & E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 635767,
          "date": "Sun 24 Jul 2022 00:27",
          "username": "hilft",
          "content": "B for sure<br>It's between A and E.  I initially thought it was A but the forum suggests that the reader endpoint is for Aurora not RDS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 585382,
          "date": "Wed 13 Apr 2022 20:13",
          "username": "bkrish",
          "content": "B --> ALB with health check<br>E --> ElastiCache for DB read performance and to offload huge traffic",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 538429,
          "date": "Wed 02 Feb 2022 08:23",
          "username": "HellGatetracyli",
          "content": "My answer is C and E. <br><br>Simple health check like TCP check (ping) will be enough because R53 also perform full health check.but did it mention R53 here yet?",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 646478,
          "date": "Sun 14 Aug 2022 02:22",
          "username": "tracyli",
          "content": "but did it mention R53 here yet?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494659,
          "date": "Sun 05 Dec 2021 21:53",
          "username": "AzureDP900",
          "content": "BE for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435734,
          "date": "Sun 07 Nov 2021 17:48",
          "username": "tgv",
          "content": "BBB EEE<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 426830,
          "date": "Wed 03 Nov 2021 09:32",
          "username": "nerdicbynature",
          "content": "AB:<br><br>A: Single reader endpoint will allow for easy future growths by simply adding more replicas. Costs aren't mentioned. Thus I would prefer A to D<br>B: Monitoring should be as cheap as possible. Compared to C, HTTP-Checks are more reliable.<br>D: Does not work directly.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411527,
          "date": "Tue 02 Nov 2021 10:23",
          "username": "WhyIronMan",
          "content": "I'll go with B,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 356829,
          "date": "Fri 22 Oct 2021 16:27",
          "username": "victordun",
          "content": "opt for B&E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348541,
          "date": "Fri 22 Oct 2021 12:38",
          "username": "Waiweng",
          "content": "BE for me",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 346799,
          "date": "Tue 19 Oct 2021 08:52",
          "username": "blackgamer",
          "content": "BE for me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 327292,
          "date": "Thu 14 Oct 2021 05:46",
          "username": "CarisB",
          "content": "B and E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 320405,
          "date": "Sun 03 Oct 2021 15:53",
          "username": "wasabidev",
          "content": "BE for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 309374,
          "date": "Mon 27 Sep 2021 08:08",
          "username": "nitinz",
          "content": "B and E",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#533",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an email application across multiple AWS Regions. The company uses Ohio (us-east-2) as the primary Region and Northern Virginia (us- east-1) as the Disaster Recovery (DR) Region. The data is continuously replicated from the primary Region to the DR Region by a single instance on the public subnet in both Regions. The replication messages between the Regions have a significant backlog during certain times of the day. The backlog clears on its own after a short time, but it affects the application's RPO.<br>Which of the following solutions should help remediate this performance problem? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#533",
          "answers": [
            {
              "choice": "<p>A. Increase the size of the instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Have the instance in the primary Region write the data to an Amazon SQS queue in the primary Region instead, and have the instance in the DR Region poll from this queue.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use multiple instances on the primary and DR Regions to send and receive the replication data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Change the DR Region to Oregon (us-west-2) instead of the current DR Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Attach an additional elastic network interface to each of the instances in both Regions and set up load balancing between the network interfaces.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 311547,
          "date": "Mon 27 Sep 2021 06:31",
          "username": "kalyan_krishna742020epomattiMrCarterstudent2020kirrim",
          "content": "Note: The answers are independent. It does not say which COMBINATION of answers would remediate this problem.<br>A - correct - someone hires a useless architect who suggested a t2.nano in region 1 and region 2. Network performance is attrocious (low). Wonder if increasing instance size will help message replication throughput? of course it will. both compute and networking.<br>B - incorrect - less correct than A and C.  If concerned about RPO, i'd fire myself if i suggested storing replication messages in the source region, rather than in the SQS queue in the target region - definitely an RPO pleaser.<br>C - correct - if network and compute was an issue, potentially doubles the speed of replicating and processing messages<br>D - incorrect - increases latency if networking was the root cause<br>E - incorrect - teaming doesn't improve network performanceWhere does it say t2.nano?? Doesn't show to me.amazing explanation. But please dont fire yourself!!Teaming is not supported in AWSAgree with the above reasoning.In addition, if you assume the backlogs are occurring because the primary region servers are under heavy load, then if the primary region servers are too busy to replicate data to the secondary region, they're probably too busy to chunk up email data to place into SQS queue messages, too.",
          "upvote_count": "171211",
          "selected_answers": ""
        },
        {
          "id": 661577,
          "date": "Tue 06 Sep 2022 20:52",
          "username": "epomatti",
          "content": "Where does it say t2.nano?? Doesn't show to me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 395697,
          "date": "Mon 11 Oct 2021 12:53",
          "username": "MrCarter",
          "content": "amazing explanation. But please dont fire yourself!!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 414169,
          "date": "Sun 17 Oct 2021 19:47",
          "username": "student2020",
          "content": "Teaming is not supported in AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 460186,
          "date": "Thu 04 Nov 2021 13:08",
          "username": "kirrim",
          "content": "Agree with the above reasoning.In addition, if you assume the backlogs are occurring because the primary region servers are under heavy load, then if the primary region servers are too busy to replicate data to the secondary region, they're probably too busy to chunk up email data to place into SQS queue messages, too.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 627702,
          "date": "Wed 06 Jul 2022 05:44",
          "username": "aandc",
          "content": "Go for AC, E makes no sense regarding load balancing",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 623495,
          "date": "Mon 27 Jun 2022 22:36",
          "username": "kangtamo",
          "content": "I will go with AC. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 508723,
          "date": "Fri 24 Dec 2021 19:32",
          "username": "vbal",
          "content": "A & C looks good.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 498594,
          "date": "Fri 10 Dec 2021 13:08",
          "username": "cldy",
          "content": "A.  Increase the size of the instances.<br>C.  Use multiple instances on the primary and DR Regions to send and receive the replication data.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 488997,
          "date": "Sun 28 Nov 2021 09:48",
          "username": "acloudguruRiho",
          "content": "For B, SQS can decouple the message for DR to reduce the backlog,For C ,it will increase the capacityIt can't be B.  \\\"Amazon SQS queue in the primary Region instead\\\". Queue is in primary region and if something happens - the data is also still in primary region. No RPO improvement..",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 513273,
          "date": "Thu 30 Dec 2021 12:05",
          "username": "Riho",
          "content": "It can't be B.  \\\"Amazon SQS queue in the primary Region instead\\\". Queue is in primary region and if something happens - the data is also still in primary region. No RPO improvement..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 468271,
          "date": "Fri 05 Nov 2021 23:43",
          "username": "Salmariaz",
          "content": "It’s between AC and CE<br>A Increasing instance size would help but we don’t know what’s the current size and assuming that it’s already small. Though this might help, but not sure if this would be a good option in the long run to keep increasing vertically<br>B SQS is ruled out since it is in the source region and counterfeits DR purpose<br>D is shifting the problem<br>C might help for faster processing since there are multiple instances to send and receive.<br>E would also increase the bandwidth as<br>Instances with multiple network cards provide higher network performance, including bandwidth capabilities above 100 Gbps and improved packet rate performance<br>Also AWS supports load balancing to multiple IPs of the same instance<br><br>https://aws.amazon.com/about-aws/whats-new/2017/09/elastic-load-balancing-network-load-balancer-now-supports-load-balancing-to-ip-addresses-as-targets-for-aws-and-on-premises-resources/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450873,
          "date": "Mon 01 Nov 2021 23:17",
          "username": "Kopa",
          "content": "im for A,C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449865,
          "date": "Mon 01 Nov 2021 14:27",
          "username": "nodogoshi",
          "content": "BC.  SQS is durable for backlog",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 446820,
          "date": "Wed 27 Oct 2021 17:31",
          "username": "student22",
          "content": "A,C<br><br>Why not C? The SQS queue in the source region would not improve RPO.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 429387,
          "date": "Sun 24 Oct 2021 03:22",
          "username": "denccc",
          "content": "go for A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 427861,
          "date": "Sat 23 Oct 2021 21:41",
          "username": "kevin1024",
          "content": "Go with C, E<br>C => multiple instances means more bandwidth too<br>E => Instances with multiple network cards provide higher network performance, including bandwidth capabilities above 100 Gbps and improved packet rate performance.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411529,
          "date": "Sun 17 Oct 2021 05:42",
          "username": "WhyIronMan",
          "content": "I'll go with A,C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349047,
          "date": "Mon 11 Oct 2021 12:42",
          "username": "Waiweng",
          "content": "it's A and C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346803,
          "date": "Fri 08 Oct 2021 12:27",
          "username": "blackgamer",
          "content": "A and C to me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 330415,
          "date": "Fri 08 Oct 2021 01:15",
          "username": "SD13",
          "content": "B & C.  <br>A is not correct as vertical scaling is not recommended.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 327297,
          "date": "Thu 07 Oct 2021 21:46",
          "username": "CarisB",
          "content": "Agree on A and C.  SQS max message size (256 kb) is not suited for replicating email data.<br>Teaming may have been interesting, but E just mentions \\\"load balancing\\\", not \\\"teaming\\\" (aggregate throughput).",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#534",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has implemented AWS Organizations. It has recently set up a number of new accounts and wants to deny access to a specific set of AWS services in these new accounts.<br>How can this be controlled MOST efficiently?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#534",
          "answers": [
            {
              "choice": "<p>A. Create an IAM policy in each account that denies access to the services. Associate the policy with an IAM group, and add all IAM users to the group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a service control policy that denies access to the services. Add all of the new accounts to a single organizational unit (OU), and apply the policy to that OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM policy in each account that denies access to the services. Associate the policy with an IAM role, and instruct users to log in using their corporate credentials and assume the IAM role.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a service control policy that denies access to the services, and apply the policy to the root of the organization.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13542,
          "date": "Wed 22 Sep 2021 13:51",
          "username": "donathon",
          "content": "B<br>A\\C: Not efficient.<br>D: Would affect all accounts.",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 624475,
          "date": "Wed 29 Jun 2022 09:23",
          "username": "TechX",
          "content": "B for sure",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 534119,
          "date": "Thu 27 Jan 2022 21:52",
          "username": "Ni_yot",
          "content": "B is correct.Applying at the root will affect all accounts.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 532283,
          "date": "Tue 25 Jan 2022 18:00",
          "username": "drwprch",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 494663,
          "date": "Sun 05 Dec 2021 21:57",
          "username": "AzureDP900",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 415790,
          "date": "Sun 31 Oct 2021 14:18",
          "username": "TiredDad",
          "content": "is this an actual exam question!???",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411532,
          "date": "Thu 28 Oct 2021 10:32",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 349048,
          "date": "Wed 27 Oct 2021 00:24",
          "username": "Waiweng",
          "content": "it' s B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 316589,
          "date": "Mon 25 Oct 2021 23:47",
          "username": "awsexamprep47",
          "content": "B is the answer.<br>Apply SCP on separate OU instead of applying it at root level, this will allow the solution to be scalable in case new accounts are added in the future",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291750,
          "date": "Fri 22 Oct 2021 18:08",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290064,
          "date": "Tue 19 Oct 2021 18:14",
          "username": "ujizane",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279000,
          "date": "Mon 18 Oct 2021 08:28",
          "username": "Ebi",
          "content": "B is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 252396,
          "date": "Thu 14 Oct 2021 22:30",
          "username": "Bulti",
          "content": "Answer is B.  Services not explicitly allowed in the SCP and associated with an OU deny access to the accounts within that OU.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243573,
          "date": "Thu 14 Oct 2021 00:08",
          "username": "T14102020",
          "content": "Correct B.  service control policy for new OU",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231189,
          "date": "Fri 08 Oct 2021 13:19",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230263,
          "date": "Tue 05 Oct 2021 16:05",
          "username": "oopsy",
          "content": "B for sure",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 152281,
          "date": "Sun 03 Oct 2021 22:28",
          "username": "fullaws",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#535",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has deployed an application to multiple environments in AWS, including production and testing. The company has separate accounts for production and testing, and users are allowed to create additional application users for team members or services, as needed. The Security team has asked the Operations team for better isolation between production and testing with centralized controls on security credentials and improved management of permissions between environments.<br>Which of the following options would MOST securely accomplish this goal?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#535",
          "answers": [
            {
              "choice": "<p>A. Create a new AWS account to hold user and service accounts, such as an identity account. Create users and groups in the identity account. Create roles with appropriate permissions in the production and testing accounts. Add the identity account to the trust policies for the roles.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Modify permissions in the production and testing accounts to limit creating new IAM users to members of the Operations team. Set a strong IAM password policy on each account. Create new IAM users and groups in each account to limit developer access to just the services required to complete their job function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a script that runs on each account that checks user accounts for adherence to a security policy. Disable any user or service accounts that do not comply.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create all user accounts in the production account. Create roles for access in the production account and testing accounts. Grant cross-account access from the production account to the testing account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14183,
          "date": "Mon 20 Sep 2021 14:30",
          "username": "donathonall_past",
          "content": "A<br>A: By centralizing users to a single account, a user can access the prod and test using assume role. This ensures that all actions are properly logged and is the most secure. Adapted from this article: https://aws.amazon.com/blogs/security/how-to-centralize-and-automate-iam-policy-creation-in-sandbox-development-and-test-environments/<br>B: This means the test users will still need to be created. The problem with test users is always security. Who is the actual person behind the scene carrying out that specific actions? This is unlikely the most secure option.<br>C: Any answers that is asking you to write a script is very unlikely to be the answer.<br>D: This seems to be able to work too which is similar to A.  But the Security team already asked for “better isolation with centralized controls”. Hence I chose A. I would prefer this one as well, B is not a centralized solution for me.",
          "upvote_count": "361",
          "selected_answers": ""
        },
        {
          "id": 21993,
          "date": "Sun 26 Sep 2021 19:43",
          "username": "all_past",
          "content": "I would prefer this one as well, B is not a centralized solution for me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715685,
          "date": "Fri 11 Nov 2022 02:56",
          "username": "Aum",
          "content": "A is the same as how AWS SSO works, where permission set in on an identity account (Master Account for SSO). Then corresponding roles get created in target accounts with IAM permission as defined in permission set with trust policies to allow principal to assume (Federated for SSO)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 577906,
          "date": "Wed 30 Mar 2022 00:41",
          "username": "jj22222",
          "content": "A.  Create a new AWS account to hold user and service accounts, such as an identity account. Create users and groups in the identity account. Create roles with appropriate permissions in the production and testing accounts. Add the identity account to the trust policies for the roles.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 532288,
          "date": "Tue 25 Jan 2022 18:11",
          "username": "drwprch",
          "content": "A.  for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 494666,
          "date": "Sun 05 Dec 2021 21:59",
          "username": "AzureDP900",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 460191,
          "date": "Wed 03 Nov 2021 02:59",
          "username": "kirrim",
          "content": "It's definitely A, see bottom of p2 here: https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Security_Strategy.pdf<br><br>It's so commonly used, AWS even provided a way to color-code the console when you assume a role, so it shows up red when you're working in a prod role, green in dev/test role, etc:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-console.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 429388,
          "date": "Tue 02 Nov 2021 05:46",
          "username": "denccc",
          "content": "will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411549,
          "date": "Tue 02 Nov 2021 01:04",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349051,
          "date": "Mon 01 Nov 2021 13:57",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 309741,
          "date": "Sat 30 Oct 2021 17:11",
          "username": "ItsmeP",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 299888,
          "date": "Sat 30 Oct 2021 09:09",
          "username": "kiev",
          "content": "The Answer is A.  I always knew it and have confirmed it with my questions and answer from Neal Davis. Guys choose A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291752,
          "date": "Wed 27 Oct 2021 05:42",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290217,
          "date": "Tue 26 Oct 2021 09:09",
          "username": "ujizane",
          "content": "A is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279005,
          "date": "Sun 24 Oct 2021 07:14",
          "username": "Ebi",
          "content": "Answer is A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 252407,
          "date": "Sat 23 Oct 2021 10:24",
          "username": "Bulti",
          "content": "Answer is A since it is the most elegant solution when it comes to centralizing user management and granting access to the services in prod and test account. However is it important to note that granting trust to the identity account alone will not prevent unauthorized access to services. You will need to create a trust policy on IAM roles in prod and test accounts that grant assumeRole permission to specific groups or users in the 'identity account.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 249531,
          "date": "Fri 22 Oct 2021 22:11",
          "username": "petebear55Kian1",
          "content": "AB CAN NOT BE RIGHT BECAUSE THE QUESTION ASKS MOST SECURE !!! NOW HAVING 'GROUPS' ACCESS IS NOT SECURE BECAUSE THERE IS NO WAY OF NOING WHICH USER IN THE GROUP ACCESSED THE RECOURCES.THUS A WHICH IS BEST PRACTICE 'ROLES'so what is your best answer?",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 281893,
          "date": "Mon 25 Oct 2021 22:47",
          "username": "Kian1",
          "content": "so what is your best answer?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243574,
          "date": "Thu 21 Oct 2021 19:16",
          "username": "T14102020",
          "content": "For sure A.  One centralized account+ roles",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#536",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>The CISO of a large enterprise with multiple IT departments, each with its own AWS account, wants one central place where AWS permissions for users can be managed and users authentication credentials can be synchronized with the company's existing on-premises solution.<br>Which solution will meet the CISO's requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#536",
          "answers": [
            {
              "choice": "<p>A. Define AWS IAM roles based on the functional responsibilities of the users in a central account. Create a SAML-based identity management provider. Map users in the on-premises groups to IAM roles. Establish trust relationships between the other accounts and the central account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy a common set of AWS IAM users, groups, roles, and policies in all of the AWS accounts using AWS Organizations. Implement federation between the on-premises identity provider and the AWS accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Organizations in a centralized account to define service control policies (SCPs). Create a SAML-based identity management provider in each account and map users in the on-premises groups to AWS IAM roles.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Perform a thorough analysis of the user base and create AWS IAM users accounts that have the necessary permissions. Set up a process to provision and deprovision accounts based on data in the on-premises solution.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14185,
          "date": "Mon 20 Sep 2021 02:06",
          "username": "donathondonathonG3PacoDereksam422Muskmanoj101chandler",
          "content": "C<br>To help you manage federation for multiple AWS accounts centrally, you can use AWS Single Sign-On to manage SSO access for all of your accounts in AWS Organizations. https://aws.amazon.com/identity/federation/<br>A: The fact that the answer did not explain how “trust relationships” are created means I would avoid this answer if there is a better answer. In this case C.  You will also need to use a lot of assume roles in each and every account which can be tedious. This was what it used to be before AWS Organization was launched.<br>B: Accounts are not centralized. (“one central place”)<br>D: There is no federation.A<br>B\\C: Accounts are not centralized. (“one central place”). Also SAML must be done in one account.<br>D: There is no federation.I feel it has to be C.  SCPs offer central control over the maximum available permissions for all accounts in your organization.A doesnt provide to centrally manage permissions.C.  finally i got u once @donathon：D<br>SCPs are necessary but not sufficient for granting access in the accounts in your organization. Attaching an SCP to the organization root or an organizational unit (OU) defines a guardrail for what actions accounts within the organization root or OU can do. You still need to attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.htmlAlthough A looks bit vague on trust relationships, it is how aws asks to do it having an central account. For C, SCP offers perimeter control. I go with AC says \\\"identity management provider in each account\\\" which I think is wrong, because you just need one, not one per account.C is not correct. you can't have SAML across each account. That is not going to centralise access.Probably A is the answer: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html and search \\\"trust\\\" on the page. <br>Also, question asks about \\\"AWS permissions for users can be managed\\\", SCP won't help too much about that. It's more like IAM's job.",
          "upvote_count": "20385114215",
          "selected_answers": ""
        },
        {
          "id": 15121,
          "date": "Tue 21 Sep 2021 16:20",
          "username": "donathonG3PacoDereksam422",
          "content": "A<br>B\\C: Accounts are not centralized. (“one central place”). Also SAML must be done in one account.<br>D: There is no federation.I feel it has to be C.  SCPs offer central control over the maximum available permissions for all accounts in your organization.A doesnt provide to centrally manage permissions.C.  finally i got u once @donathon：D<br>SCPs are necessary but not sufficient for granting access in the accounts in your organization. Attaching an SCP to the organization root or an organizational unit (OU) defines a guardrail for what actions accounts within the organization root or OU can do. You still need to attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.htmlAlthough A looks bit vague on trust relationships, it is how aws asks to do it having an central account. For C, SCP offers perimeter control. I go with A",
          "upvote_count": "38511",
          "selected_answers": ""
        },
        {
          "id": 18966,
          "date": "Wed 22 Sep 2021 15:07",
          "username": "G3",
          "content": "I feel it has to be C.  SCPs offer central control over the maximum available permissions for all accounts in your organization.A doesnt provide to centrally manage permissions.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 42516,
          "date": "Mon 27 Sep 2021 16:33",
          "username": "PacoDerek",
          "content": "C.  finally i got u once @donathon：D<br>SCPs are necessary but not sufficient for granting access in the accounts in your organization. Attaching an SCP to the organization root or an organizational unit (OU) defines a guardrail for what actions accounts within the organization root or OU can do. You still need to attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183390,
          "date": "Sat 23 Oct 2021 01:30",
          "username": "sam422",
          "content": "Although A looks bit vague on trust relationships, it is how aws asks to do it having an central account. For C, SCP offers perimeter control. I go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 48503,
          "date": "Fri 01 Oct 2021 11:30",
          "username": "Musk",
          "content": "C says \\\"identity management provider in each account\\\" which I think is wrong, because you just need one, not one per account.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 192662,
          "date": "Sun 24 Oct 2021 19:59",
          "username": "manoj101",
          "content": "C is not correct. you can't have SAML across each account. That is not going to centralise access.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 33864,
          "date": "Mon 27 Sep 2021 11:45",
          "username": "chandler",
          "content": "Probably A is the answer: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html and search \\\"trust\\\" on the page. <br>Also, question asks about \\\"AWS permissions for users can be managed\\\", SCP won't help too much about that. It's more like IAM's job.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 737000,
          "date": "Tue 06 Dec 2022 17:32",
          "username": "SureNot",
          "content": "one central place",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 707749,
          "date": "Sun 30 Oct 2022 10:55",
          "username": "nsvijay04b1",
          "content": "each account IAM identity provider and role for SAML access createdand it should be trusted external IDP provider.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 661592,
          "date": "Tue 06 Sep 2022 21:11",
          "username": "epomatti",
          "content": "A<br>One central place to synchronize users.<br><br>C is wrong.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 647796,
          "date": "Tue 16 Aug 2022 21:22",
          "username": "Ni_yot",
          "content": "Will go with A.  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_saml.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626062,
          "date": "Sat 02 Jul 2022 11:13",
          "username": "aandc",
          "content": "A:C says \\\" identity management provider in each account\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 536072,
          "date": "Sun 30 Jan 2022 11:44",
          "username": "cannottellname",
          "content": "A Does not scale that well. Establishing trust and all, while also creating IAM role for each account because access limits can be different for different accounts + person from 1 department should not access other departments accounts though they need same permissions. Hence, there will be multiple IAM role + multiple account trusts....<br><br>Also, logging into central account and then assuming role for human resources does not seem a good option. This needs to be done at each and every account level only.... and what kind of services will be needed that way...<br><br>C seems better to me here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495533,
          "date": "Tue 07 Dec 2021 03:09",
          "username": "vbal",
          "content": "why C ? Create an IDP in each Account..?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441702,
          "date": "Sat 06 Nov 2021 19:36",
          "username": "student22",
          "content": "A is correct<br>Map on premise users to AWS Roles through SAML federation<br>C is similar but not centralized.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 429389,
          "date": "Fri 05 Nov 2021 13:41",
          "username": "denccc",
          "content": "I think it's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 425291,
          "date": "Tue 02 Nov 2021 22:11",
          "username": "DerekKey",
          "content": "A correct - https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/<br>B wrong - \\\"The CISO\\\" + \\\"wants one central place where AWS permissions ... can be managed and users authentication credentials can be synchronized with the company’s existing on-premises solution\\\"<br>C wrong - \\\"The CISO\\\" + \\\"wants one central place where AWS permissions ... can be managed and users authentication credentials can be synchronized with the company’s existing on-premises solution\\\"<br>D wrong",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411550,
          "date": "Tue 02 Nov 2021 05:33",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349053,
          "date": "Mon 01 Nov 2021 21:03",
          "username": "Waiweng",
          "content": "it;s C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291759,
          "date": "Sat 30 Oct 2021 16:36",
          "username": "Kian1",
          "content": "will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279061,
          "date": "Sat 30 Oct 2021 05:39",
          "username": "Ebi",
          "content": "Answer is A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 260704,
          "date": "Fri 29 Oct 2021 09:57",
          "username": "01037",
          "content": "Either A or C needs to create roles for all accounts, so neither can really control permissions centrally.<br>But SCP defines boundaries, so it can provide central permission control to some extent, and simpler.<br>So I'm inclined to C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 252415,
          "date": "Thu 28 Oct 2021 06:18",
          "username": "Bulti",
          "content": "A is the right answer. There should be only one SAML Identity Provider for the organization which would federate with the on-prem solution to maps the groups to the IAM roles in the centralized account. Using these IAM roles the user can assume a role in other accounts like prod and test to access services using trust and permission policies associated with the assumed role in those accounts.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#537",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company has increased its utilization of AWS over time in an unmanaged way. As such, they have a large number of independent AWS accounts across different business units, projects, and environments. The company has created a Cloud Center of Excellence team, which is responsible for managing all aspects of the AWS Cloud, including their AWS accounts.<br>Which of the following should the Cloud Center of Excellence team do to BEST address their requirements in a centralized way? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: DE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#537",
          "answers": [
            {
              "choice": "<p>A. Control all AWS account root user credentials. Assign AWS IAM users in the account of each user who needs to access AWS resources. Follow the policy of least privilege in assigning permissions to each user.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Tag all AWS resources with details about the business unit, project, and environment. Send all AWS Cost and Usage reports to a central Amazon S3 bucket, and use tools such as Amazon Athena and Amazon QuickSight to collect billing details by business unit.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the AWS Marketplace to choose and deploy a Cost Management tool. Tag all AWS resources with details about the business unit, project, and environment. Send all AWS Cost and Usage reports for the AWS accounts to this tool for analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up AWS Organizations. Enable consolidated billing, and link all existing AWS accounts to a master billing account. Tag all AWS resources with details about the business unit, project and environment. Analyze Cost and Usage reports using tools such as Amazon Athena and Amazon QuickSight, to collect billing details by business unit.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Using a master AWS account, create IAM users within the master account. Define IAM roles in the other AWS accounts, which cover each of the required functions in the account. Follow the policy of least privilege in assigning permissions to each role, then enable the IAM users to assume the roles that they need to use.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 309373,
          "date": "Mon 20 Sep 2021 01:35",
          "username": "ajeeshb",
          "content": "Answers: DE<br>E because it manages IAM users centrally and uses roles in other accounts to which the IAM user can switch. Option A says to create IAM users in all accounts which is not the best solution",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 476756,
          "date": "Fri 12 Nov 2021 08:35",
          "username": "nseitkanmani76cen007gerhardblaandc",
          "content": "A & D are the right answers. <br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html<br>We recommend that you use the management account and its users and roles only for tasks that can be performed only by that account. Store all of your AWS resources in other AWS accounts in the organization and keep them out of the management account... Therefore E is incorrectAgree on this - A& DRead the question carefully.<br>The requirement is the central account wants to manage all ASPECT OF THE CLOUD INCLUDING THE AWS ACCOUNTS.<br>Therefore ROLES will be created in the child accounts that the master will assume.<br>So D & E is the correct answer.Creating all IAM Users in the main AWS Organization accounts and having no IAM Users in the sub-accounts is the consequence of what you are saying (E). That means only IAM Roles in the sub-accounts. That seems very counter intuitive. You cannot even have users and teams manage their own account and login to the Console as an IAM User in that case. For me, central management is more that you at least control all the sub-accounts at the Root level, bring them under an Organization, and can then enforce security settings, SCPs, Service Catalog etc. That is more A than E. agree on A D",
          "upvote_count": "81111",
          "selected_answers": ""
        },
        {
          "id": 529669,
          "date": "Sat 22 Jan 2022 08:29",
          "username": "tkanmani76cen007gerhardbl",
          "content": "Agree on this - A& DRead the question carefully.<br>The requirement is the central account wants to manage all ASPECT OF THE CLOUD INCLUDING THE AWS ACCOUNTS.<br>Therefore ROLES will be created in the child accounts that the master will assume.<br>So D & E is the correct answer.Creating all IAM Users in the main AWS Organization accounts and having no IAM Users in the sub-accounts is the consequence of what you are saying (E). That means only IAM Roles in the sub-accounts. That seems very counter intuitive. You cannot even have users and teams manage their own account and login to the Console as an IAM User in that case. For me, central management is more that you at least control all the sub-accounts at the Root level, bring them under an Organization, and can then enforce security settings, SCPs, Service Catalog etc. That is more A than E. ",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 636338,
          "date": "Mon 25 Jul 2022 05:19",
          "username": "cen007gerhardbl",
          "content": "Read the question carefully.<br>The requirement is the central account wants to manage all ASPECT OF THE CLOUD INCLUDING THE AWS ACCOUNTS.<br>Therefore ROLES will be created in the child accounts that the master will assume.<br>So D & E is the correct answer.Creating all IAM Users in the main AWS Organization accounts and having no IAM Users in the sub-accounts is the consequence of what you are saying (E). That means only IAM Roles in the sub-accounts. That seems very counter intuitive. You cannot even have users and teams manage their own account and login to the Console as an IAM User in that case. For me, central management is more that you at least control all the sub-accounts at the Root level, bring them under an Organization, and can then enforce security settings, SCPs, Service Catalog etc. That is more A than E. ",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 648115,
          "date": "Wed 17 Aug 2022 16:33",
          "username": "gerhardbl",
          "content": "Creating all IAM Users in the main AWS Organization accounts and having no IAM Users in the sub-accounts is the consequence of what you are saying (E). That means only IAM Roles in the sub-accounts. That seems very counter intuitive. You cannot even have users and teams manage their own account and login to the Console as an IAM User in that case. For me, central management is more that you at least control all the sub-accounts at the Root level, bring them under an Organization, and can then enforce security settings, SCPs, Service Catalog etc. That is more A than E. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626053,
          "date": "Sat 02 Jul 2022 11:05",
          "username": "aandc",
          "content": "agree on A D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713239,
          "date": "Mon 07 Nov 2022 19:31",
          "username": "mrgreatnessdmscountera",
          "content": "100% D and E , I'm an architect and this is standardGTFO mr \\\"architect\\\"",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 715211,
          "date": "Thu 10 Nov 2022 13:25",
          "username": "dmscountera",
          "content": "GTFO mr \\\"architect\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 690619,
          "date": "Mon 10 Oct 2022 03:00",
          "username": "Jonfernz",
          "content": "i do this only a daily basis. it's definitely D, E",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 679774,
          "date": "Mon 26 Sep 2022 15:10",
          "username": "EgaHa",
          "content": "Would go with D & E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 661595,
          "date": "Tue 06 Sep 2022 21:14",
          "username": "epomatti",
          "content": "A, D<br><br>E is wrong, it is a bad practice to use the master account for creating users.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 538346,
          "date": "Wed 02 Feb 2022 03:50",
          "username": "zoliv",
          "content": "D & E.  AWS Org & IAM Roles",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 494669,
          "date": "Sun 05 Dec 2021 22:03",
          "username": "AzureDP900",
          "content": "D & E makes most sense.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450745,
          "date": "Thu 04 Nov 2021 19:03",
          "username": "AWSum1",
          "content": "DE <br>Centrally manage. The wording of the other rotations being \\\"each account\\\" shows no central management",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 416374,
          "date": "Wed 03 Nov 2021 13:29",
          "username": "DanShone",
          "content": "D and E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411552,
          "date": "Wed 20 Oct 2021 16:58",
          "username": "WhyIronMan",
          "content": "I'll go with D,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 401648,
          "date": "Thu 14 Oct 2021 00:30",
          "username": "Kopa",
          "content": "Im also for DE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348547,
          "date": "Tue 05 Oct 2021 08:46",
          "username": "Waiweng",
          "content": "D and E",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 346865,
          "date": "Thu 30 Sep 2021 00:19",
          "username": "blackgamer",
          "content": "DE is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 330288,
          "date": "Tue 28 Sep 2021 20:44",
          "username": "KnightVictor",
          "content": "Would go with D & E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 327308,
          "date": "Mon 27 Sep 2021 14:47",
          "username": "CarisB",
          "content": "D and E in my opinion. A is not really a centralized solution.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 317772,
          "date": "Wed 22 Sep 2021 22:02",
          "username": "nitinz",
          "content": "D & E makes most sense.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#538",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>To abide by industry regulations, a Solutions Architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The Solutions Architect is required to provide access to the data stored in AWS to the company's global WAN network. The Security team mandates that no traffic accessing this data should traverse the public internet.<br>How should the Solutions Architect design a highly available solution that meets the requirements and is cost-effective?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#538",
          "answers": [
            {
              "choice": "<p>A. Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14187,
          "date": "Mon 20 Sep 2021 08:12",
          "username": "donathonJoeyleeKuroshammousDashL",
          "content": "D<br>This feature also allows you to connect to any of the participating VPCs from any Direct Connect location, further reducing your costs for making using AWS services on a cross-region basis.<br>https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/<br>A: There is only a single DC and hence is not highly available.<br>B: VPC peering means there are additional cost charges when data transfer between region. Also there is a 125 VPC peering limit. Data transferred across Inter-Region VPC Peering connections is charged at the standard inter-region data transfer rates. https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-support-for-inter-region-vpc-peering/<br>C: Similar to B. <br>D: Remember one caveat which the question did not state is if there are multiple accounts: The VPCs that reference a particular Direct Connect Gateway must have IP address ranges that do not overlap. Today, the VPCs must all be in the same AWS account; we plan to make this more flexible in the future. https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/Agree on DClearly explaination.Another argument is that there was no mention to have transit feature between VPCs, which is an extra reason to choose D over C (from apart being more cost-effective)As per aws announcement in Mar 2019 (https://aws.amazon.com/about-aws/whats-new/2019/03/announcing-multi-account-support-for-direct-connect-gateway/):<br><br>With the launch of multi-account support for Direct Connect gateway, you can associate up to 10 Amazon VPCs from multiple accounts with a Direct Connect gateway. The Amazon VPCs and the Direct Connect gateway must be owned by AWS Accounts that belong to the same AWS payer account ID. ",
          "upvote_count": "531123",
          "selected_answers": ""
        },
        {
          "id": 77132,
          "date": "Sun 24 Oct 2021 21:43",
          "username": "Joeylee",
          "content": "Agree on D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 223312,
          "date": "Thu 04 Nov 2021 08:46",
          "username": "Kuro",
          "content": "Clearly explaination.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278431,
          "date": "Sat 06 Nov 2021 06:32",
          "username": "shammous",
          "content": "Another argument is that there was no mention to have transit feature between VPCs, which is an extra reason to choose D over C (from apart being more cost-effective)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 403524,
          "date": "Sat 06 Nov 2021 23:14",
          "username": "DashL",
          "content": "As per aws announcement in Mar 2019 (https://aws.amazon.com/about-aws/whats-new/2019/03/announcing-multi-account-support-for-direct-connect-gateway/):<br><br>With the launch of multi-account support for Direct Connect gateway, you can associate up to 10 Amazon VPCs from multiple accounts with a Direct Connect gateway. The Amazon VPCs and the Direct Connect gateway must be owned by AWS Accounts that belong to the same AWS payer account ID. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 16800,
          "date": "Thu 23 Sep 2021 03:07",
          "username": "bebo",
          "content": "Question 144 <br>A.  Use Amazon CloudFront with Amazon ECS for hosting the website. Use AWS Secrets Manager for provide<br>user management and authentication functions. Use ECS Docker containers to build an API.<br>B.  Use Amazon Route 53 latency routing with an Application Load Balancer and AWS Fargate in different<br>regions for hosting the website. use Amazon Cognito to provide user management and authentication<br>functions. Use Amazon EKS containers.<br>C.  Use Amazon CloudFront with Amazon S3 for hosting static web resources. Use Amazon Cognito to provide<br>user management authentication functions. Use Amazon API Gateway with AWS Lambda to build an API.<br>D.  Use AWS Direct Connect with Amazon CloudFront and Amazon S3 for hosting static web resource. Use<br>Amazon Cognito to provide user management authentication functions. Use AWS Lambda to build an API.<br>Correct Answer: C",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 572539,
          "date": "Mon 21 Mar 2022 23:23",
          "username": "HellGate",
          "content": "B, C, D are all right way... D > C > B<br>D is the best answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 531280,
          "date": "Mon 24 Jan 2022 13:18",
          "username": "GeniusMikeLiuHellGate",
          "content": "why need two AWS Direct Connect connections?HA purpose",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 537800,
          "date": "Tue 01 Feb 2022 12:36",
          "username": "HellGate",
          "content": "HA purpose",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494673,
          "date": "Sun 05 Dec 2021 22:06",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441707,
          "date": "Sun 07 Nov 2021 14:04",
          "username": "student22",
          "content": "D<br>AWS region 1 --> VIF --> Direct Connect Gateway --> multiple VIF --> multiple AWS Regions",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 411557,
          "date": "Sun 07 Nov 2021 05:15",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348553,
          "date": "Sat 06 Nov 2021 12:55",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 318673,
          "date": "Sat 06 Nov 2021 12:00",
          "username": "Pupu86",
          "content": "A: Doesn’t fulfill HA<br>B: charges are implemented for both inbound and outbound peering<br>C: charges are implemented for outbound only but solution only suitable for VPC transits purpose to overcome VPC peering mesh<br>D: Correct answer to link multiple regional traffic",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 286858,
          "date": "Sat 06 Nov 2021 09:10",
          "username": "bnagaraja9099",
          "content": "D<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 280085,
          "date": "Sat 06 Nov 2021 08:47",
          "username": "Firststack",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279065,
          "date": "Sat 06 Nov 2021 06:34",
          "username": "Ebi",
          "content": "I go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 253624,
          "date": "Fri 05 Nov 2021 19:56",
          "username": "MichaelHuang",
          "content": "D<br>See the link for Direct Connect Gateway for multi-regions: https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 252442,
          "date": "Fri 05 Nov 2021 13:44",
          "username": "Bulti",
          "content": "Answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231258,
          "date": "Thu 04 Nov 2021 18:26",
          "username": "jackdryanGopiSivanathanrcher",
          "content": "I'll go with Ddata is in the AWS public regions, so Direct connect Gateway can't be used. it should Transit VPCYou can create Public VIF between Direct Connect and Direct Connect Gateway. and access AWS public services like S3. Transit VPC works for VPN, which means IPSEC that work over the internet :)",
          "upvote_count": "321",
          "selected_answers": ""
        },
        {
          "id": 247029,
          "date": "Fri 05 Nov 2021 13:22",
          "username": "GopiSivanathanrcher",
          "content": "data is in the AWS public regions, so Direct connect Gateway can't be used. it should Transit VPCYou can create Public VIF between Direct Connect and Direct Connect Gateway. and access AWS public services like S3. Transit VPC works for VPN, which means IPSEC that work over the internet :)",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 276859,
          "date": "Sat 06 Nov 2021 04:19",
          "username": "rcher",
          "content": "You can create Public VIF between Direct Connect and Direct Connect Gateway. and access AWS public services like S3. Transit VPC works for VPN, which means IPSEC that work over the internet :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229106,
          "date": "Thu 04 Nov 2021 12:38",
          "username": "Edgecrusher77",
          "content": "C, Transit VPC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183122,
          "date": "Wed 03 Nov 2021 17:45",
          "username": "ipindado2020",
          "content": "A.  Multiple DX connections very expensive: KO<br>B.  Transitive peering not allowed: KO. <br>C.  transit VPC: OK<br>D.  Direct Connect Gateway: OK <br><br>As for the additional costs of the transit VPC solution....<br> D seems to be correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#539",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS.<br>The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology. Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB, though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often for several hours.<br>Which is the MOST cost-effective solution?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#539",
          "answers": [
            {
              "choice": "<p>A. Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 104820,
          "date": "Sun 26 Sep 2021 03:54",
          "username": "meenu2225kirrim",
          "content": "I will go with B, B makes more sense out of the others. All of these apps are begging to be containerised :)Agree, ECS is the most cost-effective answer up there. Lambda would be great except that reporting app that runs for 2 hrs, which is a no go for Lambda with a max execution time of 900 seconds (15 mins):https://docs.aws.amazon.com/whitepapers/latest/serverless-architectures-lambda/timeout.html<br><br>Side note: It's not an available choice, but I'd argue that since these apps are only sporadically used, Fargate would likely be even more cost effective than EC2-based ECS:https://aws.amazon.com/blogs/containers/theoretical-cost-optimization-by-amazon-ecs-launch-type-fargate-vs-ec2/",
          "upvote_count": "232",
          "selected_answers": ""
        },
        {
          "id": 460198,
          "date": "Fri 05 Nov 2021 05:42",
          "username": "kirrim",
          "content": "Agree, ECS is the most cost-effective answer up there. Lambda would be great except that reporting app that runs for 2 hrs, which is a no go for Lambda with a max execution time of 900 seconds (15 mins):https://docs.aws.amazon.com/whitepapers/latest/serverless-architectures-lambda/timeout.html<br><br>Side note: It's not an available choice, but I'd argue that since these apps are only sporadically used, Fargate would likely be even more cost effective than EC2-based ECS:https://aws.amazon.com/blogs/containers/theoretical-cost-optimization-by-amazon-ecs-launch-type-fargate-vs-ec2/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 642999,
          "date": "Fri 05 Aug 2022 16:05",
          "username": "Sumit_Kumar",
          "content": "single deployment approach - elastic beanstalk",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 614391,
          "date": "Fri 10 Jun 2022 09:50",
          "username": "foxrj21",
          "content": "its B, ECS with ec2 or spot instances",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 581997,
          "date": "Wed 06 Apr 2022 21:17",
          "username": "bobsmith2000",
          "content": "It's B.  No-brainer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 577334,
          "date": "Tue 29 Mar 2022 09:21",
          "username": "Hasitha99",
          "content": "Based on the given answers,I will go with B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 535844,
          "date": "Sun 30 Jan 2022 01:24",
          "username": "jyrajan69jason2009",
          "content": "The key word is 'Most Cost Effective' so why no one looking at D, it says Reserved Instances which is designed to reduce costs, for me thats the answer.D is incorrect because by purchasing RIs based on the Max value set in an ASG will for sure waste resources because you are paying for the extra capacity unused. As matter of fact, because ECS does not have any additional cost and can utilize the RIs if you choose EC2, B for sure is cheaper than D. ",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 567160,
          "date": "Sun 13 Mar 2022 20:55",
          "username": "jason2009",
          "content": "D is incorrect because by purchasing RIs based on the Max value set in an ASG will for sure waste resources because you are paying for the extra capacity unused. As matter of fact, because ECS does not have any additional cost and can utilize the RIs if you choose EC2, B for sure is cheaper than D. ",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 520380,
          "date": "Sun 09 Jan 2022 18:43",
          "username": "Duke_YUuser0001",
          "content": "C is what Amazon want you to use, B is what most architect will choose in real life.see is right",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 597826,
          "date": "Fri 06 May 2022 20:55",
          "username": "user0001",
          "content": "see is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494678,
          "date": "Sun 05 Dec 2021 22:08",
          "username": "AzureDP900",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 453962,
          "date": "Mon 01 Nov 2021 00:12",
          "username": "tonikustonikus",
          "content": "I think it's C based on \\\"...while standardizing by using a single deployment methodology.\\\"hmmm, I guess I'm changing to B",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 454729,
          "date": "Fri 05 Nov 2021 05:33",
          "username": "tonikus",
          "content": "hmmm, I guess I'm changing to B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449892,
          "date": "Fri 29 Oct 2021 21:32",
          "username": "nodogoshi",
          "content": "B.  Microservice's question. ECS is for it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411560,
          "date": "Mon 25 Oct 2021 21:16",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 399210,
          "date": "Mon 25 Oct 2021 16:55",
          "username": "tuananhngo",
          "content": "what does cost-effective means? does it relate to price or lead time? we are not sure abt it. <br>I think the correct is C because B would take much effort to transform application to container type.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 361577,
          "date": "Wed 20 Oct 2021 09:25",
          "username": "kpcertStanM",
          "content": "Why not use the Lambda? The question has the follwing keywords<br>1. Node.js and Java <br>2. Infrequent Usage and memory requirement is max 2.5 GB<br>3. Less concurrency <br>4. Need Cost-effective solutionbecause one of the applications can run up to several hours while lambda can run only up to 15 mins. We could think of making an exception for this application and run everything else as lambda, but the question insists on standardizing.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 371368,
          "date": "Thu 21 Oct 2021 02:43",
          "username": "StanM",
          "content": "because one of the applications can run up to several hours while lambda can run only up to 15 mins. We could think of making an exception for this application and run everything else as lambda, but the question insists on standardizing.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 348554,
          "date": "Tue 19 Oct 2021 21:31",
          "username": "Waiweng",
          "content": "it's B fof cost effectiveness",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 346867,
          "date": "Tue 19 Oct 2021 05:14",
          "username": "blackgamer",
          "content": "B is the correct answer because it is more cost effective.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291820,
          "date": "Sun 17 Oct 2021 07:17",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 278317,
          "date": "Fri 15 Oct 2021 21:26",
          "username": "Ebi",
          "content": "B and C both seem correct, I will go with B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#540",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must build a highly available infrastructure for a popular global video game that runs on a mobile phone platform. The application runs on<br>Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The database tier is an Amazon RDS MySQL Multi-AZ instance. The entire application stack is deployed in both us-east-1 and eu-central-1. Amazon Route 53 is used to route traffic to the two installations using a latency-based routing policy. A weighted routing policy is configured in Route 53 as a fail over to another region in case the installation in a region becomes unresponsive.<br>During the testing of disaster recovery scenarios, after blocking access to the Amazon RDS MySQL instance in eu-central-1 from all the application instances running in that region. Route 53 does not automatically failover all traffic to us-east-1.<br>Based on this situation, which changes would allow the infrastructure to failover to us-east-1? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#540",
          "answers": [
            {
              "choice": "<p>A. Specify a weight of 100 for the record pointing to the primary Application Load Balancer in us-east-1 and a weight of 60 for the pointing to the primary Application Load Balancer in eu-central-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Specify a weight of 100 for the record pointing to the primary Application Load Balancer in us-east-1 and a weight of 0 for the record pointing to the primary Application Load Balancer in eu-central-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set the value of Evaluate Target Health to Yes on the latency alias resources for both eu-central-1 and us-east-1.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Write a URL in the application that performs a health check on the database layer. Add it as a health check within the weighted routing policy in both regions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Disable any existing health checks for the resources in the policies and set a weight of 0 for the records pointing to primary in both eu-central-1 and us-east-1, and set a weight of 100 for the primary Application Load Balancer only in the region that has healthy resources.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 75605,
          "date": "Wed 22 Sep 2021 02:16",
          "username": "haah",
          "content": "support CD",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 425186,
          "date": "Wed 03 Nov 2021 04:02",
          "username": "Madhu654",
          "content": "Health check page targeting a database is a bad idea. The question talks about a weighted routing policy. <br><br>If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.<br><br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-how-route-53-chooses-records.html<br><br>Answer is BC",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 609820,
          "date": "Tue 31 May 2022 18:54",
          "username": "ArreRaja",
          "content": "BC<br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 606724,
          "date": "Tue 24 May 2022 15:00",
          "username": "bobsmith2000",
          "content": "No-brainer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 495190,
          "date": "Mon 06 Dec 2021 15:13",
          "username": "Ronon",
          "content": "RDS won't use ALB, so A & B are incorrect, but we need to failover traffic to us-east-1. only B & C work.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494681,
          "date": "Sun 05 Dec 2021 22:10",
          "username": "AzureDP900",
          "content": "I will go with CD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 489732,
          "date": "Mon 29 Nov 2021 10:14",
          "username": "acloudguru",
          "content": "Question number is 357.<br>D.  Write a URL in the application that performs a health check on the database layer. Add it as a health check within the weighted routing policy in both regions.<br>Did we see the same question?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448247,
          "date": "Sun 07 Nov 2021 07:55",
          "username": "moon2351",
          "content": "I'll go with C&D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 435738,
          "date": "Wed 03 Nov 2021 07:13",
          "username": "tgv",
          "content": "CCC DDD<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411564,
          "date": "Tue 02 Nov 2021 02:37",
          "username": "WhyIronMan",
          "content": "I'll go with C,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348556,
          "date": "Thu 28 Oct 2021 10:26",
          "username": "Waiweng",
          "content": "it's C and D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 347017,
          "date": "Wed 27 Oct 2021 19:28",
          "username": "blackgamer",
          "content": "C and D is correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294914,
          "date": "Sat 16 Oct 2021 02:09",
          "username": "kiev",
          "content": "CD and yes it is a repeat question. I like some of these as it helps consolidate my understanding of the questions.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278331,
          "date": "Wed 13 Oct 2021 21:08",
          "username": "Ebi",
          "content": "I will go with CD",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243886,
          "date": "Tue 12 Oct 2021 03:41",
          "username": "T14102020",
          "content": "Correct is CD.  Evaluate target health+ health check DB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231550,
          "date": "Sat 09 Oct 2021 03:48",
          "username": "jackdryan",
          "content": "I'll go with C,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 133402,
          "date": "Fri 01 Oct 2021 17:08",
          "username": "NikkyDickyPhat",
          "content": "CD, dup of Q 142dup with q140, not 142.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 170888,
          "date": "Sun 03 Oct 2021 18:57",
          "username": "Phat",
          "content": "dup with q140, not 142.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#541",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An online e-commerce business is running a workload on AWS. The application architecture includes a web tier, an application tier for business logic, and a database tier for user and transactional data management. The database server has a 100 GB memory requirement. The business requires cost-efficient disaster recovery for the application with an RTO of 5 minutes and an RPO of 1 hour. The business also has a regulatory for out-of-region disaster recovery with a minimum distance between the primary and alternate sites of 250 miles.<br>Which of the following options can the Solutions Architect design to create a comprehensive solution for this customer that meets the disaster recovery requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#541",
          "answers": [
            {
              "choice": "<p>A. Back up the application and database data frequently and copy them to Amazon S3. Replicate the backups using S3 cross-region replication, and use AWS CloudFormation to instantiate infrastructure for disaster recovery and restore data from Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Employ a pilot light environment in which the primary database is configured with mirroring to build a standby database on m4.large in the alternate region. Use AWS CloudFormation to instantiate the web servers, application servers and load balancers in case of a disaster to bring the application up in the alternate region. Vertically resize the database to meet the full production demands, and use Amazon Route 53 to switch traffic to the alternate region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use a scaled-down version of the fully functional production environment in the alternate region that includes one instance of the web server, one instance of the application server, and a replicated instance of the database server in standby mode. Place the web and the application tiers in an Auto Scaling behind a load balancer, which can automatically scale when the load arrives to the application. Use Amazon Route 53 to switch traffic to the alternate region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Employ a multi-region solution with fully functional web, application, and database tiers in both regions with equivalent capacity. Activate the primary database in one region only and the standby database in the other region. Use Amazon Route 53 to automatically switch traffic from one region to another using health check routing policies.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 160583,
          "date": "Tue 05 Oct 2021 00:51",
          "username": "TK2019",
          "content": "This should be C.  As RTO is in minutes (https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html)<br>Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down version of a fully functional environment always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load.",
          "upvote_count": "20",
          "selected_answers": ""
        },
        {
          "id": 75604,
          "date": "Sat 25 Sep 2021 07:28",
          "username": "haah",
          "content": "support C",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 715642,
          "date": "Fri 11 Nov 2022 01:38",
          "username": "Relaxeasy",
          "content": "I will go with C. . No brainer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714442,
          "date": "Wed 09 Nov 2022 10:55",
          "username": "whuzzup",
          "content": "support C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 623540,
          "date": "Tue 28 Jun 2022 01:32",
          "username": "kangtamo",
          "content": "Agree with C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 542608,
          "date": "Mon 07 Feb 2022 19:32",
          "username": "nickstudy7Jonfernz",
          "content": "Why Not C? Pilot Light<br>https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-iii-pilot-light-and-warm-standby/All those Cloudformation stacks will take way longer than 5 minutes to deploy. Will not satisfy RTO.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 690632,
          "date": "Mon 10 Oct 2022 03:26",
          "username": "Jonfernz",
          "content": "All those Cloudformation stacks will take way longer than 5 minutes to deploy. Will not satisfy RTO.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 519128,
          "date": "Fri 07 Jan 2022 18:44",
          "username": "SmartphonePunitsolanki",
          "content": "RTO is 5 minutes. Hence is D. It should be C.  <br><br>Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. The more scaled-up the Warm Standby is, the lower RTO and control plane reliance will be. When scaled up to full scale this is known as a Hot Standby.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 542851,
          "date": "Tue 08 Feb 2022 05:14",
          "username": "Punitsolanki",
          "content": "It should be C.  <br><br>Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. The more scaled-up the Warm Standby is, the lower RTO and control plane reliance will be. When scaled up to full scale this is known as a Hot Standby.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 498454,
          "date": "Fri 10 Dec 2021 10:05",
          "username": "cldy",
          "content": "C.  Use a scaled-down version of the fully functional production environment in the alternate region that includes one instance of the web server, one instance of the application server, and a replicated instance of the database server in standby mode. Place the web and the application tiers in an Auto Scaling behind a load balancer, which can automatically scale when the load arrives to the application. Use Amazon Route 53 to switch traffic to the alternate region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494682,
          "date": "Sun 05 Dec 2021 22:13",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450760,
          "date": "Fri 29 Oct 2021 15:39",
          "username": "AWSum1",
          "content": "C - RTO is 5 min and its a scaled down environment. D would be good if it needed to be active active and cost was not a concern",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435830,
          "date": "Tue 26 Oct 2021 16:54",
          "username": "student22CotterViper57student22",
          "content": "D<br>C is more cost effective but it seems to need manual switching over which we can't guarantee to happen in 5 minutes. D is automatic.Why? you focus cost, is there the question about cost effective?The questions says \\\"The business requires cost-efficient disaster recovery for the application with an RTO of 5 minutes and an RPO of 1 hour.\\\". It's always good to read the question carefully or you will miss important details.Good point. Changing my answers to C. ",
          "upvote_count": "3131",
          "selected_answers": ""
        },
        {
          "id": 440060,
          "date": "Wed 27 Oct 2021 03:31",
          "username": "CotterViper57student22",
          "content": "Why? you focus cost, is there the question about cost effective?The questions says \\\"The business requires cost-efficient disaster recovery for the application with an RTO of 5 minutes and an RPO of 1 hour.\\\". It's always good to read the question carefully or you will miss important details.Good point. Changing my answers to C. ",
          "upvote_count": "131",
          "selected_answers": ""
        },
        {
          "id": 445419,
          "date": "Fri 29 Oct 2021 11:25",
          "username": "Viper57student22",
          "content": "The questions says \\\"The business requires cost-efficient disaster recovery for the application with an RTO of 5 minutes and an RPO of 1 hour.\\\". It's always good to read the question carefully or you will miss important details.Good point. Changing my answers to C. ",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 457618,
          "date": "Tue 02 Nov 2021 00:39",
          "username": "student22",
          "content": "Good point. Changing my answers to C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411569,
          "date": "Sun 24 Oct 2021 14:08",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411093,
          "date": "Sat 23 Oct 2021 14:38",
          "username": "jobe42",
          "content": "C, for D i miss the replication stuff, just another database in standby missing the data from the active node. Best thing is here a Auroa Global Database",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 401705,
          "date": "Sat 16 Oct 2021 12:39",
          "username": "Kopa",
          "content": "Im for C, it fulfills the requirements and its more cost effective then D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349057,
          "date": "Fri 15 Oct 2021 06:57",
          "username": "Waiweng",
          "content": "support C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 347026,
          "date": "Wed 13 Oct 2021 01:47",
          "username": "blackgamer",
          "content": "Not very good design, but closest one is C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294915,
          "date": "Sun 10 Oct 2021 02:13",
          "username": "kievnitinzsarah_t",
          "content": "C and another repeat question. Honestly guys I wanted to ask a genuine question here. Does anyone resort to cramming some of the answers? Because I do my best to analyse things and get most of them right but I have had to cram some of them and if I see those questions, I kind of know the answers already.I do 3 revisions, one first pass nothing makes sense, on second pass things kind of make sense. on 3 pass things are crystal clear but some are still not making sense. those which are not making sense after 3rd pass, I copy paste those questions and memorize the keywords in question and keywords in answer. hope it helps.No, as the questions on the exam may vary. <br>If I am confident in my answer, I check with the comments. If I'm right, I move on. <br>If I am not confident, I look for explanations, links and resources in the comments and try to figure out the exact reasons. <br>If I have no clue whatsoever, I go read the documentation on AWS and/or try it myself in my AWS account(s) before even looking at the comments. Only when I think I know the answer, I check. <br><br>This is somewhat time-consuming but that way I really learn the stuff, not cram for the exam alone.",
          "upvote_count": "133",
          "selected_answers": ""
        },
        {
          "id": 321595,
          "date": "Sun 10 Oct 2021 22:50",
          "username": "nitinz",
          "content": "I do 3 revisions, one first pass nothing makes sense, on second pass things kind of make sense. on 3 pass things are crystal clear but some are still not making sense. those which are not making sense after 3rd pass, I copy paste those questions and memorize the keywords in question and keywords in answer. hope it helps.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 333746,
          "date": "Mon 11 Oct 2021 05:08",
          "username": "sarah_t",
          "content": "No, as the questions on the exam may vary. <br>If I am confident in my answer, I check with the comments. If I'm right, I move on. <br>If I am not confident, I look for explanations, links and resources in the comments and try to figure out the exact reasons. <br>If I have no clue whatsoever, I go read the documentation on AWS and/or try it myself in my AWS account(s) before even looking at the comments. Only when I think I know the answer, I check. <br><br>This is somewhat time-consuming but that way I really learn the stuff, not cram for the exam alone.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#542",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a memory-intensive analytics application using on-demand Amazon EC2 C5 compute optimized instance. The application is used continuously and application demand doubles during working hours. The application currently scales based on CPU usage. When scaling in occurs, a lifecycle hook is used because the instance requires 4 minutes to clean the application state before terminating.<br>Because users reported poor performance during working hours, scheduled scaling actions were implemented so additional instances would be added during working hours. The Solutions Architect has been asked to reduce the cost of the application.<br>Which solution is MOST cost-effective?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#542",
          "answers": [
            {
              "choice": "<p>A. Use the existing launch configuration that uses C5 instances, and update the application AMI to include the Amazon CloudWatch agent. Change the Auto Scaling policies to scale based on memory utilization. Use Reserved Instances for the number of instances required after working hours, and use Spot Instances to cover the increased demand during working hours.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the existing launch configuration to use R5 instances, and update the application AMI to include SSM Agent. Change the Auto Scaling policies to scale based on memory utilization. Use Reserved Instances for the number of instances required after working hours, and use Spot Instances with on-Demand instances to cover the increased demand during working hours.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the existing launch configuration that uses C5 instances, and update the application AMI to include SSM Agent. Leave the Auto Scaling policies to scale based on CPU utilization. Use scheduled Reserved Instances for the number of instances required after working hours, and use Spot Instances to cover the increased demand during working hours.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new launch configuration using R5 instances, and update the application AMI to include the Amazon CloudWatch agent. Change the Auto Scaling policies to scale based on memory utilization. Use Reserved Instances for the number of instances required after working hours, and use Standard Reserved Instances with On-Demand Instances to cover the increased demand during working hours.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65080,
          "date": "Tue 21 Sep 2021 04:54",
          "username": "jay1ram2joanneli77Amitv2706",
          "content": "The \\\"When scaling in occurs, a lifecycle hook is used because the instance requires 4 minutes to clean the application state before terminating.\\\"requirement eliminates spot nodes. So, R5 with reserved and on-demand is a more suitable choice. Option DThis eliminates A, B and C. .. nicely done.good point coz spot instance termination notice time is 2 mins,",
          "upvote_count": "1713",
          "selected_answers": ""
        },
        {
          "id": 693323,
          "date": "Wed 12 Oct 2022 20:33",
          "username": "joanneli77",
          "content": "This eliminates A, B and C. .. nicely done.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 380821,
          "date": "Sat 30 Oct 2021 14:37",
          "username": "Amitv2706",
          "content": "good point coz spot instance termination notice time is 2 mins,",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 104823,
          "date": "Mon 27 Sep 2021 10:04",
          "username": "meenu2225AWSum1",
          "content": "One thing: You cannot update the existing launch configuration i.e.update the AMI inside it. Which leaves only option D. Probably the best and most direct way of answering this question",
          "upvote_count": "61",
          "selected_answers": ""
        },
        {
          "id": 450764,
          "date": "Sun 31 Oct 2021 05:33",
          "username": "AWSum1",
          "content": "Probably the best and most direct way of answering this question",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715645,
          "date": "Fri 11 Nov 2022 01:41",
          "username": "Relaxeasy",
          "content": "Option D. . Spot instance will not work here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709025,
          "date": "Tue 01 Nov 2022 11:13",
          "username": "nsvijay04b1",
          "content": "only option that doesn't say editexisting LC/LT.<br>Spot not suitable for 4 min life cycle for cleanup<br>reserved instances to save cost.<br>It wud be nice to have in the option \\\"Scheduled RIs\\\" for day time load and \\\"Standard RIs\\\" otherwise all the time.<br><br>https://aws.amazon.com/ec2/pricing/reserved-instances/<br><br>Standard RIs: These provide the most significant discount (up to 72% off On-Demand) and are best suited for steady-state usage.<br><br>Scheduled RIs: These are available to launch within the time windows you reserve. This option allows you to match your capacity reservation to a predictable recurring schedule that only requires a fraction of a day, a week, or a month.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 496742,
          "date": "Wed 08 Dec 2021 11:49",
          "username": "cldy",
          "content": "D.  Create a new launch configuration using R5 instances, and update the application AMI to include the Amazon CloudWatch agent. Change the Auto Scaling policies to scale based on memory utilization. Use Reserved Instances for the number of instances required after working hours, and use Standard Reserved Instances with On-Demand Instances to cover the increased demand during working hours.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494686,
          "date": "Sun 05 Dec 2021 22:18",
          "username": "AzureDP900",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411575,
          "date": "Sat 30 Oct 2021 21:38",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349061,
          "date": "Sat 30 Oct 2021 06:08",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 347038,
          "date": "Mon 25 Oct 2021 12:03",
          "username": "blackgamer",
          "content": "The answer is D.  SSM agent is not relevant here and the application is memory-intensive which R5 instances are memory optimised instances. Need cloudwatch agent to install to monitor memory utilisation as it is custom metric.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278415,
          "date": "Wed 20 Oct 2021 09:18",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 257632,
          "date": "Sat 16 Oct 2021 07:53",
          "username": "MichaelHuang01037ujizane",
          "content": "Why not B?<br>R5 is for memory intensive, so B or D; SSM for memory metric, so only B. \\\"the instance requires 4 minutes to clean the application state before terminating\\\" rules out Spot Instanceneed CloudWatch agent for memory metrix not ssm agent",
          "upvote_count": "123",
          "selected_answers": ""
        },
        {
          "id": 261400,
          "date": "Tue 19 Oct 2021 20:48",
          "username": "01037",
          "content": "\\\"the instance requires 4 minutes to clean the application state before terminating\\\" rules out Spot Instance",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 294695,
          "date": "Sun 24 Oct 2021 14:02",
          "username": "ujizane",
          "content": "need CloudWatch agent for memory metrix not ssm agent",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243893,
          "date": "Fri 15 Oct 2021 00:24",
          "username": "T14102020",
          "content": "Correct is D.  New lunches + RI + without Spot",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231556,
          "date": "Tue 12 Oct 2021 10:42",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230491,
          "date": "Thu 07 Oct 2021 03:10",
          "username": "srinivasaalexmena1981",
          "content": "Memory Utilization of EC2 is not a cloud watch metric.<br>C is the right answerYou are wrong https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 231418,
          "date": "Thu 07 Oct 2021 09:16",
          "username": "alexmena1981",
          "content": "You are wrong https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209729,
          "date": "Mon 04 Oct 2021 06:50",
          "username": "CYL",
          "content": "D.  Spot instance not suitable for such use case due to terminating timing and working hours expectation. Use R5 EC2 instances for memory intensive usecases.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 133424,
          "date": "Sun 03 Oct 2021 11:09",
          "username": "NikkyDicky",
          "content": "D.  Dup of Q144",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 108028,
          "date": "Wed 29 Sep 2021 21:45",
          "username": "JAWS1600VrushaliD",
          "content": "I agree to D.  <br>For folks that think that launch config cannot be updated/changed. here is the link<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.htmlreserved instances for increased load??",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 109648,
          "date": "Sun 03 Oct 2021 09:42",
          "username": "VrushaliD",
          "content": "reserved instances for increased load??",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#543",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a data center that must be migrated to AWS as quickly as possible. The data center has a 500 Mbps AWS Direct Connect link and a separate, fully available 1 Gbps ISP connection. A Solutions Architect must transfer 20 TB of data from the data center to an Amazon S3 bucket.<br>What is the FASTEST way transfer the data?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#543",
          "answers": [
            {
              "choice": "<p>A. Upload the data to the S3 bucket using the existing DX link.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Send the data to AWS using the AWS Import/Export service.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Upload the data using an 80 TB AWS Snowball device.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload the data to the S3 bucket using S3 Transfer Acceleration.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65083,
          "date": "Mon 20 Sep 2021 08:26",
          "username": "jay1ram2",
          "content": "Transfer Acceleration over a fully available 1 Gbps can theoretically move around 10TB/Day. <br><br>1 Gbps = (1024/8) MBPs = 128 MBps <br>(128 MBps * 3600 secs * 24 Hrs)/1024 = 10,800 GB/Day = 10TB/Day<br><br>Along with Transfer Acceleration, which provides aconsistent experience, the entire data can be moved in 2 days. However AWS Import/Export (now snowball) takes around a week to make the data available on AWS. The Answer is D. ",
          "upvote_count": "24",
          "selected_answers": ""
        },
        {
          "id": 714934,
          "date": "Thu 10 Nov 2022 06:28",
          "username": "janvandermerwer",
          "content": "D looks to be the best answer. - Especially with the brief stating \\\"fastest solution\\\"<br>Snowball and import/export likely will be to slow (~6 days) vs potentially 2-3 days using s3 transfer acceleration.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 680832,
          "date": "Tue 27 Sep 2022 15:54",
          "username": "tomosabc1",
          "content": "The answer should be D.  Agree with jay1ram2's analysis.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 596365,
          "date": "Tue 03 May 2022 14:48",
          "username": "aloha123",
          "content": "AWS Import/Export is now Snowball.It takes about 4-6 days to receive a Snowball. With option D we can complete in 2 days.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 538328,
          "date": "Wed 02 Feb 2022 03:26",
          "username": "HellGate",
          "content": "My answer is C<br><br>Question didn’t say about location information so it’s not easy to compare transfer rate thru S3 Transfer Acceleration .When I check aws document I could find pretty similar case with mig data of 25TB from below link on example 2.<br><br>https://aws.amazon.com/snowball/pricing/<br><br>However, it's worth to memorize 1Gbps = 10TB/Day.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 527185,
          "date": "Wed 19 Jan 2022 04:02",
          "username": "tkanmani76",
          "content": "D - Considering the data is 20 TB we can complete in 2 days using S3 Transfer acceleration - if we look at higher data say 50TB, then it would make sense to use Snowball as it would take same time (say 5 days) but almost 60% cheaper than S3 TA. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494688,
          "date": "Sun 05 Dec 2021 22:20",
          "username": "AzureDP900",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449917,
          "date": "Sat 06 Nov 2021 12:46",
          "username": "nodogoshi",
          "content": "A.  S3 Transfer Acceleration do nothing in this situation. simply upload with direct connect.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411581,
          "date": "Wed 03 Nov 2021 21:40",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349083,
          "date": "Mon 01 Nov 2021 23:52",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 347044,
          "date": "Sun 31 Oct 2021 14:12",
          "username": "blackgamer",
          "content": "D seems to be the most suitable. But very ambiguous answers still.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 330472,
          "date": "Mon 18 Oct 2021 08:11",
          "username": "Sunflyhomesarah_tWhyIronMan",
          "content": "Nooo D!<br>Please never use business internet to transfer huge data with full bandwidth. Your boss even get delay to open Google.<br>A is my choice. it doesn't need logistic and transport time for arrival, copy, shipping, etc.<br>C seems old since AWS started to offer Snowball mobile edge <br>https://aws.amazon.com/snowball/faqs/<br>https://aws.amazon.com/snowball/faqs/The question says \\\"a separate, fully available 1 Gbps ISP connection\\\". And asks for the FASTEST solution. <br><br>It may not be ideal in a real-world setting, but it IS the fastest.\\\"a separate, fully available 1 Gbps ISP connection\\\". <br>\\\"FASTEST solution\\\"<br>I'll go with D",
          "upvote_count": "221",
          "selected_answers": ""
        },
        {
          "id": 333765,
          "date": "Fri 29 Oct 2021 09:21",
          "username": "sarah_t",
          "content": "The question says \\\"a separate, fully available 1 Gbps ISP connection\\\". And asks for the FASTEST solution. <br><br>It may not be ideal in a real-world setting, but it IS the fastest.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411580,
          "date": "Wed 03 Nov 2021 14:59",
          "username": "WhyIronMan",
          "content": "\\\"a separate, fully available 1 Gbps ISP connection\\\". <br>\\\"FASTEST solution\\\"<br>I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294921,
          "date": "Sun 17 Oct 2021 05:54",
          "username": "kiev",
          "content": "Another repeat question and D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278420,
          "date": "Sat 16 Oct 2021 14:52",
          "username": "Ebi",
          "content": "Using 1Gbps internet speed you can transfer around 10TB of data per day much less than Snowball<br>D is my choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243896,
          "date": "Mon 11 Oct 2021 03:06",
          "username": "T14102020",
          "content": "Correct is D.  S3 Transfer Acceleration",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 231557,
          "date": "Sun 10 Oct 2021 06:29",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 183193,
          "date": "Fri 08 Oct 2021 06:01",
          "username": "ipindado2020",
          "content": "D for sure...<br>Public upload should be fastest that DX",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#544",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to host its website on AWS using serverless architecture design patterns for global customers. The company has outlined its requirements as follow:<br>✑ The website should be responsive.<br>✑ The website should offer minimal latency.<br>✑ The website should be highly available.<br>✑ Users should be able to authenticate through social identity providers such as Google, Facebook, and Amazon.<br>✑ There should be baseline DDoS protections for spikes in traffic.<br>How can the design requirements be met?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#544",
          "answers": [
            {
              "choice": "<p>A. Use Amazon CloudFront with Amazon ECS for hosting the website. Use AWS Secrets Manager to provide user management and authentication functions. Use ECS Docker containers to build an API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Route 53 latency routing with an Application Load Balancer and AWS Fargate in different regions for hosting the website. Use Amazon Cognito to provide user management and authentication functions. Use Amazon EKS containers to build an API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon CloudFront with Amazon S3 for hosting static web resources. Use Amazon Cognito to provide user management and authentication functions. Use Amazon API Gateway with AWS Lambda to build an API.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Direct Connect with Amazon CloudFront and Amazon S3 for hosting static web resources. Use Amazon Cognito to provide user management authentication functions. Use AWS Lambda to build an API.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 313820,
          "date": "Sat 16 Oct 2021 17:35",
          "username": "daviensAum",
          "content": "C.  The question mentions protection against DDoS attack. You can setup WAF on API gateway but you cannot set WAF on EKS, ECS nor Lambdabaseline DDos also available when using Route 53",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 715709,
          "date": "Fri 11 Nov 2022 03:35",
          "username": "Aum",
          "content": "baseline DDos also available when using Route 53",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 624499,
          "date": "Wed 29 Jun 2022 09:57",
          "username": "TechX",
          "content": "Why not B, I see B meet the requirement...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494689,
          "date": "Sun 05 Dec 2021 22:21",
          "username": "AzureDP900",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493420,
          "date": "Sat 04 Dec 2021 00:57",
          "username": "vbal",
          "content": "the ECS control plane services (those services that deliver ECS management capabilities to our customers) do not take service dependencies outside of the Region in which we are operating to avoid multi-region impact - Not sure if that means ECS Custer is Regional?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411590,
          "date": "Fri 05 Nov 2021 15:58",
          "username": "WhyIronMan",
          "content": "I'll go with C<br><br>That's why you'll use Amazon API Gateway with AWS Lambda to build an API.<br>And recall that:<br>A company wants to host its website on AWS using serverless architecture design patterns <br><br>SAM is not compatible with EKS but it is with Lambda and API Gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 367564,
          "date": "Mon 25 Oct 2021 10:48",
          "username": "digimaniacWhyIronMan",
          "content": "B, what does a website is responsive mean? It is called Responsive UI framework. This needs server side scripting, which means you can't use S3 for hosting.That's why you'll use Amazon API Gateway with AWS Lambda to build an API.<br>And recall that:<br>A company wants to host its website on AWS using serverless architecture design patterns <br>SAM is not compatible with EKS but it is with Lambda and API Gateway",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 411589,
          "date": "Tue 02 Nov 2021 16:39",
          "username": "WhyIronMan",
          "content": "That's why you'll use Amazon API Gateway with AWS Lambda to build an API.<br>And recall that:<br>A company wants to host its website on AWS using serverless architecture design patterns <br>SAM is not compatible with EKS but it is with Lambda and API Gateway",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349124,
          "date": "Sun 24 Oct 2021 23:37",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 278423,
          "date": "Thu 07 Oct 2021 17:25",
          "username": "Ebi",
          "content": "C is the answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 243900,
          "date": "Tue 05 Oct 2021 08:40",
          "username": "T14102020",
          "content": "Correct is C.  CloudFront + S3 + Cognito.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231558,
          "date": "Fri 01 Oct 2021 21:25",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209790,
          "date": "Sun 26 Sep 2021 15:11",
          "username": "CYL",
          "content": "C.  These combination of services are stateless and highly scaleable.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133431,
          "date": "Sat 25 Sep 2021 21:02",
          "username": "NikkyDicky",
          "content": "C.  Dup of Q146",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 104827,
          "date": "Thu 23 Sep 2021 07:12",
          "username": "meenu2225",
          "content": "Although I like option B, but its has both Fargate and EKS, this option needs bit more explaination. Option D is a bad design, Direct connect seriously?. Option A is missing congito/user authentication method. Which leaves only option C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 98901,
          "date": "Thu 23 Sep 2021 02:56",
          "username": "Bouji1982",
          "content": "D is the answer, Npo need for API-GW",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 80094,
          "date": "Wed 22 Sep 2021 16:13",
          "username": "fw",
          "content": "Answer is C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 68971,
          "date": "Wed 22 Sep 2021 10:23",
          "username": "LunchTimenil3112",
          "content": "This is a replication of Question #146Topic 2Hi<br>is that means this question#543is not from topic2 ?",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 337230,
          "date": "Tue 19 Oct 2021 16:34",
          "username": "nil3112",
          "content": "Hi<br>is that means this question#543is not from topic2 ?",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#545",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is currently using AWS CodeCommit for its source control and AWS CodePipeline for continuous integration. The pipeline has a build stage for building the artifacts, which is then staged in an Amazon S3 bucket.<br>The company has identified various improvement opportunities in the existing process, and a Solutions Architect has been given the following requirements:<br>✑ Create a new pipeline to support feature development<br>✑ Support feature development without impacting production applications<br>✑ Incorporate continuous testing with unit tests<br>✑ Isolate development and production artifacts<br>✑ Support the capability to merge tested code into production code.<br>How should the Solutions Architect achieve these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#545",
          "answers": [
            {
              "choice": "<p>A. Trigger a separate pipeline from CodeCommit feature branches. Use AWS CodeBuild for running unit tests. Use CodeBuild to stage the artifacts within an S3 bucket in a separate testing account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Trigger a separate pipeline from CodeCommit feature branches. Use AWS Lambda for running unit tests. Use AWS CodeDeploy to stage the artifacts within an S3 bucket in a separate testing account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Trigger a separate pipeline from CodeCommit tags. Use Jenkins for running unit tests. Create a stage in the pipeline with S3 as the target for staging the artifacts with an S3 bucket in a separate testing account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a separate CodeCommit repository for feature development and use it to trigger the pipeline. Use AWS Lambda for running unit tests. Use AWS CodeBuild to stage the artifacts within different S3 buckets in the same production account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 278425,
          "date": "Mon 18 Oct 2021 08:52",
          "username": "Ebi",
          "content": "I will go with A",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 499112,
          "date": "Sat 11 Dec 2021 04:51",
          "username": "challenger1",
          "content": "My Answer: A<br>Use codebuild the whole way",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494691,
          "date": "Sun 05 Dec 2021 22:23",
          "username": "AzureDP900",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 454784,
          "date": "Thu 04 Nov 2021 19:15",
          "username": "tonikus",
          "content": "A is the answer since all others are nonsense (considering this is AWS exam )",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 411593,
          "date": "Thu 04 Nov 2021 19:12",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349126,
          "date": "Wed 27 Oct 2021 16:46",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 294922,
          "date": "Thu 21 Oct 2021 13:56",
          "username": "kievnil3112tvs",
          "content": "A and it is another repeatHi Liev<br>Could you please help in identifying topic2 questions. i was told that after page40, topic2 questions are there but as per your comment it seems this 545 belongs to topic1Did you found from where topic 2 starts",
          "upvote_count": "123",
          "selected_answers": ""
        },
        {
          "id": 337232,
          "date": "Sat 23 Oct 2021 05:39",
          "username": "nil3112tvs",
          "content": "Hi Liev<br>Could you please help in identifying topic2 questions. i was told that after page40, topic2 questions are there but as per your comment it seems this 545 belongs to topic1Did you found from where topic 2 starts",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 351032,
          "date": "Thu 28 Oct 2021 08:35",
          "username": "tvs",
          "content": "Did you found from where topic 2 starts",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243906,
          "date": "Wed 13 Oct 2021 17:00",
          "username": "T14102020",
          "content": "Correct is A.  CodeBuild for unit tests",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 231561,
          "date": "Mon 11 Oct 2021 08:56",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209793,
          "date": "Sat 09 Oct 2021 07:26",
          "username": "CYL",
          "content": "A.  Codebuild to verify against a feature branch before merging. Store artifacts in S3.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 133433,
          "date": "Wed 06 Oct 2021 03:25",
          "username": "NikkyDicky",
          "content": "A.  Dup of Q147",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 132416,
          "date": "Mon 04 Oct 2021 22:25",
          "username": "seamas",
          "content": "A, AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 87813,
          "date": "Fri 24 Sep 2021 01:18",
          "username": "Mkumar",
          "content": "Answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 68973,
          "date": "Thu 23 Sep 2021 15:06",
          "username": "LunchTime",
          "content": "This is a replication of Question #147Topic 2",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#546",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an ordering system on AWS using Amazon SQS and AWS Lambda, with each order received as a JSON message. Recently the company had a marketing event that led to a tenfold increase in orders. With this increase, the following undesired behaviors started in the ordering system:<br>✑ Lambda failures while processing orders lead to queue backlogs.<br>✑ The same orders have been processed multiple times.<br>A Solutions Architect has been asked to solve the existing issues with the ordering system and add the following resiliency features:<br>✑ Retain problematic orders for analysis.<br>✑ Send notification if errors go beyond a threshold value.<br>How should the Solutions Architect meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#546",
          "answers": [
            {
              "choice": "<p>A. Receive multiple messages with each Lambda invocation, add error handling to message processing code and delete messages after processing, increase the visibility timeout for the messages, create a dead letter queue for messages that could not be processed, create an Amazon CloudWatch alarm on Lambda errors for notification.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Receive single messages with each Lambda invocation, put additional Lambda workers to poll the queue, delete messages after processing, increase the message timer for the messages, use Amazon CloudWatch Logs for messages that could not be processed, create a CloudWatch alarm on Lambda errors for notification.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Receive multiple messages with each Lambda invocation, use long polling when receiving the messages, log the errors from the message processing code using Amazon CloudWatch Logs, create a dead letter queue with AWS Lambda to capture failed invocations, create CloudWatch events on Lambda errors for notification.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Receive multiple messages with each Lambda invocation, add error handling to message processing code and delete messages after processing, increase the visibility timeout for the messages, create a delay queue for messages that could not be processed, create an Amazon CloudWatch metric on Lambda errors for notification.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65316,
          "date": "Wed 22 Sep 2021 19:58",
          "username": "jay1ram2RedKaneJuks",
          "content": "The correct answer is A.  <br><br>B - Single message/lambda will increase concurrency requirements and increased failure rates. There is no \\\"Lambda workers\\\" just increased concurrency limit. <br>C - There is no long polling in Lambda<br>D is incorrect, the delay queue is used to throttle incoming messages and not handle messages that could not be processed.https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html \\\"For standard queues, Lambda uses long polling to poll a queue until it becomes active.\\\"Long polling is not needed as the incoming message count is high but increasing the visibility timeout in option A will help with the processing of the message.",
          "upvote_count": "2721",
          "selected_answers": ""
        },
        {
          "id": 329068,
          "date": "Sat 23 Oct 2021 12:03",
          "username": "RedKaneJuks",
          "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html \\\"For standard queues, Lambda uses long polling to poll a queue until it becomes active.\\\"Long polling is not needed as the incoming message count is high but increasing the visibility timeout in option A will help with the processing of the message.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 494631,
          "date": "Sun 05 Dec 2021 21:19",
          "username": "Juks",
          "content": "Long polling is not needed as the incoming message count is high but increasing the visibility timeout in option A will help with the processing of the message.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 79436,
          "date": "Thu 30 Sep 2021 13:20",
          "username": "santanu7701037",
          "content": "Delay will increase unprocessed msgs on the queue.Exactly opposite of what is needed.We rather need a dead letter queue for messages that could not be handled.Option C.  <br><br>D is wrong becuase of Delay queue<br>B is wrong Single order per message will increase more messages in the queue <br>A and C are possible choices<br>A is better as increasing the visibility timeout decreases the possibility of duplicate message processing.C is wrong, because no mention of deleting message.",
          "upvote_count": "62",
          "selected_answers": ""
        },
        {
          "id": 261414,
          "date": "Sun 17 Oct 2021 08:50",
          "username": "01037",
          "content": "C is wrong, because no mention of deleting message.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 636030,
          "date": "Sun 24 Jul 2022 13:57",
          "username": "CloudHandsOn",
          "content": "A. <br>DLQ and CW Alarms is all you need :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497351,
          "date": "Thu 09 Dec 2021 06:27",
          "username": "cldy",
          "content": "A.  Receive multiple messages with each Lambda invocation, add error handling to message processing code and delete messages after processing, increase the visibility timeout for the messages, create a dead letter queue for messages that could not be processed, create an Amazon CloudWatch alarm on Lambda errors for notification.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494693,
          "date": "Sun 05 Dec 2021 22:25",
          "username": "AzureDP900",
          "content": "Correct Answer is A.  Dead Letter Queue, increased visibility timeout",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484653,
          "date": "Tue 23 Nov 2021 01:27",
          "username": "acloudguru",
          "content": "A is best option due to mention of dead-letter queue.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 411602,
          "date": "Sat 06 Nov 2021 19:36",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 383051,
          "date": "Mon 01 Nov 2021 22:26",
          "username": "sydon",
          "content": "C<br>long polling(SQS) and dead-letter queue<br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349127,
          "date": "Mon 01 Nov 2021 15:57",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 347369,
          "date": "Sat 30 Oct 2021 10:39",
          "username": "blackgamer",
          "content": "Yes, A is the answer. Dead Letter Queue for failed messages.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 337459,
          "date": "Tue 26 Oct 2021 13:07",
          "username": "Amitv2706",
          "content": "A is best option due to mention of dead-letter queue.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 330307,
          "date": "Sat 23 Oct 2021 17:21",
          "username": "KnightVictor",
          "content": "i'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278434,
          "date": "Fri 22 Oct 2021 12:18",
          "username": "Ebi",
          "content": "A is the best answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 258118,
          "date": "Sat 16 Oct 2021 09:34",
          "username": "Bulti",
          "content": "Correct Answer is A.  Dead Letter Queue, increased visibility timeout, multiple messages all satisfy the requriements.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243917,
          "date": "Fri 15 Oct 2021 02:19",
          "username": "T14102020",
          "content": "Correct is A. dead letter queue NOT delay letter queue",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 231562,
          "date": "Thu 14 Oct 2021 09:06",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209800,
          "date": "Mon 11 Oct 2021 06:26",
          "username": "CYL",
          "content": "A.  Dead letter queue for error messages.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#547",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0036100001.png\" class=\"in-exam-image\"><br>An organization has recently grown through acquisitions. Two of the purchased companies use the same IP CIDR range. There is a new short-term requirement to allow AnyCompany A (VPC-A) to communicate with a server that has the IP address 10.0.0.77 in AnyCompany B (VPC-B). AnyCompany A must also communicate with all resources in AnyCompany C (VPC-C). The Network team has created the VPC peer links, but it is having issues with communications between VPC-A and VPC-B.  After an investigation, the team believes that the routing tables in the VPCs are incorrect.<br>What configuration will allow AnyCompany A to communicate with AnyCompany C in addition to the database in AnyCompany B?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#547",
          "answers": [
            {
              "choice": "<p>A. On VPC-A, create a static route for the VPC-B CIDR range (10.0.0.0/24) across VPC peer pcx-AB.  Create a static route of 10.0.0.0/16 across VPC peer pcx-AC.  On VPC-B, create a static route for VPC-A CIDR (172.16.0.0/24) on peer pcx-AB.  On VPC-C, create a static route for VPC-A CIDR (172.16.0.0/24) across peer pcx-AC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. On VPC-A, enable dynamic route propagation on pcx-AB and pcx-AC.  On VPC-B, enable dynamic route propagation and use security groups to allow only the IP address 10.0.0.77/32 on VPC peer pcx-AB.  On VPC-C, enable dynamic route propagation with VPC-A on peer pcx-AC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. On VPC-A, create network access control lists that block the IP address 10.0.0.77/32 on VPC peer pcx-AC.  On VPC-A, create a static route for VPC-B CIDR (10.0.0.0/24) on pcx-AB and a static route for VPC-C CIDR (10.0.0.0/24) on pcx-AC.  On VPC-B, create a static route for VPC-A CIDR (172.16.0.0/24) on peer pcx-AB.  On VPC-C, create a static route for VPC-A CIDR (172.16.0.0/24) across peer pcx-AC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. On VPC-A, create a static route for the VPC-B (10.0.0.77/32) database across VPC peer pcx-AB.  Create a static route for the VPC-C CIDR on VPC peer pcx-AC.  On VPC-B, create a static route for VPC-A CIDR (172.16.0.0/24) on peer pcx-AB.  On VPC-C, create a static route for VPC-A CIDR (172.16.0.0/24) across peer pcx-AC. <br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 360920,
          "date": "Sun 10 Oct 2021 18:39",
          "username": "LCC92",
          "content": "D will work, /32 will be prioritized (Routing prioritize smaller cider).<br>However, it will not be perfect, A wont able to communicate with 10.0.0.77 in VPC-C, because it will always be routed to B for that destination IP. But it is \\\"short-term requirement \\\" in this question, so the solution is acceptable.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 316707,
          "date": "Sat 02 Oct 2021 07:05",
          "username": "awsexamprep47",
          "content": "D is correct /32 longest prefix path",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 613008,
          "date": "Wed 08 Jun 2022 03:31",
          "username": "kangtamo",
          "content": "Go with D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 494695,
          "date": "Sun 05 Dec 2021 22:27",
          "username": "AzureDP900",
          "content": "D works fine",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411606,
          "date": "Tue 26 Oct 2021 23:52",
          "username": "WhyIronMan",
          "content": "I'll go with D<br>It's the only one with makes sense from the Networking perspective",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 349128,
          "date": "Fri 08 Oct 2021 03:43",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 337526,
          "date": "Wed 06 Oct 2021 00:24",
          "username": "CarisB",
          "content": "Answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 309448,
          "date": "Wed 29 Sep 2021 02:54",
          "username": "nitinz",
          "content": "D is okay",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#548",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is designing a new highly available web application on AWS. The application requires consistent and reliable connectivity from the application servers in AWS to a backend REST API hosted in the company's on-premises environment. The backend connection between AWS and on-premises will be routed over an AWS Direct Connect connection through a private virtual interface. Amazon Route 53 will be used to manage private DNS records for the application to resolve the IP address on the backend REST API.<br>Which design would provide a reliable connection to the backend API?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#548",
          "answers": [
            {
              "choice": "<p>A. Implement at least two backend endpoints for the backend REST API, and use Route 53 health checks to monitor the availability of each backend endpoint and perform DNS-level failover.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Install a second Direct Connect connection from a different network carrier and attach it to the same virtual private gateway as the first Direct Connect connection.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Install a second cross connect for the same Direct Connect connection from the same network carrier, and join both connections to the same link aggregation group (LAG) on the same private virtual interface.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IPSec VPN connection routed over the public internet from the on-premises data center to AWS and attach it to the same virtual private gateway as the Direct Connect connection.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 261416,
          "date": "Fri 08 Oct 2021 10:56",
          "username": "01037LCC92heanyGeniusMikeLiudesertlotus1211ByrneyLCC92",
          "content": "D. <br>Why almost everyone chose B. <br>B is clearly wrong. Check below.<br>\\\"You cannot associate a virtual private gateway with more than one Direct Connect gateway\\\"<br>in<br>https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.htmlwe can associate a VPG with multiple DX.<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html<br>\\\"Figure 6 - Redundant AWS Direct Connect\\\"<br>The correct answer is B. correct.Direct connect gateway is not required here.two Direct connects can be connected to VIFs of VPGIt should be D, question is care about secure,VPN with IP Sec is secure,and Direct Connect is Private not secure.Tell me what makes a 'Direct' connection unsecure?A direct connection is private dark fiber...\\\"The application requires consistent and reliable connectivity\\\" not secure.\\\"You cannot associate a virtual private gateway with more than one Direct Connect gateway\\\"<br>Direct Connect Gateway is a different entity, we dont use it in this question.",
          "upvote_count": "2317241210",
          "selected_answers": ""
        },
        {
          "id": 360926,
          "date": "Sun 24 Oct 2021 16:39",
          "username": "LCC92heany",
          "content": "we can associate a VPG with multiple DX.<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html<br>\\\"Figure 6 - Redundant AWS Direct Connect\\\"<br>The correct answer is B. correct.Direct connect gateway is not required here.two Direct connects can be connected to VIFs of VPG",
          "upvote_count": "172",
          "selected_answers": ""
        },
        {
          "id": 687534,
          "date": "Thu 06 Oct 2022 08:07",
          "username": "heany",
          "content": "correct.Direct connect gateway is not required here.two Direct connects can be connected to VIFs of VPG",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 513929,
          "date": "Fri 31 Dec 2021 08:20",
          "username": "GeniusMikeLiudesertlotus1211Byrney",
          "content": "It should be D, question is care about secure,VPN with IP Sec is secure,and Direct Connect is Private not secure.Tell me what makes a 'Direct' connection unsecure?A direct connection is private dark fiber...\\\"The application requires consistent and reliable connectivity\\\" not secure.",
          "upvote_count": "412",
          "selected_answers": ""
        },
        {
          "id": 730958,
          "date": "Wed 30 Nov 2022 01:35",
          "username": "desertlotus1211",
          "content": "Tell me what makes a 'Direct' connection unsecure?A direct connection is private dark fiber...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712826,
          "date": "Mon 07 Nov 2022 06:53",
          "username": "Byrney",
          "content": "\\\"The application requires consistent and reliable connectivity\\\" not secure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 360928,
          "date": "Mon 25 Oct 2021 19:15",
          "username": "LCC92",
          "content": "\\\"You cannot associate a virtual private gateway with more than one Direct Connect gateway\\\"<br>Direct Connect Gateway is a different entity, we dont use it in this question.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 65321,
          "date": "Wed 22 Sep 2021 05:21",
          "username": "jay1ram2",
          "content": "My answer is B - 2 DX connection to on-prem provides more reliable connectivity between AWS and data center <br>https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/<br><br>A - The ask is, Which design would provide a \\\"reliable connection\\\" to the backend API? not to re-design the backend implementation for High Availability.<br>C - 2 DX connections from the same provider create a single point of failure<br>D - VPN over the public internet is generally less reliable than a dedicated DX connection.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 713251,
          "date": "Mon 07 Nov 2022 19:49",
          "username": "mrgreatness",
          "content": "Im going B as there is no mention of secure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 693334,
          "date": "Wed 12 Oct 2022 20:55",
          "username": "joanneli77",
          "content": "A VPN+DX is common because it is cheap, but it is not as reliable as two DX (add CGW hardware and different routing paths and it's messy).Two DX can be connected to one VGW.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 650124,
          "date": "Mon 22 Aug 2022 08:16",
          "username": "kadev",
          "content": "Maybe the Question is not exactly. <br>\\\"a secure connection \\\" =>D maybe almost matching. But why we need VPN over internet while we have Direct Connect lol",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 646745,
          "date": "Sun 14 Aug 2022 15:09",
          "username": "Harithareddynnepomatti",
          "content": "The question is about security, hence VPN connection is needed and it is DNo, it says \\\"reliable\\\", not secure. Not the same thing.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 661682,
          "date": "Wed 07 Sep 2022 00:47",
          "username": "epomatti",
          "content": "No, it says \\\"reliable\\\", not secure. Not the same thing.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 637686,
          "date": "Wed 27 Jul 2022 01:54",
          "username": "hilft",
          "content": "It's D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 634340,
          "date": "Thu 21 Jul 2022 05:50",
          "username": "KiraguJohnnexus2020",
          "content": "Which architecture would be most likely to establish a secure connection to the backend API?<br>- Did i miss anything here? i thought the question is about securityNot sure what you are trying to say here.<br>Direct Connect is private, and it is secure as no one else can see what going on there.<br>VPN is secure, based on it is encrypted, but VPN is used on public network due to the nature that the public network is public, not secure.<br><br>VPN is not more secure than Direct Connect.<br><br>so IMO, DirectConnect is fine.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 643382,
          "date": "Sat 06 Aug 2022 14:39",
          "username": "nexus2020",
          "content": "Not sure what you are trying to say here.<br>Direct Connect is private, and it is secure as no one else can see what going on there.<br>VPN is secure, based on it is encrypted, but VPN is used on public network due to the nature that the public network is public, not secure.<br><br>VPN is not more secure than Direct Connect.<br><br>so IMO, DirectConnect is fine.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 632753,
          "date": "Mon 18 Jul 2022 02:00",
          "username": "Student1950",
          "content": "here is the correct link <br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 632752,
          "date": "Mon 18 Jul 2022 01:59",
          "username": "Student1950",
          "content": "I believe Answer should be D but its not worded correctly. Basically we can enable IPSec VPN on existing DX connection using Public VIF of DX to establish secure communication between AWS and On-Prem as the link below<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 632296,
          "date": "Sat 16 Jul 2022 19:24",
          "username": "JonnyB1001",
          "content": "Not A: thats HA. <br>Not B: You cannot associate a virtual private gateway with more than one Direct Connect gateway and you cannot attach a private virtual interface to more than one Direct Connect gateway.<br>C: same-same, so not dependable. <br>D: provides secure and diverse route. So D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 628885,
          "date": "Fri 08 Jul 2022 21:33",
          "username": "hilft",
          "content": "I would go for D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 625337,
          "date": "Thu 30 Jun 2022 17:41",
          "username": "KiraguJohn",
          "content": "D: For a secure connection<br>B: For a redundancy",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 606779,
          "date": "Tue 24 May 2022 17:09",
          "username": "bobsmith2000",
          "content": "\\\"content and dependable\\\" connection.<br>It's definitively B. <br>For encryption in transit via Direct Connect in case of REST the one could use HTTPS",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 600009,
          "date": "Wed 11 May 2022 11:31",
          "username": "Alvindo",
          "content": "Answer is D<br>B is for redundancy question asked SECURE, which a VPN offers since it encrypts the networl",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 581149,
          "date": "Tue 05 Apr 2022 11:46",
          "username": "Netaji",
          "content": "when its DX, it's secure by default because it's not internet comparatively - question also says as first condition \\\" The application needs constant and dependable communication between its AWS application servers and a backend REST API housed on-premises.\\\" so its B<br><br>people are saying Direct connect gateway I agree not mentioned answer so no question and normal DX will connect 2 connection on VPG <br>as below <br>https://aws.amazon.com/directconnect/resiliency-recommendation/?nc=sn&loc=4&dn=2<br><br>if anyone want to see a direct connect gateway, please see below URL <br><br>https://www.stax.io/changelog/2020-10-06-new-direct-connect-functionality-for-stax-networks/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 581141,
          "date": "Tue 05 Apr 2022 11:28",
          "username": "Netaji",
          "content": "when its DX, it's secure by default because it's not internet comparatively - question also says as first condition \\\" The application needs constant and dependable communication between its AWS application servers and a backend REST API housed on-premises.\\\" so its B",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#549",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company is running an application that stores invoice files in an Amazon S3 bucket and metadata about the files in an Amazon DynamoDB table. The application software runs in both us-east-1 and eu-west-1. The S3 bucket and DynamoDB table are in us-east-1. The company wants to protect itself from data corruption and loss of connectivity to either Region.<br>Which option meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#549",
          "answers": [
            {
              "choice": "<p>A. Create a DynamoDB global table to replicate data between us-east-1 and eu-west-1. Enable continuous backup on the DynamoDB table in us-east-1. Enable versioning on the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function triggered by Amazon CloudWatch Events to make regular backups of the DynamoDB table. Set up S3 cross-region replication from us-east-1 to eu-west-1. Set up MFA delete on the S3 bucket in us-east-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a DynamoDB global table to replicate data between us-east-1 and eu-west-1. Enable versioning on the S3 bucket. Implement strict ACLs on the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a DynamoDB global table to replicate data between us-east-1 and eu-west-1. Enable continuous backup on the DynamoDB table in us-east-1. Set up S3 cross-region replication from us-east-1 to eu-west-1.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 278452,
          "date": "Wed 13 Oct 2021 23:16",
          "username": "Ebi",
          "content": "I go with D",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 548240,
          "date": "Wed 16 Feb 2022 03:04",
          "username": "jyrajan69",
          "content": "Must consider the fact that they want safeguard the data, and only B addresses that by adding MFA, to prevent accidental delete. So will go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 522899,
          "date": "Thu 13 Jan 2022 15:50",
          "username": "[Removed]",
          "content": "Most definitely D.  CRR requires versioning to be turned on and the requirement is connectivity to any region. D checks all the boxes",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 495616,
          "date": "Tue 07 Dec 2021 06:07",
          "username": "cldy",
          "content": "D.  Create a DynamoDB global table to replicate data between us-east-1 and eu-west-1. Enable continuous backup on the DynamoDB table in us-east-1. Set up S3 cross-region replication from us-east-1 to eu-west-1.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494697,
          "date": "Sun 05 Dec 2021 22:31",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490543,
          "date": "Tue 30 Nov 2021 10:17",
          "username": "backfringe",
          "content": "I go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411630,
          "date": "Fri 05 Nov 2021 03:19",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 370176,
          "date": "Tue 02 Nov 2021 12:52",
          "username": "Kukkuji",
          "content": "Correct answer is D.  To enable s3 replication, Both source and destination buckets must have versioning enabled.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 369863,
          "date": "Tue 02 Nov 2021 03:49",
          "username": "ss160700",
          "content": "A - it is about backup and restore to prevent data loss / corruption",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349133,
          "date": "Sat 30 Oct 2021 19:07",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 320088,
          "date": "Thu 21 Oct 2021 08:54",
          "username": "wasabidevJaps",
          "content": "A.  Amazon Route 53 supports DNSSEC for domain registration as well as DNSSEC signingIts D. You need to replicate data across regions with S3 cross region replication",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 415219,
          "date": "Fri 05 Nov 2021 07:41",
          "username": "Japs",
          "content": "Its D. You need to replicate data across regions with S3 cross region replication",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294927,
          "date": "Fri 15 Oct 2021 06:11",
          "username": "kievJaps",
          "content": "D is the answer and it is another repeatQuestion... did you start from topic 2? i.e. question 398?",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 415221,
          "date": "Fri 05 Nov 2021 13:43",
          "username": "Japs",
          "content": "Question... did you start from topic 2? i.e. question 398?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243930,
          "date": "Sat 09 Oct 2021 13:02",
          "username": "T14102020",
          "content": "Correct is D.  DynamoDB global table + S3 cross-region replication",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 231566,
          "date": "Wed 06 Oct 2021 14:46",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209807,
          "date": "Tue 05 Oct 2021 21:23",
          "username": "CYL",
          "content": "D.  Cross region replication for S3 and global dynamo tables.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 160899,
          "date": "Fri 01 Oct 2021 13:59",
          "username": "directconnect",
          "content": "Answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 133441,
          "date": "Fri 01 Oct 2021 00:07",
          "username": "NikkyDicky",
          "content": "D.  Dup of Q150",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#550",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to launch an online shopping website in multiple countries and must ensure that customers are protected against potential `man-in-the-middle` attacks.<br>Which architecture will provide the MOST secure site access?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#550",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Route 53 for domain registration and DNS services. Enable DNSSEC for all Route 53 requests. Use AWS Certificate Manager (ACM) to register TLS/SSL certificates for the shopping website, and use Application Load Balancers configured with those TLS/SSL certificates for the site. Use the Server Name Identification extension in all client requests to the site.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Register 2048-bit encryption keys from a third-party certificate service. Use a third-party DNS provider that uses the customer managed keys for DNSSec. Upload the keys to ACM, and use ACM to automatically deploy the certificates for secure web services to an EC2 front-end web server fleet by using NGINX. Use the Server Name Identification extension in all client requests to the site.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Route 53 for domain registration. Register 2048-bit encryption keys from a third-party certificate service. Use a third-party DNS service that supports DNSSEC for DNS requests that use the customer managed keys. Import the customer managed keys to ACM to deploy the certificates to Classic Load Balancers configured with those TLS/SSL certificates for the site. Use the Server Name Identification extension in all clients requests to the site.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Route 53 for domain registration, and host the company DNS root servers on Amazon EC2 instances running Bind. Enable DNSSEC for DNS requests. Use ACM to register TLS/SSL certificates for the shopping website, and use Application Load Balancers configured with those TLS/SSL certificates for the site. Use the Server Name Identification extension in all client requests to the site.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 320087,
          "date": "Mon 01 Nov 2021 03:18",
          "username": "wasabidev",
          "content": "A, now Amazon Route 53 supports DNSSEC for domain registration as well as DNSSEC signing",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 87819,
          "date": "Sun 10 Oct 2021 03:28",
          "username": "Mkumarhilft",
          "content": "Answer: DA better",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 638382,
          "date": "Thu 28 Jul 2022 03:42",
          "username": "hilft",
          "content": "A better",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 614122,
          "date": "Thu 09 Jun 2022 19:20",
          "username": "ravisar",
          "content": "The answer is A - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html \\\"You can protect your domain from this type of attack, known as DNS spoofing or a man-in-the-middle attack, by configuring Domain Name System Security Extensions (DNSSEC), a protocol for securing DNS traffic\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 608002,
          "date": "Fri 27 May 2022 10:57",
          "username": "bobsmith2000",
          "content": "Seems to be A. <br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-configuring-dnssec.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 540760,
          "date": "Sat 05 Feb 2022 04:52",
          "username": "kyo",
          "content": "Answer is A.  Bind is not good. CLB is wrong. ACM's SSL certificate cannot use in EC2 instance.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 521763,
          "date": "Tue 11 Jan 2022 21:49",
          "username": "Ni_yot",
          "content": "Agree its A.  See link https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514388,
          "date": "Sat 01 Jan 2022 06:41",
          "username": "cldy",
          "content": "A correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497780,
          "date": "Thu 09 Dec 2021 15:07",
          "username": "cldy",
          "content": "A.  Use Amazon Route 53 for domain registration and DNS services. Enable DNSSEC for all Route 53 requests. Use AWS Certificate Manager (ACM) to register TLS/SSL certificates for the shopping website, and use Application Load Balancers configured with those TLS/SSL certificates for the site. Use the Server Name Identification extension in all client requests to the site.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 433226,
          "date": "Sat 06 Nov 2021 13:44",
          "username": "denccc",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411634,
          "date": "Sat 06 Nov 2021 08:37",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 349134,
          "date": "Sat 06 Nov 2021 04:27",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 347416,
          "date": "Fri 05 Nov 2021 22:59",
          "username": "blackgamer",
          "content": "A for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 339303,
          "date": "Wed 03 Nov 2021 20:36",
          "username": "BloodCube",
          "content": "After June 2021, the answer is A<br>Before that, D is correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 337493,
          "date": "Mon 01 Nov 2021 19:40",
          "username": "Amitv2706",
          "content": "A, as now AWS supports DNSSEC on its own.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 325481,
          "date": "Mon 01 Nov 2021 14:00",
          "username": "kalyan_krishna742020",
          "content": "Answer is D since R53 started supporting DNSSEC since last December 2020 which is not over 6 months yet.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 277696,
          "date": "Thu 28 Oct 2021 10:15",
          "username": "Ebi",
          "content": "With very recent announcement from AWS answer should be A:<br>https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 261420,
          "date": "Tue 26 Oct 2021 04:04",
          "username": "01037",
          "content": "A. <br>Old question?<br>According to<br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html<br>Amazon Route 53 supports DNSSEC for domain registration as well as DNSSEC signing",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#551",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is creating an account strategy so that they can begin using AWS. The Security team will provide each team with the permissions they need to follow the principle or least privileged access. Teams would like to keep their resources isolated from other groups, and the Finance team would like each team's resource usage separated for billing purposes.<br>Which account creation process meets these requirements and allows for changes?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#551",
          "answers": [
            {
              "choice": "<p>A. Create a new AWS Organizations account. Create groups in Active Directory and assign them to roles in AWS to grant federated access. Require each team to tag their resources, and separate bills based on tags. Control access to resources through IAM granting the minimally required privilege.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create individual accounts for each team. Assign the security accountas the master account, and enable consolidated billing for all other accounts. Create a cross-account role for security to manage accounts, and send logs to a bucket in the security account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new AWS account, and use AWS Service Catalog to provide teams with the required resources. Implement a third-party billing solutionto provide the Finance team with the resource use for each team based on tagging. Isolate resources using IAM to avoid account sprawl. Security will control and monitor logs and permissions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a master account for billing using Organizations, and create each team's account from that master account. Create a security account for logs and cross-account access. Apply service control policies on each account, and grant the Security team cross-account access to all accounts. Security will create IAM policies for each account to maintain least privilege access.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 277695,
          "date": "Wed 20 Oct 2021 02:25",
          "username": "Ebi",
          "content": "D is the answer",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 253005,
          "date": "Sat 16 Oct 2021 15:33",
          "username": "Bulti",
          "content": "Answer is D.  AWS organization, separation of accounts by function is a standard AWS best practice when it comes to account creation.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 676522,
          "date": "Thu 22 Sep 2022 21:36",
          "username": "Naj_64Vinafec",
          "content": "How does D satisfies \\\"Finance team would like each team's resource usage separated for billing purposes.\\\"?You don't have to enable consolidated billing",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 695238,
          "date": "Sat 15 Oct 2022 08:59",
          "username": "Vinafec",
          "content": "You don't have to enable consolidated billing",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 543372,
          "date": "Tue 08 Feb 2022 23:15",
          "username": "jj22222",
          "content": "D.  Create a master account for billing using Organizations, and create each teamג€™s account from that master account. Create a security account for logs and cross-account access. Apply service control policies on each account, and grant the Security team cross-account access to all accounts. Security will create IAM policies for each account to maintain least privilege access.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 539654,
          "date": "Thu 03 Feb 2022 12:45",
          "username": "kyo",
          "content": "D: AWS Organizations is the best solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 526917,
          "date": "Tue 18 Jan 2022 19:28",
          "username": "cannottellname",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 515871,
          "date": "Mon 03 Jan 2022 16:28",
          "username": "Ni_yot",
          "content": "D for me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514343,
          "date": "Sat 01 Jan 2022 04:59",
          "username": "cldy",
          "content": "D correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494711,
          "date": "Sun 05 Dec 2021 23:02",
          "username": "AzureDP900",
          "content": "D is the best answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 478678,
          "date": "Mon 15 Nov 2021 14:13",
          "username": "ryu10_09",
          "content": "D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413928,
          "date": "Fri 05 Nov 2021 23:36",
          "username": "CloudChef",
          "content": "A)Reason/ Require each team to tag their resources, and separate bills based on tags.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 411636,
          "date": "Tue 02 Nov 2021 05:53",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349298,
          "date": "Sun 31 Oct 2021 21:32",
          "username": "Waiweng",
          "content": "it is D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 347466,
          "date": "Fri 29 Oct 2021 12:59",
          "username": "blackgamer",
          "content": "D is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243942,
          "date": "Sat 16 Oct 2021 11:24",
          "username": "T14102020",
          "content": "Correct is D.  master account",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231571,
          "date": "Thu 14 Oct 2021 15:50",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209612,
          "date": "Tue 12 Oct 2021 17:33",
          "username": "CYL",
          "content": "D.  Use SCP to control organizational level policies.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#552",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a 24 TB MySQL database in its on-premises data center that grows at the rate of 10 GB per day. The data center is connected to the company's<br>AWS infrastructure with a 50 Mbps VPN connection.<br>The company is migrating the application and workload to AWS. The application code is already installed and tested on Amazon EC2. The company now needs to migrate the database and wants to go live on AWS within 3 weeks.<br>Which of the following approaches meets the schedule with LEAST downtime?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#552",
          "answers": [
            {
              "choice": "<p>A. 1. Use the VM Import/Export service to import a snapshot of the on-premises database into AWS. 2. Launch a new EC2 instance from the snapshot. 3. Set up ongoing database replication from on premises to the EC2 database over the VPN. 4. Change the DNS entry to point to the EC2 database. 5. Stop the replication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. 1. Launch an AWS DMS instance. 2. Launch an Amazon RDS Aurora MySQL DB instance.3. Configure the AWS DMS instance with on-premises and Amazon RDS database information. 4. Start the replication task within AWS DMS over the VPN. 5. Change the DNS entry to point to the Amazon RDS MySQL database. 6. Stop the replication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. 1. Create a database export locally using database-native tools. 2. Import that into AWS using AWS Snowball. 3. Launch an Amazon RDS Aurora DB instance. 4. Load the data in the RDS Aurora DB instance from the export. 5. Set up database replication from the on-premises database to the RDS Aurora DB instance over the VPN. 6. Change the DNS entry to point to the RDS Aurora DB instance. 7. Stop the replication.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. 1. Take the on-premises application offline. 2. Create a database export locally using database-native tools. 3. Import that into AWS using AWS Snowball. 4. Launch an Amazon RDS Aurora DB instance. 5. Load the data in the RDS Aurora DB instance from the export. 6. Change the DNS entry to point to the Amazon RDS Aurora DB instance. 7. Put the Amazon EC2 hosted application online.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 277704,
          "date": "Sat 23 Oct 2021 01:43",
          "username": "Ebi",
          "content": "C is the answer, <br>with 50Mbps connection only around 11TB can be transferred in 3 weeks, so 24TB of data must be transferred differently which is Snowball in this case",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 65328,
          "date": "Mon 20 Sep 2021 14:41",
          "username": "jay1ram2",
          "content": "The Correct Answer is C",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 622147,
          "date": "Sat 25 Jun 2022 15:46",
          "username": "skyblue07",
          "content": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 498440,
          "date": "Fri 10 Dec 2021 09:58",
          "username": "cldy",
          "content": "C.  1. Create a database export locally using database-native tools. 2. Import that into AWS using AWS Snowball. 3. Launch an Amazon RDS Aurora DB instance. 4. Load the data in the RDS Aurora DB instance from the export. 5. Set up database replication from the on-premises database to the RDS Aurora DB instance over the VPN. 6. Change the DNS entry to point to the RDS Aurora DB instance. 7. Stop the replication.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494716,
          "date": "Sun 05 Dec 2021 23:23",
          "username": "AzureDP900",
          "content": "C is right answer!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441796,
          "date": "Sat 06 Nov 2021 04:16",
          "username": "Goram113",
          "content": "Now DMS can use snowball for entry synchronization and it would be best answer, but in available options C is best.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411639,
          "date": "Tue 02 Nov 2021 10:19",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349303,
          "date": "Sun 31 Oct 2021 12:18",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 325438,
          "date": "Sun 24 Oct 2021 07:36",
          "username": "ExtHo",
          "content": "C is correct and D required at least 1 week downtime.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 321762,
          "date": "Sat 23 Oct 2021 18:12",
          "username": "alisyech",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243945,
          "date": "Wed 20 Oct 2021 10:45",
          "username": "T14102020",
          "content": "Correct is C.  snowball + without of premise offline",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 231575,
          "date": "Sun 17 Oct 2021 07:22",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 215662,
          "date": "Sat 16 Oct 2021 16:26",
          "username": "kopper2019",
          "content": "C is the answer as soon as you see 25TB over a 25mb links no way.... based on torrent leeching experience :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 209613,
          "date": "Sat 16 Oct 2021 03:44",
          "username": "CYL",
          "content": "C.  D requires downtime.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133448,
          "date": "Sun 10 Oct 2021 21:28",
          "username": "NikkyDicky01037",
          "content": "C.  Dup of Q153149 now",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 261534,
          "date": "Wed 20 Oct 2021 14:26",
          "username": "01037",
          "content": "149 now",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 104994,
          "date": "Sat 09 Oct 2021 12:04",
          "username": "Oleksandr",
          "content": "I think it's C<br>DMS: 50Mbps = 6.25 MB/sec = ... = 0.5Tb/day. 24 Tb = 48 days, which is way above 3 weeks.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 104843,
          "date": "Thu 07 Oct 2021 00:39",
          "username": "meenu2225",
          "content": "C is the one.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#553",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to allow its Marketing team to perform SQL queries on customer records to identify market segments. The data is spread across hundreds of files. The records must be encrypted in transit and at rest. The Team Manager must have the ability to manage users and groups, but no team members should have access to services or resources not required for the SQL queries. Additionally, Administrators need to audit the queries made and receive notifications when a query violates rules defined by the Security team.<br>AWS Organizations has been used to create a new account and an AWS IAM user with administrator permissions for the Team Manager.<br>Which design meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#553",
          "answers": [
            {
              "choice": "<p>A. Apply a service control policy (SCP) that allows access to IAM, Amazon RDS, and AWS CloudTrail. Load customer records in Amazon RDS MySQL and train users to execute queries using the AWS CLI. Stream the query logs to Amazon CloudWatch Logs from the RDS database instance. Use a subscription filter with AWS Lambda functions to audit and alarm on queries against personal data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Apply a service control policy (SCP) that denies access to all services except IAM, Amazon Athena, Amazon S3, and AWS CloudTrail. Store customer record files in Amazon S3 and train users to execute queries using the CLI via Athena. Analyze CloudTrail events to audit and alarm on queries against personal data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Apply a service control policy (SCP) that denies access to all services except IAM, Amazon DynamoDB, and AWS CloudTrail. Store customer records in DynamoDB and train users to execute queries using the AWS CLI. Enable DynamoDB streams to track the queries that are issued and use an AWS Lambda function for real-time monitoring and alerting.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Apply a service control policy (SCP) that allows access to IAM, Amazon Athena, Amazon S3, and AWS CloudTrail. Store customer records as files in Amazon S3 and train users to leverage the Amazon S3 Select feature and execute queries using the AWS CLI. Enable S3 object-level logging and analyze CloudTrail events to audit and alarm on queries against personal data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65045,
          "date": "Mon 20 Sep 2021 18:45",
          "username": "jay1ram2",
          "content": "The answer is B.  This is the only option that satisfies all requirements<br><br>Encryption Rest/Transit - S3/Athena<br>Manage users and groups - IAM<br>Deny Access - Ensures the strictest access. <br>Audit Queries - CloudTrail logs<br><br>A - RDS MySQL only pushes slow query log to CLoudwatch<br>C - DynamoDB streams push only data changes not SQL<br>D - This optionup Athena but recommends using S3 select",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 671792,
          "date": "Sat 17 Sep 2022 22:18",
          "username": "dcdcdc3",
          "content": "this is what S3 Select is:<br>https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-new-features-for-s3-select/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498585,
          "date": "Fri 10 Dec 2021 13:00",
          "username": "cldy",
          "content": "B.  Apply a service control policy (SCP) that denies access to all services except IAM, Amazon Athena, Amazon S3, and AWS CloudTrail. Store customer record files in Amazon S3 and train users to execute queries using the CLI via Athena. Analyze CloudTrail events to audit and alarm on queries against personal data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494738,
          "date": "Mon 06 Dec 2021 00:45",
          "username": "AzureDP900",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 488908,
          "date": "Sun 28 Nov 2021 07:58",
          "username": "acloudguru",
          "content": "Encryption Rest/Transit - S3/Athena<br>Manage users and groups - IAM<br>Deny Access - Ensures the strictest access.<br>Audit Queries - CloudTrail logs<br><br>A - RDS MySQL only pushes slow query log to CLoudwatch<br>C - DynamoDB streams push only data changes not SQL<br>D - This option up Athena but recommends using S3 select<br><br>This is a easy one for solution type of questions, hope I can have it in my exam",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 455903,
          "date": "Sat 06 Nov 2021 14:01",
          "username": "Smartphone",
          "content": "Answer is B.  <br>Each of the following policies is an example of a deny list policy strategy. Deny list policies must be attached along with other policies that allow the approved actions in the affected accounts.<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411643,
          "date": "Thu 28 Oct 2021 19:25",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349325,
          "date": "Sun 24 Oct 2021 21:04",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 347489,
          "date": "Sun 24 Oct 2021 14:16",
          "username": "blackgamer",
          "content": "Answer is B.  Athena can query but what it S3 select.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 346313,
          "date": "Sun 24 Oct 2021 09:06",
          "username": "gswViper57",
          "content": "there is nothing to suggest in the question that it is required to pull out hundreds of queries at a time in which case why B? Surely D is ok?It is not possible to grant permissions using SCP, only deny them. This means you can ignore all questions that state \\\"Use an SCP that allows access\\\".",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 446078,
          "date": "Mon 01 Nov 2021 05:26",
          "username": "Viper57",
          "content": "It is not possible to grant permissions using SCP, only deny them. This means you can ignore all questions that state \\\"Use an SCP that allows access\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 337512,
          "date": "Sun 24 Oct 2021 00:09",
          "username": "Amitv2706AWSum1",
          "content": "B is correct. Athena can run queries on multiple files at same time. However S3 Select is applicable for only one object at a timeCorrect. And the question states 100s of files",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 450801,
          "date": "Tue 02 Nov 2021 16:46",
          "username": "AWSum1",
          "content": "Correct. And the question states 100s of files",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 277708,
          "date": "Sat 23 Oct 2021 07:54",
          "username": "Ebi",
          "content": "I will go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243947,
          "date": "Sat 23 Oct 2021 03:29",
          "username": "T14102020",
          "content": "Correct is B.  Athena + SCP denies",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 231578,
          "date": "Thu 21 Oct 2021 08:51",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209617,
          "date": "Wed 20 Oct 2021 05:55",
          "username": "CYL",
          "content": "B.  Use Deny policies in order to restrict usage of services outside the allowable ones.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133453,
          "date": "Tue 19 Oct 2021 21:31",
          "username": "NikkyDickyKopa01037",
          "content": "B, Dup of Q154why you guys write always dup of questions on 1-450 questions, i dont understand. Should we look into 1-450 questions too?150 now",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 402531,
          "date": "Wed 27 Oct 2021 16:30",
          "username": "Kopa",
          "content": "why you guys write always dup of questions on 1-450 questions, i dont understand. Should we look into 1-450 questions too?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 261541,
          "date": "Sat 23 Oct 2021 05:33",
          "username": "01037",
          "content": "150 now",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 114452,
          "date": "Tue 12 Oct 2021 11:06",
          "username": "roger8978",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#554",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is responsible for redesigning a legacy Java application to improve its availability, data durability, and scalability. Currently, the application runs on a single high-memory Amazon EC2 instance. It accepts HTTP requests from upstream clients, adds them to an in-memory queue, and responds with a<br>200 status. A separate application thread reads items from the queue, processes them, and persists the results to an Amazon RDS MySQL instance. The processing time for each item takes 90 seconds on average, most of which is spent waiting on external service calls, but the application is written to process multiple items in parallel.<br>Traffic to this service is unpredictable. During periods of high load, items may sit in the internal queue for over an hour while the application processes the backlog.<br><br>In addition, the current system has issues with availability and data loss if the single application node fails.<br>Clients that access this service cannot be modified. They expect to receive a response to each HTTP request they send within 10 seconds before they will time out and retry the request.<br>Which approach would improve the availability and durability of the system while decreasing the processing latency and minimizing costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#554",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon API Gateway REST API that uses Lambda proxy integration to pass requests to an AWS Lambda function. Migrate the core processing code to a Lambda function and write a wrapper class that provides a handler method that converts the proxy events to the internal application data model and invokes the processing module.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon API Gateway REST API that uses a service proxy to put items in an Amazon SQS queue. Extract the core processing code from the existing application and update it to pull items from Amazon SQS instead of an in-memory queue. Deploy the new processing application to smaller EC2 instances within an Auto Scaling group that scales dynamically based on the approximate number of messages in the Amazon SQS queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the application to use Amazon DynamoDB instead of Amazon RDS. Configure Auto Scaling for the DynamoDB table. Deploy the application within an Auto Scaling group with a scaling policy based on CPU utilization. Back the in-memory queue with a memory-mapped file to an instance store volume and periodically write that file to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the application to use a Redis task queue instead of the in-memory queue. Build a Docker container image for the application. Create an Amazon ECS task definition that includes the application container and a separate container to host Redis. Deploy the new task definition as an ECS service using AWS Fargate, and enable Auto Scaling.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65050,
          "date": "Sun 19 Sep 2021 20:09",
          "username": "jay1ram2VrushaliDLisXvkbajoria",
          "content": "Not sure why C is the correct answer. The obvious challenges here are long workloads, scalability based on queue load, and reliability. Almost always the defacto answer to queue related workload is SQS. Since the workloads are very long (90 minutes) Lambdas cannot be used (15 mins max timeout). So,autoscaled smaller EC2 nodes that wait on external services to complete the task makes more sense. If the task fails, the message is returned to the queue and retried. <br><br>My answer is BIts 90 seconds not minutes, but still ans is BWhat is the issue with A then using Lambda?It is not because of Lambda that question A is incorrect. It is because it didn't talk about local queue that was in used in EC2.",
          "upvote_count": "26512",
          "selected_answers": ""
        },
        {
          "id": 109656,
          "date": "Sat 09 Oct 2021 07:12",
          "username": "VrushaliDLisX",
          "content": "Its 90 seconds not minutes, but still ans is BWhat is the issue with A then using Lambda?",
          "upvote_count": "51",
          "selected_answers": ""
        },
        {
          "id": 334976,
          "date": "Wed 27 Oct 2021 09:58",
          "username": "LisX",
          "content": "What is the issue with A then using Lambda?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 368235,
          "date": "Fri 29 Oct 2021 02:21",
          "username": "vkbajoria",
          "content": "It is not because of Lambda that question A is incorrect. It is because it didn't talk about local queue that was in used in EC2.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450805,
          "date": "Sun 07 Nov 2021 09:47",
          "username": "AWSum1",
          "content": "This question is long and causes brain fatigue lol. <br><br>B , sqs will satisfy the need",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 625126,
          "date": "Thu 30 Jun 2022 10:34",
          "username": "TechX",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 611774,
          "date": "Sun 05 Jun 2022 12:33",
          "username": "ravisar",
          "content": "Will Answer B satisfy the requirement below? \\\"They anticipate receiving a response to each HTTP request they submit within 10 seconds, at which point the request will time out and be retried.\\\"Will SQS processing time depends on the subscriber?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 577811,
          "date": "Tue 29 Mar 2022 20:12",
          "username": "jj22222",
          "content": "B looks right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 495472,
          "date": "Tue 07 Dec 2021 00:52",
          "username": "vbal",
          "content": "instance store volume - better IOPS?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411647,
          "date": "Sat 06 Nov 2021 03:44",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 390064,
          "date": "Sat 30 Oct 2021 00:47",
          "username": "Chubb",
          "content": "what does it mean by upstream client?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292194,
          "date": "Fri 22 Oct 2021 17:23",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277715,
          "date": "Fri 22 Oct 2021 00:21",
          "username": "Ebi",
          "content": "I will go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253013,
          "date": "Thu 21 Oct 2021 22:09",
          "username": "Bulti",
          "content": "Answer is B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 251914,
          "date": "Thu 21 Oct 2021 20:17",
          "username": "BrittsbeebatovDashLDashL",
          "content": "Why B? Not sure if API Gateway can support HTTP (it only does HTTPS) and client application can't be changed to invoke HTTPSI believe you can make \\\"HTTP\\\" API requests<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop.html#http-api-examplesThat is a \\\"HTTP API\\\" - not making an HTTP request to API gateway endpoint. None of the answers seems right.My bad, C seems to be the only possible Answer.",
          "upvote_count": "1111",
          "selected_answers": ""
        },
        {
          "id": 332102,
          "date": "Mon 25 Oct 2021 23:37",
          "username": "beebatovDashLDashL",
          "content": "I believe you can make \\\"HTTP\\\" API requests<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop.html#http-api-examplesThat is a \\\"HTTP API\\\" - not making an HTTP request to API gateway endpoint. None of the answers seems right.My bad, C seems to be the only possible Answer.",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 409168,
          "date": "Sat 30 Oct 2021 02:48",
          "username": "DashLDashL",
          "content": "That is a \\\"HTTP API\\\" - not making an HTTP request to API gateway endpoint. None of the answers seems right.My bad, C seems to be the only possible Answer.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 409169,
          "date": "Mon 01 Nov 2021 10:47",
          "username": "DashL",
          "content": "My bad, C seems to be the only possible Answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 243958,
          "date": "Thu 14 Oct 2021 23:34",
          "username": "T14102020",
          "content": "Correct is B.  SQS + without Lambda, DynamoDB, Redis",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 231591,
          "date": "Thu 14 Oct 2021 12:43",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 209632,
          "date": "Wed 13 Oct 2021 03:43",
          "username": "CYL",
          "content": "B.  Using SQS to decouple the incoming request and processing nodes. Auto scaling to scale based on traffic. This increases reliability.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133456,
          "date": "Sun 10 Oct 2021 20:40",
          "username": "NikkyDicky",
          "content": "B.  Dup of Q155",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 87826,
          "date": "Fri 08 Oct 2021 02:05",
          "username": "Mkumar",
          "content": "My answer is B",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#555",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect needs to migrate a legacy application from on premises to AWS. On premises, the application runs on two Linux servers behind a load balancer and accesses a database that is master-master on two servers. Each application server requires a license file that is tied to the MAC address of the server's network adapter. It takes the software vendor 12 hours to send ne license files through email. The application requires configuration files to use static.<br>IPv4 addresses to access the database servers, not DNS.<br>Given these requirements, which steps should be taken together to enable a scalable architecture for the application servers? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#555",
          "answers": [
            {
              "choice": "<p>A. Create a pool of ENIs, request license files from the vendor for the pool, and store the license files within Amazon S3. Create automation to download an unused license, and attach the corresponding ENI at boot time.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a pool of ENIs, request license files from the vendor for the pool, store the license files on an Amazon EC2 instance, modify the configuration files, and create an AMI from the instance. use this AMI for all instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a bootstrap automation to request a new license file from the vendor with a unique return email. Have the server configure itself with the received license file.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create bootstrap automation to attach an ENI from the pool, read the database IP addresses from AWS Systems Manager Parameter Store, and inject those parameters into the local configuration files. Keep SSM up to date using a Lambda function.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Install the application on an EC2 instance, configure the application, and configure the IP address information. Create an AMI from this instance and use if for all instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65055,
          "date": "Mon 20 Sep 2021 23:32",
          "username": "jay1ram2",
          "content": "I choose A and D",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 362993,
          "date": "Thu 28 Oct 2021 16:02",
          "username": "vkbajoriavkbajoriaAzureDP900",
          "content": "Majority of the answers are incorrect. Is Examtopics purposely providing all the wrong answer?The answer is A & DThey might have provided intentionally wrong answers, We have to read and understand what is right vs wrong",
          "upvote_count": "531",
          "selected_answers": ""
        },
        {
          "id": 368241,
          "date": "Mon 01 Nov 2021 13:07",
          "username": "vkbajoria",
          "content": "The answer is A & D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 494743,
          "date": "Mon 06 Dec 2021 01:03",
          "username": "AzureDP900",
          "content": "They might have provided intentionally wrong answers, We have to read and understand what is right vs wrong",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626376,
          "date": "Sun 03 Jul 2022 04:42",
          "username": "aandc",
          "content": "AD is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 584875,
          "date": "Tue 12 Apr 2022 20:42",
          "username": "bkrish",
          "content": "I will go with A&D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494742,
          "date": "Mon 06 Dec 2021 00:57",
          "username": "AzureDP900",
          "content": "I will go with AD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411657,
          "date": "Sat 06 Nov 2021 19:32",
          "username": "WhyIronMan",
          "content": "I'll go with A,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349336,
          "date": "Mon 25 Oct 2021 23:31",
          "username": "Waiweng",
          "content": "It s A&D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 334628,
          "date": "Mon 25 Oct 2021 14:47",
          "username": "01037",
          "content": "A, D<br>If it's supported by the vendor, C is a better choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 325451,
          "date": "Mon 25 Oct 2021 01:50",
          "username": "ExtHo",
          "content": "A&D <br>Having the license files on an Amazon S3 bucket reduces the management overhead for the EC2 instances, as you can easily add/remove more license keys if needed.<br><br>Having the database IP addresses on Parameter Store ensures that all the EC2 instances will have a central location to retrieve the IP addresses. This also reduces the need to constantly update any script from inside the EC2 instance even if you add/remove more databases in the future.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 319042,
          "date": "Sun 24 Oct 2021 22:11",
          "username": "Pupu86",
          "content": "The objective is to tie the license file to a recognisable NIC (ENI in this case) and extract the IP and bind it accordingly. AD fulfils both requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 295486,
          "date": "Wed 20 Oct 2021 22:09",
          "username": "certainly",
          "content": "AD.  B is incorrect. the license is binded with specific MAC address. you cannot store license file in the EC2 whose NIC is assigned from the pool programmatically",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292197,
          "date": "Tue 19 Oct 2021 07:20",
          "username": "Kian1",
          "content": "going with AD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 280255,
          "date": "Sat 16 Oct 2021 21:36",
          "username": "Firststack",
          "content": "A & D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277717,
          "date": "Fri 15 Oct 2021 17:51",
          "username": "Ebi",
          "content": "I will go with AD",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 261572,
          "date": "Wed 13 Oct 2021 11:41",
          "username": "01037",
          "content": "What does it mean that tying a license file to the MAC address of the server's network adapter?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253017,
          "date": "Mon 11 Oct 2021 05:58",
          "username": "Bulti",
          "content": "For security purposes its best not to store the license files on EC2 instance. So I will go with A & D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 233036,
          "date": "Sun 10 Oct 2021 08:38",
          "username": "gookseangbinhdx",
          "content": "BD my friend says ADshould be AD, don't store license into Ec2.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 249403,
          "date": "Sun 10 Oct 2021 23:03",
          "username": "binhdx",
          "content": "should be AD, don't store license into Ec2.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#556",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an Amazon VPC that is divided into a public subnet and a private subnet. A web application runs in Amazon VPC, and each subnet has its own<br>NACL. The public subnet has a CIDR of 10.0.0.0/24. An Application Load Balancer is deployed to the public subnet. The private subnet has a CIDR of 10.0.1.0/24.<br>Amazon EC2 instances that run a web server on port 80 are launched into the private subnet.<br>Only network traffic that is required for the Application Load Balancer to access the web application can be allowed to travel between the public and private subnets.<br>What collection of rules should be written to ensure that the private subnet's NACL meets the requirement? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#556",
          "answers": [
            {
              "choice": "<p>A. An inbound rule for port 80 from source 0.0.0.0/0.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. An inbound rule for port 80 from source 10.0.0.0/24.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. An outbound rule for port 80 to destination 0.0.0.0/0.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. An outbound rule for port 80 to destination 10.0.0.0/24.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. An outbound rule for ports 1024 through 65535 to destination 10.0.0.0/24.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 514056,
          "date": "Fri 31 Dec 2021 13:00",
          "username": "Riho",
          "content": "https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-groups.html#elb-vpc-nacl - Right answer should be B,E",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 734451,
          "date": "Sat 03 Dec 2022 13:44",
          "username": "SureNot",
          "content": "BE - SOURCE port is random and uniuq for each connection from ALB.  SG automatically allows return traffic but not NACL.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 714040,
          "date": "Tue 08 Nov 2022 19:23",
          "username": "alxjandroleivaAum",
          "content": "BD, Why not?BE. . can't be D because NACL is stateless",
          "upvote_count": "21",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 716091,
          "date": "Fri 11 Nov 2022 14:46",
          "username": "Aum",
          "content": "BE. . can't be D because NACL is stateless",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 672299,
          "date": "Sun 18 Sep 2022 13:35",
          "username": "Dionenonly",
          "content": "B E would be the best answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 637380,
          "date": "Tue 26 Jul 2022 13:49",
          "username": "CloudHandsOn",
          "content": "B. E.  - First choice, and believe this is the correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 627725,
          "date": "Wed 06 Jul 2022 07:20",
          "username": "aandc",
          "content": "ephemeral ports are needed",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 627107,
          "date": "Mon 04 Jul 2022 20:27",
          "username": "JonJon03",
          "content": "ALB terminates flow/has proxy behaviour. https://aws.amazon.com/elasticloadbalancing/features/?nc=sn&loc=2&dn=1",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 604355,
          "date": "Fri 20 May 2022 12:55",
          "username": "bobsmith2000",
          "content": "NACL is stateless. So we must set up both inbound and outbound.<br>B.  An inbound rule for port 80 from source 10.0.0.0/24. Allows access from pub sub on 80.<br>E.  An outbound rule for ports 1024 through 65535 to destination 10.0.0.0/24. Allow outbound to pub sub on ephemeral ports",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 580584,
          "date": "Mon 04 Apr 2022 08:59",
          "username": "adsdadasdad",
          "content": "Its not, you made the mistake of thinking the application load balancer is an internal one. Thus the answer is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#557",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an internal AWS Elastic Beanstalk worker environment inside a VPC that must access an external payment gateway API available on an HTTPS endpoint on the public internet. Because of security policies, the payment gateway's Application team can grant access to only one public IP address.<br>Which architecture will set up an Elastic Beanstalk environment to access the company's application without making multiple changes on the company's end?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#557",
          "answers": [
            {
              "choice": "<p>A. Configure the Elastic Beanstalk application to place Amazon EC2 instances in a private subnet with an outbound route to a NAT gateway in a public subnet. Associate an Elastic IP address to the NAT gateway that can be whitelisted on the payment gateway application side.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the Elastic Beanstalk application to place Amazon EC2 instances in a public subnet with an internet gateway. Associate an Elastic IP address to the internet gateway that can be whitelisted on the payment gateway application side.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the Elastic Beanstalk application to place Amazon EC2 instances in a private subnet. Set an HTTPS_PROXY application parameter to send outbound HTTPS connections to an EC2 proxy server deployed in a public subnet. Associate an Elastic IP address to the EC2 proxy host that can be whitelisted on the payment gateway application side.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the Elastic Beanstalk application to place Amazon EC2 instances in a public subnet. Set the HTTPS_PROXY and NO_PROXY application parameters to send non-VPC outbound HTTPS connections to an EC2 proxy server deployed in a public subnet. Associate an Elastic IP address to the EC2 proxy host that can be whitelisted on the payment gateway application side.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 87829,
          "date": "Fri 24 Sep 2021 11:28",
          "username": "Mkumarsashenka",
          "content": "Answer is Ahttps://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-static-IP-address/",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 479661,
          "date": "Tue 16 Nov 2021 21:53",
          "username": "sashenka",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-static-IP-address/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278268,
          "date": "Sun 31 Oct 2021 12:04",
          "username": "Ebi",
          "content": "I will go with A",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 673725,
          "date": "Tue 20 Sep 2022 03:00",
          "username": "Rocketeer",
          "content": "Both A and C will work. However A is much easier to set up and least impact.<br>Hence A is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 672304,
          "date": "Sun 18 Sep 2022 13:42",
          "username": "Dionenonly",
          "content": "A.  Plain and simple",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 671099,
          "date": "Fri 16 Sep 2022 21:49",
          "username": "Israel",
          "content": "Elastic Beanstalk doesn't support proxy settings like HTTPS_PROXY for configuring a web proxy.<br><br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/vpc.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 636895,
          "date": "Mon 25 Jul 2022 20:29",
          "username": "hilft",
          "content": "Why not C?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 596018,
          "date": "Mon 02 May 2022 13:44",
          "username": "tartarus23",
          "content": "A.  offers security the the EC2 instance as they are in private subnet and internet connection to public subnet is via NAT gateway is secured. elastic IP of the NAT gateway can then be whitelisted by the payment gateway app",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 580289,
          "date": "Sun 03 Apr 2022 15:29",
          "username": "HellGate",
          "content": "Need Proxy service for HTTPS communication.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 561418,
          "date": "Sat 05 Mar 2022 13:22",
          "username": "Ni_yot",
          "content": "A for me.<br>makes sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494747,
          "date": "Mon 06 Dec 2021 01:09",
          "username": "AzureDP900",
          "content": "I will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411666,
          "date": "Wed 03 Nov 2021 12:24",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349349,
          "date": "Wed 03 Nov 2021 02:36",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 243979,
          "date": "Thu 28 Oct 2021 20:19",
          "username": "T14102020",
          "content": "Correct is A.  NAT + Elastic IP",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 231603,
          "date": "Mon 25 Oct 2021 06:33",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 209717,
          "date": "Wed 20 Oct 2021 08:29",
          "username": "CYL",
          "content": "A.  Simplest approach to having all the requests originating to have the same public IP.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 142470,
          "date": "Mon 11 Oct 2021 20:43",
          "username": "MultiAZsam422",
          "content": "What's wrong with B? IGW also has Elastic IP that can be whitelisted.Elastic beanstalk in private vpc or not exposed to internet looking at question",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 183617,
          "date": "Mon 11 Oct 2021 23:21",
          "username": "sam422",
          "content": "Elastic beanstalk in private vpc or not exposed to internet looking at question",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 133467,
          "date": "Mon 11 Oct 2021 17:51",
          "username": "NikkyDicky",
          "content": "A.  Dup of Q158",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#558",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a website that enables users to upload videos. Company policy states the uploaded videos must be analyzed for restricted content. An uploaded video is placed in Amazon S3, and a message is pushed to an Amazon SQS queue with the video's location. A backend application pulls this location from<br>Amazon SQS and analyzes the video.<br>The video analysis is compute-intensive and occurs sporadically during the day. The website scales with demand. The video analysis application runs on a fixed number of instances. Peak demand occurs during the holidays, so the company must add instances to the application during this time. All instances used are currently on-demand Amazon EC2 T2 instances. The company wants to reduce the cost of the current solution.<br>Which of the following solutions is MOST cost-effective?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#558",
          "answers": [
            {
              "choice": "<p>A. Keep the website on T2 instances. Determine the minimum number of website instances required during off-peak times and use Spot Instances to cover them while using Reserved Instances to cover peak demand. Use Amazon EC2 R4 and Amazon EC2 R5 Reserved Instances in an Auto Scaling group for the video analysis application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Keep the website on T2 instances. Determine the minimum number of website instances required during off-peak times and use Reserved Instances to cover them while using On-Demand Instances to cover peak demand. Use Spot Fleet for the video analysis application comprised of Amazon EC2 C4 and Amazon EC2 C5 Spot Instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the website to AWS Elastic Beanstalk and Amazon EC2 C4 instances. Determine the minimum number of website instances required during off-peak times and use On-Demand Instances to cover them while using Spot capacity to cover peak demand. Use Spot Fleet for the video analysis application comprised of C4 and Amazon EC2 C5 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the website to AWS Elastic Beanstalk and Amazon EC2 R4 instances. Determine the minimum number of website instances required during off-peak times and use Reserved Instances to cover them while using On-Demand Instances to cover peak demand. Use Spot Fleet for the video analysis application comprised of R4 and Amazon EC2 R5 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 319044,
          "date": "Mon 25 Oct 2021 03:01",
          "username": "Pupu86joe16",
          "content": "B is correct as it uses C4 and C5 (compute intensive instances) while D uses R4 and R5 (memory intensive instances) even if beanstalk auto-scaling is taken into account.B. <br>Main points to support this answer - Reserved Instances for off-peak load, spot for video processing, C4/5 for compute optimized video processing.",
          "upvote_count": "122",
          "selected_answers": ""
        },
        {
          "id": 455158,
          "date": "Fri 29 Oct 2021 21:06",
          "username": "joe16",
          "content": "B. <br>Main points to support this answer - Reserved Instances for off-peak load, spot for video processing, C4/5 for compute optimized video processing.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349354,
          "date": "Tue 26 Oct 2021 02:49",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 506346,
          "date": "Tue 21 Dec 2021 19:26",
          "username": "AzureDP900",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 496573,
          "date": "Wed 08 Dec 2021 07:03",
          "username": "cldy",
          "content": "B.  Keep the website on T2 instances. Determine the minimum number of website instances required during off-peak times and use Reserved Instances to cover them while using On-Demand Instances to cover peak demand. Use Spot Fleet for the video analysis application comprised of Amazon EC2 C4 and Amazon EC2 C5 Spot Instances.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494749,
          "date": "Mon 06 Dec 2021 01:13",
          "username": "AzureDP900",
          "content": "B is right..use Reserved Instances to cover them while using On-Demand Instances to cover peak demand. Use Spot Fleet",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411681,
          "date": "Tue 26 Oct 2021 04:37",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289467,
          "date": "Sun 24 Oct 2021 13:24",
          "username": "hezll",
          "content": "why not D.  Beanstalk has auto-scalling,",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 278273,
          "date": "Sun 24 Oct 2021 13:00",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243984,
          "date": "Sat 23 Oct 2021 22:03",
          "username": "T14102020",
          "content": "Correct is B.  T2 + RI for off-peak + without Beanstalk",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 231604,
          "date": "Sat 23 Oct 2021 08:48",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209720,
          "date": "Fri 22 Oct 2021 07:19",
          "username": "CYL",
          "content": "B.  Correct usage of reserved, on-demand and spot instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 133469,
          "date": "Thu 21 Oct 2021 05:30",
          "username": "NikkyDicky",
          "content": "B.  Dup of Q159",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 104848,
          "date": "Mon 18 Oct 2021 19:12",
          "username": "meenu2225",
          "content": "B seems right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 87830,
          "date": "Mon 18 Oct 2021 03:24",
          "username": "Mkumar",
          "content": "Answer is B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 68989,
          "date": "Wed 22 Sep 2021 22:13",
          "username": "LunchTimeshyamexamprep",
          "content": "This is a replication of Question #159Topic 2where is topic 2 questions pls confirm? <br>I am preparing for solution architect professional exam ,kindly confirm examtopics questions from 390-953 are enough to pass the exam.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 692824,
          "date": "Wed 12 Oct 2022 09:46",
          "username": "shyamexamprep",
          "content": "where is topic 2 questions pls confirm? <br>I am preparing for solution architect professional exam ,kindly confirm examtopics questions from 390-953 are enough to pass the exam.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#559",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that uses Amazon EC2 instances in an Auto Scaling group. The Quality Assurance (QA) department needs to launch a large number of short-lived environments to test the application. The application environments are currently launched by the Manager of the department using an AWS<br>CloudFormation template. To launch the stack, the Manager uses a role with permission to use CloudFormation, EC2, and Auto Scaling APIs. The Manager wants to allow testers to launch their own environments, but does not want to grant broad permissions to each user.<br>Which set up would achieve these goals?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#559",
          "answers": [
            {
              "choice": "<p>A. Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to assume the Manager's role and add a policy that restricts the permissions to the template and the resources it creates. Train users to launch the template from the CloudFormation console.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Service Catalog product from the environment template. Add a launch constraint to the product with the existing role. Give users in the QA department permission to use AWS Service Catalog APIs only. Train users to launch the templates from the AWS Service Catalog console.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Upload the AWS CloudFormation template to Amazon S3. Give users in the QA department permission to use CloudFormation and S3 APIs, with conditions that restrict the permission to the template and the resources it creates. Train users to launch the template from the CloudFormation console.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Elastic Beanstalk application from the environment template. Give users in the QA department permission to use Elastic Beanstalk permissions only. Train users to launch Elastic Beanstalk environment with the Elastic Beanstalk CLI, passing the existing role to the environment as a service role.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 65077,
          "date": "Mon 20 Sep 2021 02:09",
          "username": "jay1ram2",
          "content": "A? Grant QA users access to Manager Role is a blatant violation of Security.<br><br>B makes more sense to me as it restricts users to create services through the catalog.",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 694931,
          "date": "Fri 14 Oct 2022 19:19",
          "username": "Blair77",
          "content": "I'll go with B!!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 494751,
          "date": "Mon 06 Dec 2021 01:17",
          "username": "AzureDP900",
          "content": "Service Catalog is right option. B for sure right answer.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 430104,
          "date": "Sun 07 Nov 2021 05:41",
          "username": "dencccAzureDP900",
          "content": "It's B, not sure what other discussion is going on below.Below discussions doesn't seems related to this question, I am not sure what they are taking :)",
          "upvote_count": "34",
          "selected_answers": ""
        },
        {
          "id": 494752,
          "date": "Mon 06 Dec 2021 01:18",
          "username": "AzureDP900",
          "content": "Below discussions doesn't seems related to this question, I am not sure what they are taking :)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 411688,
          "date": "Sun 07 Nov 2021 04:15",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 336220,
          "date": "Sun 07 Nov 2021 02:12",
          "username": "chuck_leejoe16",
          "content": "needs to improve the scalable performance and availability of the database.<br>Which solution meets these requirements?<br><br>A.  Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold<br>B.  Migrate the database to Amazon Aurora, and add a read replica Add a database connection pool outside of the Lambda handler function<br>C.  Migrate the database to Amazon Aurora, and add a read replica Use Amazon Route 53 weighted records<br>D.  Migrate the database to Amazon Aurora, and add an Aurora Replica Configure Amazon RDS Proxy to manage database connection poolsD. <br>Lambdas are stateless and can't rely on connection pool. To get over this problem, AWS provide RDS proxy for connection pool management.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 455159,
          "date": "Sun 07 Nov 2021 11:13",
          "username": "joe16",
          "content": "D. <br>Lambdas are stateless and can't rely on connection pool. To get over this problem, AWS provide RDS proxy for connection pool management.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 336219,
          "date": "Sun 07 Nov 2021 00:03",
          "username": "chuck_lee",
          "content": "A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events the application has a much higher workload than normal Users notice slow response times during the peak periods because of many database connections. The company needs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253044,
          "date": "Sat 06 Nov 2021 08:15",
          "username": "Bulti",
          "content": "Answer is B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243988,
          "date": "Sat 06 Nov 2021 06:37",
          "username": "T14102020",
          "content": "Correct is B.  Service Catalog product",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231607,
          "date": "Fri 05 Nov 2021 18:36",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 228546,
          "date": "Fri 05 Nov 2021 08:52",
          "username": "bbnbnuyh",
          "content": "B makes sense as AWS Service Catalog is created for a use-case like this.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 209722,
          "date": "Thu 04 Nov 2021 21:02",
          "username": "CYL",
          "content": "B, using service catalog to show what are the allowed services will be the easiest way to approach the restrictions.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 160580,
          "date": "Sun 31 Oct 2021 11:51",
          "username": "JBRIANPhatFualifebegins",
          "content": "A.  Verify the AWS IoT Device Shadow service is subscribed to the appropriate topic and is executing the AWS Lambda function.<br>B.  Verify that AWS IoT monitoring shows that the appropriate AWS IoT rules are being executed, and that the AWS IoT rules are enabled with the correct rule actions.<br>C.  Check the AWS IoT Fleet indexing service and verify that the thing group has the appropriate IAM role to update DynamoDB. <br>D.  Verify that AWS IoT things are using MQTT instead of MQTT over WebScocket, then check that the provisioning has the appropriate policy attached.D is correctexplain pleaseAnswer is B: IoT Rules. https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html",
          "upvote_count": "1111",
          "selected_answers": ""
        },
        {
          "id": 179346,
          "date": "Tue 02 Nov 2021 09:17",
          "username": "PhatFualifebegins",
          "content": "D is correctexplain pleaseAnswer is B: IoT Rules. https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 181384,
          "date": "Tue 02 Nov 2021 17:12",
          "username": "Fualifebegins",
          "content": "explain pleaseAnswer is B: IoT Rules. https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 561402,
          "date": "Sat 05 Mar 2022 12:42",
          "username": "lifebegins",
          "content": "Answer is B: IoT Rules. https://docs.aws.amazon.com/iot/latest/developerguide/iot-rules.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 160579,
          "date": "Sun 31 Oct 2021 10:39",
          "username": "JBRIAN",
          "content": "NO.80 An IoT company has rolled out a fleet of sensors for monitoring temperatures in remote locations. Each device connect to AWS IoT Core and sends a message 30 seconds, updating an Amazon DynamoDB table. A System Administrator users AWS IoT to verify the devices are still sending messages to AWS IoT Core: the database is not updating.<br>What should a Solution Architect check to determine why the database is not being updated?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 160576,
          "date": "Sun 31 Oct 2021 07:30",
          "username": "JBRIANSadioManeSadioManeNit_1",
          "content": "A.  Store the data in Amazon DocumentDB Create a single global Amazon CloudFront distribution with a custom origin built on edge-optimized Amazon API Gateway and AWS Lambda Assign the company's domain as an alternate domain for the distribution. and configure Amazon Route 53 with an alias to the CloudFront distribution<br>B.  Store the data in replicated Amazon S3 buckets in two Regions Create an Amazon CloudFront distribution in each Region, with custom origins built on Amazon API Gateway and AWS Lambda launched in each Region Assign the company's domain as an alternate domain for both distributions and configure Amazon Route 53 with a failover routing policy between them<br>C.  Store the data in an Amazon DynamoDB global table in two Regions using on-demand capacity mode In both Regions, run the web service as Amazon ECS Fargate tasks in an Auto Scaling ECS service behind an Application Load Balancer (ALB) In Amazon Route 53, configure an alias record in the company's domain and a Route 53 latency-based routing policy with health checks to distribute traffic between the two ALBsAnswer is ABDSorry. The answer is meant for Q #63What is the ans for Q77",
          "upvote_count": "2111",
          "selected_answers": ""
        },
        {
          "id": 171376,
          "date": "Tue 02 Nov 2021 01:49",
          "username": "SadioManeSadioMane",
          "content": "Answer is ABDSorry. The answer is meant for Q #63",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 171377,
          "date": "Tue 02 Nov 2021 02:45",
          "username": "SadioMane",
          "content": "Sorry. The answer is meant for Q #63",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 206629,
          "date": "Thu 04 Nov 2021 18:40",
          "username": "Nit_1",
          "content": "What is the ans for Q77",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 160575,
          "date": "Sun 31 Oct 2021 05:04",
          "username": "JBRIAN",
          "content": "NO.77 A company is refactoring an existing web service that provides read and write access to structured data. The service must respond to short but significant spikes in the system load The service must be fault tolerant across multiple AWS Regions.<br>Which actions should be taken to meet these requirements?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 160574,
          "date": "Sun 31 Oct 2021 02:45",
          "username": "JBRIANDuyPhanperioNkemNkemNNHANA_New_Guy",
          "content": "A.  Apply environment, cost center, and application name tags to all taggable resources<br>B.  Configure custom budgets and define thresholds using Cost Explorer<br>C.  Configure AWS Trusted Advisor to obtain weekly emails with cost-saving estimates<br>D.  Create a portfolio for each business unit and add products to the portfolios using AWS<br>CloudFormation in AWS Service Catalog<br>E.  Configure a billing alarm in Amazon CloudWatch.<br>F.  Configure SCPs in AWS Organizations to allow services available using AWSthe correct answer is ABDI agree. <br>'A' for calculating the costs for each project, environment.<br>'B' for limiting the usage of resources in the dev account.<br>'D' for business units deploying pre-approved IT services only.ADF<br>A: Calculating costs for each project, environment<br>D: Limiting deployed resources<br>F: Centrally managing IT services and limiting AWS resources in the development accountReversed myself. Answer is ABDB is wrong, threshold should be defined in Budget service.The correct answer is ADF",
          "upvote_count": "1114121",
          "selected_answers": ""
        },
        {
          "id": 180236,
          "date": "Tue 02 Nov 2021 14:39",
          "username": "DuyPhanperio",
          "content": "the correct answer is ABDI agree. <br>'A' for calculating the costs for each project, environment.<br>'B' for limiting the usage of resources in the dev account.<br>'D' for business units deploying pre-approved IT services only.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 181529,
          "date": "Tue 02 Nov 2021 20:09",
          "username": "perio",
          "content": "I agree. <br>'A' for calculating the costs for each project, environment.<br>'B' for limiting the usage of resources in the dev account.<br>'D' for business units deploying pre-approved IT services only.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 182812,
          "date": "Wed 03 Nov 2021 00:50",
          "username": "NkemNkemNNHANA_New_Guy",
          "content": "ADF<br>A: Calculating costs for each project, environment<br>D: Limiting deployed resources<br>F: Centrally managing IT services and limiting AWS resources in the development accountReversed myself. Answer is ABDB is wrong, threshold should be defined in Budget service.The correct answer is ADF",
          "upvote_count": "4121",
          "selected_answers": ""
        },
        {
          "id": 182815,
          "date": "Wed 03 Nov 2021 04:16",
          "username": "NkemNNHANA_New_Guy",
          "content": "Reversed myself. Answer is ABDB is wrong, threshold should be defined in Budget service.The correct answer is ADF",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 205355,
          "date": "Thu 04 Nov 2021 09:45",
          "username": "NNHANA_New_Guy",
          "content": "B is wrong, threshold should be defined in Budget service.The correct answer is ADF",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 224354,
          "date": "Fri 05 Nov 2021 00:09",
          "username": "A_New_Guy",
          "content": "The correct answer is ADF",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#560",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has several teams, and each team has their own Amazon RDS database that totals 100 TB.  The company is building a data query platform for<br>Business Intelligence Analysts to generate a weekly business report. The new system must run ad-hoc SQL queries.<br>What is the MOST cost-effective solution?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#560",
          "answers": [
            {
              "choice": "<p>A. Create a new Amazon Redshift cluster. Create an AWS Glue ETL job to copy data from the RDS databases to the Amazon Redshift cluster. Use Amazon Redshift to run the query.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon EMR cluster with enough core nodes. Run an Apache Spark job to copy data from the RDS databases to a Hadoop Distributed File System (HDFS). Use a local Apache Hive metastore to maintain the table definition. Use Spark SQL to run the query.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an AWS Glue ETL job to copy all the RDS databases to a single Amazon Aurora PostgreSQL database. Run SQL queries on the Aurora PostgreSQL database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an AWS Glue crawler to crawl all the databases and create tables in the AWS Glue Data Catalog. Use an AWS Glue ETL job to load data from the RDS databases to Amazon S3, and use Amazon Athena to run the queries.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154150,
          "date": "Wed 22 Sep 2021 09:31",
          "username": "Nemer",
          "content": "D.  ad-hoc queries + cost advantage over Redshift -> Athena.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 276797,
          "date": "Fri 29 Oct 2021 10:28",
          "username": "Ebi",
          "content": "Cheapest option is D",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 636845,
          "date": "Mon 25 Jul 2022 19:11",
          "username": "hilft",
          "content": "Badly formed question. each team got 100tb of data set and you still not using Redshift? The right answer is D.  because it is asking for the most cost-efficient way of querying.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 626436,
          "date": "Sun 03 Jul 2022 07:53",
          "username": "aandc",
          "content": "keyword: ad-hoc SQL queries, cost -> Athena",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 532704,
          "date": "Wed 26 Jan 2022 09:37",
          "username": "shotty1",
          "content": "correct answer is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 529347,
          "date": "Fri 21 Jan 2022 21:01",
          "username": "CloudChef",
          "content": "D via Digital Cloud Training",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 527454,
          "date": "Wed 19 Jan 2022 12:31",
          "username": "pititcu667",
          "content": "d because cheap + ad-hoc",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 494753,
          "date": "Mon 06 Dec 2021 01:21",
          "username": "AzureDP900",
          "content": "I will go with D, This is most cost -effective solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411690,
          "date": "Sun 07 Nov 2021 17:53",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349362,
          "date": "Tue 02 Nov 2021 19:59",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 347310,
          "date": "Sun 31 Oct 2021 00:08",
          "username": "digimaniac",
          "content": "D.  Redshift, EMR, and Anthena can all do the job. Read this article.<br>https://aws.amazon.com/athena/faqs/#When_to_use_Athena_vs_other_big_data_services",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 292212,
          "date": "Sat 30 Oct 2021 06:58",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 266158,
          "date": "Fri 29 Oct 2021 02:27",
          "username": "rkbala",
          "content": "D.  Athena is cheap over Redshift",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253049,
          "date": "Wed 27 Oct 2021 17:07",
          "username": "Bulti",
          "content": "Correct answer is D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 250760,
          "date": "Thu 21 Oct 2021 18:06",
          "username": "Brittspetebear55",
          "content": "The question clearly states, that the purpose is to have Business analytics kind of queries. pushes this towards Redshift. i.e. A.  Otherwise why would somebody ever need a redshift cluster, if S3 and Athena could have been used\\\"QUERIES\\\"!!!!IS THE KEY HERE!!... This indicates simple things like .. how many blue jumpers were sold etc.Redshift would be more appropriate for DEEP statistical analysis... such as plotting flight routes.. answer is D",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 253445,
          "date": "Thu 28 Oct 2021 13:27",
          "username": "petebear55",
          "content": "\\\"QUERIES\\\"!!!!IS THE KEY HERE!!... This indicates simple things like .. how many blue jumpers were sold etc.Redshift would be more appropriate for DEEP statistical analysis... such as plotting flight routes.. answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 243990,
          "date": "Thu 21 Oct 2021 17:53",
          "username": "T14102020",
          "content": "Correct is D.  Athena + Glue Crawler",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231610,
          "date": "Thu 21 Oct 2021 12:37",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#561",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company provides AWS solutions to its users with AWS CloudFormation templates. Users launch the templates in their accounts to have different solutions provisioned for them. The users want to improve the deployment strategy for solutions while retaining the ability to do the following:<br>✑ Add their own features to a solution for their specific deployments.<br>✑ Run unit tests on their changes.<br>✑ Turn features on and off for their deployments.<br>✑ Automatically update with code changes.<br>✑ Run security scanning tools for their deployments.<br>Which strategies should the Solutions Architect use to meet the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#561",
          "answers": [
            {
              "choice": "<p>A. Allow users to download solution code as Docker images. Use AWS CodeBuild and AWS CodePipeline for the CI/CD pipeline. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use AWS CodeDeploy to run unit tests and security scans, and for deploying and updating a solution with changes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use AWS Amplify plugins for different solution features and user prompts to turn features on and off. Use AWS Lambda to run unit tests and security scans, and AWS CodeBuild for deploying and updating a solution with changes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Allow users to download solution code artifacts in their Amazon S3 buckets. Use Amazon S3 and AWS CodePipeline for the CI/CD pipelines. Use CloudFormation StackSets for different solution features and to turn features on and off. Use AWS Lambda to run unit tests and security scans, and CloudFormation for deploying and updating a solution with changes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use the AWS Cloud Development Kit constructs for different solution features, and use the manifest file to turn features on and off. Use AWS CodeBuild to run unit tests and security scans, and for deploying and updating a solution with changes.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 211007,
          "date": "Tue 21 Sep 2021 04:39",
          "username": "bbnbnuyh",
          "content": "D.  AWS CDK enables you to define your infrastructure with code and provision it through AWS CloudFormation. You get all the benefits of CloudFormation, including repeatable deployment, easy rollback, and drift detection.",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 231594,
          "date": "Wed 22 Sep 2021 00:53",
          "username": "cloudgcKelvin1477cloudgcdijesim222",
          "content": "Answer-C. <br>A - codedeploy - not used for unit tests and security scans<br>B - codebuild - not used for deploying and updating<br>D - codebuild - not used for deploying and updatingsomewhat agree with stacksets use case for providing various flavor of the solution templatelooks like a keyword is missing in Answer-D. <br><br>and XXXX for deploying and updating a solution with changes.<br><br>if this is true then the answer can be D. CDK as in answer D can EITHER output cloudformation tempaltes OR deploy the stack immediately (which is totally feasible to do with codebuild). IF the output was a cloudformation template (which was NOT in answer D) it is totally feasible to deploy cloudformation templates with codepipeline alone, no codedeploy etc. is needed. -> anwser D is perfect",
          "upvote_count": "6132",
          "selected_answers": ""
        },
        {
          "id": 236543,
          "date": "Sat 02 Oct 2021 11:39",
          "username": "Kelvin1477",
          "content": "somewhat agree with stacksets use case for providing various flavor of the solution template",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 237818,
          "date": "Wed 06 Oct 2021 23:59",
          "username": "cloudgcdijesim222",
          "content": "looks like a keyword is missing in Answer-D. <br><br>and XXXX for deploying and updating a solution with changes.<br><br>if this is true then the answer can be D. CDK as in answer D can EITHER output cloudformation tempaltes OR deploy the stack immediately (which is totally feasible to do with codebuild). IF the output was a cloudformation template (which was NOT in answer D) it is totally feasible to deploy cloudformation templates with codepipeline alone, no codedeploy etc. is needed. -> anwser D is perfect",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 341167,
          "date": "Fri 22 Oct 2021 14:42",
          "username": "dijesim222",
          "content": "CDK as in answer D can EITHER output cloudformation tempaltes OR deploy the stack immediately (which is totally feasible to do with codebuild). IF the output was a cloudformation template (which was NOT in answer D) it is totally feasible to deploy cloudformation templates with codepipeline alone, no codedeploy etc. is needed. -> anwser D is perfect",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 632406,
          "date": "Sun 17 Jul 2022 01:47",
          "username": "rahulseth",
          "content": "A. CodeBuild can't be use for AWSrun unit tests and security scans, and for deploying and updating a solution with changes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 559958,
          "date": "Thu 03 Mar 2022 10:24",
          "username": "bobokyo",
          "content": "D is correct.<br>https://docs.aws.amazon.com/solutions/latest/smart-product-solution/components.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497344,
          "date": "Thu 09 Dec 2021 06:13",
          "username": "cldy",
          "content": "D.  Allow users to download solution code artifacts. Use AWS CodeCommit and AWS CodePipeline for the CI/CD pipeline. Use the AWS Cloud Development Kit constructs for different solution features, and use the manifest file to turn features on and off. Use AWS CodeBuild to run unit tests and security scans, and for deploying and updating a solution with changes.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494754,
          "date": "Mon 06 Dec 2021 01:24",
          "username": "AzureDP900",
          "content": "I will go with D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484644,
          "date": "Tue 23 Nov 2021 01:17",
          "username": "acloudguru",
          "content": "Correct is D.  CodePipeline + Cloud Development Kits for turn features on and off + CodeBuild to run unit tests",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 411692,
          "date": "Sat 06 Nov 2021 07:42",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350032,
          "date": "Wed 03 Nov 2021 07:36",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 348171,
          "date": "Mon 01 Nov 2021 01:55",
          "username": "blackgamer",
          "content": "D seems to be correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 321758,
          "date": "Mon 18 Oct 2021 21:49",
          "username": "alisyech",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293092,
          "date": "Mon 18 Oct 2021 05:12",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 290123,
          "date": "Fri 15 Oct 2021 08:17",
          "username": "lechuk",
          "content": "Seems there is somthing missing in D.  CodeBuild is not intended to deploy",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 265324,
          "date": "Wed 13 Oct 2021 08:37",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 255265,
          "date": "Mon 11 Oct 2021 13:16",
          "username": "Bulti",
          "content": "D is the correct answer. No other options allows the developers to modify the solution code and deploy it using CodePipeline the way D does.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244843,
          "date": "Fri 08 Oct 2021 09:37",
          "username": "T14102020",
          "content": "Correct is D.  CodePipeline + Cloud Development Kits for turn features on and off + CodeBuild to run unit tests",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 234391,
          "date": "Fri 24 Sep 2021 14:07",
          "username": "ting_66",
          "content": "CodeBuild can build, test, and ofc run scanning job. CDK is a CloudFormation for developers.<br>D is correct",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#562",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses Amazon S3 to host a web application. Currently, the company uses a continuous integration tool running on an Amazon EC2 instance that builds and deploys the application by uploading it to an S3 bucket. A Solutions Architect needs to enhance the security of the company's platform with the following requirements:<br>✑ A build process should be run in a separate account from the account hosting the web application.<br>✑ A build process should have minimal access in the account it operates in.<br>✑ Long-lived credentials should not be used.<br>As a start, the Development team created two AWS accounts: one for the application named web account process; other is a named build account.<br>Which solution should the Solutions Architect use to meet the security requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#562",
          "answers": [
            {
              "choice": "<p>A. In the build account, create a new IAM role, which can be assumed by Amazon EC2 only. Attach the role to the EC2 instance running the continuous integration process. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the build account to use s3:PutObject calls.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the build account, create a new IAM role, which can be assumed by Amazon EC2 only. Attach the role to the EC2 instance running the continuous integration process. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the newly created IAM role to use s3:PutObject calls.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the build account, create a new IAM user. Store the access key and secret access key in AWS Secrets Manager. Modify the continuous integration process to perform a lookup of the IAM user credentials from Secrets Manager. Create an IAM policy to allow s3: PutObject calls on the S3 bucket in the web account, and attack it to the user. In the web account, create an S3 bucket policy attached to the S3 bucket that allows the newly created IAM user to use s3:PutObject calls.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the build account, modify the continuous integration process to perform a lookup of the IAM user credentials from AWS Secrets Manager. In the web account, create a new IAM user. Store the access key and secret access key in Secrets Manager. Attach the PowerUserAccess IAM policy to the IAM user.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154191,
          "date": "Tue 21 Sep 2021 09:37",
          "username": "Nemerjoe16",
          "content": "B.  No long term credentials -> use roles. Bucket policies to grant permissions to the role, not the account itself.B. <br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-cross-account-upload-access/",
          "upvote_count": "204",
          "selected_answers": ""
        },
        {
          "id": 455766,
          "date": "Thu 04 Nov 2021 21:13",
          "username": "joe16",
          "content": "B. <br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-cross-account-upload-access/",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 652938,
          "date": "Sun 28 Aug 2022 14:55",
          "username": "gnic",
          "content": "It's B.  \\\"allow new role to use the API putObject\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 626464,
          "date": "Sun 03 Jul 2022 09:25",
          "username": "aandc",
          "content": "B BBBBB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 580868,
          "date": "Mon 04 Apr 2022 19:43",
          "username": "roka_ua",
          "content": "Vote B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 551260,
          "date": "Sat 19 Feb 2022 20:07",
          "username": "cannottellname",
          "content": "When DEV account assumes role in PROD account, the s3 only has to allow the role of PROD account to make any changes. 2 way trust is not required in S3 policy - it is at IAM level which should already be taken care.<br><br>BBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532761,
          "date": "Wed 26 Jan 2022 10:53",
          "username": "shotty1",
          "content": "I am pretty sure it is A.  Using a role as a trusted Principal for cross account access has never worked for me, even though the documentation is sometimes a bit vague on that topic.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 527835,
          "date": "Wed 19 Jan 2022 20:01",
          "username": "pititcu667",
          "content": "roles should be used no?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 507386,
          "date": "Wed 22 Dec 2021 22:13",
          "username": "bwestpha",
          "content": "It's A.  B is just creating a policy, but not a role which can be used by anything. Additionally there should be a two way trust established, but isn't. <br>It's not good to enable the complete build-account to write into the bucket by the bucket policy, but at least this scenario will work and fullfills the requirements. A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494758,
          "date": "Mon 06 Dec 2021 01:31",
          "username": "AzureDP900",
          "content": "I am going with B.  Initially I thought of D , however that doesn't make any sense.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 448502,
          "date": "Tue 02 Nov 2021 19:34",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 422842,
          "date": "Tue 26 Oct 2021 15:00",
          "username": "dencccdenccc",
          "content": "I would think it's A, no? Can you allow a remote role in your bucket policy?Okay it's B: https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 422844,
          "date": "Sat 30 Oct 2021 07:41",
          "username": "denccc",
          "content": "Okay it's B: https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 411747,
          "date": "Tue 26 Oct 2021 04:26",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 350034,
          "date": "Mon 25 Oct 2021 07:11",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 348183,
          "date": "Sat 23 Oct 2021 08:45",
          "username": "blackgamer",
          "content": "Going with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 295537,
          "date": "Sun 17 Oct 2021 11:19",
          "username": "certainlysarah_t",
          "content": "A.  is correct. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.htmlYour link says B: <br>\\\"At the end of this tutorial, you have the following:<br>- Users in the Development account (the trusted account) that are allowed to assume a specific role in the Production account.<br>- A role in the Production account (the trusting account) that is allowed to access a specific Amazon S3 bucket.<br>- The productionapp bucket in the Production account.\\\"",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 333883,
          "date": "Mon 18 Oct 2021 00:53",
          "username": "sarah_t",
          "content": "Your link says B: <br>\\\"At the end of this tutorial, you have the following:<br>- Users in the Development account (the trusted account) that are allowed to assume a specific role in the Production account.<br>- A role in the Production account (the trusting account) that is allowed to access a specific Amazon S3 bucket.<br>- The productionapp bucket in the Production account.\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292470,
          "date": "Wed 06 Oct 2021 03:08",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281180,
          "date": "Sat 02 Oct 2021 16:36",
          "username": "Firststack",
          "content": "Bis correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#563",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A fleet of Amazon ECS instances is used to poll an Amazon SQS queue and update items in an Amazon DynamoDB database. Items in the table are not being updated, and the SQS queue is filling up. Amazon CloudWatch Logs are showing consistent 400 errors when attempting to update the table. The provisioned write capacity units are appropriately configured, and no throttling is occurring.<br>What is the LIKELY cause of the failure?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#563",
          "answers": [
            {
              "choice": "<p>A. The ECS service was deleted.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The ECS configuration does not contain an Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The ECS instance task execution IAM role was modified.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. The ECS task role was modified.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154258,
          "date": "Sat 25 Sep 2021 12:00",
          "username": "NemerMarcChartouny",
          "content": "D. Between executionRoleArn (option C) and taskRoleArn (D), only the latter is used to interact with DynamoDB.  The former is used to download images or write logs to Cloudwatch. <br><br>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html<br><br>Status 400 with DynamoDB.  Here,probably an authn failure due to someone messing up the role.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodesNemer in Arabic means 'Tiger'... And it seems you are a real AWS Tiger Man!! #Guru_Level",
          "upvote_count": "329",
          "selected_answers": ""
        },
        {
          "id": 378043,
          "date": "Fri 29 Oct 2021 20:52",
          "username": "MarcChartouny",
          "content": "Nemer in Arabic means 'Tiger'... And it seems you are a real AWS Tiger Man!! #Guru_Level",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 530180,
          "date": "Sun 23 Jan 2022 02:06",
          "username": "RVivek",
          "content": "D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498336,
          "date": "Fri 10 Dec 2021 07:15",
          "username": "GeniusMikeLiu",
          "content": "https://sysadmins.co.za/difference-with-ecs-task-and-execution-iam-roles-on-aws/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 494760,
          "date": "Mon 06 Dec 2021 01:32",
          "username": "AzureDP900",
          "content": "I will go with D, This question is part of Neal Davis practice tests.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 484896,
          "date": "Tue 23 Nov 2021 10:19",
          "username": "backfringe",
          "content": "I go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 482330,
          "date": "Sat 20 Nov 2021 08:44",
          "username": "acloudguru",
          "content": "C is only for agent related tasks such as cloudwatch, secret manager ,ECR, while this is 400 error, must be something wrong between DynamoDB, so such role should be D. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 411749,
          "date": "Thu 04 Nov 2021 09:30",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 398889,
          "date": "Tue 02 Nov 2021 13:59",
          "username": "tuananhngo",
          "content": "D IS CORRECT",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 350039,
          "date": "Tue 26 Oct 2021 18:11",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 348185,
          "date": "Sat 23 Oct 2021 05:13",
          "username": "blackgamer",
          "content": "D is the answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 295538,
          "date": "Thu 21 Oct 2021 21:57",
          "username": "certainly",
          "content": "agree D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292475,
          "date": "Thu 21 Oct 2021 20:44",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281182,
          "date": "Wed 20 Oct 2021 16:06",
          "username": "Firststack",
          "content": "D - Task role modification",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 276810,
          "date": "Mon 18 Oct 2021 16:58",
          "username": "Ebi",
          "content": "I will go with D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253062,
          "date": "Mon 18 Oct 2021 16:14",
          "username": "Bulti",
          "content": "D is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244010,
          "date": "Thu 07 Oct 2021 05:22",
          "username": "T14102020",
          "content": "Correct is D. Task Role",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231613,
          "date": "Tue 05 Oct 2021 19:20",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#564",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A mobile gaming application publishes data continuously to Amazon Kinesis Data Streams. An AWS Lambda function processes records from the data stream and writes to an Amazon DynamoDB table. The DynamoDB table has an auto scaling policy enabled with the target utilization set to 70%.<br>For several minutes at the start and end of each day, there is a spike in traffic that often exceeds five times the normal load. The company notices the<br>GetRecords.IteratorAgeMilliseconds metric of the Kinesis data stream temporarily spikes to over a minute for several minutes. The AWS Lambda function writes<br>ProvisionedThroughputExceededException messages to Amazon CloudWatch Logs during these times, and some records are redirected to the dead letter queue.<br>No exceptions are thrown by the Kinesis producer on the gaming application.<br>What change should the company make to resolve this issue?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#564",
          "answers": [
            {
              "choice": "<p>A. Use Application Auto Scaling to set a scaling schedule to scale out write capacity on the DynamoDB table during predictable load spikes.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon CloudWatch Events to monitor the dead letter queue and invoke a Lambda function to automatically retry failed records.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Reduce the DynamoDB table auto scaling policy's target utilization to 20% to more quickly respond to load spikes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Increase the number of shards in the Kinesis data stream to increase throughput capacity.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 175455,
          "date": "Fri 24 Sep 2021 02:30",
          "username": "hailiangsam422sarah_tsarah_t",
          "content": "Its A.  The alerts clearly indicate the problem was caused by sudden spike in traffic. Autoscaling on DDB didnt work because the suddenness of the spike, which is why you need to scale out the DDB before the traffic spike comes in rather than wait for the actual spike to trigger the scalingIt makes sense to auto scale dynamodb when cpu utilisation is being spiked, rather than predicting the spike timeThis https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html points to C, not AHowever, after reading this https://aws.amazon.com/about-aws/whats-new/2017/11/scheduled-scaling-now-available-for-application-auto-scaling/ I am probably going with A. ..",
          "upvote_count": "18212",
          "selected_answers": ""
        },
        {
          "id": 184008,
          "date": "Fri 24 Sep 2021 10:36",
          "username": "sam422",
          "content": "It makes sense to auto scale dynamodb when cpu utilisation is being spiked, rather than predicting the spike time",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 333898,
          "date": "Fri 29 Oct 2021 12:37",
          "username": "sarah_tsarah_t",
          "content": "This https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html points to C, not AHowever, after reading this https://aws.amazon.com/about-aws/whats-new/2017/11/scheduled-scaling-now-available-for-application-auto-scaling/ I am probably going with A. ..",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 333899,
          "date": "Sat 30 Oct 2021 12:58",
          "username": "sarah_t",
          "content": "However, after reading this https://aws.amazon.com/about-aws/whats-new/2017/11/scheduled-scaling-now-available-for-application-auto-scaling/ I am probably going with A. ..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 168028,
          "date": "Tue 21 Sep 2021 17:18",
          "username": "b3llman",
          "content": "Ans: C<br>Although it had auto scaling enabled in Dynamodb, it did not scale quick enough. Dynamodb's auto scaling relies on cloudwatch alarms and it takes at least a minute to trigger each scaling based on the 70% utilisation target. This was explained in the GetRecords.IteratorAgeMilliseconds matrix from Kinesis that lambda was not getting records from Kinesis quick enough. <br>https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html<br><br>Since the spikes were huge and it hit the provisioned WCU during that time before auto-scaling could kick in. It resulted in ProvisionedThroughputExceededException from Dynamodb. As a result, it took a few rounds (a few mins) to scale to the desired utilisation target. <br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html<br><br>So, the solution is to lower the utilisation target and let it scale ASAP.",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 688360,
          "date": "Fri 07 Oct 2022 08:00",
          "username": "JohnPi",
          "content": "DynamoDB auto scaling modifies provisioned throughput settings only when the actual workload stays elevated (or depressed) for a sustained period of several minutes. The Application Auto Scaling target tracking algorithm seeks to keep the target utilization at or near your chosen value over the long term.<br><br>Sudden, short-duration spikes of activity are accommodated by the table's built-in burst capacity.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 676140,
          "date": "Thu 22 Sep 2022 14:51",
          "username": "AwsBRFan",
          "content": "Sinceissue can be related to consumers, then changing to A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 676117,
          "date": "Thu 22 Sep 2022 14:33",
          "username": "AwsBRFan",
          "content": "https://aws.amazon.com/pt/premiumsupport/knowledge-center/kinesis-data-streams-iteratorage-metric/<br><br>\\\"However, if the processing time cannot be reduced, then consider upscaling the Kinesis stream by increasing the number of shards.\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 577481,
          "date": "Tue 29 Mar 2022 12:26",
          "username": "jj22222",
          "content": "A.  Use Application Auto Scaling to set a scaling schedule to scale out write capacity on the DynamoDB table during predictable load spikes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 521874,
          "date": "Wed 12 Jan 2022 03:22",
          "username": "limeboi18",
          "content": "I think it's A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 508191,
          "date": "Fri 24 Dec 2021 00:26",
          "username": "tkanmani76tkanmani76",
          "content": "Option A. <br>This is a case of piling records for processing. Kinesis GetRecords.IteratorAgeMilliseconds increasing indicates that records are being processed slowly and this higlights the risk of records expiring. ProvisionedThroughputExceededException indicates request rate is too high. AWS API Doc says - Reduce the frequency of requests and use exponential backoff so they can be processed. To ensure the records are processed quickly during surge times which is known aheadwrite capacity should be increased.Related information - When Kinesis Producer is writing to KDS - the capacity is determined by the number of shards ( provisioned mode where the load is known). AWS supports on-demand mode where the shards are scaled up/down. Each shard for writing is able to handle 1MB/Sec. So if we need to increase write we need to increase the shards.This is not relevant in our case as the data is getting written and Lambda is able to read from the shards.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 508196,
          "date": "Fri 24 Dec 2021 00:40",
          "username": "tkanmani76",
          "content": "Related information - When Kinesis Producer is writing to KDS - the capacity is determined by the number of shards ( provisioned mode where the load is known). AWS supports on-demand mode where the shards are scaled up/down. Each shard for writing is able to handle 1MB/Sec. So if we need to increase write we need to increase the shards.This is not relevant in our case as the data is getting written and Lambda is able to read from the shards.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494764,
          "date": "Mon 06 Dec 2021 01:37",
          "username": "AzureDP900",
          "content": "A is right answer based on traffic surge that often surpasses five times the average load",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 461277,
          "date": "Sat 06 Nov 2021 04:23",
          "username": "kirrim",
          "content": "You can tell the issue is with DynamoDB because Lambda is reporting a ProvisionedThroughputExceededException, which is part of the DynamoDB SDK that Lambda code is using, indicating DynamoDB cannot keep up.So you know you're dealing with A or C. The root of the problem is that even though DynamoDB is set up for autoscaling, it takes a few minutes for it to happen.Merely adjusting the auto scaling policy thresholds can't change that fact, it's still going to take a while to scale up.If the traffic was a slow ramp up, you might be able to get away with C, but this is a sudden flood that happens twice per day.Since this is very predictable and on a schedule, the easiest method is to schedule the scale-up to happen in advance of the flood hitting.(A)<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-data-streams-iteratorage-metric/<br>https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/dynamodbv2/model/ProvisionedThroughputExceededException.html<br>https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-scheduled-scaling.html",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 435739,
          "date": "Fri 05 Nov 2021 17:56",
          "username": "tgv",
          "content": "AAA<br>---",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411761,
          "date": "Thu 04 Nov 2021 22:32",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 402713,
          "date": "Thu 04 Nov 2021 20:08",
          "username": "Kopa",
          "content": "Im for A, it happens on scheduled time so why not choose schedule automatic scale...",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350064,
          "date": "Thu 04 Nov 2021 13:16",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 348190,
          "date": "Wed 03 Nov 2021 05:05",
          "username": "blackgamer",
          "content": "My answer is A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 347715,
          "date": "Sun 31 Oct 2021 14:30",
          "username": "digimaniac",
          "content": "A<br>First you need to figure out where the congestion is. It is between Lamda and Dynamo DB.  Then, you need to understand the Autoscaling of Dynamo DB.  it only react after a few min of sustained spike. Adjusting the target down actually wont do anything. In reality, Dynamo DB has Using Burst Capacity which can handle 5 min burst.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 302501,
          "date": "Fri 29 Oct 2021 04:07",
          "username": "AJBA",
          "content": "A is the Answer<br>\\\"The first is predictable traffic, which means the scheduled actions. An example of predictable traffic is when your Kinesis Data Stream endpoint sees growing traffic in specific time window. In this case, you can make sure that an Application Auto Scaling scheduled action increases the number of Kinesis Data Stream shards to meet the demand. For instance, you might increase the number of shards at 12:00 p.m. and decrease them at 8:00 p.m.\\\"<br>https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/#aws-comment-trigger-5929:~:text=The%20first%20is%20predictable%20traffic%2C%20which,and%20decrease%20them%20at%208%3A00%20p.m.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#565",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB. <br>What can a Solutions Architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#565",
          "answers": [
            {
              "choice": "<p>A. Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload API objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user s3:PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154368,
          "date": "Mon 27 Sep 2021 07:15",
          "username": "Nemer",
          "content": "C. S3 Transfer Acceleration + multipart upload for performance, presigned URLs for access.",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 497381,
          "date": "Thu 09 Dec 2021 07:22",
          "username": "cldy",
          "content": "C.  Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494765,
          "date": "Mon 06 Dec 2021 01:40",
          "username": "AzureDP900",
          "content": "S3 Transfer Acceleration is right choice, I will go with C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448515,
          "date": "Sat 06 Nov 2021 13:03",
          "username": "moon2351",
          "content": "Answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411765,
          "date": "Tue 02 Nov 2021 13:36",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 353385,
          "date": "Mon 01 Nov 2021 16:01",
          "username": "Chibuzo1",
          "content": "The answer is C. <br>. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 350069,
          "date": "Thu 28 Oct 2021 11:42",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 292480,
          "date": "Thu 28 Oct 2021 01:59",
          "username": "Kian1",
          "content": "ofc going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277537,
          "date": "Tue 26 Oct 2021 00:45",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 253082,
          "date": "Sun 24 Oct 2021 03:16",
          "username": "Bulti",
          "content": "The question is about uploading the object faster not about retrieving uploaded objects faster and hence the answer is C.  When using CloudFront to upload objects with S3 as origin the request goes through the Edge servers but doesn't use the S3 Transfer acceleration feature to accelerate the upload. Uploading speeds from slow to fast - direct S3-> Cloudfront to S3-> S3 transfer acceleration",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 250745,
          "date": "Thu 21 Oct 2021 14:17",
          "username": "Britts",
          "content": "No brainer. C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244023,
          "date": "Sun 17 Oct 2021 22:24",
          "username": "T14102020",
          "content": "Correct is C.  S3 Transfer Acceleration",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231619,
          "date": "Sun 17 Oct 2021 07:30",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209592,
          "date": "Fri 15 Oct 2021 16:22",
          "username": "CYL",
          "content": "C.  Multipart upload and S3 transfer acceleration to handle the upload challenge. Presigned URL to ensure only the right users can do the upload.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 199191,
          "date": "Tue 12 Oct 2021 08:26",
          "username": "Paitan",
          "content": "C is the right option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 168953,
          "date": "Thu 07 Oct 2021 21:58",
          "username": "kanavpeersam422",
          "content": "D could be the answer, but POST and PUT methods are not supported as cache in cloudfront<br>https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CachedMethods.htmlIssue is with S3 upload right , I didn't see a cache issue?",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 184013,
          "date": "Tue 12 Oct 2021 01:47",
          "username": "sam422",
          "content": "Issue is with S3 upload right , I didn't see a cache issue?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 157006,
          "date": "Thu 07 Oct 2021 01:45",
          "username": "Anila_Dhharisi",
          "content": "C is right answer",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#566",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's CISO has asked a Solutions Architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its applications can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.<br>The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.<br>What CI/CD configuration meets all of the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#566",
          "answers": [
            {
              "choice": "<p>A. Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154397,
          "date": "Mon 20 Sep 2021 09:37",
          "username": "Nemer",
          "content": "B seems about right:blue/green deployments to minimize downtime (as opposed to in-place deployments) + deployments can be rolled back automatically or manually with CodeDeploy.<br>https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 168066,
          "date": "Wed 22 Sep 2021 21:17",
          "username": "b3llmanangelsrppetebear55sashsz",
          "content": "Ans: A<br>Since the requirement is \\\"as quickly as possible with minimal downtime\\\". Blue/green is not as quick and the question didn't ask for zero downtime.Any links?Your missing the point of the question \\\"push another code update.\\\"is NOT a ROLLBACK.... Its important to READ the question in the exam ... B is right as it is very fluid ..You are missing the question's requirements.",
          "upvote_count": "5131",
          "selected_answers": ""
        },
        {
          "id": 207082,
          "date": "Thu 23 Sep 2021 21:31",
          "username": "angelsrp",
          "content": "Any links?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253627,
          "date": "Tue 12 Oct 2021 08:09",
          "username": "petebear55sashsz",
          "content": "Your missing the point of the question \\\"push another code update.\\\"is NOT a ROLLBACK.... Its important to READ the question in the exam ... B is right as it is very fluid ..You are missing the question's requirements.",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 580140,
          "date": "Sun 03 Apr 2022 09:01",
          "username": "sashsz",
          "content": "You are missing the question's requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 652989,
          "date": "Sun 28 Aug 2022 16:50",
          "username": "kadev",
          "content": "\\\"push another code update\\\" i dont like that =>need to rollback to latest stable version => B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497391,
          "date": "Thu 09 Dec 2021 07:31",
          "username": "cldy",
          "content": "B.  Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494767,
          "date": "Mon 06 Dec 2021 01:42",
          "username": "AzureDP900",
          "content": "I will go with B, Blue/Green is fast to rollback.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 411769,
          "date": "Fri 05 Nov 2021 12:41",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350071,
          "date": "Tue 02 Nov 2021 01:43",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 330611,
          "date": "Sat 23 Oct 2021 11:26",
          "username": "KnightVictor",
          "content": "No brainer. going with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292483,
          "date": "Fri 22 Oct 2021 17:28",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281187,
          "date": "Fri 22 Oct 2021 01:04",
          "username": "Firststack",
          "content": "B Blue/Green",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 277538,
          "date": "Mon 18 Oct 2021 17:07",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 265289,
          "date": "Sun 17 Oct 2021 18:13",
          "username": "kopper2019",
          "content": "B as well",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253107,
          "date": "Mon 04 Oct 2021 14:23",
          "username": "Bulti",
          "content": "B is the right answer as it provides the least downtime option.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244024,
          "date": "Thu 30 Sep 2021 19:58",
          "username": "T14102020",
          "content": "Correct is B.  blue/green deployments",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231622,
          "date": "Mon 27 Sep 2021 22:56",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209595,
          "date": "Sat 25 Sep 2021 21:43",
          "username": "CYL",
          "content": "B.  Use blue/green deployment to minimize downtime. The rest of the options do not allow for low downtime during deployment.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 157009,
          "date": "Mon 20 Sep 2021 10:27",
          "username": "Anila_Dhharisi",
          "content": "B is right answer. Yes Blue/Green deployments has minimal downtime when compared to in-place and can be rollback automatically.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#567",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to analyze log data using date ranges with a custom application running on AWS. The application generates about 10 GB of data every day, which is expected to grow. A Solutions Architect is tasked with storing the data in Amazon S3 and using Amazon Athena to analyze the data.<br>Which combination of steps will ensure optimal performance as the data grows? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#567",
          "answers": [
            {
              "choice": "<p>A. Store each object in Amazon S3 with a random string at the front of each key.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the data in multiple S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the data in Amazon S3 in a columnar format, such as Apache Parquet or Apache ORC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the data in Amazon S3 in objects that are smaller than 10 MB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Store the data using Apache Hive partitioning in Amazon S3 using a key that includes a date, such as dt=2019-02.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154414,
          "date": "Tue 21 Sep 2021 05:27",
          "username": "Nemer",
          "content": "C & E:Optimal performance with Athena is achieved with columnar storage and partitioning the data. <br>https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 717092,
          "date": "Sun 13 Nov 2022 05:45",
          "username": "et22s",
          "content": "C: \\\"Use an efficient file format such as parquet or ORC – To dramatically reduce query running time and costs, use compressed Parquet or ORC files to store your data. To convert your existing dataset to those formats in Athena, you can use CTAS. For more information, see Using CTAS and INSERT INTO for ETL and data analysis.\\\"<br>https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html<br><br>E: \\\"By partitioning your data, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost.\\\" + \\\"Athena can use Apache Hive style partitions\\\" <br>https://docs.aws.amazon.com/athena/latest/ug/partitions.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 513372,
          "date": "Thu 30 Dec 2021 13:55",
          "username": "cldy",
          "content": "C and E. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499322,
          "date": "Sat 11 Dec 2021 11:21",
          "username": "cldy",
          "content": "C.  Store the data in Amazon S3 in a columnar format, such as Apache Parquet or Apache ORC. <br>E.  Store the data using Apache Hive partitioning in Amazon S3 using a key that includes a date, such as dt=2019-02.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498995,
          "date": "Sat 11 Dec 2021 00:10",
          "username": "challenger1",
          "content": "My Answer: C & E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 494768,
          "date": "Mon 06 Dec 2021 01:45",
          "username": "AzureDP900",
          "content": "C, E is correct . This question is part of Neal Davis practice test",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 490398,
          "date": "Tue 30 Nov 2021 04:48",
          "username": "acloudguru",
          "content": "C & E: Optimal performance with Athena is achieved with columnar storage and partitioning the data.<br>https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 448517,
          "date": "Sun 07 Nov 2021 07:15",
          "username": "moon2351",
          "content": "CE is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411770,
          "date": "Sun 07 Nov 2021 06:27",
          "username": "WhyIronMan",
          "content": "I'll go with C,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 362533,
          "date": "Sat 06 Nov 2021 08:10",
          "username": "oscargeetkanmani76",
          "content": "B & C: Athena is used for S3 query only. In question they mentioned Athena not HIVE, so don't chose E. https://docs.aws.amazon.com/athena/latest/ug/partitions.html<br>This will clarify why E. ",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 514848,
          "date": "Sun 02 Jan 2022 06:54",
          "username": "tkanmani76",
          "content": "https://docs.aws.amazon.com/athena/latest/ug/partitions.html<br>This will clarify why E. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350075,
          "date": "Fri 05 Nov 2021 19:50",
          "username": "Waiweng",
          "content": "it's C&E",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 348434,
          "date": "Mon 25 Oct 2021 11:25",
          "username": "blackgamer",
          "content": "C & E.  Don't confuse Apache Hive bucketing with AWS S3 Bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 295081,
          "date": "Sat 23 Oct 2021 04:32",
          "username": "kiev",
          "content": "Full House with CE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292485,
          "date": "Thu 21 Oct 2021 22:43",
          "username": "Kian1",
          "content": "going with CE",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281189,
          "date": "Thu 21 Oct 2021 18:37",
          "username": "Firststack",
          "content": "C & E is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277547,
          "date": "Wed 20 Oct 2021 20:03",
          "username": "Ebi",
          "content": "Definitely C,E are correct answers",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253126,
          "date": "Thu 07 Oct 2021 06:03",
          "username": "Bulti",
          "content": "Answer is C & E- Optimize Columnar data store + Partition to improve performance.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#568",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An advisory firm is creating a secure data analytics solution for its regulated financial services users. Users will upload their raw data to an Amazon S3 bucket, where they have PutObject permissions only. Data will be analyzed by applications running on an Amazon EMR cluster launched in a VPC.  The firm requires that the environment be isolated from the internet. All data at rest must be encrypted using keys controlled by the firm.<br>Which combination of actions should the Solutions Architect take to meet the user's security requirements? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#568",
          "answers": [
            {
              "choice": "<p>A. Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for AWS KMS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and a NAT gateway to access AWS KMS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch the Amazon EMR cluster in a private subnet configured to use an AWS CloudHSM appliance for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for CloudHSM.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the S3 endpoint policies to permit access to the necessary data buckets only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure the S3 bucket policies to permit access using an aws:sourceVpce condition to match the S3 endpoint ID. <br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 183212,
          "date": "Tue 05 Oct 2021 05:30",
          "username": "ipindado2020oraldevelangelsrptiana528arulrajjayarajQCOSunflyhomejoe16",
          "content": "A.  KEYS not controlled by the firm(AWS KMS). KO<br>B.  KEYS not controlled by the firm(AWS KMS) and access through internet. KO<br>C.  KEYS controlled by the firm (CloudHSM) and access to AWS public resources trhough internal VPC endpoints. OK.<br>D.  This restricts that financial service users can access just to this bucket trhough the vpc link, does not prevent anybody else to read the bucket. KO<br>E.  This will enforce the access to the bucket from the financial users vpc. OK.<br><br>Then CEDid you check about CMK??He is right, i think you are talking about costumer managed CMK which isnt mentioned in the answers.The question mentioned KMS CMK, which has two types, AWS-managed and customer-managed. So A is correct. Using KMS CMK as the firm's key, no problem at all.I think the requirement here is \\\" All data at rest must be encrypted using keys controlled by the firm \\\" ,I think KMS would do that , CloudHSM may be ideal for Customer Supplied Encryption keys with extra hardware security with no one has access to that .There is AWS managed CMK and customer managed CMKs. In this case as it applies to the question, the firm can use customer managed CMK. Based on this, A is correctBy default, AWS KMS creates the key material for a CMK. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the CMK. **** However, you can import your own key material into a CMK ****A. E - Correct<br>C is incorrect. Here is the snippted from CloudHSM FAQ page that clearly states that you need to import the CloudHSM managed key into the AWS KMS to use SSE -<br>\\\"AWS services integrate with AWS Key Management Service, which in turn is integrated with AWS CloudHSM through the KMS custom key store feature. If you want to use the server-side encryption offered by many AWS services (such as EBS, S3, or Amazon RDS), you can do so by configuring a custom key store in AWS KMS.\\\"",
          "upvote_count": "282221834",
          "selected_answers": ""
        },
        {
          "id": 197446,
          "date": "Fri 08 Oct 2021 17:59",
          "username": "oraldevelangelsrptiana528",
          "content": "Did you check about CMK??He is right, i think you are talking about costumer managed CMK which isnt mentioned in the answers.The question mentioned KMS CMK, which has two types, AWS-managed and customer-managed. So A is correct. Using KMS CMK as the firm's key, no problem at all.",
          "upvote_count": "222",
          "selected_answers": ""
        },
        {
          "id": 205806,
          "date": "Sat 09 Oct 2021 12:44",
          "username": "angelsrptiana528",
          "content": "He is right, i think you are talking about costumer managed CMK which isnt mentioned in the answers.The question mentioned KMS CMK, which has two types, AWS-managed and customer-managed. So A is correct. Using KMS CMK as the firm's key, no problem at all.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 488721,
          "date": "Sun 28 Nov 2021 03:03",
          "username": "tiana528",
          "content": "The question mentioned KMS CMK, which has two types, AWS-managed and customer-managed. So A is correct. Using KMS CMK as the firm's key, no problem at all.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 226847,
          "date": "Wed 13 Oct 2021 07:05",
          "username": "arulrajjayaraj",
          "content": "I think the requirement here is \\\" All data at rest must be encrypted using keys controlled by the firm \\\" ,I think KMS would do that , CloudHSM may be ideal for Customer Supplied Encryption keys with extra hardware security with no one has access to that .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 284852,
          "date": "Sun 24 Oct 2021 07:50",
          "username": "QCOSunflyhome",
          "content": "There is AWS managed CMK and customer managed CMKs. In this case as it applies to the question, the firm can use customer managed CMK. Based on this, A is correctBy default, AWS KMS creates the key material for a CMK. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the CMK. **** However, you can import your own key material into a CMK ****",
          "upvote_count": "83",
          "selected_answers": ""
        },
        {
          "id": 331366,
          "date": "Sat 30 Oct 2021 14:51",
          "username": "Sunflyhome",
          "content": "By default, AWS KMS creates the key material for a CMK. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the CMK. **** However, you can import your own key material into a CMK ****",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 456038,
          "date": "Sat 06 Nov 2021 23:09",
          "username": "joe16",
          "content": "A. E - Correct<br>C is incorrect. Here is the snippted from CloudHSM FAQ page that clearly states that you need to import the CloudHSM managed key into the AWS KMS to use SSE -<br>\\\"AWS services integrate with AWS Key Management Service, which in turn is integrated with AWS CloudHSM through the KMS custom key store feature. If you want to use the server-side encryption offered by many AWS services (such as EBS, S3, or Amazon RDS), you can do so by configuring a custom key store in AWS KMS.\\\"",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 155735,
          "date": "Wed 22 Sep 2021 07:46",
          "username": "Nemer",
          "content": "A & E. VPC endpoints and bucket policies...without removing the existing PutObject permissions for the users who are uploading. <br>https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 632322,
          "date": "Sat 16 Jul 2022 20:21",
          "username": "Student1950",
          "content": "I would go with C and E<br>Reason for C: Encryption in transit is required not encryption at rest<br>https://docs.aws.amazon.com/cloudhsm/latest/userguide/data-protection.html<br>Application connects to CloudHSM using interface endpoint and S3 with gateway endpoint<br>Reason for E: Gateway endpoint need bucket policy to restrict from VPCE<br><br>Reason for E:",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626979,
          "date": "Mon 04 Jul 2022 14:05",
          "username": "aandc",
          "content": "vote AE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 495825,
          "date": "Tue 07 Dec 2021 10:27",
          "username": "cldy",
          "content": "A.  Launch the Amazon EMR cluster in a private subnet configured to use an AWS KMS CMK for at-rest encryption. Configure a gateway VPC endpoint for Amazon S3 and an interface VPC endpoint for AWS KMS.<br>E.  Configure the S3 bucket policies to permit access using an aws:sourceVpce condition to match the S3 endpoint ID. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494770,
          "date": "Mon 06 Dec 2021 01:49",
          "username": "AzureDP900",
          "content": "A,E is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441011,
          "date": "Sat 06 Nov 2021 22:33",
          "username": "wakame",
          "content": "Perhaps the issue is wrong.<br>I found that there were the following releases for CloudHSM:<br>https://aws.amazon.com/about-aws/whats-new/2021/02/introducing-amazon-vpc-endpoints-aws-cloudhsm/?nc1=h_ls<br>In other words, until February of this year, it was not possible to create a VPC endpoint in CloudHSM.<br>Therefore A & E is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 438233,
          "date": "Sat 06 Nov 2021 02:34",
          "username": "Suresh108",
          "content": "SSE-S3: AWS manages both data key and master key<br><br>SSE-KMS: AWS manages data key and you manage master key<br><br>SSE-C: You manage both data key and master key<br><br>See this doc for more details: http://amzn.to/2iVsGvMA ) Server-Side Encryption<br><br>SSE-S3 (AWS-Managed Keys) => When the requirement is to keep the encryption work simple and minimise the maintenance overhead then use SSE-S3.<br><br>SSE-KMS (AWS KMS Keys) => When the requirement is to maintain a security audit trail then use SSE-KMS Keys.<br><br>SSE-C (Customer-Provided Keys) => When end-to-end encryption is not required and the client wants full control of his/her security keys, then use SSE-C. <br><br>B) Client-Side Encryption<br><br>AWS KMS-managed, customer master key => When the requirement is to maintain end-to-end encryption plus a security audit trail, then use AWS KMS Keys.<br><br>Client Managed Master Key => When the requirement is to maintain end-to-end encryption but the client wants full control of his/her security keys, then use Client Managed Master Key.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 430120,
          "date": "Fri 05 Nov 2021 16:24",
          "username": "denccc",
          "content": "A and E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411772,
          "date": "Thu 04 Nov 2021 18:03",
          "username": "WhyIronMan",
          "content": "I'll go with A,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 398713,
          "date": "Thu 04 Nov 2021 15:56",
          "username": "pradhyumna",
          "content": "A E<br>While C looks like a close one it is not a complete one, the cluster instances would need HSM client software to make it work which is missing from the answer. On the otherhand A just meets the requirements.<br>https://aws.amazon.com/cloudhsm/features/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350082,
          "date": "Thu 04 Nov 2021 15:34",
          "username": "Waiweng",
          "content": "it's A ,E",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 338294,
          "date": "Wed 03 Nov 2021 00:26",
          "username": "Amitv2706",
          "content": "For those who are voting for C, <br>Doubt that EMR supports CloudHSM based encryption option.<br><br>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 300097,
          "date": "Thu 28 Oct 2021 01:39",
          "username": "kiev",
          "content": "Guys inpindado is correct. I have confirmed with my materials from Neal Davis. The key requirements is to keep environment isolated from the Internet and with that we could use AWS CLOUDHSM and VPC condition should match S3 endpoints ID. ",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 295100,
          "date": "Tue 26 Oct 2021 10:49",
          "username": "kiev",
          "content": "A and E for me. CMK is managed by firm and E is no question.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292489,
          "date": "Mon 25 Oct 2021 22:46",
          "username": "Kian1",
          "content": "going for A,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277552,
          "date": "Sun 24 Oct 2021 06:18",
          "username": "EbiEbiRedKane",
          "content": "I will go with AEKMS CMK is managed by firm, you don't need CloudHSM,<br>Also interface endpoint does not support CloudHSM, so C is NOT CORRECT answerCloudHSM does support interface endpoint<br>https://docs.aws.amazon.com/vpc/latest/privatelink/integrated-services-vpce-list.html",
          "upvote_count": "441",
          "selected_answers": ""
        },
        {
          "id": 286574,
          "date": "Sun 24 Oct 2021 14:04",
          "username": "EbiRedKane",
          "content": "KMS CMK is managed by firm, you don't need CloudHSM,<br>Also interface endpoint does not support CloudHSM, so C is NOT CORRECT answerCloudHSM does support interface endpoint<br>https://docs.aws.amazon.com/vpc/latest/privatelink/integrated-services-vpce-list.html",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 329270,
          "date": "Fri 29 Oct 2021 21:29",
          "username": "RedKane",
          "content": "CloudHSM does support interface endpoint<br>https://docs.aws.amazon.com/vpc/latest/privatelink/integrated-services-vpce-list.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#569",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>While debugging a backend application for an IoT system that supports globally distributed devices, a Solutions Architect notices that stale data is occasionally being sent to user devices. Devices often share data, and stale data does not cause issues in most cases. However, device operations are disrupted when a device reads the stale data after an update.<br>The global system has multiple identical application stacks deployed in different AWS Regions. If a user device travels out of its home geographic region, it will always connect to the geographically closest AWS Region to write or read data. The same data is available in all supported AWS Regions using an Amazon<br>DynamoDB global table.<br>What change should be made to avoid causing disruptions in device operations?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#569",
          "answers": [
            {
              "choice": "<p>A. Update the backend to use strongly consistent reads. Update the devices to always write to and read from their home AWS Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable strong consistency globally on a DynamoDB global table. Update the backend to use strongly consistent reads.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Switch the backend data store to Amazon Aurora MySQL with cross-region replicas. Update the backend to always write to the master endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Select one AWS Region as a master and perform all writes in that AWS Region only. Update the backend to use strongly consistent reads.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154541,
          "date": "Wed 29 Sep 2021 22:31",
          "username": "Nemer",
          "content": "A.  DynamoDB does not support strongly consistent reads ACROSS REGIONS. The stale data comes from writing to one region & reading from another.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 717093,
          "date": "Sun 13 Nov 2022 05:51",
          "username": "et22s",
          "content": "An application can read and write data to any replica table. If your application only uses eventually consistent reads and only issues reads against one AWS Region, it will work without any modification. However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.<br><br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html#V2globaltables_HowItWorks.conflict-resolution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 496728,
          "date": "Wed 08 Dec 2021 11:35",
          "username": "cldy",
          "content": "A.  Update the backend to use strongly consistent reads. Update the devices to always write to and read from their home AWS Region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494771,
          "date": "Mon 06 Dec 2021 01:53",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448526,
          "date": "Wed 03 Nov 2021 08:02",
          "username": "moon2351",
          "content": "Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441828,
          "date": "Sat 30 Oct 2021 21:26",
          "username": "Goram113",
          "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411774,
          "date": "Fri 29 Oct 2021 17:43",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350089,
          "date": "Tue 26 Oct 2021 13:07",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281197,
          "date": "Fri 22 Oct 2021 08:13",
          "username": "Firststack",
          "content": "A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 277559,
          "date": "Tue 19 Oct 2021 14:13",
          "username": "Ebi",
          "content": "A is the correct answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263537,
          "date": "Sat 16 Oct 2021 22:18",
          "username": "SD13",
          "content": "A - However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.<br>Doc link - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253142,
          "date": "Wed 13 Oct 2021 02:58",
          "username": "Bulti",
          "content": "A is the right answer. Dynamo DB doesn't support strong consistency on global tables cross-region. In order for strong consistency to work , the application needs to write and read data from the same region.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 250740,
          "date": "Fri 08 Oct 2021 01:18",
          "username": "Britts",
          "content": "A can't be right unless the the dynamodb global table gets replaced by a regional table first?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244043,
          "date": "Thu 07 Oct 2021 00:15",
          "username": "T14102020",
          "content": "Correct is A.  Read only from Home Region",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231627,
          "date": "Tue 05 Oct 2021 16:48",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 215973,
          "date": "Mon 04 Oct 2021 17:18",
          "username": "smartassX",
          "content": "A.  ——> <br>“ An application can read and write data to any replica table. If your application only uses eventually consistent reads and only issues reads against one AWS Region, it will work without any modification. However, if your application requires strongly consistent reads, it must perform all of its strongly consistent reads and writes in the same Region. DynamoDB does not support strongly consistent reads across Regions. Therefore, if you write to one Region and read from another Region, the read response might include stale data that doesn't reflect the results of recently completed writes in the other Region.<br><br>If applications update the same item in different Regions at about the same time, conflicts can arise. To help ensure eventual consistency, DynamoDB global tables use a last writer wins reconciliation between concurrent updates, in which DynamoDB makes a best effort to determine the last writer. With this conflict resolution mechanism, all the replicas will agree on the latest update and converge toward a state in which they all have identical data. “",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 209602,
          "date": "Mon 04 Oct 2021 14:58",
          "username": "CYL",
          "content": "A.  This combination allows for less restriction and impact to overall performance and allows for consistent read requirements. Tie a particular user back to the home region.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#570",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A software as a service (SaaS) company offers a cloud solution for document management to private law firms and the public sector. A local government client recently mandated that highly confidential documents cannot be stored outside the country. The company CIO asks a Solutions Architect to ensure the application can adapt to this new requirement. The CIO also wants to have a proper backup plan for these documents, as backups are not currently performed.<br>What solution meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#570",
          "answers": [
            {
              "choice": "<p>A. Tag documents that are not highly confidential as regular in Amazon S3. Create individual S3 buckets for each user. Upload objects to each user's bucket. Set S3 bucket replication from these buckets to a central S3 bucket in a different AWS account and AWS Region. Configure an AWS Lambda function triggered by scheduled events in Amazon CloudWatch to delete objects that are tagged as secret in the S3 backup bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Create a cross- region S3 bucket in a separate AWS account. Set proper IAM roles to allow cross-region permissions to the S3 buckets. Configure an AWS Lambda function triggered by Amazon CloudWatch scheduled events to copy objects that are tagged as secret to the S3 backup bucket and objects tagged as normal to the cross-region S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to an S3 bucket in a different AWS Region. Configure an AWS Lambda function that triggers when new S3 objects are created in the main bucket to replicate only documents tagged as secret into the S3 bucket in the same AWS Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Tag highly confidential documents as secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to a different AWS Region. Create an Amazon CloudWatch Events rule for new S3 objects tagged as secret to trigger an AWS Lambda function to replicate them into a separate bucket in the same AWS Region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 277560,
          "date": "Tue 26 Oct 2021 13:07",
          "username": "Ebi",
          "content": "Answer is C",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 712159,
          "date": "Sun 06 Nov 2022 06:12",
          "username": "Blackfry",
          "content": "The difference between C and D is that both regular and confidential documents are tagged or only confidential documents are tagged. But we can use Object tags, if there are any tag.<br>So when we wants to use Selective Cross-Region Replication based on Object Tags about regular documents, we should tag 'regular documents'(or both).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 688969,
          "date": "Sat 08 Oct 2022 03:16",
          "username": "WayneYitomosabc1wassb",
          "content": "I will go with C.  Because option D says that we are moving regular documents into a different region, it makes no sense at allIn fact, to move regular documents is an implicit requirement from the question. Even option C says \\\"......to move regular documents to an S3 bucket in a different AWS Region\\\".@Bulti answer : Answer is C.  D looks like an option except for the fact that the regular objects are not tagged.",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 689804,
          "date": "Sun 09 Oct 2022 03:06",
          "username": "tomosabc1wassb",
          "content": "In fact, to move regular documents is an implicit requirement from the question. Even option C says \\\"......to move regular documents to an S3 bucket in a different AWS Region\\\".@Bulti answer : Answer is C.  D looks like an option except for the fact that the regular objects are not tagged.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 692220,
          "date": "Tue 11 Oct 2022 17:33",
          "username": "wassb",
          "content": "@Bulti answer : Answer is C.  D looks like an option except for the fact that the regular objects are not tagged.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 646526,
          "date": "Sun 14 Aug 2022 04:17",
          "username": "Harithareddynn",
          "content": "The only difference between C and D is S3 events/Cloud watch events - In case of C,S3 events cannot be triggered selectively based on tag, so it would call Lambda for all documents - hence D is better.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 503662,
          "date": "Fri 17 Dec 2021 12:57",
          "username": "ciki",
          "content": "I think answer is D cloudwatch verify that the application can adapt to this new demand",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 503094,
          "date": "Thu 16 Dec 2021 18:37",
          "username": "vbal",
          "content": "https://aws.amazon.com/blogs/mt/monitor-tag-changes-on-aws-resources-with-serverless-workflows-and-amazon-cloudwatch-events/<br>Even after reading above page I am still not sure if it would work or not. But If I can trigger even based upon Each Object's Tag being Put into S3, I would prefer D just because this is more efficient as Lambda would be triggered only for Secret documents and not for ALL the PUT Object Events which is in-efficient IMO.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 495685,
          "date": "Tue 07 Dec 2021 07:45",
          "username": "cldy",
          "content": "C.  Tag documents as either regular or secret in Amazon S3. Create an individual S3 backup bucket in the same AWS account and AWS Region. Use S3 selective cross-region replication based on object tags to move regular documents to an S3 bucket in a different AWS Region. Configure an AWS Lambda function that triggers when new S3 objects are created in the main bucket to replicate only documents tagged as secret into the S3 bucket in the same AWS Region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494777,
          "date": "Mon 06 Dec 2021 01:59",
          "username": "AzureDP900",
          "content": "Use S3 selective cross-region replication, Answer is C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486043,
          "date": "Wed 24 Nov 2021 15:51",
          "username": "pcops",
          "content": "I will go for C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 482993,
          "date": "Sun 21 Nov 2021 04:21",
          "username": "acloudguru",
          "content": "Answer = C. <br>https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 411775,
          "date": "Fri 05 Nov 2021 04:06",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 353855,
          "date": "Sat 30 Oct 2021 23:36",
          "username": "aws_arn_namewahlbergusa",
          "content": "I think answer is D.  With C only new confidential object will be backup, what about old confidentialobject. Addition , S3 event has lag , although rarely but still can cause lost data, CloudWatch is more reliableWrong. Althought I got confused on the same point as well. The trick is for CloudWatch Event Rule you first need to enable CloudTrail Data Events = > https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html<br><br>Hence C is correct.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 496189,
          "date": "Tue 07 Dec 2021 17:47",
          "username": "wahlbergusa",
          "content": "Wrong. Althought I got confused on the same point as well. The trick is for CloudWatch Event Rule you first need to enable CloudTrail Data Events = > https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source-console.html<br><br>Hence C is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350093,
          "date": "Fri 29 Oct 2021 01:32",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 348484,
          "date": "Thu 28 Oct 2021 01:07",
          "username": "blackgamer",
          "content": "The answer is C.  https://aws.amazon.com/about-aws/whats-new/2018/09/amazon-s3-announces-selective-crr-based-on-object-tags/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263223,
          "date": "Sun 24 Oct 2021 05:15",
          "username": "01037",
          "content": "C. <br>Region is treated as a country, though there are several Regions in US.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253151,
          "date": "Wed 13 Oct 2021 23:28",
          "username": "BultiLiamNg",
          "content": "Answer is C.  D looks like an option except for the fact that the regular objects are not tagged. Only highly confidential objects are tagged. Otherwise it's possible to setup a CloudWatch Event rule on an S3 object load event and specify in the action to invoke Lambda function to copy the secret files into a backup S3 bucket.Thank you for pointing this out",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 587469,
          "date": "Mon 18 Apr 2022 06:43",
          "username": "LiamNg",
          "content": "Thank you for pointing this out",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 250739,
          "date": "Sun 10 Oct 2021 13:33",
          "username": "Britts",
          "content": "Not sure why lambda is needed here as we can do cross zone replication in AWS in the same account. Anyway will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#571",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that runs on a fleet of Amazon EC2 instances and stores 70 GB of device data for each instance in Amazon S3. Recently, some of the S3 uploads have been failing. At the same time, the company is seeing an unexpected increase in storage data costs. The application code cannot be modified.<br>What is the MOST efficient way to upload the device data to Amazon S3 while managing storage costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#571",
          "answers": [
            {
              "choice": "<p>A. Upload device data using a multipart upload. Use the AWS CLI to list incomplete parts to address the failed S3 uploads. Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Upload device data using S3 Transfer Acceleration. Use the AWS Management Console to address the failed S3 uploads. Use the Multi-Object Delete operation nightly to delete the old uploads.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Upload device data using a multipart upload. Use the AWS Management Console to list incomplete parts to address the failed S3 uploads. Configure a lifecycle policy to archive continuously to Amazon S3 Glacier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload device data using S3 Transfer Acceleration. Use the AWS Management Console to list incomplete parts to address the failed S3 uploads. Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 275185,
          "date": "Wed 20 Oct 2021 10:14",
          "username": "EbiExtHokirrim",
          "content": "Answer is ASupporting EbiIt should be A, because the most critical problem is that the console cannot display the information that your multipart upload failed. This can only be viewed through the SDK/API.<br>And the title said that there are many unexpected data costs, which should refer to the storage fee caused by the failure of multipart upload (because if you don’t use multipart upload, the entire file upload will fail if it fails, and there is no such part of the cost). It can be concluded that the original program has already written code for multipart upload. No additional code changes are required.Technically, you can view failed multi-part uploads in the console using AWS Storage Lens:<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens-optimize-storage.html#locate-incomplete-mpu<br>https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br><br>I still think A is the best answer, though!",
          "upvote_count": "11101",
          "selected_answers": ""
        },
        {
          "id": 326243,
          "date": "Sat 23 Oct 2021 09:15",
          "username": "ExtHokirrim",
          "content": "Supporting EbiIt should be A, because the most critical problem is that the console cannot display the information that your multipart upload failed. This can only be viewed through the SDK/API.<br>And the title said that there are many unexpected data costs, which should refer to the storage fee caused by the failure of multipart upload (because if you don’t use multipart upload, the entire file upload will fail if it fails, and there is no such part of the cost). It can be concluded that the original program has already written code for multipart upload. No additional code changes are required.Technically, you can view failed multi-part uploads in the console using AWS Storage Lens:<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens-optimize-storage.html#locate-incomplete-mpu<br>https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br><br>I still think A is the best answer, though!",
          "upvote_count": "101",
          "selected_answers": ""
        },
        {
          "id": 461283,
          "date": "Sat 06 Nov 2021 15:25",
          "username": "kirrim",
          "content": "Technically, you can view failed multi-part uploads in the console using AWS Storage Lens:<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-lens-optimize-storage.html#locate-incomplete-mpu<br>https://aws.amazon.com/blogs/aws-cloud-financial-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br><br>I still think A is the best answer, though!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253158,
          "date": "Fri 15 Oct 2021 14:57",
          "username": "BultiBulti0103701037",
          "content": "Between A and D, I will go with D only because A will require a code change. It is assumed that the application currently does not use multi-part upload. Using S3 Transfer acceleration does not require code change. Identifying multi-part object failures is possible using both CLI and console so I will go with D. On reviewing the Option D again, I realized that it is assuming we are using multipart upload with S3 TA.  This will also require a code change. The only option then which will not require a code change is option B.  So my final answer is Option B. Yes, only B doesn't need code change.But how to find out a failed upload?<br>Isn't the upload is a 0 or 1 operation if it isn't multi part upload?",
          "upvote_count": "6211",
          "selected_answers": ""
        },
        {
          "id": 258186,
          "date": "Sun 17 Oct 2021 06:08",
          "username": "Bulti0103701037",
          "content": "On reviewing the Option D again, I realized that it is assuming we are using multipart upload with S3 TA.  This will also require a code change. The only option then which will not require a code change is option B.  So my final answer is Option B. Yes, only B doesn't need code change.But how to find out a failed upload?<br>Isn't the upload is a 0 or 1 operation if it isn't multi part upload?",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 263233,
          "date": "Mon 18 Oct 2021 18:47",
          "username": "01037",
          "content": "Yes, only B doesn't need code change.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 263236,
          "date": "Wed 20 Oct 2021 08:48",
          "username": "01037",
          "content": "But how to find out a failed upload?<br>Isn't the upload is a 0 or 1 operation if it isn't multi part upload?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713065,
          "date": "Mon 07 Nov 2022 14:35",
          "username": "sou123454",
          "content": "Answer is DDDD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 624368,
          "date": "Wed 29 Jun 2022 04:00",
          "username": "TechX",
          "content": "Agree with A, best solution here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 622813,
          "date": "Sun 26 Jun 2022 22:48",
          "username": "kangtamo",
          "content": "Agree with A: AWS CLI.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 520373,
          "date": "Sun 09 Jan 2022 18:32",
          "username": "CloudChef",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494781,
          "date": "Mon 06 Dec 2021 02:04",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486046,
          "date": "Wed 24 Nov 2021 15:56",
          "username": "pcops",
          "content": "I will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 478392,
          "date": "Mon 15 Nov 2021 01:05",
          "username": "sashenkasashenka",
          "content": "DDD - main reason it is NOT A is because \\\"Modifications to the application's code are not permitted.\\\" and taking advantage of S3 multipart uploads REQUIRES modification to your code. SDK/API is provided and the S3 multipart upload function is different than the PUT of the S3 upload. Take a look here:<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpu-upload-object.html<br><br>Additionally,TA is best practice for transferring large files to S3 buckets. As data arrives at the closest edge location, the data is routed to Amazon S3 over an optimized network path. This will insure more device uploads will not end up in a failed state.AAA - CORRECTION. It appears that if using the AWS SDK/CLI by DEFAULT when uploading a >5Mb file to an AWS S3 bucket multipart upload will be used. That and I missed that listing of failed multipart upload objects CAN'T be viewed in the Management Console.",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 478395,
          "date": "Mon 15 Nov 2021 01:15",
          "username": "sashenka",
          "content": "AAA - CORRECTION. It appears that if using the AWS SDK/CLI by DEFAULT when uploading a >5Mb file to an AWS S3 bucket multipart upload will be used. That and I missed that listing of failed multipart upload objects CAN'T be viewed in the Management Console.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 448531,
          "date": "Sat 06 Nov 2021 06:05",
          "username": "moon2351",
          "content": "I think Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 428762,
          "date": "Thu 04 Nov 2021 21:38",
          "username": "DerekKey",
          "content": "\\\"Use the AWS Management Console to list incomplete parts to address the failed S3 uploads\\\" - not possible with Management Console<br>C & D - wrong<br>\\\"Upload device data using S3 Transfer Acceleration\\\" - can be used to move data between Regions. Not in this case<br>B & D - wrong<br>\\\"Use the AWS Management Console to address the failed S3 uploads.\\\" - there is no functionality<br>B - wrong<br>\\\"Use the AWS CLI to list incomplete parts to address the failed S3 uploads\\\" - correct<br>\\\"Enable the lifecycle policy for the incomplete multipart uploads on the S3 bucket to delete the old uploads and prevent new failed uploads from accumulating.\\\" - correct<br>A - correct. I assume that they will not change the application and use CLI to upload files",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 411796,
          "date": "Wed 03 Nov 2021 12:12",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 402439,
          "date": "Wed 03 Nov 2021 06:03",
          "username": "Desailly",
          "content": "Well described here https://aws.amazon.com/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 397754,
          "date": "Tue 02 Nov 2021 23:52",
          "username": "SPRao",
          "content": "If some of uploads are failing and cost is getting increased means upload is already multipart<br>hence only ask is to how to reduce the cost and that can be done by deleting failed uploads<br>from S3. Hence A makes sense.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 362542,
          "date": "Thu 28 Oct 2021 14:54",
          "username": "oscargeeRich_Richbharadhwaj",
          "content": "Note: You aren’t able to view the parts of your incomplete multipart upload in the AWS Management Console.<br>https://aws.amazon.com/cn/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br><br>So A is the only one.Lifecycle policies for failed uploads discussed in this blog: https://aws.amazon.com/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br>(A)s3 storage lens is through the console. The real thing is deletion of the file and not a move to glacier. hence A is correct",
          "upvote_count": "212",
          "selected_answers": ""
        },
        {
          "id": 377738,
          "date": "Fri 29 Oct 2021 21:58",
          "username": "Rich_Rich",
          "content": "Lifecycle policies for failed uploads discussed in this blog: https://aws.amazon.com/blogs/aws-cost-management/discovering-and-deleting-incomplete-multipart-uploads-to-lower-amazon-s3-costs/<br>(A)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 380966,
          "date": "Mon 01 Nov 2021 16:16",
          "username": "bharadhwaj",
          "content": "s3 storage lens is through the console. The real thing is deletion of the file and not a move to glacier. hence A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350103,
          "date": "Mon 25 Oct 2021 07:18",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 333936,
          "date": "Sun 24 Oct 2021 10:44",
          "username": "sarah_t",
          "content": "S3 Transfer Acceleration is used for data transfer from remote clients by routing them through AWS edge locations. How would that help when the data is already uploaded from within an AWS region?",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#572",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB.  The<br>Developers account resides in a dedicated organizational unit (OU). The Solutions Architect has implemented the following SCP on the Developers account:<br><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0037900001.png\" class=\"in-exam-image\"><br>When this policy is deployed, IAM users in the Developers account are still able to use AWS services that are not listed in the policy.<br>What should the Solutions Architect do to eliminate the Developers' ability to use services outside the scope of this policy?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#572",
          "answers": [
            {
              "choice": "<p>A. Create an explicit deny statement for each AWS service that should be constrained.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the FullAWSAccess SCP from the Developer account's OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the FullAWSAccess SCP to explicitly deny all services.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add an explicit deny statement using a wildcard to the end of the SCP.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 350115,
          "date": "Tue 19 Oct 2021 23:19",
          "username": "Waiweng",
          "content": "B is correct",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 415682,
          "date": "Sat 23 Oct 2021 05:47",
          "username": "student2020joe16tekkart",
          "content": "Answer is A - You cannot remove the FullAWSAccess SCP that is inherited from root. Test it and see.Yes, you can.(Ans - B)<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist<br>\\\"To use SCPs as an allow list, you must replace the AWS managed FullAWSAccess SCP with an SCP that explicitly permits only those services and actions that you want to allow. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit.\\\"Answer is A, because as soon as an SCP was created, the FullAWSAccess SCP was already overruled (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist) and (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist), because Explicit Deny > Explicit Allow > Implicit Deny > Implicit Allow, the only way to overcome Explicit Allow is to add Explicit Deny statements.Answers C and D would work too good, of course everything would be blocked !",
          "upvote_count": "884",
          "selected_answers": ""
        },
        {
          "id": 456073,
          "date": "Sun 07 Nov 2021 02:02",
          "username": "joe16",
          "content": "Yes, you can.(Ans - B)<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist<br>\\\"To use SCPs as an allow list, you must replace the AWS managed FullAWSAccess SCP with an SCP that explicitly permits only those services and actions that you want to allow. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit.\\\"",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 423623,
          "date": "Fri 05 Nov 2021 05:32",
          "username": "tekkart",
          "content": "Answer is A, because as soon as an SCP was created, the FullAWSAccess SCP was already overruled (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist) and (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist), because Explicit Deny > Explicit Allow > Implicit Deny > Implicit Allow, the only way to overcome Explicit Allow is to add Explicit Deny statements.Answers C and D would work too good, of course everything would be blocked !",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 713303,
          "date": "Mon 07 Nov 2022 21:18",
          "username": "mrgreatness",
          "content": "100% B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 692252,
          "date": "Tue 11 Oct 2022 18:23",
          "username": "wassb",
          "content": "This question doesnt make sense AT ALL. <br>{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": [<br>\\\"ec2:*\\\",<br>\\\"cloudwatch:*\\\"<br>],<br>\\\"Resource\\\": \\\"*\\\"<br>}<br>]<br>}<br>An allow list policy might look like the following example, which enables account users to perform operations for Amazon Elastic Compute Cloud (Amazon EC2) and Amazon CloudWatch, ****but no other service****. <br> + The FullAWSAccess SCP doesnt need to be deleted, the fact defining a new SCP is enough..<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626224,
          "date": "Sat 02 Jul 2022 18:11",
          "username": "aandc",
          "content": "B<br>To support this, AWS Organizations attaches an AWS managed SCP named FullAWSAccess to every root and OU when it's created. This policy allows all services and actions. It's always available for you to attach or detach from the entities in your organization as needed. Because the policy is an AWS managed SCP, you can't modify or delete it.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 527484,
          "date": "Wed 19 Jan 2022 13:34",
          "username": "tkanmani76",
          "content": "B is correct - An allow list strategy has you remove the FullAWSAccess SCP that is attached by default to every OU and account. This means that no APIs are permitted anywhere unless you explicitly allow them.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494784,
          "date": "Mon 06 Dec 2021 02:08",
          "username": "AzureDP900",
          "content": "it should be B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486053,
          "date": "Wed 24 Nov 2021 16:03",
          "username": "pcops",
          "content": "Ans is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441794,
          "date": "Fri 05 Nov 2021 09:44",
          "username": "student22",
          "content": "B<br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 329315,
          "date": "Sun 10 Oct 2021 04:13",
          "username": "RedKane",
          "content": "Ignore the messages below - it looks like access has to be granted at each level : root, any intermediate OUs and ACCOUNT so removing FullAWSAccess SCP from any of the nodes in the hierarchy will do the job.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 329309,
          "date": "Sat 09 Oct 2021 12:37",
          "username": "RedKane",
          "content": "To add to the previous post - each higher OU higher in the hierarchy, including organization root will also have FullAWSAccess SCP attached and each of those SCPs will be inherited by each account below in the hierarchy. So each account inherits multiple copies of FullAWSAccess SCP. In order to get rid of it one would need to remove FullAWSAccess SCP from every OU (higher in the hierarchy) and the root as well as the ACCOUNT itself.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 329304,
          "date": "Wed 06 Oct 2021 07:06",
          "username": "RedKane",
          "content": "FullAWSAccess SCP is attached automatically by default not only to each OU but also to each account individually so removing FullAWSAccess SCP from Developers-OU will change nothing as the one attached directly to the Developers-ACCOUNT will still remain. That would only leave option A as valid although I'm not sure if the author of this question considered the behavior I described. Also in real scenarios one would rather attach SCP with DENY's and leave FullAWSAccess SCP untouched.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 321754,
          "date": "Sat 02 Oct 2021 15:41",
          "username": "alisyech",
          "content": "it should B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 314992,
          "date": "Sun 26 Sep 2021 15:30",
          "username": "didek1986",
          "content": "answ B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 314378,
          "date": "Fri 24 Sep 2021 02:50",
          "username": "beber3564",
          "content": "B.  allow list <br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_denylist",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 311938,
          "date": "Thu 23 Sep 2021 18:54",
          "username": "M_Asep",
          "content": "I Support D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 309805,
          "date": "Thu 23 Sep 2021 10:19",
          "username": "nitinzheyheyheinitinz",
          "content": "D is correct answer.D cannot be the answer. It will also override the allowed statements.<br><br>A request results in an explicit deny if an applicable policy includes a Deny statement. If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.<br>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.htmlYou are absolutely correct. Thanks for the link I change my answer to B. ",
          "upvote_count": "144",
          "selected_answers": ""
        },
        {
          "id": 316698,
          "date": "Mon 27 Sep 2021 03:58",
          "username": "heyheyheinitinz",
          "content": "D cannot be the answer. It will also override the allowed statements.<br><br>A request results in an explicit deny if an applicable policy includes a Deny statement. If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.<br>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.htmlYou are absolutely correct. Thanks for the link I change my answer to B. ",
          "upvote_count": "44",
          "selected_answers": ""
        },
        {
          "id": 318406,
          "date": "Tue 28 Sep 2021 11:11",
          "username": "nitinz",
          "content": "You are absolutely correct. Thanks for the link I change my answer to B. ",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#573",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company developed a Java application and deployed it to an Apache Tomcat server that runs on Amazon EC2 instances. The company's Engineering team has implemented AWS CloudFormation and Chef Automate to automate the provisioning of and updates to the infrastructure and configuration of the application in the development, test, and production environments. These implementations have led to significantly improves reliability in releasing changes. The Engineering team reports there are frequent service disruptions due to unexpected errors when updating the application of the Apache Tomcat server.<br>Which solution will increase the reliability of all releases?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#573",
          "answers": [
            {
              "choice": "<p>A. Implement a blue/green deployment methodology.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Implement the canary release methodology.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon CloudFront to serve all requests from the cache while deploying the updates.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement the all at once deployment methodology.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 282976,
          "date": "Mon 18 Oct 2021 05:01",
          "username": "Trap_D0_rsashsz",
          "content": "B<br>Stunning how few people can read. Opsworks isn't even mentioned anywhere in the question. This question has nothing to do with Opsworks. It's cloudformation + Chef Automate (which indicates a Chef Serve, NOT a full Opsworks stack). Chef isn't an aws tools and doesn't require Opsworks to work, and it supports Blue/Green, Phoenix, and Canary deployments (https://blog.chef.io/watch-chef-aws-your-path-to-devops). The answer is B. Stunning how you are judging the other people and you yourself didn't provide any reason behind your answer.",
          "upvote_count": "265",
          "selected_answers": ""
        },
        {
          "id": 580337,
          "date": "Sun 03 Apr 2022 16:51",
          "username": "sashsz",
          "content": "Stunning how you are judging the other people and you yourself didn't provide any reason behind your answer.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 156199,
          "date": "Tue 21 Sep 2021 18:06",
          "username": "NemerGmail78Alexey79",
          "content": "A.  Blue/green deployment with Opsworks for Chef Automate, using separate stacks for each environment.https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.htmlhttps://aws.amazon.com/opsworks/chefautomate/",
          "upvote_count": "1221",
          "selected_answers": ""
        },
        {
          "id": 200168,
          "date": "Sat 25 Sep 2021 20:33",
          "username": "Gmail78",
          "content": "https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 563836,
          "date": "Wed 09 Mar 2022 08:50",
          "username": "Alexey79",
          "content": "https://aws.amazon.com/opsworks/chefautomate/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 643746,
          "date": "Sun 07 Aug 2022 16:07",
          "username": "nexus2020",
          "content": "Blue/green will ensure the upgrade is tested before launch. - better reliability = A is right<br>Canary will track the change, but without doing anything else will not make it more reliable. = b is not the better answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 613563,
          "date": "Thu 09 Jun 2022 03:09",
          "username": "AnhddAnhddAnhdd",
          "content": "B goodIt's say that with the blue/green deployment, the cons is: \\\"Cost is a drawback to blue-green deployments. Replicating a production environment can be complex and expensive, especially when working with microservices. Quality assurance and user acceptance testing may not identify all of the anomalies or regressions either, and so shifting all user traffic at once can present risks. An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made.\\\"Althought that the question not require to low cost, I will always prefer the solution that take lower cost. Plus that, with the blue/green an outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made.<br>So C run out -> B for me",
          "upvote_count": "111",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 613565,
          "date": "Thu 09 Jun 2022 03:10",
          "username": "Anhdd",
          "content": "It's say that with the blue/green deployment, the cons is: \\\"Cost is a drawback to blue-green deployments. Replicating a production environment can be complex and expensive, especially when working with microservices. Quality assurance and user acceptance testing may not identify all of the anomalies or regressions either, and so shifting all user traffic at once can present risks. An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 613566,
          "date": "Thu 09 Jun 2022 03:10",
          "username": "Anhdd",
          "content": "Althought that the question not require to low cost, I will always prefer the solution that take lower cost. Plus that, with the blue/green an outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made.<br>So C run out -> B for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 605045,
          "date": "Sun 22 May 2022 01:08",
          "username": "user0001",
          "content": "it is A in this case, it would be B if the question is about cost",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 552840,
          "date": "Mon 21 Feb 2022 13:23",
          "username": "futen0326",
          "content": "Don't let the top comment fool you.. the answer is A.  OpsWorks has 3 modes: Puppet Enterprise, Chef Automate, and OpsWorks..<br><br>For the exam you will default for OpsWorks if you see those keywords.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 531376,
          "date": "Mon 24 Jan 2022 15:44",
          "username": "AMKazi",
          "content": "A is the answer.<br>B: is not suitable in this scenario as they are facing issues with upgrading Apache TOMCAT server. You cannot do Canary deployments for server infrastructure. Canary is best suited for toggle features/releases that can we toggled on /off. Since this is TOMCAT, you cannot opt in or opt out.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 529646,
          "date": "Sat 22 Jan 2022 07:18",
          "username": "GeniusMikeLiu",
          "content": "after read so many comment, I still confused. what the main diffenrent between Blue/green and canary deloyment?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 526366,
          "date": "Tue 18 Jan 2022 08:13",
          "username": "cannottellname",
          "content": "A is reliable. nothing is mentioned about costs.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 515368,
          "date": "Mon 03 Jan 2022 03:56",
          "username": "GeniusMikeLiu",
          "content": "quesion is care about 'reliable', so A is best then B.  Blue/Green deployment can roll back to old version if something goes wrong.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 513403,
          "date": "Thu 30 Dec 2021 14:29",
          "username": "cldy",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 500998,
          "date": "Tue 14 Dec 2021 03:20",
          "username": "JuksAnhdd",
          "content": "Blue/Green deployment is more reliable as it will never cause an outage. Using Canary you are still causing outage for a set of users.Blue/green deployment: An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made",
          "upvote_count": "51",
          "selected_answers": ""
        },
        {
          "id": 613567,
          "date": "Thu 09 Jun 2022 03:12",
          "username": "Anhdd",
          "content": "Blue/green deployment: An outage or issue could also have a wide-scale business impact before a rollback is triggered, and depending on the implementation, in-flight user transactions may be lost when the shift in traffic is made",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496448,
          "date": "Wed 08 Dec 2021 02:35",
          "username": "tkanmani76",
          "content": "A - The answer is Blue/Green. The question requires a \\\"Reliable\\\" soluton - With Canary you would still be routing to a small subset of user base who would be impacted if there is an issue with upgrade. With Blue/Green you would test in one environment and once it works fine you could swing over - that way there will be no customer impact or production issue.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 492679,
          "date": "Thu 02 Dec 2021 17:36",
          "username": "sappers",
          "content": "Its B Canary - yep stunning that so many head-in-cloud Architects dont understand DevOps - Think if YOU were responsible for \\\"service outages occur(ing) OFTEN as a result of unanticipated issues\\\" e.g. a known intermittently flawed App - would you really do Blue/Green (no mention in Q of testing) ? then just swap over ? Good luck w that :@)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 492595,
          "date": "Thu 02 Dec 2021 15:50",
          "username": "sappers",
          "content": "Its B Canary - yep stunning that so many head-in-cloud Architects dont understand DevOps - Think if YOU were responsible for \\\"service outages occur(ing) OFTEN as a result of unanticipated issues\\\" e.g. a known intermittently flawed App - would you really do Blue/Green (no mention in Q of testing) ? then just swap over ? Good lcuck w that :@)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 430136,
          "date": "Tue 02 Nov 2021 22:11",
          "username": "denccc",
          "content": "Issue: \\\"The Engineering team reports there are frequent service disruptions due to unexpected errors when updating the application of the Apache Tomcat server.\\\" This can be prevented by using B/G deployments. Only when everything is fine a switch will happen. So will go with A. ",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 427944,
          "date": "Tue 02 Nov 2021 11:59",
          "username": "Cotterjoe16joe16",
          "content": "A or B I am very confuse!A and B are similar but B more suited in this scenario. Please read this - <br>https://martinfowler.com/bliki/CanaryRelease.htmland this -<br>https://circleci.com/blog/canary-vs-blue-green-downtime/",
          "upvote_count": "311",
          "selected_answers": ""
        },
        {
          "id": 456086,
          "date": "Wed 03 Nov 2021 06:18",
          "username": "joe16joe16",
          "content": "A and B are similar but B more suited in this scenario. Please read this - <br>https://martinfowler.com/bliki/CanaryRelease.htmland this -<br>https://circleci.com/blog/canary-vs-blue-green-downtime/",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 456088,
          "date": "Sat 06 Nov 2021 21:31",
          "username": "joe16",
          "content": "and this -<br>https://circleci.com/blog/canary-vs-blue-green-downtime/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#574",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>During a security audit of a Service team's application, a Solutions Architect discovers that a username and password for an Amazon RDS database and a set of<br>AWS IAM user credentials can be viewed in the AWS Lambda function code. The Lambda function uses the username and password to run queries on the database, and it uses the IAM credentials to call AWS services in a separate management account.<br>The Solutions Architect is concerned that the credentials could grant inappropriate access to anyone who can view the Lambda code. The management account and the Service team's account are in separate AWS Organizations organizational units (OUs).<br>Which combination of changes should the Solutions Architect make to improve the solution's security? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#574",
          "answers": [
            {
              "choice": "<p>A. Configure Lambda to assume a role in the management account with appropriate access to AWS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Lambda to use the stored database credentials in AWS Secrets Manager and enable automatic rotation.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Lambda function to rotate the credentials every hour by deploying a new Lambda version with the updated credentials.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an SCP on the management account's OU to prevent IAM users from accessing resources in the Service team's account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Enable AWS Shield Advanced on the management account to shield sensitive resources from unauthorized IAM access.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 168699,
          "date": "Thu 23 Sep 2021 21:59",
          "username": "LunchTime",
          "content": "A & B are correct.<br>Concenus on B being correct.<br>Regarding A verse D: SCP is too restrictive. As mentioned by khksoma, the issue is only around the Lambda function. D also does not provide a way to support the Lambda calling AWS services in the separate account. As such, D is not correct. Option \\\"A\\\" addresses this and is supported by the link given by balisongjam.",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 275809,
          "date": "Fri 22 Oct 2021 22:10",
          "username": "Ebi",
          "content": "Answer is AB",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 598028,
          "date": "Sat 07 May 2022 10:18",
          "username": "tartarus23",
          "content": "A.  Seems a better option than using AWS organizations to address the requirements<br>B.  AWS Secrets Manager enables lifecycle management, key rotation and securely storing the database credentials.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 537609,
          "date": "Tue 01 Feb 2022 04:02",
          "username": "HellGate",
          "content": "My answer is B and D. <br><br>in the question, mentioned as “The Solutions Architect is afraid that the credentials might be misused by anybody who can examine the Lambda code”, so proper access control is needed here.We need D for this.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 523931,
          "date": "Sat 15 Jan 2022 05:50",
          "username": "CloudChef",
          "content": "Seems AWS has people who put a bunch of wrong answers at about the same time. Careful what you believe.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494786,
          "date": "Mon 06 Dec 2021 02:13",
          "username": "AzureDP900",
          "content": "A, B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 455499,
          "date": "Fri 05 Nov 2021 09:00",
          "username": "tonikus",
          "content": "Q: Answers here.. marked as \\\"Correct\\\" with randomizer?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411861,
          "date": "Thu 04 Nov 2021 18:00",
          "username": "WhyIronMan",
          "content": "I'll go with A,B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 371457,
          "date": "Sat 30 Oct 2021 09:14",
          "username": "ss160700",
          "content": "A&B - D will prevent Lambda to function correctly",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 360339,
          "date": "Fri 29 Oct 2021 12:43",
          "username": "pradhyumnapradhyumna",
          "content": "B and D is correct. The question says, \\\"which combination\\\", obviously both AB are solving the same lambda problem, hence not a good \\\"combination\\\". On top of it, it does not help lambda assuming a role in mgmt account while the application is completely running in service account with lambda and RDS. Second part of the problem is how to prevent users from using the IAM credentials which can be viewed in the code. This is what SCP is addressing, anyways SCP doesn't affect the IAM users in the mgmt account and so this SCP would prevent IAM users from the service account . I would go with B and D \\\"combination\\\".Changing to A & B rds credentials in secrets manager, use roles to eliminate mgmt creds",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 398742,
          "date": "Thu 04 Nov 2021 05:01",
          "username": "pradhyumna",
          "content": "Changing to A & B rds credentials in secrets manager, use roles to eliminate mgmt creds",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 350120,
          "date": "Tue 26 Oct 2021 11:59",
          "username": "Waiweng",
          "content": "it's A&B",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 292516,
          "date": "Sat 23 Oct 2021 15:25",
          "username": "Kian1",
          "content": "going with AB",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 281210,
          "date": "Sat 23 Oct 2021 05:57",
          "username": "Firststack",
          "content": "A & B is the most secure approach",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 268998,
          "date": "Fri 22 Oct 2021 17:25",
          "username": "Justu",
          "content": "AB, You need to fix lambda getting credentials directly from the code and allow it to use mgmt account resources. <br><br>D: There's no need to restrict ServiceAccount resources by SCP. <br><br>Kanavpeer is right.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 254553,
          "date": "Thu 21 Oct 2021 11:00",
          "username": "Cantaloupe",
          "content": "A/B<br>Assuming the role is the right way to do it. And SSM is good for storing DB credentials<br>https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/<br><br>D is wrong as users from one account cannot access resources from another account if not allowed through cross-account access using assumed roles. There's no need to use SCP for deny<br>E is wrong as shield is used for ddos protection<br>C does not make sense with hourly redeploying of lambda",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253665,
          "date": "Sun 17 Oct 2021 19:31",
          "username": "petebear55",
          "content": "BEST PRACTICE WOULD BE B AND D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 253440,
          "date": "Sat 16 Oct 2021 18:13",
          "username": "Bulti",
          "content": "A & B is the right answer. SCP will prevent an IAM user credentials to access the services which will cause the Lambda function to fail. We don't want the Lambda function to fail wile calling AWS services. Option A provides an elegant and standard solution to allow Lambda in one account to access AWS services in another account by assuming the IAM role that provides it access to call those AWS services.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#575",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is having issues with a newly deployed serverless infrastructure that uses Amazon API Gateway, Amazon Lambda, and Amazon DynamoDB. <br>In a steady state, the application performs as expected. However, during peak load, tens of thousands of simultaneous invocations are needed and user requests fail multiple times before succeeding. The company has checked the logs for each component, focusing specifically on Amazon CloudWatch Logs for Lambda.<br>There are no errors logged by the services or applications.<br>What might cause this problem?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#575",
          "answers": [
            {
              "choice": "<p>A. Lambda has very low memory assigned, which causes the function to fail at peak load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Lambda is in a subnet that uses a NAT gateway to reach out of the internet, and the function instance does not have sufficient Amazon EC2 resources in the VPC to scale with the load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The throttle limit set on API Gateway is very low. During peak load, the additional requests are not making their way through to Lambda.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. DynamoDB is set up in an auto scaling mode. During peak load, DynamoDB adjusts capacity and throughput behind the scenes, which is causing the temporary downtime. Once the scaling completes, the retries go through successfully.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 191938,
          "date": "Wed 29 Sep 2021 21:16",
          "username": "proxyolism",
          "content": "the answer is C.  question says<br><br>The company has checked the logs for each component, focusing specifically on Amazon CloudWatch Logs for Lambda.<br><br>and it means there is no error log from lambda. the company actuallydid not check API gateway's cloudwatch log. if lambda fails, the company could check it is the lambda problem with cloudwatch logs. furthermore, A is completely wrong because lambda runs pararell with concurrency. and question says this problem only occurs when during maximum loads. if lambda memory is the cause of problem, it can be failed whenever under maximum loads or not.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 154674,
          "date": "Mon 20 Sep 2021 09:06",
          "username": "Nemersam422student22",
          "content": "Fine with C.  Nothing wrong with Lambda. Increase API gateway throttle limits.There are no errors logged in from services or application, why we think gateway throttle errors, it will log if an issue. I go with AC<br>No errors from Lambda because requests were throttled at API Gateway.",
          "upvote_count": "922",
          "selected_answers": ""
        },
        {
          "id": 184096,
          "date": "Tue 21 Sep 2021 18:53",
          "username": "sam422student22",
          "content": "There are no errors logged in from services or application, why we think gateway throttle errors, it will log if an issue. I go with AC<br>No errors from Lambda because requests were throttled at API Gateway.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 436820,
          "date": "Wed 03 Nov 2021 15:44",
          "username": "student22",
          "content": "C<br>No errors from Lambda because requests were throttled at API Gateway.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 640144,
          "date": "Sun 31 Jul 2022 16:29",
          "username": "Ni_yot",
          "content": "C makes perfect sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532239,
          "date": "Tue 25 Jan 2022 16:36",
          "username": "AMKazi",
          "content": "Ans is C: <br>cannot be D: https://aws.amazon.com/about-aws/whats-new/2017/06/announcing-amazon-dynamodb-auto-scaling/#:~:text=Starting%20today%2C%20when%20you%20create,request%20volumes%2C%20with%20zero%20downtime.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513947,
          "date": "Fri 31 Dec 2021 09:28",
          "username": "cldy",
          "content": "C is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494787,
          "date": "Mon 06 Dec 2021 02:15",
          "username": "AzureDP900",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411866,
          "date": "Sat 30 Oct 2021 08:30",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 404542,
          "date": "Fri 29 Oct 2021 20:01",
          "username": "KopaKopa",
          "content": "Im for A, was for D but doesnt make sense as if it was scaling issue on Dynamo it will log errors on lambda.sorry i mean C",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 478683,
          "date": "Mon 15 Nov 2021 14:18",
          "username": "Kopa",
          "content": "sorry i mean C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350125,
          "date": "Thu 28 Oct 2021 06:39",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 295261,
          "date": "Wed 27 Oct 2021 01:30",
          "username": "kiev",
          "content": "It is C.  I thought it was A but reading again the last line that says lambda has cloud watch has been checked and there is no problem with lambda implies the problem isn't with lambda. Now between API gateway and Dynamodb, I think it is clear there is a problem with throttle limit in API gateway that's causing the issue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292518,
          "date": "Sat 23 Oct 2021 13:12",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281211,
          "date": "Sat 23 Oct 2021 00:49",
          "username": "Firststack",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 275814,
          "date": "Fri 22 Oct 2021 12:51",
          "username": "Ebi",
          "content": "Answer is C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 258878,
          "date": "Thu 21 Oct 2021 08:24",
          "username": "kopper2019",
          "content": "C API GW limit",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253449,
          "date": "Tue 19 Oct 2021 20:53",
          "username": "Bulti",
          "content": "Answer is C.  When throttle limits are low on API Gateway, concurrent requests beyond that threshold limit are dropped and they need to be retried. As a result after repeated retries the request succeeds when the concurrent request count drops below the throttle limit.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244062,
          "date": "Tue 19 Oct 2021 11:09",
          "username": "T14102020",
          "content": "Correct is C.  If no errors in logs so need increase API gateway throttle limits",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 239050,
          "date": "Mon 18 Oct 2021 13:12",
          "username": "joos",
          "content": "A right",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#576",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company with hundreds of AWS accounts has a newly established centralized internal process for purchasing new or modifying existing Reserved<br>Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement or execution. Previously, business units would directly purchase or modify Reserved Instances in their own respective AWS accounts autonomously.<br>Which combination of steps should be taken to proactively enforce the new process in the MOST secure way possible? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#576",
          "answers": [
            {
              "choice": "<p>A. Ensure all AWS accounts are part of an AWS Organizations structure operating in all features mode.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In each AWS account, create an IAM policy with a DENY rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an SCP that contains a deny rule to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions. Attach the SCP to each organizational unit (OU) of the AWS Organizations structure.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Ensure that all AWS accounts are part of an AWS Organizations structure operating in consolidated billing features mode.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154673,
          "date": "Tue 21 Sep 2021 13:04",
          "username": "Nemerpetebear55",
          "content": "A&D.  AWS Org operating in all features mode, to be able to use SCP with deny list (blacklist).<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp-strategies.htmlWell done Nemer",
          "upvote_count": "312",
          "selected_answers": ""
        },
        {
          "id": 253678,
          "date": "Sat 23 Oct 2021 05:15",
          "username": "petebear55",
          "content": "Well done Nemer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 577806,
          "date": "Tue 29 Mar 2022 20:07",
          "username": "jj22222",
          "content": "A & D for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 547862,
          "date": "Tue 15 Feb 2022 17:01",
          "username": "pititcu667",
          "content": "I agree with nemer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 494830,
          "date": "Mon 06 Dec 2021 04:07",
          "username": "AzureDP900",
          "content": "A, D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411868,
          "date": "Sun 07 Nov 2021 14:11",
          "username": "WhyIronMan",
          "content": "I'll go with A,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350177,
          "date": "Wed 03 Nov 2021 22:23",
          "username": "Waiweng",
          "content": "it's A&D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 349043,
          "date": "Sun 31 Oct 2021 11:18",
          "username": "blackgamer",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 295271,
          "date": "Sat 30 Oct 2021 00:08",
          "username": "kiev",
          "content": "AD is the right answer. Now to those saying why A and not E, it is because AWS organisations in all feature mode include consolidated billing.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292522,
          "date": "Fri 29 Oct 2021 03:25",
          "username": "Kian1",
          "content": "going with AD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 275819,
          "date": "Thu 28 Oct 2021 17:33",
          "username": "Ebi",
          "content": "A and D are correct answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 269328,
          "date": "Wed 27 Oct 2021 20:36",
          "username": "kopper2019",
          "content": "A and D, Orgs and SCP the way to go",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253451,
          "date": "Wed 20 Oct 2021 13:27",
          "username": "Bulti",
          "content": "A & D is the right answer. Forst put all accounts into OU and the apply SCP to deny access to the EC2 API that procure new reserved instances or modify existing reserved instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251319,
          "date": "Mon 18 Oct 2021 17:52",
          "username": "darthvoodoo",
          "content": "With D in place, I wonder how the procurement team would now be able to purchase reserved instances...I know you can have exceptions in SCPs but c'mon AWS...",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 244065,
          "date": "Sat 16 Oct 2021 12:54",
          "username": "T14102020",
          "content": "Correct is AD.  A and D.  Use AWS Organization together with SCP",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242021,
          "date": "Sat 16 Oct 2021 09:01",
          "username": "rscloud",
          "content": "A and D<br>SCPs are available only in an organization that has all features enabled.<br>An SCP restricts permissions for IAM users and roles in member accounts",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231633,
          "date": "Wed 06 Oct 2021 16:27",
          "username": "jackdryan",
          "content": "I'll go with A,D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229078,
          "date": "Wed 06 Oct 2021 12:29",
          "username": "cloudgc",
          "content": "A - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html<br>D",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#577",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The Solutions<br>Architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.<br>How can the Solutions Architect design the API Gateway access control and perform request inspections?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#577",
          "answers": [
            {
              "choice": "<p>A. For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. For the API Gateway resource, set CORS to enabled and only return the company's domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154706,
          "date": "Mon 20 Sep 2021 08:37",
          "username": "Nemerjoe16",
          "content": "A.  Access control using Role, and request inspection with X-Ray.A<br>https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/",
          "upvote_count": "242",
          "selected_answers": ""
        },
        {
          "id": 456114,
          "date": "Sat 06 Nov 2021 01:48",
          "username": "joe16",
          "content": "A<br>https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 275821,
          "date": "Thu 21 Oct 2021 04:06",
          "username": "Ebi",
          "content": "Answer is A",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 497352,
          "date": "Thu 09 Dec 2021 06:29",
          "username": "cldy",
          "content": "A.  For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494847,
          "date": "Mon 06 Dec 2021 04:42",
          "username": "AzureDP900",
          "content": "A is right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484661,
          "date": "Tue 23 Nov 2021 01:35",
          "username": "acloudguru",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 484660,
          "date": "Tue 23 Nov 2021 01:34",
          "username": "acloudguru",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 411869,
          "date": "Fri 05 Nov 2021 13:46",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350185,
          "date": "Mon 01 Nov 2021 12:00",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 349054,
          "date": "Sun 31 Oct 2021 03:12",
          "username": "blackgamer",
          "content": "A is the answer, XRay is needed here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 333869,
          "date": "Sat 30 Oct 2021 04:39",
          "username": "Pupu86",
          "content": "End-to-end request already hints towards the usage of AWS X-ray. Automatically filtering out option B and D.  Further the authorisation via role rather than parsing secrets through AWS clients - so A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 295275,
          "date": "Sat 30 Oct 2021 01:32",
          "username": "kiev",
          "content": "A is the correct answer. Role +X-ray for better analysis",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292527,
          "date": "Fri 29 Oct 2021 04:43",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253453,
          "date": "Sun 17 Oct 2021 10:48",
          "username": "Bulti",
          "content": "A is correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 244068,
          "date": "Wed 13 Oct 2021 12:08",
          "username": "T14102020",
          "content": "Correct is A.  AWS Signature + X-Ray",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 231635,
          "date": "Mon 11 Oct 2021 10:33",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 215837,
          "date": "Mon 11 Oct 2021 06:06",
          "username": "taoteching1",
          "content": "A is correct - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-using-xray-maps.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 211100,
          "date": "Fri 08 Oct 2021 13:30",
          "username": "liono",
          "content": "A<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#578",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect needs to design a highly available application that will allow authenticated users to stay connected to the application even when there are underlying failures.<br>Which solution will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#578",
          "answers": [
            {
              "choice": "<p>A. Deploy the application on Amazon EC2 instances. Use Amazon Route 53 to forward requests to the EC2 instances. Use Amazon DynamoDB to save the authenticated connection details.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer to handle requests. Use Amazon DynamoDB to save the authenticated connection details.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer on the front end. Use EC2 instances to save the authenticated connection details.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the application on Amazon EC2 instances in an Auto Scaling group. Use an internet-facing Application Load Balancer on the front end. Use EC2 instances hosting a MySQL database to save the authenticated connection details.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 154713,
          "date": "Mon 20 Sep 2021 12:36",
          "username": "Nemeroscargeekirrim",
          "content": "B.  ALB + ASG + DynamoDBmake sense.The requirement said, stay connection after certification. Which means the info should be cached in backend. That's why DB is not necessary. Otherwise every time user submit request will trigger a DB query which is really slow.Could front-end DDB with ElasticCache if you're worried about the DDB queries being too slow and you truly need that level of performance on auth/session data and are willing to pay for it.But still need DDB behind it to populate the cache misses in that scenario.",
          "upvote_count": "2411",
          "selected_answers": ""
        },
        {
          "id": 362922,
          "date": "Wed 03 Nov 2021 20:03",
          "username": "oscargeekirrim",
          "content": "The requirement said, stay connection after certification. Which means the info should be cached in backend. That's why DB is not necessary. Otherwise every time user submit request will trigger a DB query which is really slow.Could front-end DDB with ElasticCache if you're worried about the DDB queries being too slow and you truly need that level of performance on auth/session data and are willing to pay for it.But still need DDB behind it to populate the cache misses in that scenario.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 461571,
          "date": "Sat 06 Nov 2021 21:31",
          "username": "kirrim",
          "content": "Could front-end DDB with ElasticCache if you're worried about the DDB queries being too slow and you truly need that level of performance on auth/session data and are willing to pay for it.But still need DDB behind it to populate the cache misses in that scenario.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 595341,
          "date": "Sun 01 May 2022 01:15",
          "username": "tartarus23",
          "content": "B.  DynamoDB is a better option to save the authenticated connection details rather than a standard EC2 instance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 586314,
          "date": "Fri 15 Apr 2022 13:20",
          "username": "tartarus23",
          "content": "B is highly available, scalable, ALB allows connection stickiness and handling with help of DDB to save the connections and sessions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 532705,
          "date": "Wed 26 Jan 2022 09:39",
          "username": "shotty1",
          "content": "B is most right I would say. I am pretty certain it is the answer that AWS wants to hear",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 529349,
          "date": "Fri 21 Jan 2022 21:03",
          "username": "CloudChef",
          "content": "B or not 2 B, that is the question, and the answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 522092,
          "date": "Wed 12 Jan 2022 12:40",
          "username": "pititcu667",
          "content": "Voting B.  the answer given makes no sense. If you store session data on an ec2 and you lost it you lost the session.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 494851,
          "date": "Mon 06 Dec 2021 04:49",
          "username": "AzureDP900",
          "content": "I will go with DynamoDB ,B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448543,
          "date": "Sat 06 Nov 2021 14:08",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411870,
          "date": "Fri 05 Nov 2021 16:08",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350191,
          "date": "Wed 03 Nov 2021 00:57",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 331527,
          "date": "Sun 31 Oct 2021 12:14",
          "username": "KnightVictor",
          "content": "Reading highly available, A & D are ruled out. <br>B, C talk about auto scaling group+application load balancer, so highly available. <br>Between B & C, since DynamoDB makes more sense in this case, so going for B<br><br>My take: B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 295279,
          "date": "Thu 28 Oct 2021 12:11",
          "username": "kiev",
          "content": "Absolutely B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292530,
          "date": "Mon 25 Oct 2021 01:03",
          "username": "Kian1",
          "content": "going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281217,
          "date": "Sat 23 Oct 2021 20:53",
          "username": "Firststack",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 276773,
          "date": "Mon 18 Oct 2021 13:37",
          "username": "Ebi",
          "content": "No option other than B makes sense, answer is B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 267861,
          "date": "Thu 14 Oct 2021 20:40",
          "username": "Superomamsarah_t",
          "content": "C.  the question is asking: \\\"stay connected to the application even when there are underlying failures\\\", it means when the DynamoDB fails so the only way is to save the connection details locally on the EC2 instances.DynamoDB is HA by default, EC2 instances can fail.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 333958,
          "date": "Mon 01 Nov 2021 00:43",
          "username": "sarah_t",
          "content": "DynamoDB is HA by default, EC2 instances can fail.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253466,
          "date": "Tue 12 Oct 2021 03:16",
          "username": "Bulti",
          "content": "Answer is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#579",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company experienced a breach of highly confidential personal information due to permission issues on an Amazon S3 bucket. The Information Security team has tightened the bucket policy to restrict access. Additionally, to be better prepared for future attacks, these requirements must be met:<br>✑ Identify remote IP addresses that are accessing the bucket objects.<br>✑ Receive alerts when the security policy on the bucket is changed.<br>✑ Remediate the policy changes automatically.<br>Which strategies should the Solutions Architect use?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#579",
          "answers": [
            {
              "choice": "<p>A. Use Amazon CloudWatch Logs with CloudWatch filters to identify remote IP addresses. Use CloudWatch Events rules with AWS Lambda to automatically remediate S3 bucket policy changes. Use Amazon SES with CloudWatch Events rules for alerts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Athena with S3 access logs to identify remote IP addresses. Use AWS Config rules with AWS Systems Manager Automation to automatically remediate S3 bucket policy changes. Use Amazon SNS with AWS Config rules for alerts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use S3 access logs with Amazon Elasticsearch Service and Kibana to identify remote IP addresses. Use an Amazon Inspector assessment template to automatically remediate S3 bucket policy changes. Use Amazon SNS for alerts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Macie with an S3 bucket to identify access patterns and remote IP addresses. Use AWS Lambda with Macie to automatically remediate S3 bucket policy changes. Use Macie automatic alerting capabilities for alerts.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155023,
          "date": "Tue 21 Sep 2021 07:47",
          "username": "Nemerkpcert",
          "content": "B.  1)To id remote IPs, need to look at S3 access logs. Athena helps in analyzing those logs.<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html<br><br>2) For auto-remediation, use AWS Config with Systems Manager.<br>https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/<br><br>4) For alerting, use SNS with AWS Config.<br>https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.htmlAgree. Answer is B. ",
          "upvote_count": "443",
          "selected_answers": ""
        },
        {
          "id": 362624,
          "date": "Thu 04 Nov 2021 19:36",
          "username": "kpcert",
          "content": "Agree. Answer is B. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 714976,
          "date": "Thu 10 Nov 2022 07:42",
          "username": "janvandermerwer",
          "content": "B - Need to retrive \\\"remote IP addreses\\\", alerts when the bucket changes and remediate the changes automatically<br>--- Config rules --> detect change --> send sns alert + trigger config remediation.<br>--> S3 acess logs search, Athena can probably do the job here.<br><br>D - Macie is good but wont' meet the criteria to detect changes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 676347,
          "date": "Thu 22 Sep 2022 18:11",
          "username": "AwsBRFan",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html<br>Key word S3 server access<br>Ispector is for EC2 and ECS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 495039,
          "date": "Mon 06 Dec 2021 11:23",
          "username": "cldy",
          "content": "B.  Use Amazon Athena with S3 access logs to identify remote IP addresses. Use AWS Config rules with AWS Systems Manager Automation to automatically remediate S3 bucket policy changes. Use Amazon SNS with AWS Config rules for alerts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486056,
          "date": "Wed 24 Nov 2021 16:17",
          "username": "pcops",
          "content": "B: Athena + S3 access logs to identify IP address. SNS for notifications and SM to automate the requests.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 481264,
          "date": "Fri 19 Nov 2021 02:44",
          "username": "sashenka",
          "content": "One actually CAN get the IP ADDRESS using Amazon Macie:<br>policyDetails.actor.ipAddressDetails.ipAddressV4<br><br>https://docs.aws.amazon.com/de_de/macie/latest/user/findings-filter-fields.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411876,
          "date": "Fri 05 Nov 2021 04:20",
          "username": "WhyIronMan",
          "content": "I'll go with B<br>For those choosing D, read the question again. Twice.<br>✑ Identify remote IP addresses that are accessing the bucket objects.<br>✑ Receive alerts when the security policy on the bucket is changed.<br>✑ Remediate the policy changes automatically.<br>^this is called \\\"Requirements\\\"^<br>Macie is about the DATA itself; question wants to prevent a series of events like public explicit buckets, notify and set they private again. <br>Typical use case of AWS Config rules",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 348140,
          "date": "Thu 04 Nov 2021 18:22",
          "username": "digimaniac",
          "content": "D<br>B can't monitor S3 policy change. versus Macie can \\\"Macie generates policy findings when the policies or settings for an S3 bucket are changed in a way that reduces the security of the bucket and its objects. Macie does this only if the change occurs after you enable your Macie account.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 333867,
          "date": "Thu 04 Nov 2021 07:13",
          "username": "Pupu86blackgamer",
          "content": "Answer is D.  AWS macie is built specifically for protecting of PII informationAnswer is B.  please refer to below link for details explanation.<br><br>https://aws.amazon.com/blogs/mt/using-aws-systems-manager-opscenter-and-aws-config-for-compliance-monitoring/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 422136,
          "date": "Sat 06 Nov 2021 23:36",
          "username": "blackgamer",
          "content": "Answer is B.  please refer to below link for details explanation.<br><br>https://aws.amazon.com/blogs/mt/using-aws-systems-manager-opscenter-and-aws-config-for-compliance-monitoring/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 321756,
          "date": "Sun 24 Oct 2021 01:10",
          "username": "alisyech",
          "content": "i go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292534,
          "date": "Fri 22 Oct 2021 12:50",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 276782,
          "date": "Thu 21 Oct 2021 23:40",
          "username": "Ebi",
          "content": "I will go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253704,
          "date": "Tue 19 Oct 2021 21:15",
          "username": "petebear55",
          "content": "Change my mind to B. . D can not do the last point in the question.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253698,
          "date": "Mon 18 Oct 2021 21:01",
          "username": "petebear55",
          "content": "D: https://aws.amazon.com/blogs/security/how-to-create-custom-alerts-with-amazon-macie/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253697,
          "date": "Sun 17 Oct 2021 16:03",
          "username": "petebear55vbal",
          "content": "D is correct as its designed for just this scenario with S3Amazon Macie is a security service that makes it easy for you to discover, classify, and protect sensitive data in Amazon Simple Storage Service (Amazon S3). Question is About Bucket Policy Changes...Can Macie look for changes in AWS resources Configuration???",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 502467,
          "date": "Wed 15 Dec 2021 21:39",
          "username": "vbal",
          "content": "Amazon Macie is a security service that makes it easy for you to discover, classify, and protect sensitive data in Amazon Simple Storage Service (Amazon S3). Question is About Bucket Policy Changes...Can Macie look for changes in AWS resources Configuration???",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253486,
          "date": "Thu 14 Oct 2021 16:30",
          "username": "BultiBulti",
          "content": "B is the correct answer. Only AWS config can continuously monitor changes to bucket polices and enable automaticremediation.https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 269582,
          "date": "Thu 21 Oct 2021 00:39",
          "username": "Bulti",
          "content": "https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251327,
          "date": "Mon 11 Oct 2021 05:42",
          "username": "darthvoodooSD13SD13",
          "content": "D is correct because:<br>1. Macie can detect the source IP (https://docs.aws.amazon.com/macie/latest/user/monitoring.html)<br>2. It can easily send alerts out<br>3. Can integrate with event bridge to trigger lambda for remediation (https://docs.aws.amazon.com/macie/latest/user/findings-monitor.html)D seems latest and accurate: Supporting link: https://aws.amazon.com/blogs/security/deploy-an-automated-chatops-solution-for-remediating-amazon-macie-findings/?nc1=b_rpCorrect answer is B, Macie cannot detect remote IP Athena can",
          "upvote_count": "422",
          "selected_answers": ""
        },
        {
          "id": 264188,
          "date": "Wed 20 Oct 2021 07:59",
          "username": "SD13SD13",
          "content": "D seems latest and accurate: Supporting link: https://aws.amazon.com/blogs/security/deploy-an-automated-chatops-solution-for-remediating-amazon-macie-findings/?nc1=b_rpCorrect answer is B, Macie cannot detect remote IP Athena can",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 329909,
          "date": "Mon 25 Oct 2021 01:36",
          "username": "SD13",
          "content": "Correct answer is B, Macie cannot detect remote IP Athena can",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#580",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing a deployment strategy for an application tier and has the following requirements:<br>✑ The application code will need a 500 GB static dataset to be present before application startup.<br>✑ The application tier must be able to scale up and down based on demand with as little startup time as possible.<br>✑ The Development team should be able to update the code multiple times each day.<br>✑ Critical operating system (OS) patches must be installed within 48 hours of being released.<br>Which deployment strategy meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#580",
          "answers": [
            {
              "choice": "<p>A. Use AWS Systems Manager to create a new AMI with the updated OS patches. Update the Auto Scaling group to use the patched AMI and replace existing unpatched instances. Use AWS CodeDeploy to push the application code to the instances. Store the static data in Amazon EFS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Systems Manager to create a new AMI with updated OS patches. Update the Auto Scaling group to use the patched AMI and replace existing unpatched instances. Update the OS patches and the application code as batch job every night. Store the static data in Amazon EFS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an Amazon-provided AMI for the OS. Configure an Auto Scaling group set to a static instance count. Configure an Amazon EC2 user data script to download the data from Amazon S3. Install OS patches with AWS Systems Manager when they are released. Use AWS CodeDeploy to push the application code to the instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon-provided AMI for the OS. Configure an Auto Scaling group. Configure an Amazon EC2 user data script to download the data from Amazon S3. Replace existing instances after each updated Amazon-provided AMI release. Use AWS CodeDeploy to push the application code to the instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155030,
          "date": "Wed 22 Sep 2021 12:09",
          "username": "Nemerrcher",
          "content": "A. Systems Managerto update the ASGwith patched AMI, CodeDeploy to push the code, and EFS for the 500 GB static data.Second this, System manager have pre-defined run book that can allow you to patch os :)",
          "upvote_count": "264",
          "selected_answers": ""
        },
        {
          "id": 276929,
          "date": "Thu 28 Oct 2021 22:41",
          "username": "rcher",
          "content": "Second this, System manager have pre-defined run book that can allow you to patch os :)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 276788,
          "date": "Tue 26 Oct 2021 17:25",
          "username": "Ebi",
          "content": "Answer is A, <br>B although is correct as well but does not satisfy multiple deployments per day",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 689003,
          "date": "Sat 08 Oct 2022 05:21",
          "username": "WayneYi",
          "content": "The issue with option B is that it only pushes application code once per day, but we need multiple deployments per day.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 574176,
          "date": "Thu 24 Mar 2022 09:49",
          "username": "Bennycy",
          "content": "C Has all 4 requirements",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 523501,
          "date": "Fri 14 Jan 2022 12:20",
          "username": "pititcu667",
          "content": "A the keywords are automatic multiple releases -> CodeDeploy, Least amount of startup time shared efs for data is faster than downloading 500 gb from s3.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 514370,
          "date": "Sat 01 Jan 2022 06:02",
          "username": "cldy",
          "content": "A: CodeDeploy + EFS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494857,
          "date": "Mon 06 Dec 2021 04:58",
          "username": "AzureDP900",
          "content": "I will go with OPTION A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491039,
          "date": "Tue 30 Nov 2021 23:42",
          "username": "acloudguru",
          "content": "codedeploy is better than B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 411879,
          "date": "Tue 02 Nov 2021 03:03",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350231,
          "date": "Mon 01 Nov 2021 22:48",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349096,
          "date": "Sat 30 Oct 2021 05:42",
          "username": "blackgamer",
          "content": "A seems to be better option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292535,
          "date": "Fri 29 Oct 2021 16:34",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253500,
          "date": "Wed 20 Oct 2021 22:24",
          "username": "Bulti",
          "content": "I will go with A instead of B.  B is a bit confusing because it appears that the AWS System Manager would be able to create new AMI as new OS patches are released and replace the existing ones. Not sure why there is a need to do that when deploying the application code as well in B.  So I will go with A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244539,
          "date": "Wed 20 Oct 2021 04:48",
          "username": "T14102020",
          "content": "Correct is A.  CodeDeploy + EFS(faster then S3)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244522,
          "date": "Wed 20 Oct 2021 03:45",
          "username": "T14102020",
          "content": "Correct is A.  CodeDeploy + EFS",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242024,
          "date": "Sat 16 Oct 2021 07:56",
          "username": "rscloud",
          "content": "A<br>Code deploy for multiply deploy and EFS for static data",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 233137,
          "date": "Wed 06 Oct 2021 12:04",
          "username": "gookseang",
          "content": "A, CodeDeploy + EFS",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#581",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is operating a large customer service call center, and stores and processes call recordings with a custom application. Approximately 2% of the call recordings are transcribed by an offshore team for quality assurance purposes. These recordings take up to 72 hours to be transcribed. The recordings are stored on an NFS share before they are archived to an offsite location after 90 days. The company uses Linux servers for processing the call recordings and managing the transcription queue. There is also a web application for the quality assurance staff to review and score call recordings.<br>The company plans to migrate the system to AWS to reduce storage costs and the time required to transcribe calls.<br>Which set of actions should be taken to meet the company's objectives?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#581",
          "answers": [
            {
              "choice": "<p>A. Upload the call recordings to Amazon S3 from the call center. Set up an S3 lifecycle policy to move the call recordings to Amazon S3 Glacier after 90 days. Use an AWS Lambda trigger to transcribe the call recordings with Amazon Transcribe. Use Amazon S3, Amazon API Gateway, and Lambda to host the review and scoring application.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Upload the call recordings to Amazon S3 from the call center. Set up an S3 lifecycle policy to move the call recordings to Amazon S3 Glacier after 90 days. Use an AWS Lambda trigger to transcribe the call recordings with Amazon Mechanical Turk. Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer to host the review and scoring application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer to host the review and scoring application. Upload the call recordings to this application from the call center and store them on an Amazon EFS mount point. Use AWS Backup to archive the call recordings after 90 days. Transcribe the call recordings with Amazon Transcribe.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload the call recordings to Amazon S3 from the call center and put the object key in an Amazon SQS queue. Set up an S3 lifecycle policy to move the call recordings to Amazon S3 Glacier after 90 days. Use Amazon EC2 instances in an Auto Scaling group to send the recordings to Amazon Mechanical Turk for transcription. Use the number of objects in the queue as the scaling metric. Use Amazon S3, Amazon API Gateway, and AWS Lambda to host the review and scoring application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 513421,
          "date": "Thu 30 Dec 2021 14:41",
          "username": "AwsSuperTrooper",
          "content": "Answer is A",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 707290,
          "date": "Sat 29 Oct 2022 17:35",
          "username": "awsguru1998",
          "content": "A is straightforward. MTurk is not as instant as Transcribe. Reduce storage costs with NFS so avoid EFS and EC2 and move to serverless with S3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 594023,
          "date": "Thu 28 Apr 2022 21:51",
          "username": "Yamchi",
          "content": "A<br>S3 + Glacier + API GW + Lambda + Amazon Transcribe",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#582",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is building a containerized .NET Core application that will run in AWS Fargate. The backend of the application requires Microsoft SQL Server with high availability. All tiers of the application must be highly available. The credentials used for the connection string to SQL Server should not be stored on disk within the .NET Core front-end containers.<br>Which strategies should the Solutions Architect use to meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#582",
          "answers": [
            {
              "choice": "<p>A. Set up SQL Server to run in Fargate with Service Auto Scaling. Create an Amazon ECS task execution role that allows the Fargate task definition to get the secret value for the credentials to SQL Server running in Fargate. Specify the ARN of the secret in AWS Secrets Manager in the secrets section of the Fargate task definition so the sensitive data can be injected into the containers as environment variables on startup for reading into the application to construct the connection string. Set up the .NET Core service using Service Auto Scaling behind an Application Load Balancer in multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Multi-AZ deployment of SQL Server on Amazon RDS. Create a secret in AWS Secrets Manager for the credentials to the RDS database. Create an Amazon ECS task execution role that allows the Fargate task definition to get the secret value for the credentials to the RDS database in Secrets Manager. Specify the ARN of the secret in Secrets Manager in the secrets section of the Fargate task definition so the sensitive data can be injected into the containers as environment variables on startup for reading into the application to construct the connection string. Set up the .NET Core service in Fargate using Service Auto Scaling behind an Application Load Balancer in multiple Availability Zones.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Auto Scaling group to run SQL Server on Amazon EC2. Create a secret in AWS Secrets Manager for the credentials to SQL Server running on EC2. Create an Amazon ECS task execution role that allows the Fargate task definition to get the secret value for the credentials to SQL Server on EC2. Specify the ARN of the secret in Secrets Manager in the secrets section of the Fargate task definition so the sensitive data can be injected into the containers as environment variables on startup for reading into the application to construct the connection string. Set up the .NET Core service using Service Auto Scaling behind an Application Load Balancer in multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Multi-AZ deployment of SQL Server on Amazon RDS. Create a secret in AWS Secrets Manager for the credentials to the RDS database. Create non- persistent empty storage for the .NET Core containers in the Fargate task definition to store the sensitive information. Create an Amazon ECS task execution role that allows the Fargate task definition to get the secret value for the credentials to the RDS database in Secrets Manager. Specify the ARN of the secret in Secrets Manager in the secrets section of the Fargate task definition so the sensitive data can be written to the non-persistent empty storage on startup for reading into the application to construct the connection string. Set up the .NET Core service using Service Auto Scaling behind an Application Load Balancer in multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155191,
          "date": "Thu 23 Sep 2021 09:49",
          "username": "Nemerrcher",
          "content": "B. Secrets Manager natively supports SQL Server on RDS. No real need to create additional'ephemeral storage' to fetch credentials, as these can be injected to containers as environment variables.<br>https://aws.amazon.com/premiumsupport/knowledge-center/ecs-data-security-container-task/agreed, deploying something similar in Fargate that required secrets from secret manger.",
          "upvote_count": "333",
          "selected_answers": ""
        },
        {
          "id": 276935,
          "date": "Fri 22 Oct 2021 10:15",
          "username": "rcher",
          "content": "agreed, deploying something similar in Fargate that required secrets from secret manger.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 244551,
          "date": "Sun 10 Oct 2021 16:58",
          "username": "T14102020",
          "content": "Correct is B.  RDS + Secret Manager + Without non- persistent empty storage",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 676367,
          "date": "Thu 22 Sep 2022 18:29",
          "username": "AwsBRFan",
          "content": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data-secrets.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 623850,
          "date": "Tue 28 Jun 2022 10:44",
          "username": "TechX",
          "content": "Answer: B<br>Explanation:<br>By default tasks in Fargate are assigned ephemeral storage. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/fargate-task-storage.html https://aws.amazon.com/premiumsupport/knowledge-center/ecs-data-security-container-task/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 622775,
          "date": "Sun 26 Jun 2022 21:37",
          "username": "kangtamo",
          "content": "It should be B, retrieving RDS credentials from Secret Manager.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 557822,
          "date": "Mon 28 Feb 2022 04:19",
          "username": "jyrajan69",
          "content": "Unless there is a specific reasons for using non-persistent storage the answer must be B.  Best practice is to use roles, and B is the only answer with that",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499010,
          "date": "Sat 11 Dec 2021 00:49",
          "username": "challenger1",
          "content": "My Answer: B<br>B uses ECS - containers solution for Fargate",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494864,
          "date": "Mon 06 Dec 2021 05:05",
          "username": "AzureDP900",
          "content": "I will go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 439335,
          "date": "Sat 30 Oct 2021 11:56",
          "username": "Suresh108",
          "content": "method of elimination -. <br><br>application must be highly available = MULTI-AZ (ONLY B and D has). <br>out of those two environment variables works good, hence B. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 411885,
          "date": "Fri 29 Oct 2021 16:31",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350241,
          "date": "Thu 28 Oct 2021 17:01",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350100,
          "date": "Wed 27 Oct 2021 11:39",
          "username": "blackgamer",
          "content": "B is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345578,
          "date": "Tue 26 Oct 2021 18:31",
          "username": "gsw",
          "content": "whats the problem with D?",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 292550,
          "date": "Fri 22 Oct 2021 13:28",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 269338,
          "date": "Fri 22 Oct 2021 03:09",
          "username": "Ebi",
          "content": "B is the correct answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263724,
          "date": "Tue 19 Oct 2021 18:27",
          "username": "01037",
          "content": "D doesn't work?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253520,
          "date": "Mon 18 Oct 2021 21:56",
          "username": "Bulti",
          "content": "Correct answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#583",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An enterprise company wants to implement cost controls for all its accounts in AWS Organizations, which has full features enabled. The company has mapped organizational units (OUs) to its business units, and it wants to bill these business units for their individual AWS spending. There has been a recent spike in the company's AWS bill, which is generating attention from the Finance team. A Solutions Architect needs to investigate the cause of the spike while designing a solution that will track AWS costs in Organizations and generate a notification to the required teams if costs from a business unit exceed a specific monetary threshold.<br>Which solution will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#583",
          "answers": [
            {
              "choice": "<p>A. Use Cost Explorer to troubleshoot the reason for the additional costs. Set up an AWS Lambda function to monitor the company's AWS bill by each AWS account in an OU. Store the threshold amount set by the Finance team in the AWS Systems Manager Parameter Store. Write the custom rules in the Lambda function to verify any hidden costs for the AWS accounts. Trigger a notification from the Lambda function to an Amazon SNS topic when a budget threshold is breached.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Trusted Advisor to troubleshoot the reason for the additional costs. Set up an AWS Lambda function to monitor the company's AWS bill by each AWS account in an OU. Store the threshold amount set by the Finance team in the AWS Systems Manager Parameter Store. Write custom rules in the Lambda function to verify any hidden costs for the AWS accounts. Trigger an email to the required teams from the Lambda function using Amazon SNS when a budget threshold is breached.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Cost Explorer to troubleshoot the reason for the additional costs. Create a budget using AWS Budgets with the monetary amount set by the Finance team for each OU by grouping the linked accounts. Configure an Amazon SNS notification to the required teams in the budget.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Trusted Advisor to troubleshoot the reason for the additional costs. Create a budget using AWS Budgets with the monetary amount set by the Finance team for each OU by grouping the linked accounts. Add the Amazon EC2 instance types to be used in the company as a budget filter. Configure an Amazon SNS topic with a subscription for the Finance team email address to receive budget notifications.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155199,
          "date": "Wed 22 Sep 2021 11:36",
          "username": "Nemer",
          "content": "C. Typical Cost Explorer & AWS Budget use case.",
          "upvote_count": "24",
          "selected_answers": ""
        },
        {
          "id": 270175,
          "date": "Sat 30 Oct 2021 03:21",
          "username": "Ebi",
          "content": "I will go with C",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 650823,
          "date": "Tue 23 Aug 2022 15:05",
          "username": "sathishleorai",
          "content": "Best answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 497755,
          "date": "Thu 09 Dec 2021 14:39",
          "username": "cldy",
          "content": "C.  Use Cost Explorer to troubleshoot the reason for the additional costs. Create a budget using AWS Budgets with the monetary amount set by the Finance team for each OU by grouping the linked accounts. Configure an Amazon SNS notification to the required teams in the budget.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494867,
          "date": "Mon 06 Dec 2021 05:09",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411888,
          "date": "Thu 04 Nov 2021 08:54",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 350244,
          "date": "Sun 31 Oct 2021 04:17",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 253527,
          "date": "Wed 27 Oct 2021 21:25",
          "username": "Bulti",
          "content": "Answer is C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244557,
          "date": "Mon 25 Oct 2021 22:30",
          "username": "T14102020",
          "content": "Correct is C.  Cost Explorer + Budget",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 231651,
          "date": "Sat 16 Oct 2021 00:31",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 208631,
          "date": "Sun 10 Oct 2021 09:27",
          "username": "CYL",
          "content": "C.  Cost explorer give the breakdown by OU. AWS Budget to set limits and alerts.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 199504,
          "date": "Fri 08 Oct 2021 05:29",
          "username": "Paitan",
          "content": "Definitely C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 158450,
          "date": "Mon 04 Oct 2021 13:51",
          "username": "shakthi000005",
          "content": "Ans is C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 157130,
          "date": "Tue 28 Sep 2021 19:46",
          "username": "Anila_Dhharisiqaz12wsxpetebear55",
          "content": "Ater re-reading the question, I think its C and not B. <br>Cost Explorer – See patterns in AWS spend over time, project future costs, identify areas that need further inquiry, observe Reserved Instance utilization, observe Reserved Instance coverage, and receive Reserved Instance recommendations.<br><br>AWS Trusted Advisor – Get real-time identification of potential areas for optimization.<br><br>AWS Budgets – Set custom budgets that trigger alerts when cost or usage exceed (or are forecasted to exceed) a budgeted amount. Budgets can be set based on tags and accounts as well as resource types.before writing something make sure that it's correctStop being a school teacher .. everyone is entitled to make mistakes",
          "upvote_count": "6813",
          "selected_answers": ""
        },
        {
          "id": 160859,
          "date": "Mon 04 Oct 2021 14:55",
          "username": "qaz12wsxpetebear55",
          "content": "before writing something make sure that it's correctStop being a school teacher .. everyone is entitled to make mistakes",
          "upvote_count": "813",
          "selected_answers": ""
        },
        {
          "id": 253715,
          "date": "Fri 29 Oct 2021 12:12",
          "username": "petebear55",
          "content": "Stop being a school teacher .. everyone is entitled to make mistakes",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 157126,
          "date": "Sun 26 Sep 2021 01:37",
          "username": "Anila_Dhharisi",
          "content": "B is the answer. As Cost Explorer allows users to examine usage patterns over time. Trusted Advisor alerts users about resources with low utilization. and as well Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#584",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.<br>Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#584",
          "answers": [
            {
              "choice": "<p>A. Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB.  Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLB.  Create a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB.  Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB.  Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155232,
          "date": "Thu 23 Sep 2021 02:07",
          "username": "Nemer",
          "content": "C. NLB with one Elastic IP per AZto handle TCP traffic. Alias record set named my.service.com. <br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 173729,
          "date": "Wed 06 Oct 2021 18:00",
          "username": "examguru2020",
          "content": "C is correct.<br>If you create an internet-facing load balancer, you can select an Elastic IP address for each Availability Zone. This provides your load balancer with static IP addresses. <br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-network-load-balancer.html",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 734569,
          "date": "Sat 03 Dec 2022 17:42",
          "username": "SureNot",
          "content": "Btw is it possible to reach the goal with ECS(and Fargate)?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 650116,
          "date": "Mon 22 Aug 2022 07:51",
          "username": "kadevByrney",
          "content": "\\\"The service must use fixed address assignments in order for other businesses to add the addresses to their allow list\\\" => That mean outbound traffic need through fix IP.<br>=> C also wrong, IP public fixed of NL not related to outboundC is right - the allow list on the 'other business' side includes the fixed IPs for the NLB as allowed destinations.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 712983,
          "date": "Mon 07 Nov 2022 11:39",
          "username": "Byrney",
          "content": "C is right - the allow list on the 'other business' side includes the fixed IPs for the NLB as allowed destinations.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 648336,
          "date": "Thu 18 Aug 2022 09:13",
          "username": "kadev",
          "content": "yep, C<br>https://aws.amazon.com/premiumsupport/knowledge-center/elb-attach-elastic-ip-to-public-nlb/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 637687,
          "date": "Wed 27 Jul 2022 01:54",
          "username": "jyrajan69",
          "content": "Answer C is the only one with Alias DNS record which is needed to access AWS Resources",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 606765,
          "date": "Tue 24 May 2022 16:40",
          "username": "bobsmith2000",
          "content": "No-brainer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 529885,
          "date": "Sat 22 Jan 2022 14:51",
          "username": "Devgela",
          "content": "C.  Assign the Elastic IP addresses to the NLB make the answers correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499216,
          "date": "Sat 11 Dec 2021 09:51",
          "username": "cldy",
          "content": "C.  Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB.  Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494871,
          "date": "Mon 06 Dec 2021 05:12",
          "username": "AzureDP900",
          "content": "C is perfect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 492872,
          "date": "Fri 03 Dec 2021 01:49",
          "username": "Rho_Ohm",
          "content": ">> Ans: C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490311,
          "date": "Tue 30 Nov 2021 02:08",
          "username": "acloudguru",
          "content": "C, only make sense one",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 411889,
          "date": "Mon 01 Nov 2021 15:31",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 351554,
          "date": "Fri 29 Oct 2021 07:43",
          "username": "blackgamer",
          "content": "C is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350250,
          "date": "Fri 29 Oct 2021 07:28",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 270192,
          "date": "Fri 29 Oct 2021 05:13",
          "username": "Ebi",
          "content": "I will go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253616,
          "date": "Wed 27 Oct 2021 14:38",
          "username": "Bulti",
          "content": "Answer is C most probably. In A, B and D, ELB is created but Route53 never uses that as the resource in the A-record,Instead routes traffic directly to the underlying EC2 or ECS instances. However there is no mention of providing Elastic IP addresses of the EC2 instances to the external services that will use them for whitelisting. So I am a bit confused.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#585",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a web application with On-Demand Amazon EC2 instances in Auto Scaling groups that scale dynamically based on custom metrics. After extensive testing, the company determines that the m5.2xlarge instance size is optimal for the workload. Application data is stored in db.r4.4xlarge Amazon RDS instances that are confirmed to be optimal. The traffic to the web application spikes randomly during the day.<br>What other cost-optimization methods should the company implement to further reduce costs without impacting the reliability of the application?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#585",
          "answers": [
            {
              "choice": "<p>A. Double the instance count in the Auto Scaling groups and reduce the instance size to m5.large.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Reserve capacity for the RDS database and the minimum number of EC2 instances that are constantly running.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Reduce the RDS instance size to db.r4.xlarge and add five equivalently sized read replicas to provide reliability.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Reserve capacity for all EC2 instances and leverage Spot Instance pricing for the RDS database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155235,
          "date": "Thu 23 Sep 2021 04:21",
          "username": "Nemer",
          "content": "B-is the reasonable option, as there is no point in changing instances sizes that are already optimal, and a spot instance for the db is a bad idea.",
          "upvote_count": "20",
          "selected_answers": ""
        },
        {
          "id": 272722,
          "date": "Wed 13 Oct 2021 05:56",
          "username": "EbiEbi",
          "content": "B is the right answerI change to A after reading all the comments, <br>As the Autoscaling is based on metrics A can be the right answer if and only if minimum number of instances is 1, otherwise with min=2 this solution does not help reducing cost",
          "upvote_count": "53",
          "selected_answers": ""
        },
        {
          "id": 285991,
          "date": "Mon 18 Oct 2021 22:26",
          "username": "Ebi",
          "content": "I change to A after reading all the comments, <br>As the Autoscaling is based on metrics A can be the right answer if and only if minimum number of instances is 1, otherwise with min=2 this solution does not help reducing cost",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 658439,
          "date": "Sat 03 Sep 2022 14:21",
          "username": "AYANtheGLADIATOR",
          "content": "B is the answer here is the link .<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithReservedDBInstances.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 530317,
          "date": "Sun 23 Jan 2022 08:13",
          "username": "Nano803",
          "content": "I like B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 496555,
          "date": "Wed 08 Dec 2021 06:34",
          "username": "cldy",
          "content": "B.  Reserve capacity for the RDS database and the minimum number of EC2 instances that are constantly running.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494872,
          "date": "Mon 06 Dec 2021 05:15",
          "username": "AzureDP900",
          "content": "I will go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 456671,
          "date": "Sun 07 Nov 2021 15:35",
          "username": "Viper57",
          "content": "The answer is clearly B.  People are being confused by the term 'reserve capacity'. This is not the same as an on-demand capacity reservation. <br><br>This article by AWS clearly states that by 'reserving capacity' you are reserving the instances and reducing your costs. See - https://aws.amazon.com/aws-cost-management/aws-cost-optimization/reserved-instances/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 432324,
          "date": "Sat 06 Nov 2021 12:46",
          "username": "Kopa",
          "content": "Im more for A, costs can be minimal and change more dynamic because for lower model type of instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412617,
          "date": "Mon 01 Nov 2021 08:26",
          "username": "jobe42",
          "content": "B, the fact that RDS and EC2 is defined here as \\\"optimal\\\", no need to change them, so just reduce the cost with reserved instances for RDS and EC2",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411890,
          "date": "Sun 31 Oct 2021 18:22",
          "username": "WhyIronMan",
          "content": "I'll go with B<br>You can easily safe money reserving the DB instance and the minimum number of ec2",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350252,
          "date": "Mon 25 Oct 2021 21:58",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 311923,
          "date": "Sat 23 Oct 2021 13:57",
          "username": "ItsmeP",
          "content": "Ans B<br>A is incorrect as there is no change in billing if we go with double instance count with half capacity, it can minor degrade performance as well.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292558,
          "date": "Fri 22 Oct 2021 20:40",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281374,
          "date": "Sat 16 Oct 2021 00:59",
          "username": "rastibnh_fediQCO",
          "content": "Answer is A - to have more smaller instances to better react on dynamic scaling<br>The answer B is not giving you any discount. It's \\\"Reserve capacity\\\", not \\\"Reserved Instance\\\"and the minimum number of EC2 instance**Checked the pricing on the m5.2xlarge vs m5.xlarge and they are $0.384 and 0.192 per Hour. There is no further reduction in cost (no savings), however with B, reserving capacity makes cost very granular which can result in cost reduction no matter how little. The question is more focused on cost reduction rather than reliability/performance of the solution",
          "upvote_count": "524",
          "selected_answers": ""
        },
        {
          "id": 290369,
          "date": "Tue 19 Oct 2021 02:36",
          "username": "bnh_fedi",
          "content": "and the minimum number of EC2 instance**",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 285029,
          "date": "Sat 16 Oct 2021 09:41",
          "username": "QCO",
          "content": "Checked the pricing on the m5.2xlarge vs m5.xlarge and they are $0.384 and 0.192 per Hour. There is no further reduction in cost (no savings), however with B, reserving capacity makes cost very granular which can result in cost reduction no matter how little. The question is more focused on cost reduction rather than reliability/performance of the solution",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 274417,
          "date": "Fri 15 Oct 2021 03:58",
          "username": "ju0nbnh_fedi",
          "content": "Answer is A.  Reserved capacity doesn't offer any billing discount. <br><br>https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/and the minimum number of EC2 instance**",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 290370,
          "date": "Tue 19 Oct 2021 23:27",
          "username": "bnh_fedi",
          "content": "and the minimum number of EC2 instance**",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253620,
          "date": "Tue 12 Oct 2021 03:14",
          "username": "Bulti",
          "content": "Correct answer is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244582,
          "date": "Sat 09 Oct 2021 00:39",
          "username": "T14102020",
          "content": "Correct is B.  reserve capacity",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#586",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS<br>CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.<br>Which solution will ensure that the credentials are appropriately secured automatically?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#586",
          "answers": [
            {
              "choice": "<p>A. Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155266,
          "date": "Thu 23 Sep 2021 18:04",
          "username": "Nemerrcher",
          "content": "D.  CodeCommit trigger with Lambda.<br>https://docs.aws.amazon.com/lambda/latest/dg/services-codecommit.htmlSample code here https://github.com/aws-samples/discover-sensitive-data-in-aws-codecommit-with-aws-lambda/tree/main/src/handlers<br><br>Running regex after all hehe",
          "upvote_count": "273",
          "selected_answers": ""
        },
        {
          "id": 276942,
          "date": "Wed 13 Oct 2021 19:58",
          "username": "rcher",
          "content": "Sample code here https://github.com/aws-samples/discover-sensitive-data-in-aws-codecommit-with-aws-lambda/tree/main/src/handlers<br><br>Running regex after all hehe",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 192999,
          "date": "Fri 01 Oct 2021 21:41",
          "username": "MMARTINEZ85misterfaustGmail78bbnbnuyhymengxingkirrim",
          "content": "C.  Macie can be used with CodeCommit. <br>https://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.html\\\"Use advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.\\\"which it exclude C from my understanding...D is then the answerMacie can only scan S3 buckets. D is the answerThat's right!<br>AWS CodeCommit stores your repositories in Amazon S3 and Amazon DynamoDB. <br>So use Macie.<br>See https://aws.amazon.com/codecommit/features/<br>High Availability and Durability.CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I don't think they're stored in buckets that you can see or point Macie to.In fact, there are even solutions out there describing how to copy your repo from CodeCommit into S3 to back it up:https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html<br><br>D: AWS has an exact architecture for doing this:https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "911344",
          "selected_answers": ""
        },
        {
          "id": 196258,
          "date": "Fri 01 Oct 2021 23:58",
          "username": "misterfaustGmail78bbnbnuyh",
          "content": "\\\"Use advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.\\\"which it exclude C from my understanding...D is then the answerMacie can only scan S3 buckets. D is the answer",
          "upvote_count": "113",
          "selected_answers": ""
        },
        {
          "id": 200428,
          "date": "Sat 02 Oct 2021 05:23",
          "username": "Gmail78bbnbnuyh",
          "content": "which it exclude C from my understanding...D is then the answerMacie can only scan S3 buckets. D is the answer",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 207319,
          "date": "Tue 05 Oct 2021 14:41",
          "username": "bbnbnuyh",
          "content": "Macie can only scan S3 buckets. D is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 425718,
          "date": "Wed 27 Oct 2021 18:52",
          "username": "ymengxingkirrim",
          "content": "That's right!<br>AWS CodeCommit stores your repositories in Amazon S3 and Amazon DynamoDB. <br>So use Macie.<br>See https://aws.amazon.com/codecommit/features/<br>High Availability and Durability.CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I don't think they're stored in buckets that you can see or point Macie to.In fact, there are even solutions out there describing how to copy your repo from CodeCommit into S3 to back it up:https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html<br><br>D: AWS has an exact architecture for doing this:https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "44",
          "selected_answers": ""
        },
        {
          "id": 461654,
          "date": "Sun 07 Nov 2021 14:27",
          "username": "kirrim",
          "content": "CodeCommit may use S3 on the back end (and it also uses DynamoDB on the back end) but I don't think they're stored in buckets that you can see or point Macie to.In fact, there are even solutions out there describing how to copy your repo from CodeCommit into S3 to back it up:https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html<br><br>D: AWS has an exact architecture for doing this:https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 644386,
          "date": "Tue 09 Aug 2022 09:06",
          "username": "Santo99",
          "content": "Macke is only for S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 536097,
          "date": "Sun 30 Jan 2022 12:43",
          "username": "cannottellname",
          "content": "Amazon Macie is only used for S3. Hence, D seems good :):)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 529505,
          "date": "Sat 22 Jan 2022 01:34",
          "username": "tkanmani76",
          "content": "D - https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494873,
          "date": "Mon 06 Dec 2021 05:16",
          "username": "AzureDP900",
          "content": "D is right answer, I think this question in Neal Davis practice tests",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490842,
          "date": "Tue 30 Nov 2021 17:46",
          "username": "ryu10_09",
          "content": "https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 450519,
          "date": "Fri 05 Nov 2021 03:53",
          "username": "nodogoshi",
          "content": "D.  Amazon Macie is for S3 Service, not for CodeCommit.<br>https://docs.aws.amazon.com/codecommit/latest/userguide/data-protection.html<br>”Use advanced managed security services such as Amazon Macie, which assists in discovering and securing personal data that is stored in Amazon S3.”<br>[stored in Amazon S3.]",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441598,
          "date": "Tue 02 Nov 2021 08:46",
          "username": "TomPaschendastudent22",
          "content": "For D, there is a blog post describing that exact solution: https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/<br>For C: I dont think Macie works directly with CodeCommitGood link, thanks. <br>Answer is D",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 441855,
          "date": "Wed 03 Nov 2021 21:27",
          "username": "student22",
          "content": "Good link, thanks. <br>Answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 439343,
          "date": "Tue 02 Nov 2021 04:23",
          "username": "Suresh108",
          "content": "I am choosing DDDDDD.  <br><br>https://aws.amazon.com/blogs/compute/discovering-sensitive-data-in-aws-codecommit-with-aws-lambda-2/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 436859,
          "date": "Fri 29 Oct 2021 10:37",
          "username": "student22",
          "content": "D<br>Not C - Macie is for s3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411893,
          "date": "Tue 26 Oct 2021 23:39",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 405271,
          "date": "Tue 26 Oct 2021 13:18",
          "username": "Kopa",
          "content": "Only D is a promptly and immediate solution regarding security.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351571,
          "date": "Fri 22 Oct 2021 10:47",
          "username": "blackgamer",
          "content": "D is answer. C is not relevant , it is to scan S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350259,
          "date": "Thu 21 Oct 2021 11:57",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 332108,
          "date": "Wed 20 Oct 2021 03:04",
          "username": "PredaOvde",
          "content": "Cannot be D.  It will check only for newly commited code, not for old code, which is required. I pick A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 323235,
          "date": "Mon 18 Oct 2021 14:11",
          "username": "Pupu86",
          "content": "Answer is D. <br>CodeCommit itself is a repository. <br><br>Using Mercie means you are saving your code artefacts to S3 instead.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#587",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS<br>CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts.<br>As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.<br>How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#587",
          "answers": [
            {
              "choice": "<p>A. Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to execute in a non-production environment before approving the change for production.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions and generate notifications on errors. Deploy to a test environment and execute a manual test plan before approving the change for production.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155279,
          "date": "Tue 21 Sep 2021 19:56",
          "username": "Nemer",
          "content": "B.  Why do manual testing in option D when it can be automated with CodeBuild? CF Change sets to preview changes, andCodeDeploy b/g deployment with ASG.<br><br>https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 637642,
          "date": "Tue 26 Jul 2022 23:56",
          "username": "hilft",
          "content": "of course, it's B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 610602,
          "date": "Thu 02 Jun 2022 14:57",
          "username": "xyzman",
          "content": "it is B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 498696,
          "date": "Fri 10 Dec 2021 15:29",
          "username": "cldy",
          "content": "B.  Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 494874,
          "date": "Mon 06 Dec 2021 05:19",
          "username": "AzureDP900",
          "content": "B is right , this is straight forward question",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 489106,
          "date": "Sun 28 Nov 2021 13:07",
          "username": "acloudguru",
          "content": "codebuild can provide automatic test",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 450520,
          "date": "Sat 06 Nov 2021 08:29",
          "username": "nodogoshi",
          "content": "B.  automated testing always best",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412619,
          "date": "Wed 03 Nov 2021 20:00",
          "username": "jobe42",
          "content": "B: fully automated, with ChangeSets, all other answers have way to much room for human errors.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 411905,
          "date": "Mon 01 Nov 2021 04:12",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351578,
          "date": "Wed 27 Oct 2021 18:22",
          "username": "blackgamer",
          "content": "It is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350263,
          "date": "Mon 25 Oct 2021 18:51",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 292561,
          "date": "Sun 17 Oct 2021 04:29",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 272725,
          "date": "Fri 08 Oct 2021 16:11",
          "username": "Ebi",
          "content": "I will go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 258983,
          "date": "Fri 08 Oct 2021 06:27",
          "username": "kopper2019",
          "content": "Answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253628,
          "date": "Fri 08 Oct 2021 00:22",
          "username": "Bulti",
          "content": "B is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244589,
          "date": "Mon 04 Oct 2021 13:42",
          "username": "T14102020",
          "content": "Correct is B. automated testing using AWS CodeBuild",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 231661,
          "date": "Fri 01 Oct 2021 08:17",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#588",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A financial services company is moving to AWS and wants to enable developers to experiment and innovate while preventing access to production applications.<br>The company has the following requirements:<br>✑ Production workloads cannot be directly connected to the internet.<br>✑ All workloads must be restricted to the us-west-2 and eu-central-1 Regions.<br>✑ Notification should be sent when developer sandboxes exceed $500 in AWS spending monthly.<br>Which combination of actions needs to be taken to create a multi-account structure that meets the company's requirements? (Choose three.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#588",
          "answers": [
            {
              "choice": "<p>A. Create accounts for each production workload within an organization in AWS Organizations. Place the production accounts within an organizational unit (OU). For each account, delete the default VPC.  Create an SCP with a Deny rule for the attach an internet gateway and create a default VPC actions. Attach the SCP to the OU for the production accounts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create accounts for each production workload within an organization in AWS Organizations. Place the production accounts within an organizational unit (OU). Create an SCP with a Deny rule on the attach an internet gateway action. Create an SCP with a Deny rule to prevent use of the default VPC.  Attach the SCPs to the OU for the production accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a SCP containing a Deny Effect for cloudfront:*, iam:*, route53:*, and support:* with a StringNotEquals condition on an aws:RequestedRegion condition key with us-west-2 and eu-central-1 values. Attach the SCP to the organization's root.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM permission boundary containing a Deny Effect for cloudfront:*, iam:*, route53:*, and support:* with a StringNotEquals condition on an aws:RequestedRegion condition key with us-west-2 and eu-central-1 values. Attach the permission boundary to an IAM group containing the development and production users.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create accounts for each development workload within an organization in AWS Organizations. Place the development accounts within an organizational unit (OU). Create a custom AWS Config rule to deactivate all IAM users when an account's monthly bill exceeds $500.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create accounts for each development workload within an organization in AWS Organizations. Place the development accounts within an organizational unit (OU). Create a budget within AWS Budgets for each development account to monitor and report on monthly spending exceeding $500.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 156252,
          "date": "Sun 19 Sep 2021 19:01",
          "username": "Nemerpablobairatstudent22ipindado2020",
          "content": "BCF - Production and dev accounts in separate OUs, AWS Budget for notifications. <br>Between A & B, deleting default VPC seems excessive. SCP should be able to prevent using it. Not 100% sure.B is wrong for one simple reason. You can delete the default VPC and create a new one. The new one will have a new arn so the SCP will not have effect on it.<br>Just denying to create an IG does not prevent to create a new default VPC with the IG attached.<br>From here: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html<br>\\\"Amazon creates the above resources on your behalf. IAM policies do not apply to these actions because you do not perform these actions. For example, if you have an IAM policy that denies the ability to call CreateInternetGateway, and then you call CreateDefaultVpc, the internet gateway in the default VPC is still created.\\\"<br><br>In conclusion, ACFGood point. <br>Answer: ACFI get the point... both questions want to reflect equivalent actions, but for me the redaction of B is very confusing...<br><br>\\\"and create a default VPC actions. Create an SCP with a Deny rule to prevent use of the default VPC\\\"<br><br>Obviousy it can be understood that \\\"create default vpc actions\\\" means the default vpc for the prod environment....<br><br>And when it is said that...\\\"Create an SCP with a Deny rule to prevent use of the default VPC\\\"... It can be understood that it is talking about th original \\\"default VPC\\\" no the new one... isn´t it?<br><br>In any case It is too much \\\"It can be understood\\\"... So I go for ACF, nobody will use never that VPC so I for me it has more sense cleaning the entire network structure of prod (consdering B syntax).",
          "upvote_count": "25811",
          "selected_answers": ""
        },
        {
          "id": 427526,
          "date": "Wed 27 Oct 2021 09:16",
          "username": "pablobairatstudent22",
          "content": "B is wrong for one simple reason. You can delete the default VPC and create a new one. The new one will have a new arn so the SCP will not have effect on it.<br>Just denying to create an IG does not prevent to create a new default VPC with the IG attached.<br>From here: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html<br>\\\"Amazon creates the above resources on your behalf. IAM policies do not apply to these actions because you do not perform these actions. For example, if you have an IAM policy that denies the ability to call CreateInternetGateway, and then you call CreateDefaultVpc, the internet gateway in the default VPC is still created.\\\"<br><br>In conclusion, ACFGood point. <br>Answer: ACF",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 436878,
          "date": "Thu 04 Nov 2021 08:16",
          "username": "student22",
          "content": "Good point. <br>Answer: ACF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183261,
          "date": "Sat 25 Sep 2021 03:00",
          "username": "ipindado2020",
          "content": "I get the point... both questions want to reflect equivalent actions, but for me the redaction of B is very confusing...<br><br>\\\"and create a default VPC actions. Create an SCP with a Deny rule to prevent use of the default VPC\\\"<br><br>Obviousy it can be understood that \\\"create default vpc actions\\\" means the default vpc for the prod environment....<br><br>And when it is said that...\\\"Create an SCP with a Deny rule to prevent use of the default VPC\\\"... It can be understood that it is talking about th original \\\"default VPC\\\" no the new one... isn´t it?<br><br>In any case It is too much \\\"It can be understood\\\"... So I go for ACF, nobody will use never that VPC so I for me it has more sense cleaning the entire network structure of prod (consdering B syntax).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 272740,
          "date": "Wed 13 Oct 2021 11:30",
          "username": "Ebiheanygpark",
          "content": "ACF is the right answer.<br>B can not be the answer, there is no way to have one single SCP at OU or root level to deny using of default VPC in each accountShould be ADF. As there could be other types of workload which could be in other org, e.g. sandbox workloads in CTO org, etc. The question doesn't imply there are only two orgs in this companyTouche",
          "upvote_count": "2322",
          "selected_answers": ""
        },
        {
          "id": 686621,
          "date": "Wed 05 Oct 2022 07:56",
          "username": "heany",
          "content": "Should be ADF. As there could be other types of workload which could be in other org, e.g. sandbox workloads in CTO org, etc. The question doesn't imply there are only two orgs in this company",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 297432,
          "date": "Fri 15 Oct 2021 06:04",
          "username": "gpark",
          "content": "Touche",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 698395,
          "date": "Tue 18 Oct 2022 19:30",
          "username": "JohnPiJohnPi",
          "content": "BCF<br>A does not scaleACF you cannot \\\"Create an SCP with a Deny rule to prevent use of the default VPC\\\"",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 698405,
          "date": "Tue 18 Oct 2022 19:44",
          "username": "JohnPi",
          "content": "ACF you cannot \\\"Create an SCP with a Deny rule to prevent use of the default VPC\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 687689,
          "date": "Thu 06 Oct 2022 12:56",
          "username": "tomosabc1",
          "content": "The answer should be ACF. <br><br>B(wrong): \\\"Create an SCP with a Deny rule to prevent use of the default VPC. \\\" It is impossible to do this.<br>D(wrong): Permission boundary can only be attached to user or role, rather than IAM group.<br>E(wrong): Obviously wrong. AWS Budgets should be used.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 678668,
          "date": "Sun 25 Sep 2022 11:46",
          "username": "Azerty1313",
          "content": "C isn't recommended see: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html#scp-warning-testing-effect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 676441,
          "date": "Thu 22 Sep 2022 19:53",
          "username": "AwsBRFan",
          "content": "AWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. <br><br>But if tested why not?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 637040,
          "date": "Tue 26 Jul 2022 03:15",
          "username": "hilft",
          "content": "It's BDF. <br>Don't mess around with IGW<br>AWS don't recommend SCP on root account",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626471,
          "date": "Sun 03 Jul 2022 09:54",
          "username": "aandc",
          "content": "Cannot find how to \\\"Deny rule to prevent use of the default VPC\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 580846,
          "date": "Mon 04 Apr 2022 18:49",
          "username": "roka_ua",
          "content": "Vote ACF",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 555204,
          "date": "Thu 24 Feb 2022 12:10",
          "username": "futen0326",
          "content": "D instead of C.  You don't have to attach an SCP to the root, it's bad practice, you can be a little more granular with D.  It works better for the requirement.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 523258,
          "date": "Fri 14 Jan 2022 02:53",
          "username": "tkanmani76",
          "content": "A - Why not B ? Tried searching SCP for VPC - we can deny creation of default VPC (CreateDefaultVpc), there are none to stop using it. So only way is to delete.<br>D - Why not C ? Per AWS it is not a good practice to attach SCP to root.<br>F - No contention with E here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494876,
          "date": "Mon 06 Dec 2021 05:25",
          "username": "AzureDP900",
          "content": "I have to revisit this question and confirm between ACF vs BCF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 486517,
          "date": "Thu 25 Nov 2021 09:34",
          "username": "ryu10_09ryu10_09",
          "content": "why A, you cannot delete the default VPC.  so A is not valid. It is BCFI change my mind. i have checked and you can delete default VPC",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 486519,
          "date": "Thu 25 Nov 2021 09:35",
          "username": "ryu10_09",
          "content": "I change my mind. i have checked and you can delete default VPC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 479386,
          "date": "Tue 16 Nov 2021 14:08",
          "username": "Kopa",
          "content": "A,C,F should be",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 446329,
          "date": "Fri 05 Nov 2021 06:56",
          "username": "near22",
          "content": "ADF<br>for c, AWS don't recommend apply SCP to root",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 427955,
          "date": "Fri 29 Oct 2021 02:05",
          "username": "littlecurly",
          "content": "B,D,F<br>D denies the root to the global services including IAM, which doesn't make sense...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 416493,
          "date": "Sun 24 Oct 2021 01:30",
          "username": "student2020",
          "content": "ACF is the answer<br>There is no action to prevent use of default VPC<br>https://docs.aws.amazon.com/AWSEC2/latest/APIReference/OperationList-query-vpc.html",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#589",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.<br>Which steps should the solutions architect take to design an appropriate solution?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#589",
          "answers": [
            {
              "choice": "<p>A. Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company's domain to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 157479,
          "date": "Mon 27 Sep 2021 00:36",
          "username": "NemerhilftoscargeeGeniusMikeLiu",
          "content": "B.  Web appneeds ALB.  Multi-AZ deployment should address HA.  Retain deletion policy to not delete the db with the stack.i think B is better than AI don't think web app combined with ALB.  My understanding is access heavy using NLB, calculation heavy using ALB. why not C?",
          "upvote_count": "22132",
          "selected_answers": ""
        },
        {
          "id": 638934,
          "date": "Fri 29 Jul 2022 02:45",
          "username": "hilft",
          "content": "i think B is better than A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 363053,
          "date": "Tue 26 Oct 2021 05:30",
          "username": "oscargee",
          "content": "I don't think web app combined with ALB.  My understanding is access heavy using NLB, calculation heavy using ALB. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 530403,
          "date": "Sun 23 Jan 2022 10:12",
          "username": "GeniusMikeLiu",
          "content": "why not C?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 272743,
          "date": "Mon 18 Oct 2021 07:59",
          "username": "Ebi",
          "content": "I will go with B",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 703302,
          "date": "Mon 24 Oct 2022 21:11",
          "username": "Blair77",
          "content": "BBB - Web App need ALB not NLB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 696957,
          "date": "Mon 17 Oct 2022 07:49",
          "username": "ToanVN1988",
          "content": "Have to use CNAME record on R53 to maping with ALB.  C is wrong",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 689011,
          "date": "Sat 08 Oct 2022 05:56",
          "username": "WayneYi",
          "content": "200,000 accesses per day is really no big deal, no reason to use NLB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 659231,
          "date": "Sun 04 Sep 2022 13:51",
          "username": "kadev",
          "content": "Question forcus on HA <br>Amazon Aurora is designed for spead storage on three AZ => HA more than RDS only",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 654870,
          "date": "Wed 31 Aug 2022 09:51",
          "username": "Sathish1412",
          "content": "B is best option for the requirement",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 646634,
          "date": "Sun 14 Aug 2022 09:51",
          "username": "MikeyJSathish1412",
          "content": "NLB in A is overkill.<br><br>daily demands of 200,000 users < Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies.<br>https://aws.amazon.com/elasticloadbalancing/network-load-balancer/You are correct!",
          "upvote_count": "21",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 654546,
          "date": "Wed 31 Aug 2022 02:03",
          "username": "Sathish1412",
          "content": "You are correct!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 628870,
          "date": "Fri 08 Jul 2022 19:47",
          "username": "Millari",
          "content": "A.  EB is already .NET ready<br>AWS Elastic Beanstalk for .NET makes it easier to deploy, manage, and scale your ASP.NET web applications that use Amazon Web Services.<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.html<br><br>you can also AAdding an Amazon RDS DB instance to your .NET application environment",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 624375,
          "date": "Wed 29 Jun 2022 04:21",
          "username": "TechXgerhardbl",
          "content": "A for me, we have 200.000 users which is heavy access, NLB will go over ALBI'd start thinking about a NLB if I had 200K users per second, not per day.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 646760,
          "date": "Sun 14 Aug 2022 15:31",
          "username": "gerhardbl",
          "content": "I'd start thinking about a NLB if I had 200K users per second, not per day.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 622814,
          "date": "Sun 26 Jun 2022 22:55",
          "username": "kangtamo",
          "content": "B sounds better.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 543447,
          "date": "Wed 09 Feb 2022 01:42",
          "username": "Bigbearcn",
          "content": "It's B. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 543106,
          "date": "Tue 08 Feb 2022 15:41",
          "username": "HellGate",
          "content": "You can deploy system with Beanstalk since it has its source code of .Net.And there's no DR requirement (D).",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 494878,
          "date": "Mon 06 Dec 2021 05:29",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 477445,
          "date": "Sat 13 Nov 2021 13:41",
          "username": "kaleen_bhaiya",
          "content": "Answer is A<br>Couple of reasons; 1) NLB is high performing and 2) You cannot have an A record for Route 53 alias, ALB doesn't have IP (A type) so answer would be NLB.  Let me know if I am missing anything.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 412494,
          "date": "Sun 31 Oct 2021 23:48",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 351584,
          "date": "Mon 25 Oct 2021 00:44",
          "username": "blackgamerRocketeerRocketeer",
          "content": "I will go with B.  A is incorrect as NLB doesn't have listener for Http and Https, it only works at layer 4 TCP and TLS only.NLB will act as a passthrough for the traffic and hence will work for http or httpsNever mind. I think we need to use ALB for http or https traffic.",
          "upvote_count": "511",
          "selected_answers": ""
        },
        {
          "id": 675096,
          "date": "Wed 21 Sep 2022 13:56",
          "username": "RocketeerRocketeer",
          "content": "NLB will act as a passthrough for the traffic and hence will work for http or httpsNever mind. I think we need to use ALB for http or https traffic.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 675105,
          "date": "Wed 21 Sep 2022 14:02",
          "username": "Rocketeer",
          "content": "Never mind. I think we need to use ALB for http or https traffic.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#590",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing a publicly accessible web application that is on an Amazon CloudFront distribution with an Amazon S3 website endpoint as the origin. When the solution is deployed, the website returns an Error 403: Access Denied message.<br>Which steps should the solutions architect take to correct the issue? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#590",
          "answers": [
            {
              "choice": "<p>A. Remove the S3 block public access option from the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the requester pays option from the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Remove the origin access identity (OAI) from the CloudFront distribution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Change the storage class from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Disable S3 object versioning.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 157833,
          "date": "Tue 05 Oct 2021 10:47",
          "username": "Anila_Dhharisiviet1991tekkartjoe16",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/A&B. <br>Amazon S3 Block Public Access must be disabled on the bucket.<br>If Requester Pays is enabled, then the request must include the request-payer parameter.In your link, it is written \\\": If you don't want to allow public (anonymous) access to your S3 objects, then change your configuration to use the S3 REST API endpoint as the origin of your distribution. Then, configure your distribution and S3 bucket to restrict access using an origin access identity (OAI). \\\"Then the answer would be A&C, Requester Pays is useful to prevent DDoS attacks, just remove it (answer B) would remove functionality with itRequestor pays is a feature to share cost of storing/accessing S3 objects, not DDoS prevention.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "19731",
          "selected_answers": ""
        },
        {
          "id": 375390,
          "date": "Wed 27 Oct 2021 07:23",
          "username": "viet1991tekkartjoe16",
          "content": "A&B. <br>Amazon S3 Block Public Access must be disabled on the bucket.<br>If Requester Pays is enabled, then the request must include the request-payer parameter.In your link, it is written \\\": If you don't want to allow public (anonymous) access to your S3 objects, then change your configuration to use the S3 REST API endpoint as the origin of your distribution. Then, configure your distribution and S3 bucket to restrict access using an origin access identity (OAI). \\\"Then the answer would be A&C, Requester Pays is useful to prevent DDoS attacks, just remove it (answer B) would remove functionality with itRequestor pays is a feature to share cost of storing/accessing S3 objects, not DDoS prevention.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "731",
          "selected_answers": ""
        },
        {
          "id": 389679,
          "date": "Wed 27 Oct 2021 13:07",
          "username": "tekkartjoe16",
          "content": "In your link, it is written \\\": If you don't want to allow public (anonymous) access to your S3 objects, then change your configuration to use the S3 REST API endpoint as the origin of your distribution. Then, configure your distribution and S3 bucket to restrict access using an origin access identity (OAI). \\\"Then the answer would be A&C, Requester Pays is useful to prevent DDoS attacks, just remove it (answer B) would remove functionality with itRequestor pays is a feature to share cost of storing/accessing S3 objects, not DDoS prevention.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 456143,
          "date": "Tue 02 Nov 2021 20:23",
          "username": "joe16",
          "content": "Requestor pays is a feature to share cost of storing/accessing S3 objects, not DDoS prevention.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 500025,
          "date": "Sun 12 Dec 2021 14:25",
          "username": "tkanmani76AzureDP900bermoPunitsolanki",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403/ This will settle the answer in favour of A and B. This is great information, thanks for sharing.Thanks indeed for this useful linkBut the link is when you access the S3 directly, not via cloudfront.",
          "upvote_count": "7111",
          "selected_answers": ""
        },
        {
          "id": 500367,
          "date": "Mon 13 Dec 2021 05:08",
          "username": "AzureDP900",
          "content": "This is great information, thanks for sharing.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 502188,
          "date": "Wed 15 Dec 2021 14:38",
          "username": "bermo",
          "content": "Thanks indeed for this useful link",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 545117,
          "date": "Fri 11 Feb 2022 07:36",
          "username": "Punitsolanki",
          "content": "But the link is when you access the S3 directly, not via cloudfront.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 645216,
          "date": "Thu 11 Aug 2022 03:11",
          "username": "Rocky2222",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 625616,
          "date": "Fri 01 Jul 2022 09:03",
          "username": "TechX",
          "content": "AB for me. <br>If you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 570543,
          "date": "Fri 18 Mar 2022 14:36",
          "username": "Hari008",
          "content": "Here the key word is publicly available, i will go with A&C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 544944,
          "date": "Fri 11 Feb 2022 02:21",
          "username": "peddyua",
          "content": "A is weird, it can work with block public access as well (deployed on a previous project)<br>AB for me",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 494788,
          "date": "Mon 06 Dec 2021 02:18",
          "username": "AzureDP900",
          "content": "A and B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 446699,
          "date": "Tue 02 Nov 2021 18:48",
          "username": "Viper57",
          "content": "A and C are correct.<br><br>The question says it is using a S3 Website Endpoint. OAI can only be used when Cloudfront needs to access a REST API endpoint, so removing OAI would fix this problem. <br><br>See using S3 to host a static website with Cloudfront: https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/<br><br>- Using a REST API endpoint as the origin, with access restricted by an origin access identity (OAI)<br>- Using a website endpoint as the origin, with anonymous (public) access allowed<br>- Using a website endpoint as the origin, with access restricted by a Referer header",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 436888,
          "date": "Sun 31 Oct 2021 18:51",
          "username": "student22student22",
          "content": "A and C<br>If you don't want to allow public (anonymous) access to your S3 objects, then change your configuration to use the S3 REST API endpoint as the origin of your distribution. Then, configure your distribution and S3 bucket to restrict access using an origin access identity (OAI).<br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/Changing to A and B",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 458157,
          "date": "Sun 07 Nov 2021 08:57",
          "username": "student22",
          "content": "Changing to A and B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412498,
          "date": "Thu 28 Oct 2021 12:59",
          "username": "WhyIronMan",
          "content": "I'll go with A,B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 351587,
          "date": "Sun 24 Oct 2021 18:02",
          "username": "blackgamer",
          "content": "A & B is the better option but it is not the best too. A is not a very good solution.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350282,
          "date": "Fri 22 Oct 2021 19:29",
          "username": "Waiweng",
          "content": "it's A&B",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 323253,
          "date": "Fri 22 Oct 2021 01:23",
          "username": "Pupu86",
          "content": "Origin Access Identity feature is to control only Cloudfront has access (read) permissions to S3 so users can only access S3 contents via a valid OAI in Cloudfront, making Cloudfront the only point of entry. So the eventual state of access logs in Cloudfront would also be accurately reflective.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292565,
          "date": "Thu 21 Oct 2021 04:44",
          "username": "Kian1",
          "content": "going with AB",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 272755,
          "date": "Wed 20 Oct 2021 02:02",
          "username": "Ebi",
          "content": "Answer is AB",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 259046,
          "date": "Wed 20 Oct 2021 01:47",
          "username": "kopper2019",
          "content": "A and B<br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-website-cloudfront-error-403/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 258293,
          "date": "Sat 16 Oct 2021 12:26",
          "username": "Bulti",
          "content": "A and B are the right answers",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#591",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A web application is hosted in a dedicated VPC that is connected to a company's on-premises data center over a Site-to-Site VPN connection. The application is accessible from the company network only. This is a temporary non-production application that is used during business hours. The workload is generally low with occasional surges.<br>The application has an Amazon Aurora MySQL provisioned database cluster on the backend. The VPC has an internet gateway and a NAT gateways attached.<br>The web servers are in private subnets in an Auto Scaling group behind an Elastic Load Balancer. The web servers also upload data to an Amazon S3 bucket through the internet.<br>A solutions architect needs to reduce operational costs and simplify the architecture.<br>Which strategy should the solutions architect use?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#591",
          "answers": [
            {
              "choice": "<p>A. Review the Auto Scaling group settings and ensure the scheduled actions are specified to operate the Amazon EC2 instances during business hours only. Use 3-year scheduled Reserved Instances for the web server EC2 instances. Detach the internet gateway and remove the NAT gateways from the VPC.  Use an Aurora Serverless database and set up a VPC endpoint for the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Review the Auto Scaling group settings and ensure the scheduled actions are specified to operate the Amazon EC2 instances during business hours only. Detach the internet gateway and remove the NAT gateways from the VPC.  Use an Aurora Serverless database and set up a VPC endpoint for the S3 bucket, then update the network routing and security rules and policies related to the changes.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Review the Auto Scaling group settings and ensure the scheduled actions are specified to operate the Amazon EC2 instances during business hours only. Detach the internet gateway from the VPC, and use an Aurora Serverless database. Set up a VPC endpoint for the S3 bucket, then update the network routing and security rules and policies related to the changes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use 3-year scheduled Reserved Instances for the web server Amazon EC2 instances. Remove the NAT gateways from the VPC, and set up a VPC endpoint for the S3 bucket. Use Amazon CloudWatch and AWS Lambda to stop and start the Aurora DB cluster so it operates during business hours only. Update the network routing and security rules and policies related to the changes.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155560,
          "date": "Mon 20 Sep 2021 01:19",
          "username": "NemeroscargeeViper57",
          "content": "B.  We are charged for each “NAT Gateway-hour\\\" even without data going through it.<br>https://aws.amazon.com/vpc/pricing/How would you handle The web servers also upload data to an Amazon S3 bucket through the internet? NAT has to been keep.Using an S3 VPC endpoint that goes over the AWS backend solves this problem.",
          "upvote_count": "1915",
          "selected_answers": ""
        },
        {
          "id": 363064,
          "date": "Tue 02 Nov 2021 10:42",
          "username": "oscargeeViper57",
          "content": "How would you handle The web servers also upload data to an Amazon S3 bucket through the internet? NAT has to been keep.Using an S3 VPC endpoint that goes over the AWS backend solves this problem.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 456734,
          "date": "Sat 06 Nov 2021 11:29",
          "username": "Viper57",
          "content": "Using an S3 VPC endpoint that goes over the AWS backend solves this problem.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 207037,
          "date": "Thu 30 Sep 2021 07:45",
          "username": "amaltareJohnPiMichaelRbesopleasespammelatercloudgc",
          "content": "Guys.. has anyone notice that a site-to-site VPN is in place and for this to work, an internet gateway is required. but ABC all the three options are saying to remove internet gateway, I dont think it will work then.. I will go with Dno IGW required https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario4.htmlJust fount this from AWS S2S docs:\\\"An Internet gateway is not required to establish a Site-to-Site VPN connection.\\\"a site-to-site VPN creates an IPSEC tunnel from an EC2 instance to a device of your choice on-prem. The IGW is required for that tunnel to have a route out of the VPCIt would make sense that a site-to-site VPN needs an IGW. But that's incorrect. <br>\\\"An internet gateway is not required to establish an AWS Site-to-Site VPN connection.\\\" https://aws.amazon.com/vpc/faqs/#ConnectivityB - https://aws.amazon.com/vpn/faqs/#:~:text=Amazon%20supports%20Internet%20Protocol%20security,-to-Site%20VPN%20connection.",
          "upvote_count": "1122143",
          "selected_answers": ""
        },
        {
          "id": 688445,
          "date": "Fri 07 Oct 2022 09:58",
          "username": "JohnPi",
          "content": "no IGW required https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario4.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 207078,
          "date": "Thu 30 Sep 2021 17:11",
          "username": "MichaelRbeso",
          "content": "Just fount this from AWS S2S docs:\\\"An Internet gateway is not required to establish a Site-to-Site VPN connection.\\\"a site-to-site VPN creates an IPSEC tunnel from an EC2 instance to a device of your choice on-prem. The IGW is required for that tunnel to have a route out of the VPC",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 227654,
          "date": "Wed 06 Oct 2021 03:38",
          "username": "beso",
          "content": "a site-to-site VPN creates an IPSEC tunnel from an EC2 instance to a device of your choice on-prem. The IGW is required for that tunnel to have a route out of the VPC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 207085,
          "date": "Sat 02 Oct 2021 08:47",
          "username": "pleasespammelater",
          "content": "It would make sense that a site-to-site VPN needs an IGW. But that's incorrect. <br>\\\"An internet gateway is not required to establish an AWS Site-to-Site VPN connection.\\\" https://aws.amazon.com/vpc/faqs/#Connectivity",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230846,
          "date": "Wed 06 Oct 2021 04:40",
          "username": "cloudgc",
          "content": "B - https://aws.amazon.com/vpn/faqs/#:~:text=Amazon%20supports%20Internet%20Protocol%20security,-to-Site%20VPN%20connection.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 710206,
          "date": "Thu 03 Nov 2022 04:04",
          "username": "Heer",
          "content": "The right answer between B and C is B .The ques says we do have IGW and NAT and Option C is removing IGW only .NAT cannot operate without IGW and that is why option C doesn't makes sense.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 676489,
          "date": "Thu 22 Sep 2022 20:54",
          "username": "AwsBRFan",
          "content": "VPN requires Virtual Private Gateway",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 622815,
          "date": "Sun 26 Jun 2022 22:58",
          "username": "kangtamo",
          "content": "Go with B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 494886,
          "date": "Mon 06 Dec 2021 05:36",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 492042,
          "date": "Thu 02 Dec 2021 01:32",
          "username": "vbal",
          "content": "A is wrong becoz SRI can't have 3-year reservation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 398000,
          "date": "Sat 06 Nov 2021 05:34",
          "username": "Pb55",
          "content": "S3 VPC endpoint means no need for IGW or NAT. So B. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 363065,
          "date": "Tue 02 Nov 2021 17:20",
          "username": "oscargeejobe42",
          "content": "C! B and C are almost same. But you need NAT to allow web servers in VPC private sub net to upload data to an Amazon S3 bucket through the internet.B. .. \\\"and set up a VPC endpoint for the S3 bucket\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 412633,
          "date": "Sat 06 Nov 2021 07:56",
          "username": "jobe42",
          "content": "B. .. \\\"and set up a VPC endpoint for the S3 bucket\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 352325,
          "date": "Tue 02 Nov 2021 01:21",
          "username": "blackgamer",
          "content": "Going with B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350921,
          "date": "Thu 28 Oct 2021 20:46",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 333799,
          "date": "Wed 27 Oct 2021 21:45",
          "username": "Pupu86",
          "content": "https://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html<br><br>This link shows you how to create a site-to-site VPN connection to your AWS VPCs. No internet gateway or NAT gateway is required",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292572,
          "date": "Sun 24 Oct 2021 03:13",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 272760,
          "date": "Thu 21 Oct 2021 09:11",
          "username": "Ebi",
          "content": "Answer is B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 259119,
          "date": "Tue 19 Oct 2021 21:30",
          "username": "kopper2019",
          "content": "B what would you need a Internet GW and NAT GW when all is private using a VPN and RI is not needed since is temporary so buying RIs for 3 years would mean losing money",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 253703,
          "date": "Tue 19 Oct 2021 14:08",
          "username": "Bulti",
          "content": "B is the right answer. A and D are out because scheduled reserved instances are not required as it is a temporary application. C is identical to B but it keeps the NAT Gateway which has extra unnecessary cost when we are using VPC endpoint to talk to S3.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 245972,
          "date": "Mon 18 Oct 2021 02:55",
          "username": "srinivasa",
          "content": "NAT gateway and internet gateway are not required.<br>B is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#592",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:<br>✑ It should allow changes to be released several times every hour.<br>✑ It should be able to roll back the changes as quickly as possible.<br>Which design will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#592",
          "answers": [
            {
              "choice": "<p>A. Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances, and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155569,
          "date": "Sun 19 Sep 2021 19:23",
          "username": "Nemer",
          "content": "B.  blue/green deployment with Beanstalk.",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 636799,
          "date": "Mon 25 Jul 2022 17:49",
          "username": "hilft",
          "content": "B.  'staging' blue/green",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626428,
          "date": "Sun 03 Jul 2022 07:35",
          "username": "aandc",
          "content": "key words :\\\"It should be able to roll back the changes as quickly as possible\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 522070,
          "date": "Wed 12 Jan 2022 12:12",
          "username": "pititcu667",
          "content": "B Blue Green no outage min interf.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 508550,
          "date": "Fri 24 Dec 2021 15:58",
          "username": "Baji000",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 494889,
          "date": "Mon 06 Dec 2021 05:39",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412526,
          "date": "Wed 03 Nov 2021 04:09",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 352334,
          "date": "Tue 02 Nov 2021 13:35",
          "username": "blackgamer",
          "content": "B is the right answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350924,
          "date": "Tue 26 Oct 2021 04:27",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 292575,
          "date": "Sat 23 Oct 2021 07:56",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 268601,
          "date": "Wed 20 Oct 2021 07:23",
          "username": "Ebi",
          "content": "Although there is no clarification of the platform and development env used, but the closest answer in here is B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 264533,
          "date": "Thu 14 Oct 2021 21:38",
          "username": "01037",
          "content": "Can AWS Systems Manager be used to provision infrastructure?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253710,
          "date": "Sat 09 Oct 2021 23:08",
          "username": "Bulti",
          "content": "B is the right answer. It is the fastest when it comes to rollback and deploying changes every hour. C is good but it falls short to meet the requirement of frequent deployments as it is pretty heavy in terms of having to build a new infrastructure each time a new deployment is needed.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244633,
          "date": "Sat 02 Oct 2021 09:55",
          "username": "T14102020",
          "content": "Correct is B.  Beanstalk",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242097,
          "date": "Sat 02 Oct 2021 07:53",
          "username": "rscloud",
          "content": "B.  blue/green deployment is good fit here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 232026,
          "date": "Sat 02 Oct 2021 00:09",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 208590,
          "date": "Wed 29 Sep 2021 15:37",
          "username": "CYL",
          "content": "B.  Allows for fallback.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#593",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company currently has data hosted in an IBM Db2 database. A web application calls an API that runs stored procedures on the database to retrieve user information data that is read-only. This data is historical in nature and changes on a daily basis. When a user logs in to the application, this data needs to be retrieved within 3 seconds. Each time a user logs in, the stored procedures run. Users log in several times a day to check stock prices.<br>Running this database has become cost-prohibitive due to Db2 CPU licensing. Performance goals are not being met. Timeouts from Db2 are common due to long-running queries.<br>Which approach should a solutions architect take to migrate this solution to AWS?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#593",
          "answers": [
            {
              "choice": "<p>A. Rehost the Db2 database in Amazon Fargate. Migrate all the data. Enable caching in Fargate. Refactor the API to use the Fargate Db2 database. Implement Amazon API Gateway and enable API caching.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS DMS to migrate data to Amazon DynamoDB using a continuous replication task. Refactor the API to use the DynamoDB data. Implement the refactored API in Amazon API Gateway and enable API caching.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a local cache on the mainframe to store query outputs. Use SFTP to sync to Amazon S3 on a daily basis. Refactor the API to use Amazon EFS. Implement Amazon API Gateway and enable API caching.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Extract data daily and copy the data to AWS Snowball for storage on Amazon S3. Sync daily. Refactor the API to use the S3 data. Implement Amazon API Gateway and enable API caching.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155588,
          "date": "Thu 23 Sep 2021 16:16",
          "username": "Nemer",
          "content": "B.  DMS supports DB2 as source and DynamoDB as target. Along with API caching, retrieval should be under 3 seconds.",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 268605,
          "date": "Tue 12 Oct 2021 18:30",
          "username": "Ebi",
          "content": "Answer is B<br>All other answer do not make any sense",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 676492,
          "date": "Thu 22 Sep 2022 20:59",
          "username": "AwsBRFan",
          "content": "B makes sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 653330,
          "date": "Mon 29 Aug 2022 08:57",
          "username": "gnic",
          "content": "It's B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 636119,
          "date": "Sun 24 Jul 2022 17:55",
          "username": "CloudHandsOn",
          "content": "B.  is the correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497394,
          "date": "Thu 09 Dec 2021 07:35",
          "username": "cldy",
          "content": "B.  Use AWS DMS to migrate data to Amazon DynamoDB using a continuous replication task. Refactor the API to use the DynamoDB data. Implement the refactored API in Amazon API Gateway and enable API caching.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494891,
          "date": "Mon 06 Dec 2021 05:41",
          "username": "AzureDP900",
          "content": "It is B, This question is part of Neal Davis practice test",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412557,
          "date": "Wed 03 Nov 2021 17:25",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 352333,
          "date": "Sun 31 Oct 2021 06:49",
          "username": "blackgamer",
          "content": "B is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350930,
          "date": "Sun 31 Oct 2021 03:11",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 341831,
          "date": "Mon 18 Oct 2021 06:03",
          "username": "Bemi",
          "content": "https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292576,
          "date": "Fri 15 Oct 2021 09:16",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253723,
          "date": "Mon 11 Oct 2021 13:52",
          "username": "Bulti",
          "content": "B is the answer. DMS supports migration from DB2 to Dynamo DB. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244637,
          "date": "Sun 10 Oct 2021 10:30",
          "username": "T14102020",
          "content": "Correct is B.  DMS supports DB2 as source and DynamoDB as target.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 232087,
          "date": "Fri 08 Oct 2021 04:31",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230854,
          "date": "Mon 04 Oct 2021 03:03",
          "username": "cloudgc",
          "content": "B - https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/#:~:text=AWS%20Database%20Migration%20Service%20Supports%20IBM%20Db2%20as%20a%20Source,-Posted%20On%3A%20Apr&text=AWS%20Database%20Migration%20Service%20(DMS,to%20any%20DMS%20supported%20target.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 208594,
          "date": "Sat 02 Oct 2021 19:35",
          "username": "CYL",
          "content": "B.  DMS supports relational DB to DynamoDB migration. https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB. html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#594",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to deploy a new business analytics application that requires 10,000 hours of compute time each month. The compute resources can have flexible availability, but must be as cost-effective as possible. The company will also provide a reporting service to distribute analytics reports, which needs to run at all times.<br>How should the Solutions Architect design a solution that meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#594",
          "answers": [
            {
              "choice": "<p>A. Deploy the reporting service on a Spot Fleet. Deploy the analytics application as a container in Amazon ECS with AWS Fargate as the compute option. Set the analytics application to use a custom metric with Service Auto Scaling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the reporting service on an On-Demand Instance. Deploy the analytics application as a container in AWS Batch with AWS Fargate as the compute option. Set the analytics application to use a custom metric with Service Auto Scaling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the reporting service as a container in Amazon ECS with AWS Fargate as the compute option. Deploy the analytics application on a Spot Fleet. Set the analytics application to use a custom metric with Amazon EC2 Auto Scaling applied to the Spot Fleet.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the reporting service as a container in Amazon ECS with AWS Fargate as the compute option. Deploy the analytics application on an On-Demand Instance and purchase a Reserved Instance with a 3-year term. Set the analytics application to use a custom metric with Amazon EC2 Auto Scaling applied to the On-Demand Instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 155624,
          "date": "Mon 27 Sep 2021 06:50",
          "username": "Nemer",
          "content": "C seems OK.Reporting service runs constantly, and as such should be separated from the analytics application running on a Spot Fleet for cost savings.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 710209,
          "date": "Thu 03 Nov 2022 04:09",
          "username": "Heer",
          "content": "Analytics service:Key words<br>Flexible availability i.s can be interrupted <br>Cost effective<br><br>Reporting service :Key words<br>Need to run all the time so Containers <br><br>So the right ans is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 555733,
          "date": "Fri 25 Feb 2022 04:25",
          "username": "jyrajan69tobstar86",
          "content": "There seems to be a tendency to jump in with an answer. First of all this is a steady state application, not short term, hence automatically you start thinking Reserved Instance, and the clearly stated 'the organization will offer a reporting service for distributing analytics results, which must be available at all times.\\\", which will take Spot out of the picture. Therefore the answer has to be D.  Unless someone can give proper justification tht is the answerWell, Spot instances in answer C are only used for the analytics application and not the reporting app. So thats ok.<br>Only difference between C & D is : C uses spot instances for the analytics app and D uses mixture of on demand on longterm reserved instances. <br><br>So pretty much both answers are valid. But, considering the business perspective: it's a new application. Would you want to commit yourself for the next 3 years with unknown outcome? Sure you can modify them later or resell, but still. <br>I'd choose C. ",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 559864,
          "date": "Thu 03 Mar 2022 07:46",
          "username": "tobstar86",
          "content": "Well, Spot instances in answer C are only used for the analytics application and not the reporting app. So thats ok.<br>Only difference between C & D is : C uses spot instances for the analytics app and D uses mixture of on demand on longterm reserved instances. <br><br>So pretty much both answers are valid. But, considering the business perspective: it's a new application. Would you want to commit yourself for the next 3 years with unknown outcome? Sure you can modify them later or resell, but still. <br>I'd choose C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 539677,
          "date": "Thu 03 Feb 2022 13:18",
          "username": "lucesaranoet22s",
          "content": "I do not understand why A and B are wrong. They both look feasible to me.A uses Spot fleet for the reporting service -> not suitable as the reporting service needs to be running at all times.<br>B uses Fargate for the analytics application vs C which uses a Spot Fleet for the analytics application. In this case, C is the more cost effective option as the analytics application can have flexible availability.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 717119,
          "date": "Sun 13 Nov 2022 07:29",
          "username": "et22s",
          "content": "A uses Spot fleet for the reporting service -> not suitable as the reporting service needs to be running at all times.<br>B uses Fargate for the analytics application vs C which uses a Spot Fleet for the analytics application. In this case, C is the more cost effective option as the analytics application can have flexible availability.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494894,
          "date": "Mon 06 Dec 2021 05:44",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412561,
          "date": "Mon 01 Nov 2021 15:24",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 352343,
          "date": "Mon 01 Nov 2021 09:00",
          "username": "blackgamer",
          "content": "C is cost effective compared to D.  Spot instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350939,
          "date": "Fri 29 Oct 2021 16:44",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 331546,
          "date": "Fri 29 Oct 2021 15:20",
          "username": "KnightVictor",
          "content": "going with C, keyword compute resources- \\\"cost effective\\\", \\\"flexible availability,\\\"<br>reporting service ->needs to run at all times",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292578,
          "date": "Thu 28 Oct 2021 09:11",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 268612,
          "date": "Sat 23 Oct 2021 11:59",
          "username": "Ebi",
          "content": "I will go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 253732,
          "date": "Wed 20 Oct 2021 18:23",
          "username": "Bulti",
          "content": "Correct answer is C for Spot Fleet",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244643,
          "date": "Wed 20 Oct 2021 17:57",
          "username": "T14102020",
          "content": "Correct is C.  report as Fargate + analytics as Spot",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 232089,
          "date": "Wed 20 Oct 2021 17:03",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 208596,
          "date": "Sun 10 Oct 2021 00:49",
          "username": "CYL",
          "content": "C.  Use spot instances for analytics workload. Reporting services need to be up all the time, hence should run on a reliable instance type that will not terminate on its own.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 207950,
          "date": "Thu 07 Oct 2021 05:39",
          "username": "SamAWSExam99",
          "content": "C for spot instances",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 157842,
          "date": "Thu 30 Sep 2021 04:30",
          "username": "Anila_Dhharisi",
          "content": "C is better option to use Spot for analytics",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#595",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating its three-tier web application from on-premises to the AWS Cloud. The company has the following requirements for the migration process:<br>✑ Ingest machine images from the on-premises environment.<br>✑ Synchronize changes from the on-premises environment to the AWS environment until the production cutover.<br>✑ Minimize downtime when executing the production cutover.<br>✑ Migrate the virtual machines' root volumes and data volumes.<br>Which solution will satisfy these requirements with minimal operational overhead?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#595",
          "answers": [
            {
              "choice": "<p>A. Use AWS Server Migration Service (SMS) to create and launch a replication job for each tier of the application. Launch instances from the AMIs created by AWS SMS. After initial testing, perform a final replication and create new instances from the updated AMIs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS CLI VM Import/Export script to migrate each virtual machine. Schedule the script to run incrementally to maintain changes in the application. Launch instances from the AMIs created by VM Import/Export. Once testing is done, rerun the script to do a final import and launch the instances from the AMIs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Server Migration Service (SMS) to upload the operating system volumes. Use the AWS CLI import-snapshot command for the data volumes. Launch instances from the AMIs created by AWS SMS and attach the data volumes to the instances. After initial testing, perform a final replication, launch new instances from the replicated AMIs, and attach the data volumes to the instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Application Discovery Service and AWS Migration Hub to group the virtual machines as an application. Use the AWS CLI VM Import/Export script to import the virtual machines as AMIs. Schedule the script to run incrementally to maintain changes in the application. Launch instances from the AMIs. After initial testing, perform a final virtual machine import and launch new instances from the AMIs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 156313,
          "date": "Tue 21 Sep 2021 16:16",
          "username": "Nemer",
          "content": "A.  SMS for automated, live incremental replication of live server volumes.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 160371,
          "date": "Fri 24 Sep 2021 14:29",
          "username": "LunchTime",
          "content": "A is correct. <br>SMS can handle migrating the data volumes: https://aws.amazon.com/about-aws/whats-new/2018/09/aws-server-migration-service-adds-support-for-migrating-larger-data-volumes/",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 710211,
          "date": "Thu 03 Nov 2022 04:12",
          "username": "Heer",
          "content": "Server Migration Service solely can do the following :<br>Migrate Virtual machinei.e the volumes are also getting migrated ,<br>AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2.<br><br>Right option is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 580874,
          "date": "Mon 04 Apr 2022 19:58",
          "username": "roka_ua",
          "content": "Vote A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532767,
          "date": "Wed 26 Jan 2022 10:58",
          "username": "shotty1",
          "content": "It is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 494896,
          "date": "Mon 06 Dec 2021 05:48",
          "username": "AzureDP900",
          "content": "A is good",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412567,
          "date": "Tue 02 Nov 2021 08:05",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 368738,
          "date": "Sat 30 Oct 2021 09:54",
          "username": "syc1205",
          "content": "Ingest machine images from the on-premises environment. So B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 352429,
          "date": "Thu 28 Oct 2021 15:31",
          "username": "blackgamer",
          "content": "A is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 350940,
          "date": "Mon 25 Oct 2021 06:19",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 321750,
          "date": "Sat 23 Oct 2021 19:47",
          "username": "alisyech",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292581,
          "date": "Tue 19 Oct 2021 05:38",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 283371,
          "date": "Fri 15 Oct 2021 00:11",
          "username": "Trap_D0_rEbiEbi",
          "content": "C<br>I wouldn't wrap up every drive for every vm into one giant ami and the phrasing of A \\\"launch a replication job for each tier of the application\\\" is a little vague and sounds fishy to me. Personally C looks better--use SMS to create ami's from root volumes, snapshot and import data drives, create EC2s from amis and reattach data drives. Test and deploy. That makes more sense to me.Thirdly how you manage incremental changes on data volume?<br>Answer is NOT CFirst of all snapshot import is a feature of VM Export/Import not a feature of SMS:<br>https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-import-snapshot.html<br>Secondly, if you import snapshot you need a create a volume from snapshot,",
          "upvote_count": "164",
          "selected_answers": ""
        },
        {
          "id": 285422,
          "date": "Sat 16 Oct 2021 09:02",
          "username": "Ebi",
          "content": "Thirdly how you manage incremental changes on data volume?<br>Answer is NOT C",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 285420,
          "date": "Fri 15 Oct 2021 14:48",
          "username": "Ebi",
          "content": "First of all snapshot import is a feature of VM Export/Import not a feature of SMS:<br>https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-import-snapshot.html<br>Secondly, if you import snapshot you need a create a volume from snapshot,",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 268619,
          "date": "Wed 13 Oct 2021 17:02",
          "username": "Ebi",
          "content": "A is the answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 244645,
          "date": "Sun 10 Oct 2021 15:25",
          "username": "T14102020T14102020",
          "content": "Correct is A.  SMS with minimum steps opposite Cwithout AWS CLI",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 244646,
          "date": "Wed 13 Oct 2021 14:22",
          "username": "T14102020",
          "content": "without AWS CLI",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 232321,
          "date": "Wed 06 Oct 2021 22:03",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 208598,
          "date": "Wed 29 Sep 2021 05:48",
          "username": "CYL",
          "content": "A.  SMS for migration of VMs.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#596",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An enterprise company's data science team wants to provide a safe, cost-effective way to provide easy access to Amazon SageMaker. The data scientists have limited AWS knowledge and need to be able to launch a Jupyter notebook instance. The notebook instance needs to have a preconfigured AWS KMS key to encrypt data at rest on the machine learning storage volume without exposing the complex setup requirements.<br>Which approach will allow the company to set up a self-service mechanism for the data scientists to launch Jupyter notebooks in its AWS accounts with the<br>LEAST amount of operational overhead?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#596",
          "answers": [
            {
              "choice": "<p>A. Create a serverless front end using a static Amazon S3 website to allow the data scientists to request a Jupyter notebook instance by filling out a form. Use Amazon API Gateway to receive requests from the S3 website and trigger a central AWS Lambda function to make an API call to Amazon SageMaker that will launch a notebook instance with a preconfigured KMS key for the data scientists. Then call back to the front-end website to display the URL to the notebook instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS CloudFormation template to launch a Jupyter notebook instance using the AWS::SageMaker::NotebookInstance resource type with a preconfigured KMS key. Add a user-friendly name to the CloudFormation template. Display the URL to the notebook using the Outputs section. Distribute the CloudFormation template to the data scientists using a shared Amazon S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS CloudFormation template to launch a Jupyter notebook instance using the AWS::SageMaker::NotebookInstance resource type with a preconfigured KMS key. Simplify the parameter names, such as the instance size, by mapping them to Small, Large, and X-Large using the Mappings section in CloudFormation. Display the URL to the notebook using the Outputs section, then upload the template into an AWS Service Catalog product in the data scientist's portfolio, and share it with the data scientist's IAM role.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS CLI script that the data scientists can run locally. Provide step-by-step instructions about the parameters to be provided while executing the AWS CLI script to launch a Jupyter notebook with a preconfigured KMS key. Distribute the CLI script to the data scientists using a shared Amazon S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 156315,
          "date": "Fri 24 Sep 2021 23:51",
          "username": "Nemer",
          "content": "C.  Service Catalog<br>https://aws.amazon.com/blogs/mt/enable-self-service-secured-data-science-using-amazon-sagemaker-notebooks-and-aws-service-catalog/",
          "upvote_count": "20",
          "selected_answers": ""
        },
        {
          "id": 670858,
          "date": "Fri 16 Sep 2022 15:10",
          "username": "spdracr713",
          "content": "A seems to fit the simplicity",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 636904,
          "date": "Mon 25 Jul 2022 20:38",
          "username": "hilft",
          "content": "Why it's not B?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626450,
          "date": "Sun 03 Jul 2022 08:21",
          "username": "aandc",
          "content": "keyword \\\"The data scientists are unfamiliar with AWS\\\" -> service catalog",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 596034,
          "date": "Mon 02 May 2022 14:32",
          "username": "tartarus23",
          "content": "C.  Using service catalog is easier than S3 bucket for the data scientists",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 532733,
          "date": "Wed 26 Jan 2022 10:15",
          "username": "shotty1",
          "content": "I think it is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 522146,
          "date": "Wed 12 Jan 2022 13:32",
          "username": "pititcu667",
          "content": "C just because service catalog simplifies it.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 494898,
          "date": "Mon 06 Dec 2021 05:49",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412596,
          "date": "Sat 06 Nov 2021 10:03",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 352439,
          "date": "Mon 01 Nov 2021 15:35",
          "username": "blackgamer",
          "content": "The answer is C for sure, service catalog.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350944,
          "date": "Fri 29 Oct 2021 19:04",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 296087,
          "date": "Wed 27 Oct 2021 20:09",
          "username": "kiev",
          "content": "Service catalog is the key. C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292585,
          "date": "Tue 19 Oct 2021 22:26",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 269306,
          "date": "Tue 19 Oct 2021 02:13",
          "username": "Ebi",
          "content": "I will go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 244652,
          "date": "Wed 13 Oct 2021 17:53",
          "username": "T14102020",
          "content": "Correct is C.  CloudFormat + ServiceCatalog",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 232322,
          "date": "Tue 12 Oct 2021 05:41",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229467,
          "date": "Mon 11 Oct 2021 14:08",
          "username": "oopsy",
          "content": "seems C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#597",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating its applications to AWS. The applications will be deployed to AWS accounts owned by business units. The company has several teams of developers who are responsible for the development and maintenance of all applications. The company is expecting rapid growth in the number of users.<br>The company's chief technology officer has the following requirements:<br>✑ Developers must launch the AWS infrastructure using AWS CloudFormation.<br>Developers must not be able to create resources outside of CloudFormation.<br><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0039500002.png\" class=\"in-exam-image\"><br>✑ The solution must be able to scale to hundreds of AWS accounts.<br>Which of the following would meet these requirements? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#597",
          "answers": [
            {
              "choice": "<p>A. Using CloudFormation, create an IAM role that can be assumed by CloudFormation that has permissions to create all the resources the company needs. Use CloudFormation StackSets to deploy this template to each AWS account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. In a central account, create an IAM role that can be assumed by developers, and attach a policy that allows interaction with CloudFormation. Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Using CloudFormation, create an IAM role that can be assumed by developers, and attach policies that allow interaction with and passing a role to CloudFormation. Attach an inline policy to deny access to all other AWS services. Use CloudFormation StackSets to deploy this template to each AWS account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Using CloudFormation, create an IAM role for each developer, and attach policies that allow interaction with CloudFormation. Use CloudFormation StackSets to deploy this template to each AWS account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. In a central AWS account, create an IAM role that can be assumed by CloudFormation that has permissions to create the resources the company requires. Create a CloudFormation stack policy that allows the IAM role to manage resources. Use CloudFormation StackSets to deploy the CloudFormation stack policy to each AWS account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 157966,
          "date": "Wed 22 Sep 2021 17:38",
          "username": "Kibana01",
          "content": "A&C seems a better combination.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 157864,
          "date": "Wed 22 Sep 2021 04:46",
          "username": "Anila_Dhharisiviet1991",
          "content": "Between A & E, its better to go with A.  In E, they gave option of stack policy. We use stack policy only for updates and as well to avoid any unintentional updates. In this scenario, they had not discussed the requirement of updates on the resources of CloudFormation stack.<br>Between B,C,D - its better to go with B.  <br>In option C, they mentioned about inline policy which is not appropriate as we need to embed the policy not attach it and better to use managed policies than inline policies. Inline policies are assigned to service linked roles which is inherited from the parent or user . <br>In option D, its saying to create role to each of the developers which is not the right way in assigning the permissions. A role can be used by multiple developers instead of creating each role to each developer.A&C<br>B is wrong.<br>\\\"Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.\\\" => this sentence is wrong.<br><br>\\\"AssumeRolePolicyDocument<br>The trust policy that is associated with this role. Trust policies define which entities can assume the role.\\\" <br><br>We need to use iam:Passrole to pass the role from developer to cloudformation.<br>AssumeRolePolicyDocument is used for assume the role only.",
          "upvote_count": "114",
          "selected_answers": ""
        },
        {
          "id": 377690,
          "date": "Sat 30 Oct 2021 12:13",
          "username": "viet1991",
          "content": "A&C<br>B is wrong.<br>\\\"Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.\\\" => this sentence is wrong.<br><br>\\\"AssumeRolePolicyDocument<br>The trust policy that is associated with this role. Trust policies define which entities can assume the role.\\\" <br><br>We need to use iam:Passrole to pass the role from developer to cloudformation.<br>AssumeRolePolicyDocument is used for assume the role only.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 694921,
          "date": "Fri 14 Oct 2022 19:04",
          "username": "Blair77",
          "content": "AAA CCC 110% sure!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 687601,
          "date": "Thu 06 Oct 2022 09:57",
          "username": "tomosabc1",
          "content": "The answer is AC. <br><br>B(wrong):\\\"Modify the AssumeRolePolicyDocument action to allow the IAM role to be passed to CloudFormation.\\\" => this sentence is wrong.<br>\\\"AssumeRolePolicyDocument<br>The trust policy that is associated with this role. Trust policies define which entities can assume the role.\\\"<br>We need to use iam:Passrole to pass the role from developer to cloudformation.<br><br>D(wrong): \\\"create an IAM role for each developer\\\". This sentence is wrong.<br>E(wrong): The newly created role in central account cannot be directly used by CloudFormation to create resources in other account. In addition, similar to S3 bucket policy, CloudFormation stack policy is used to control who can update the stack, rather than allowing the stack to create/manage AWS resource.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 636281,
          "date": "Mon 25 Jul 2022 00:58",
          "username": "hilft",
          "content": "C and E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626419,
          "date": "Sun 03 Jul 2022 07:18",
          "username": "aandc",
          "content": "agreed with AC",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 553381,
          "date": "Tue 22 Feb 2022 03:23",
          "username": "jyrajan69",
          "content": "First we start by looking at either B or E,here E is more detailed and complete answer, so will go with E.  Then is between A,C and D.  D is not compliant with AWS not best practice to create role for each developer, so then between A and C.  My answer would be C as this has an inline policy that prevents the developer from accessing the services directly. So answer is C and E",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 494901,
          "date": "Mon 06 Dec 2021 05:53",
          "username": "AzureDP900",
          "content": "I'll go with A,C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412598,
          "date": "Thu 04 Nov 2021 16:27",
          "username": "WhyIronMan",
          "content": "I'll go with A,C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 389906,
          "date": "Sat 30 Oct 2021 15:10",
          "username": "tekkarttkanmani76tkanmani76",
          "content": "Here is a proposition of reasoning. <br>First you must start from an account. Between B and E, you choose E because B is tempting (the statement about AssumeRolePolicyDocument looks right if I look this example : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html#aws-resource-iam-role--examples) but B does not allow you to export your configuration to other accounts. With E,CloudFormation gives itself the rights needed to auto-assign IAM.<br>Then you use CloudFormation (options A, C, D). Remains to give rights to developers (C, D), butbetween these options, to respect the statement that \\\"developers must not be able to create resources outside of CloudFormation\\\", the only option is C \\\"attach an inline policy to deny access to all other AWS services\\\" not very fine grained, but the only present. Plus D has the keywords \\\"for each developer\\\" which as said by Anila is tedious.<br>Therefore CE would be the right answers (as in the autocorrection, looking at the comments 90% of the answers are supposed to be good and I see discussion about the proposed answers on 50% !)Agree with Tekkart, C&E are the right choices.Correcting to A and C.  C is theonly choice which limits access to use of other services. And A deploys the template. (E deploys only stack set policy which is not correct).",
          "upvote_count": "612",
          "selected_answers": ""
        },
        {
          "id": 519255,
          "date": "Sat 08 Jan 2022 01:13",
          "username": "tkanmani76tkanmani76",
          "content": "Agree with Tekkart, C&E are the right choices.Correcting to A and C.  C is theonly choice which limits access to use of other services. And A deploys the template. (E deploys only stack set policy which is not correct).",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 548384,
          "date": "Wed 16 Feb 2022 08:44",
          "username": "tkanmani76",
          "content": "Correcting to A and C.  C is theonly choice which limits access to use of other services. And A deploys the template. (E deploys only stack set policy which is not correct).",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350950,
          "date": "Fri 29 Oct 2021 00:45",
          "username": "Waiweng",
          "content": "it's A&C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346663,
          "date": "Wed 27 Oct 2021 03:00",
          "username": "ppshein",
          "content": "I would go with A & B.  Because C is kinda duplicated with A and developers can manually amend policy by itself if required.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292588,
          "date": "Tue 26 Oct 2021 12:55",
          "username": "Kian1",
          "content": "going with AC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 269311,
          "date": "Thu 21 Oct 2021 13:50",
          "username": "Ebi",
          "content": "I will go with AC",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 254303,
          "date": "Thu 14 Oct 2021 13:35",
          "username": "petebear55",
          "content": "A AND C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 253745,
          "date": "Wed 13 Oct 2021 22:43",
          "username": "Bulti",
          "content": "A& C is the right answer. E is a misleading option. You need to deploy the CloudFormation template and not just the Stack policy. Moreover, the purpose of the stack policy is to prevent accidental changes to the resources being created by the CloudFormation template which is not the requirement. So A&C is correct.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 232349,
          "date": "Sun 10 Oct 2021 00:53",
          "username": "jackdryan",
          "content": "I'll go with A,C",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#598",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A media company has a static web application that is generated programmatically. The company has a build pipeline that generates HTML content that is uploaded to an Amazon S3 bucket served by Amazon CloudFront. The build pipeline runs inside a Build Account. The S3 bucket and CloudFront distribution are in a Distribution Account. The build pipeline uploads the files to Amazon S3 using an IAM role in the Build Account. The S3 bucket has a bucket policy that only allows CloudFront to read objects using an origin access identity (OAI). During testing all attempts to access the application using the CloudFront URL result in an<br>HTTP 403 Access Denied response.<br>What should a solutions architect suggest to the company to allow access the objects in Amazon S3 through CloudFront?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#598",
          "answers": [
            {
              "choice": "<p>A. Modify the S3 upload process in the Build Account to add the bucket-owner-full-control ACL to the objects at upload.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new cross-account IAM role in the Distribution Account with write access to the S3 bucket. Modify the build pipeline to assume this role to upload the files to the Distribution Account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the S3 upload process in the Build Account to set the object owner to the Distribution Account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new IAM role in the Distribution Account with read access to the S3 bucket. Configure CloudFront to use this new role as its OAI. Modify the build pipeline to assume this role when uploading files from the Build Account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 156437,
          "date": "Sun 19 Sep 2021 19:20",
          "username": "NemerByrneyviet1991",
          "content": "A.  <br>https://aws.amazon.com/tw/premiumsupport/knowledge-center/s3-bucket-owner-access/B: This exact scenario is detailed here: <br>https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-artifacts-s3/<br><br>The cross account role sets the owner as the distribution account.A is right.<br>By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. To get access to the object, the object owner must explicitly grant you (the bucket owner) access.<br><br>aws s3 cp BuildAccountFile s3://DistributionAccountS3/ --acl bucket-owner-full-control",
          "upvote_count": "2114",
          "selected_answers": ""
        },
        {
          "id": 713006,
          "date": "Mon 07 Nov 2022 12:35",
          "username": "Byrney",
          "content": "B: This exact scenario is detailed here: <br>https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-artifacts-s3/<br><br>The cross account role sets the owner as the distribution account.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 378387,
          "date": "Sun 31 Oct 2021 14:43",
          "username": "viet1991",
          "content": "A is right.<br>By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. To get access to the object, the object owner must explicitly grant you (the bucket owner) access.<br><br>aws s3 cp BuildAccountFile s3://DistributionAccountS3/ --acl bucket-owner-full-control",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 241915,
          "date": "Mon 11 Oct 2021 16:06",
          "username": "darthvoodoopetebear55tekkart01037oscargee",
          "content": "The answer is definitely A. ..this is one of the questions that always pops up in the security specialty examhttps://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/<br>If B was the issue, you would have been getting a 404 error instead.WELL DONEAt first I thought it was B, then I changed my mind to A. <br>It is not the Pipeline in Build Account which cannot access the object (answer B).<br>It is CloudFront, together with S3 bucket in Distribution Account, activated by OAI, which cannot access the object. Because, by giving cross-account permission, it lost its bucket full control : https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-8<br>I go with AGood pointIt's not A because it means S3 bucket owner asks permission of object. In this case, S3 belongs to CloudFront but accessor is app. So app won't have same permission as Cloud Front. You have to chose B. ",
          "upvote_count": "101111",
          "selected_answers": ""
        },
        {
          "id": 254305,
          "date": "Sat 16 Oct 2021 07:33",
          "username": "petebear55tekkart",
          "content": "WELL DONEAt first I thought it was B, then I changed my mind to A. <br>It is not the Pipeline in Build Account which cannot access the object (answer B).<br>It is CloudFront, together with S3 bucket in Distribution Account, activated by OAI, which cannot access the object. Because, by giving cross-account permission, it lost its bucket full control : https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-8<br>I go with A",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 389914,
          "date": "Mon 01 Nov 2021 06:25",
          "username": "tekkart",
          "content": "At first I thought it was B, then I changed my mind to A. <br>It is not the Pipeline in Build Account which cannot access the object (answer B).<br>It is CloudFront, together with S3 bucket in Distribution Account, activated by OAI, which cannot access the object. Because, by giving cross-account permission, it lost its bucket full control : https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-8<br>I go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 264831,
          "date": "Sun 17 Oct 2021 03:34",
          "username": "01037",
          "content": "Good point",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 363180,
          "date": "Sun 31 Oct 2021 12:04",
          "username": "oscargee",
          "content": "It's not A because it means S3 bucket owner asks permission of object. In this case, S3 belongs to CloudFront but accessor is app. So app won't have same permission as Cloud Front. You have to chose B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696032,
          "date": "Sun 16 Oct 2022 08:15",
          "username": "Dionenonly",
          "content": "Answer is A.  No brainer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 615453,
          "date": "Sun 12 Jun 2022 19:56",
          "username": "CloudHell",
          "content": "My initial instinct was B, but after reading the comments A sounds like a better choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 554932,
          "date": "Thu 24 Feb 2022 00:02",
          "username": "johnnsmith",
          "content": "B is correct. There are three possible Object Ownership settings: (1)Bucket owner enforced: bucket owner always owns the object. That is not the case we have here. (2) Bucket owner preferred. If an object upload includes the bucket-owner-full-control canned ACL, the bucket owner owns the object. Objects uploaded with other ACLs are owned by the writing account. Answer A would only work with this setting. But we don't if the bucket used this setting. (3) Object writer: Object writer owns the object. Answer B works regardless of the Object Ownership setting of the bucket. Details at https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 497329,
          "date": "Thu 09 Dec 2021 05:45",
          "username": "cldy",
          "content": "A.  Modify the S3 upload process in the Build Account to add the bucket-owner-full-control ACL to the objects at upload.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494903,
          "date": "Mon 06 Dec 2021 05:55",
          "username": "AzureDP900",
          "content": "A is fine",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437016,
          "date": "Fri 05 Nov 2021 12:30",
          "username": "student22",
          "content": "A<br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 433142,
          "date": "Fri 05 Nov 2021 10:49",
          "username": "blackgamer",
          "content": "It definitely is A.  This document explains it - <br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html#object-ownership-replication",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 412600,
          "date": "Thu 04 Nov 2021 10:50",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 403166,
          "date": "Tue 02 Nov 2021 05:49",
          "username": "SJain50",
          "content": "B<br>https://aws.amazon.com/premiumsupport/knowledge-center/codepipeline-artifacts-s3/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350957,
          "date": "Sat 30 Oct 2021 08:20",
          "username": "Waiweng",
          "content": "it's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 308309,
          "date": "Wed 27 Oct 2021 18:32",
          "username": "kiev",
          "content": "Neal Davis went for B.  I am going for my exam next week. I am lost now. I know both both A and B works",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 296802,
          "date": "Wed 27 Oct 2021 10:46",
          "username": "ele",
          "content": "A is correct. By assuming cross-account role, the pipeline would give up any permissions in Build account, that it might need to complete build actions. So it must keep with its own role and the answer is A. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 292592,
          "date": "Wed 27 Oct 2021 01:46",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 269319,
          "date": "Wed 20 Oct 2021 12:41",
          "username": "EbiEbi",
          "content": "I will go with AActually A and B both are correct answer, but A is more straight forward<br>One of those questions from AWS which evaluates ability to pick the BEST answer not onlythe right one",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 285438,
          "date": "Sat 23 Oct 2021 23:55",
          "username": "Ebi",
          "content": "Actually A and B both are correct answer, but A is more straight forward<br>One of those questions from AWS which evaluates ability to pick the BEST answer not onlythe right one",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 265525,
          "date": "Mon 18 Oct 2021 19:06",
          "username": "01037",
          "content": "A. <br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-owner-access/",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#599",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in<br>Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1,000 EC2 instances, overall performance was well below expectations.<br>Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#599",
          "answers": [
            {
              "choice": "<p>A. Ensure the HPC cluster is launched within a single Availability Zone.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Launch the EC2 instances and attach elastic network interfaces in multiples of four.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Ensure the clusters is launched across multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Replace Amazon EFS win multiple Amazon EBS volumes in a RAID array.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Replace Amazon EFS with Amazon FSx for Lustre.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 156754,
          "date": "Mon 04 Oct 2021 02:22",
          "username": "easytooblackgamer",
          "content": "A.  High performance computing (HPC) workload cluster should be in a single AZ.<br>C.  Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instances to accelerate High Performance Computing (HPC)<br>F.  Amazon FSx for Lustre - Use it for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.<br><br>Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.htmlYes, definitely ACF.  Well explained.",
          "upvote_count": "412",
          "selected_answers": ""
        },
        {
          "id": 433144,
          "date": "Sat 30 Oct 2021 08:43",
          "username": "blackgamer",
          "content": "Yes, definitely ACF.  Well explained.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 269329,
          "date": "Sat 23 Oct 2021 09:17",
          "username": "Ebi",
          "content": "I will go with ACF",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 653346,
          "date": "Mon 29 Aug 2022 09:38",
          "username": "gnic",
          "content": "ACF<br>A - for network perfomance (single AZ is better than multiple AZ because the latency)<br>C- EFA no brain for HPC<br>F - EFS Lustre is for HPC, no brain too",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 575341,
          "date": "Sat 26 Mar 2022 03:31",
          "username": "ka1tw",
          "content": "Why NOT B with multiple ENI?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 521758,
          "date": "Tue 11 Jan 2022 21:29",
          "username": "Ni_yot",
          "content": "Yes ACF. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514385,
          "date": "Sat 01 Jan 2022 06:35",
          "username": "cldy",
          "content": "A. C. F.  <br>EFA + FSx Lustre + single AZ.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 502248,
          "date": "Wed 15 Dec 2021 15:36",
          "username": "Tan0k",
          "content": "ACF got to be",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 494904,
          "date": "Mon 06 Dec 2021 05:56",
          "username": "AzureDP900",
          "content": "ACF is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491088,
          "date": "Wed 01 Dec 2021 01:33",
          "username": "acloudguru",
          "content": "ACF is good.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 412602,
          "date": "Sat 30 Oct 2021 03:09",
          "username": "WhyIronMan",
          "content": "I'll go with A,C,F",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 350959,
          "date": "Thu 28 Oct 2021 21:04",
          "username": "Waiweng",
          "content": "it's A,C,F",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 292593,
          "date": "Sun 24 Oct 2021 13:28",
          "username": "Kian1",
          "content": "going with ACF",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 244672,
          "date": "Thu 21 Oct 2021 01:47",
          "username": "T14102020",
          "content": "Correct ACF.  Elastic Fabric Adapter + FSx Lustre for HPC + single AZ",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 232368,
          "date": "Tue 19 Oct 2021 04:54",
          "username": "jackdryan",
          "content": "I'll go with A,C,F",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 229549,
          "date": "Mon 18 Oct 2021 07:03",
          "username": "oopsy",
          "content": "seems ACF",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 208620,
          "date": "Mon 18 Oct 2021 02:44",
          "username": "CYL",
          "content": "ACF to enhance on networking and file system level optimization.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 205940,
          "date": "Fri 15 Oct 2021 15:05",
          "username": "Bulti",
          "content": "ACF is the right answer",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    }
  ]
}
