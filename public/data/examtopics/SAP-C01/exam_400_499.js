var ExamTopic_400_499 = {
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": "#400",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses Amazon S3 to store documents that may only be accessible to an Amazon EC2 instance in a certain virtual private cloud (VPC). The company fears that a malicious insider with access to this instance could also set up an EC2 instance in another VPC to access these documents.<br>Which of the following solutions will provide the required protection?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#400",
          "answers": [
            {
              "choice": "<p>A. Use an S3 VPC endpoint and an S3 bucket policy to limit access to this VPC endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use EC2 instance profiles and an S3 bucket policy to limit access to the role attached to the instance profile.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use S3 client-side encryption and store the key in the instance metadata.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use S3 server-side encryption and protect the key with an encryption context.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13934,
          "date": "Fri 24 Sep 2021 02:38",
          "username": "manhmaluc",
          "content": "A true<br><br>https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html<br>```<br>Endpoint connections cannot be extended out of a VPC.  Resources on the other side of a VPN connection, VPC peering connection, AWS Direct Connect connection, or ClassicLink connection in your VPC cannot use the endpoint to communicate with resources in the endpoint service.<br>```",
          "upvote_count": "37",
          "selected_answers": ""
        },
        {
          "id": 281178,
          "date": "Wed 27 Oct 2021 11:50",
          "username": "Ebi",
          "content": "I go with A",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 727456,
          "date": "Sat 26 Nov 2022 12:12",
          "username": "evargasbrz",
          "content": "A - VPC EndPoint+ S3 Policy with condition<br>The main objective here is to prevent a malicious insider set up an EC2 instance in \\\"another\\\" VPC. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708398,
          "date": "Mon 31 Oct 2022 13:18",
          "username": "resnef",
          "content": "Answer is A, Stephane's discussion in Udemy about exact same scenario.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700453,
          "date": "Fri 21 Oct 2022 03:36",
          "username": "Cloud_noob",
          "content": "I go with A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 653593,
          "date": "Mon 29 Aug 2022 20:30",
          "username": "epomatti",
          "content": "A - VPC Endpoint + S3 Policy",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 636816,
          "date": "Mon 25 Jul 2022 18:14",
          "username": "hilft",
          "content": "A.  Use endpoint.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626435,
          "date": "Sun 03 Jul 2022 07:49",
          "username": "aandc",
          "content": "keyword \\\"in another VPC\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 581119,
          "date": "Tue 05 Apr 2022 10:41",
          "username": "bfalbfal",
          "content": "Answer is B.  You access from another VPC, you can either access through the internet or use S3 VPC endpoints. so in this case, S3 VPC endpoints would not come into the equation, as you won't need to create one, unless if you want access to the S3 buckets via AWS backbone, so any answer with S3 vpc endpoints is incorrectit says in \\\"another\\\"vpc, not the same VPC",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 581123,
          "date": "Tue 05 Apr 2022 10:42",
          "username": "bfal",
          "content": "it says in \\\"another\\\"vpc, not the same VPC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 559001,
          "date": "Tue 01 Mar 2022 21:29",
          "username": "Ni_yot",
          "content": "A for me. since its within a VPC access, seems logical to use a VPC endpoint and ACL to control access",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532702,
          "date": "Wed 26 Jan 2022 09:33",
          "username": "shotty1",
          "content": "While in real life I would always suggest first looking into B, the customers concern in this question will be addressed by A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 498282,
          "date": "Fri 10 Dec 2021 05:23",
          "username": "fais1985",
          "content": "Correct Answer is A,<br>B looks correct, but the roles are global & they can be attached to the ec2 in different VPC as well that will rule out option B. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 458665,
          "date": "Thu 04 Nov 2021 02:27",
          "username": "chaconerw",
          "content": "The correct answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 449843,
          "date": "Tue 02 Nov 2021 06:37",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 362458,
          "date": "Sat 30 Oct 2021 08:57",
          "username": "Radhaghosh",
          "content": "A is the most secured answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 347261,
          "date": "Sat 30 Oct 2021 03:17",
          "username": "macshild",
          "content": "The correct Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 334006,
          "date": "Thu 28 Oct 2021 11:44",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#401",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>The Solutions Architect manages a serverless application that consists of multiple API gateways, AWS Lambda functions, Amazon S3 buckets, and Amazon<br>DynamoDB tables. Customers say that a few application components slow while loading dynamic images, and some are timing out with the `504 Gateway<br>Timeout` error. While troubleshooting the scenario, the Solutions Architect confirms that DynamoDB monitoring metrics are at acceptable levels.<br>Which of the following steps would be optimal for debugging these application issues? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#401",
          "answers": [
            {
              "choice": "<p>A. Parse HTTP logs in Amazon API Gateway for HTTP errors to determine the root cause of the errors.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Parse Amazon CloudWatch Logs to determine processing times for requested images at specified intervals.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Parse VPC Flow Logs to determine if there is packet loss between the Lambda function and S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Parse AWS X-Ray traces and analyze HTTP methods to determine the root cause of the HTTP errors.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Parse S3 access logs to determine if objects being accessed are from specific IP addresses to narrow the scope to geographic latency issues.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 31366,
          "date": "Sat 25 Sep 2021 01:55",
          "username": "LunchTimestudent22student22",
          "content": "B and D are correct.<br>Firstly “A 504 Gateway Timeout Error means your web server didn't receive a timely response from another server upstream when it attempted to load one of your web pages. Put simply, your web servers aren't communicating with each other fast enough”.<br>This specific issue is addressed in the AWS article “Tracing, Logging and Monitoring an API Gateway API”. https://docs.amazonaws.cn/en_us/apigateway/latest/developerguide/monitoring_overview.html<br>The article specifically discusses using AWS X-Ray, AWS CloudTrail and AWS CloudWatch as the tools to utilized for debugging in this scenario.<br>The two options that encompass using AWS CloudWatch and AWS X-Ray are B and D respectively. AWS CloudTrail is not mentioned in any of the answers.Also, http errors checked by A is already covered by D.  Another reason to select B over A. Answer B,D",
          "upvote_count": "3521",
          "selected_answers": ""
        },
        {
          "id": 452313,
          "date": "Sun 07 Nov 2021 00:45",
          "username": "student22student22",
          "content": "Also, http errors checked by A is already covered by D.  Another reason to select B over A. Answer B,D",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 452315,
          "date": "Sun 07 Nov 2021 10:16",
          "username": "student22",
          "content": "Answer B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 11341,
          "date": "Mon 20 Sep 2021 14:09",
          "username": "sb333",
          "content": "The correct answers are BD. <br>https://docs.aws.amazon.com/en_pv/apigateway/latest/developerguide/monitoring_overview.html",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 577797,
          "date": "Tue 29 Mar 2022 19:53",
          "username": "jj22222",
          "content": "b and d look right",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 491223,
          "date": "Wed 01 Dec 2021 04:47",
          "username": "AzureDP900",
          "content": "B,D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450871,
          "date": "Sat 06 Nov 2021 22:30",
          "username": "moon2351",
          "content": "Answer is B＆D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 449846,
          "date": "Fri 05 Nov 2021 23:32",
          "username": "andylogan",
          "content": "It's B D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 347267,
          "date": "Fri 05 Nov 2021 21:35",
          "username": "macshild",
          "content": "The correct answer is BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 334022,
          "date": "Fri 05 Nov 2021 02:08",
          "username": "WhyIronMan",
          "content": "I'll go with B,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 323097,
          "date": "Sat 30 Oct 2021 21:19",
          "username": "AJBA",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-504-errors/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289349,
          "date": "Sat 30 Oct 2021 01:37",
          "username": "Kian1",
          "content": "Ans B,D for me",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 283540,
          "date": "Thu 28 Oct 2021 17:09",
          "username": "bnagaraja9099",
          "content": "D for sure.X-Ray is basic for performance monitoring. <br>Between A and B, 504 is the gateway timeout returned by the API gateway. But B talks about processing time in other components which most likely is what caused the time out. So I vote for B.  B& D<br>B & D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281179,
          "date": "Wed 27 Oct 2021 15:23",
          "username": "Ebi",
          "content": "Answer is BD<br>E is irrelevant",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 258164,
          "date": "Thu 21 Oct 2021 06:13",
          "username": "SachinJha",
          "content": "B & D is the correct answer.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242232,
          "date": "Tue 19 Oct 2021 23:14",
          "username": "gookseang",
          "content": "I will go B&D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 241848,
          "date": "Tue 19 Oct 2021 10:19",
          "username": "T14102020",
          "content": "Correct answer is BD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 231294,
          "date": "Sat 16 Oct 2021 22:17",
          "username": "petebear55",
          "content": "B AND D I WOULD GO WITH https://docs.amazonaws.cn/en_us/apigateway/latest/developerguide/monitoring_overview.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 227974,
          "date": "Wed 13 Oct 2021 14:26",
          "username": "jackdryan",
          "content": "I'll go with B,D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#402",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing the storage layer for a recently purchased application. The application will be running on Amazon EC2 instances and has the following layers and requirements:<br>✑ Data layer: A POSIX file system shared across many systems.<br>✑ Service layer: Static file content that requires block storage with more than 100k IOPS.<br>Which combination of AWS services will meet these needs? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#402",
          "answers": [
            {
              "choice": "<p>A. Data layer ג€\" Amazon S3<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Data layer ג€\" Amazon EC2 Ephemeral Storage<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Data layer ג€\" Amazon EFS<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Service layer ג€\" Amazon EBS volumes with Provisioned IOPS<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Service layer ג€\" Amazon EC2 Ephemeral Storage<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11358,
          "date": "Mon 20 Sep 2021 01:18",
          "username": "sb333",
          "content": "Could this be CE? EBS with PIOPS has max of 80,000 IOPS/instance. Ephemeral storage is > 100,000 IOPS/instance. The question doesn't really mention other requirements other than static file content, which doesn't mean durable or temporary. AWS site says Ephemeral Storage is ideal \\\"for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.\\\"Without more specifics, I would think C&E fit the bill of actual requirements of the question. Trying not to think of what I would use personally. https://docs.aws.amazon.com/en_pv/AWSEC2/latest/UserGuide/InstanceStorage.html",
          "upvote_count": "37",
          "selected_answers": ""
        },
        {
          "id": 185112,
          "date": "Wed 06 Oct 2021 20:31",
          "username": "blaubeeWillCloudGopiSivanathanporlarowl",
          "content": "C,D<br>Provisioned IOPS has max of 160,000 IOPS/instance. 80,000 IOPS/instance is for previous generation volume type.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.htmlCE.  <br><br>1. Only io2 Block Express in Provisioned IOPS SSD can support 100K iops. io1 & io2 can only support up-to 64K iops.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html<br><br>2. io2 Block Express volumes are supported with R5b instances only.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html<br><br>3. The new R5b instance is powered by the AWS Nitro System to provide the best network-attached storage performance available on EC2. And the minimum requirement for 100k iops is r5b.12xlarge, which is 48 vCPUs and 384 GiB, too expensive and not realistic for most cases.<br>https://aws.amazon.com/blogs/aws/new-amazon-ec2-r5b-instances-providing-3x-higher-ebs-performance/<br><br>4. Instance store can provide high iops with much lower cost. E. g. i3.2xlarge can provide 180k write IOPS, which has 8 vCPU and 61GiB.  <br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html#i2-instances-diskperf<br>https://aws.amazon.com/ec2/instance-types/i3/D doesn't seems to be answerfor Provisioned IOPS SSD (io2)<br>16384 : Size (GiB) (Min: 4 GiB, Max: 16384 GiB) <br>IOPS: 64000(Min: 100 IOPS, Max: 64000 IOPS) <br>Volumes with greater than 32000 IOPS must be attached to a Nitro based instance to achieve p<br>rovisioned performance.I agree with you.<br>also mentioned EBS volume\\\"s\\\" on D",
          "upvote_count": "11521",
          "selected_answers": ""
        },
        {
          "id": 446160,
          "date": "Tue 02 Nov 2021 09:19",
          "username": "WillCloud",
          "content": "CE.  <br><br>1. Only io2 Block Express in Provisioned IOPS SSD can support 100K iops. io1 & io2 can only support up-to 64K iops.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html<br><br>2. io2 Block Express volumes are supported with R5b instances only.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html<br><br>3. The new R5b instance is powered by the AWS Nitro System to provide the best network-attached storage performance available on EC2. And the minimum requirement for 100k iops is r5b.12xlarge, which is 48 vCPUs and 384 GiB, too expensive and not realistic for most cases.<br>https://aws.amazon.com/blogs/aws/new-amazon-ec2-r5b-instances-providing-3x-higher-ebs-performance/<br><br>4. Instance store can provide high iops with much lower cost. E. g. i3.2xlarge can provide 180k write IOPS, which has 8 vCPU and 61GiB.  <br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html#i2-instances-diskperf<br>https://aws.amazon.com/ec2/instance-types/i3/",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 211707,
          "date": "Sat 09 Oct 2021 14:32",
          "username": "GopiSivanathan",
          "content": "D doesn't seems to be answerfor Provisioned IOPS SSD (io2)<br>16384 : Size (GiB) (Min: 4 GiB, Max: 16384 GiB) <br>IOPS: 64000(Min: 100 IOPS, Max: 64000 IOPS) <br>Volumes with greater than 32000 IOPS must be attached to a Nitro based instance to achieve p<br>rovisioned performance.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 204884,
          "date": "Fri 08 Oct 2021 04:36",
          "username": "porlarowl",
          "content": "I agree with you.<br>also mentioned EBS volume\\\"s\\\" on D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700224,
          "date": "Thu 20 Oct 2022 21:07",
          "username": "mrgreatness",
          "content": "Answer is c and D.  With Ephermal storage ( instance store) the data will be lost if instance stops for example. CD is 100% right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696586,
          "date": "Sun 16 Oct 2022 23:15",
          "username": "ashii007",
          "content": "C,D. <br><br>Provisioned IOPS can support uto 260K IOPS<br>https://aws.amazon.com/ebs/features/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 695808,
          "date": "Sun 16 Oct 2022 02:32",
          "username": "mrgreatness",
          "content": "It says FIXED storage! Ephermanl will be lost if instance is stopped. C & D for me",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 670714,
          "date": "Fri 16 Sep 2022 12:55",
          "username": "jujumomma",
          "content": "C, E<br>C.  EFS - POSIX file system ([https://docs.aws.amazon.com/ko_kr/efs/latest/ug/creating-using.html](https://docs.aws.amazon.com/ko_kr/efs/latest/ug/creating-using.html))<br>D.  Amazon EBS volumes with Provisioned IOPS is 64K. ([https://aws.amazon.com/ko/ebs/provisioned-iops/](https://aws.amazon.com/ko/ebs/provisioned-iops/))<br><br>E.  [https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/storage-optimized-instances.html](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/storage-optimized-instances.html)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 625139,
          "date": "Thu 30 Jun 2022 10:58",
          "username": "TechXTechX",
          "content": "Cause the question not mention to be HA, so we could choose C over D.  With D, we only have max IOPs 160k with INSTANCE, while with VOLUMN IOPs maximum is 64kmy typo, we could choose E over D*",
          "upvote_count": "31",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 625140,
          "date": "Thu 30 Jun 2022 10:59",
          "username": "TechX",
          "content": "my typo, we could choose E over D*",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 577820,
          "date": "Tue 29 Mar 2022 20:21",
          "username": "jj22222",
          "content": "CE look right",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 513112,
          "date": "Thu 30 Dec 2021 08:39",
          "username": "cldy",
          "content": "C and E. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 452680,
          "date": "Sun 07 Nov 2021 16:54",
          "username": "student22",
          "content": "C,E<br>---",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 449897,
          "date": "Wed 03 Nov 2021 06:04",
          "username": "andylogan",
          "content": "It's C E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 438828,
          "date": "Sun 31 Oct 2021 21:49",
          "username": "AWS_Noob",
          "content": "Going forward it might be C & D <br>Block express is GA now and no longer under preview <br><br>https://aws.amazon.com/about-aws/whats-new/2021/07/aws-announces-general-availability-amazon-ebs-block-express-volumes/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 423043,
          "date": "Sun 31 Oct 2021 04:09",
          "username": "walkwolf3walkwolf3",
          "content": "The original answer is CE, but now D also meets requirement. So the answers are either CD or CE.  If it appears in the exam, I will go with CE. Ephemeral storage<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html<br><br>EBS volume<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 423045,
          "date": "Sun 31 Oct 2021 21:26",
          "username": "walkwolf3",
          "content": "Ephemeral storage<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html<br><br>EBS volume<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 418651,
          "date": "Sat 30 Oct 2021 18:13",
          "username": "jobe42",
          "content": "C and D (IOPS in RAID)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 405617,
          "date": "Sat 30 Oct 2021 09:05",
          "username": "RichMnz",
          "content": "C - Yes. EFS supports Posix, and designed for multiple instances.<br>D? -No. EBS max IOPS is 64k per volume, io2 Block Express is only available in preview. <br>E - Yes. Ephemeral storage is also block storage and meets the IOPS requirement.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 387704,
          "date": "Thu 28 Oct 2021 18:09",
          "username": "KopaDeathFrmAbvstudent2020",
          "content": "Im going for CD, i dont understand why should be E while you can choose Provisioned IO2<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.htmlYup you are correct, io2 Block Express can now go upto 256,000 IOPS<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.htmlio2 Block Express is a feature in preview. AWS does not test on features that are still in preview.",
          "upvote_count": "322",
          "selected_answers": ""
        },
        {
          "id": 395600,
          "date": "Fri 29 Oct 2021 22:50",
          "username": "DeathFrmAbvstudent2020",
          "content": "Yup you are correct, io2 Block Express can now go upto 256,000 IOPS<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.htmlio2 Block Express is a feature in preview. AWS does not test on features that are still in preview.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 402884,
          "date": "Sat 30 Oct 2021 02:19",
          "username": "student2020",
          "content": "io2 Block Express is a feature in preview. AWS does not test on features that are still in preview.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 382071,
          "date": "Mon 25 Oct 2021 15:10",
          "username": "wem",
          "content": "I think some of you need to read up on what ephemeral storage is<br>answer is C&D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#403",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>[1]<br>[1]<br>are accessed frequently for the first 15 to 20 days, they are seldom accessed thereafter but always need to be immediately available. The CIO has asked to find ways to reduce costs.<br>Which of the following options will reduce costs? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#403",
          "answers": [
            {
              "choice": "<p>A. Purchase Reserved instances for baseline capacity requirements and use On-Demand instances for the demand spikes. [1]<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use On-Demand instances for baseline capacity requirements and use Spot Fleet instances for the demand spikes. [1]<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a script that checks the load on all web servers and terminates unnecessary On-Demand instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13864,
          "date": "Tue 21 Sep 2021 16:16",
          "username": "awsgcpazuredonathon01037",
          "content": "A company has an application that runs a web service on Amazon EC2 instances and stores .jpg images in Amazon S3. The web traffic has a predictable baseline, but often demand spikes unpredictably for short periods of time. The application is loosely coupled and stateless. The .jpg images stored in Amazon S3 are accessed frequently for the first 15 to 20 days, they are seldom accessed thereafter but always need to be immediately available. The CIO has asked to find ways to reduce costs.<br><br>Which of the following options will reduce costs? (Choose two.)<br><br>A.  Purchase Reserved instances for baseline capacity requirements and use On-Demand instances for the demand spikes.<br>B.  Configure a lifecycle policy to move the .jpg images on Amazon S3 to S3 IA after 30 days.<br>C.  Use On-Demand instances for baseline capacity requirements and use Spot Fleet instances for the demand spikes.<br>D.  Configure a lifecycle policy to move the .jpg images on Amazon S3 to Amazon Glacier after 30 days<br>E.  Create a script that checks the load on all web servers and terminates unnecessary On-Demand instances.<br><br>My choice is ABAB<br>A: This make a lot of sense<br>B\\D: This would allow the files to be immediately accessible still vs Glacier.<br>C: Spot instance is not guaranteed so is not suitable.<br>E: This should be done on CloudWatch.Yes it is AB<br><br>But depending on how seldom and how immediately it requires, Glacier could be an option",
          "upvote_count": "7271",
          "selected_answers": ""
        },
        {
          "id": 14094,
          "date": "Wed 22 Sep 2021 13:06",
          "username": "donathon",
          "content": "AB<br>A: This make a lot of sense<br>B\\D: This would allow the files to be immediately accessible still vs Glacier.<br>C: Spot instance is not guaranteed so is not suitable.<br>E: This should be done on CloudWatch.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 385851,
          "date": "Wed 03 Nov 2021 19:51",
          "username": "01037",
          "content": "Yes it is AB<br><br>But depending on how seldom and how immediately it requires, Glacier could be an option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 680043,
          "date": "Mon 26 Sep 2022 20:09",
          "username": "Ni_yot",
          "content": "Its A and B. The other just dont fit the bill as an answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 657929,
          "date": "Sat 03 Sep 2022 02:33",
          "username": "AYANtheGLADIATOR",
          "content": "A and B for sure there is no other option suitable.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513115,
          "date": "Thu 30 Dec 2021 08:42",
          "username": "cldy",
          "content": "A and B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497638,
          "date": "Thu 09 Dec 2021 11:55",
          "username": "cldy",
          "content": "A.  Purchase Reserved instances for baseline capacity requirements and use On-Demand instances for the demand spikes.<br>B.  Configure a lifecycle policy to move the .jpg images on Amazon S3 to S3 IA after 30 days",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449900,
          "date": "Sat 06 Nov 2021 09:37",
          "username": "andylogan",
          "content": "It's A B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 376660,
          "date": "Sun 31 Oct 2021 15:20",
          "username": "tekkarttekkart",
          "content": "At first, A,B<br>Then the question \\\"how to stop the on-demand instances if the spikes go unpredictably\\\", ,plus the images need always to be immediately available. Then, the answer may be A,ES3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (similar to the S3 Standard storage class)-> A,B",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 376765,
          "date": "Wed 03 Nov 2021 04:17",
          "username": "tekkart",
          "content": "S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (similar to the S3 Standard storage class)-> A,B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 334650,
          "date": "Sun 31 Oct 2021 09:20",
          "username": "WhyIronMan",
          "content": "I'll go with A,B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290691,
          "date": "Fri 29 Oct 2021 03:11",
          "username": "wind",
          "content": "go with AB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289373,
          "date": "Thu 28 Oct 2021 22:05",
          "username": "Kian1",
          "content": "Ans A,B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281196,
          "date": "Thu 28 Oct 2021 22:00",
          "username": "Ebi",
          "content": "my answer is AB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 269082,
          "date": "Tue 26 Oct 2021 13:21",
          "username": "kopper2019",
          "content": "A and B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 262844,
          "date": "Mon 25 Oct 2021 18:29",
          "username": "sanjaym",
          "content": "AB for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 258167,
          "date": "Mon 25 Oct 2021 17:44",
          "username": "SachinJha",
          "content": "I will go with AB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 252692,
          "date": "Sat 23 Oct 2021 11:48",
          "username": "sarofi",
          "content": "predictable baseline -> reserved (cheaper than on-demand) -> A<br>unpredictable for short periods -> on demand -> A<br>infrequent access after 20 days but needs immediately available -> S3 to S3 IA -> B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242234,
          "date": "Fri 22 Oct 2021 04:51",
          "username": "gookseang",
          "content": "AB for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 235529,
          "date": "Fri 22 Oct 2021 03:26",
          "username": "newmesarofi",
          "content": "Reserved and Spot instances are the best combination.<br>It's hard to say which is better, A or C. <br>Depends on how often demand spikes happen and how many instances are needed at the time, and of course how long the company is going to run the website.Reserved is cheaper than on-demand.. predictable baseline -> reserved for baseline. unpredictable spikes -> on-demand or spot both works. Considering the \\\"traffic unpredictably for short periods of time\\\" overall cost should be A cheaper than C and more durable (plus)",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 252690,
          "date": "Fri 22 Oct 2021 18:29",
          "username": "sarofi",
          "content": "Reserved is cheaper than on-demand.. predictable baseline -> reserved for baseline. unpredictable spikes -> on-demand or spot both works. Considering the \\\"traffic unpredictably for short periods of time\\\" overall cost should be A cheaper than C and more durable (plus)",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#404",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A hybrid network architecture must be used during a company's multi-year data center migration from multiple private data centers to AWS. The current data centers are linked together with private fiber. Due to unique legacy applications, NAT cannot be used. During the migration period, many applications will need access to other applications in both the data centers and AWS.<br>Which option offers a hybrid network architecture that is secure and highly available, that allows for high bandwidth and a multi-region deployment post-migration?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#404",
          "answers": [
            {
              "choice": "<p>A. Use AWS Direct Connect to each data center from different ISPs, and configure routing to failover to the other data center's Direct Connect if one fails. Ensure that no VPC CIDR blocks overlap one another or the on-premises network.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use multiple hardware VPN connections to AWS from the on-premises data center. Route different subnet traffic through different VPN connections. Ensure that no VPC CIDR blocks overlap one another or the on-premises network.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use a software VPN with clustering both in AWS and the on-premises data center, and route traffic through the cluster. Ensure that no VPC CIDR blocks overlap one another or the on-premises network.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Direct Connect and a VPN as backup, and configure both to use the same virtual private gateway and BGP. Ensure that no VPC CIDR blocks overlap one another or the on-premises network.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 75479,
          "date": "Sat 02 Oct 2021 03:36",
          "username": "JoeyleeLunchTimerb39rchermacshild",
          "content": "Due to unique legacy applications, NAT cannot be used<br>Means outbound connection to internet is not possible, so VPN is not possible.<br><br>Correct answer has to be AGreat point.A is correct - reliable with high bandwith means failover to another DX, VPN as secondary would reduce itThis have to be upvoted more.D will not be possible with VPN as a backup that required an internet interface (IPSec)A VPN connection between AWS and on prem networks doesn't utiize the NAT gate it done through Transit Gateway with VPN extentions or Virtual Private Gateway, but then again your answer is correctjust letting the masses no NAT gateway is not involved in VPN on the AWS side",
          "upvote_count": "611113",
          "selected_answers": ""
        },
        {
          "id": 116701,
          "date": "Wed 06 Oct 2021 01:14",
          "username": "LunchTime",
          "content": "Great point.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 500110,
          "date": "Sun 12 Dec 2021 17:12",
          "username": "rb39",
          "content": "A is correct - reliable with high bandwith means failover to another DX, VPN as secondary would reduce it",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 238069,
          "date": "Sun 17 Oct 2021 12:20",
          "username": "rcher",
          "content": "This have to be upvoted more.D will not be possible with VPN as a backup that required an internet interface (IPSec)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 347277,
          "date": "Thu 28 Oct 2021 20:12",
          "username": "macshild",
          "content": "A VPN connection between AWS and on prem networks doesn't utiize the NAT gate it done through Transit Gateway with VPN extentions or Virtual Private Gateway, but then again your answer is correctjust letting the masses no NAT gateway is not involved in VPN on the AWS side",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 13517,
          "date": "Tue 21 Sep 2021 14:17",
          "username": "donathon",
          "content": "A<br>https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/<br>A: This is the best way for HA and high bandwidth.<br>B\\C: VPN even if it is redundant does not allow high bandwidth where it has a limit of 1.25Gbps. https://aws.amazon.com/vpn/faqs/<br>D: This is not multi-region and not as highly available. Remember VPN has only 1.25Gbps.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 714982,
          "date": "Thu 10 Nov 2022 07:49",
          "username": "janvandermerwer",
          "content": "A - seems to be the \\\"best\\\" option.<br>Unable to use NAT i.e VPN- So that rules out B, C, D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 712220,
          "date": "Sun 06 Nov 2022 10:16",
          "username": "Ni_yot",
          "content": "A good choice here. if you cant use NAT then Direct connect is the best solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 686238,
          "date": "Tue 04 Oct 2022 16:09",
          "username": "JayF88",
          "content": "A makes more sense, VPN solution on D not possible",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 670766,
          "date": "Fri 16 Sep 2022 13:46",
          "username": "jujumomma",
          "content": "Ans: A<br>D is wrong. <br>https://aws.amazon.com/directconnect/faqs/?nc1=h_ls<br>Q: Can I use AWS Site-to-Site VPN as a backup for my AWS Direct Connect link to an AWS Local Zone?<br><br>No. Unlike connectivity to a Region, you cannot use an AWS Site-to-Site VPN as a backup to your AWS Direct Connect connection to an AWS Local Zone. For redundancy, you must use two or more AWS Direct Connect connections.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 668219,
          "date": "Tue 13 Sep 2022 17:52",
          "username": "bihani",
          "content": "I agree on option A.  There is no mention of cost reduction in the question so using a secondary connect direct connection will be better",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 624492,
          "date": "Wed 29 Jun 2022 09:46",
          "username": "Serial_X25",
          "content": "A is correct - reliable with high bandwidth means failover to another DX, VPN as secondary would reduce it.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 523153,
          "date": "Thu 13 Jan 2022 22:57",
          "username": "[Removed]",
          "content": "D is incorrect, you cannot attach a direct connect to a Virtual Private Gateway, you need a Direct Connect Gateway.<br>A is the only viable option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513122,
          "date": "Thu 30 Dec 2021 08:51",
          "username": "cldy",
          "content": "A: CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491268,
          "date": "Wed 01 Dec 2021 05:46",
          "username": "backfringe",
          "content": "I'd with A cause the question says multi region",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 482994,
          "date": "Sun 21 Nov 2021 04:25",
          "username": "acloudguru",
          "content": "Due to unique legacy applications, NAT cannot be used<br>Means outbound connection to internet is not possible, so VPN is not possible.<br><br>Correct answer has to be A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 449901,
          "date": "Fri 05 Nov 2021 21:10",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440623,
          "date": "Thu 04 Nov 2021 09:18",
          "username": "StelSen",
          "content": "Requirement is: Secure, Highly available, High bandwidth and a multi-region deployment post-migration<br>Option-A: Fulfils last 3 req<br>Option-B: Partial security, Less bandwidth, Not HA, Can meet Multi-Region.<br><br>So, I will go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 436573,
          "date": "Wed 03 Nov 2021 05:24",
          "username": "denccc",
          "content": "It's A, are linked together with private fiber points to DX",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 426578,
          "date": "Mon 01 Nov 2021 17:16",
          "username": "FERIN_01",
          "content": "D.  Make more sense as it is multi region deployment you need Virtual private Gateway to connect with multiple VPC of different region at AWS side",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 402891,
          "date": "Sun 31 Oct 2021 22:19",
          "username": "student2020",
          "content": "A and D mention using DX which is not secure, traffic in DX is not encrypted. Option C is better as it ticks all the requirements. VPN is secure, it uses instances so the throughput is limited by the instance type (not 1.25mbps for AWS VPN). It can also scale to different regions by just creating a new EC2 VPN cluster in the new region",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#405",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is currently running a production workload on AWS that is very I/O intensive. Its workload consists of a single tier with 10 c4.8xlarge instances, each with 2 TB gp2 volumes. The number of processing jobs has recently increased, and latency has increased as well. The team realizes that they are constrained on the IOPS. For the application to perform efficiently, they need to increase the IOPS by 3,000 for each of the instances.<br>Which of the following designs will meet the performance goal MOST cost effectively?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#405",
          "answers": [
            {
              "choice": "<p>A. Change the type of Amazon EBS volume from gp2 to io1 and set provisioned IOPS to 9,000.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Increase the size of the gp2 volumes in each instance to 3 TB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new Amazon EFS file system and move all the data to this new file system. Mount this file system to all 10 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new Amazon S3 bucket and move all the data to this new bucket. Allow each instance to access this S3 bucket and use it for storage.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13346,
          "date": "Wed 22 Sep 2021 19:12",
          "username": "donathonMoon01037",
          "content": "B<br>A: IO1 or provisioned IOPS SSD is more expensive.<br>B: GP1 with 2TB volumes has 6000 IOPS. If we add additional 1TB it will increase by another 3000 IOPS. Note: Bursting and I/O credits are only relevant to volumes under 1,000 GiB, where burst performance exceeds baseline performance.<br>C\\D: Not enough information is actually provided for us so we don’t know if the instance can use shared storage or not.Answer \\\"B\\\", well explained.Thanks for the explanation.<br><br>https://aws.amazon.com/ebs/pricing/<br><br>gp2<br>$0.10 per GB-month of provisioned storage<br>io1<br>$0.125 per GB-month of provisioned storage AND $0.065 per provisioned IOPS-month",
          "upvote_count": "5031",
          "selected_answers": ""
        },
        {
          "id": 14701,
          "date": "Mon 27 Sep 2021 08:15",
          "username": "Moon",
          "content": "Answer \\\"B\\\", well explained.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 386193,
          "date": "Sat 06 Nov 2021 12:29",
          "username": "01037",
          "content": "Thanks for the explanation.<br><br>https://aws.amazon.com/ebs/pricing/<br><br>gp2<br>$0.10 per GB-month of provisioned storage<br>io1<br>$0.125 per GB-month of provisioned storage AND $0.065 per provisioned IOPS-month",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 44768,
          "date": "Wed 29 Sep 2021 09:10",
          "username": "amog",
          "content": "1TB for 3000 IOPS<br>Answer is B<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 606296,
          "date": "Mon 23 May 2022 21:45",
          "username": "p2010",
          "content": "B: GP1 with 2TB volumes has 6000 IOPS. If we add additional 1TB it will increase by another 3000 IOPS. Note: Bursting and I/O credits are only relevant to volumes under 1,000 GiB, where burst performance exceeds baseline performance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513125,
          "date": "Thu 30 Dec 2021 08:53",
          "username": "cldy",
          "content": "B: CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506339,
          "date": "Tue 21 Dec 2021 19:18",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 502385,
          "date": "Wed 15 Dec 2021 18:53",
          "username": "vbal",
          "content": "gp2: baseline performance scales linearly at 3 IOPS per GiB of volume size.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494924,
          "date": "Mon 06 Dec 2021 06:38",
          "username": "cldy",
          "content": "B.  Increase the size of the gp2 volumes in each instance to 3 TB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491229,
          "date": "Wed 01 Dec 2021 05:07",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 480889,
          "date": "Thu 18 Nov 2021 18:09",
          "username": "sashenkansvijay04b1user0001",
          "content": "If you carefully read the requirements they are asking for \\\"most effectively\\\". No mention of COST!!! So anything to do with cost should not be part of the consideration. That said, an argument can be made that option A: IO1 IOPS SSD is the MOST EFFECTIVE!read ques ..\\\"most COST effective\\\"I totally agree with you, there is no mention of most cost-effective , A is a better option in this case as you wont pay for additional storage",
          "upvote_count": "312",
          "selected_answers": ""
        },
        {
          "id": 710941,
          "date": "Fri 04 Nov 2022 07:42",
          "username": "nsvijay04b1",
          "content": "read ques ..\\\"most COST effective\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 598232,
          "date": "Sat 07 May 2022 19:39",
          "username": "user0001",
          "content": "I totally agree with you, there is no mention of most cost-effective , A is a better option in this case as you wont pay for additional storage",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 449911,
          "date": "Sun 07 Nov 2021 08:03",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409365,
          "date": "Sat 06 Nov 2021 15:24",
          "username": "runtheworld",
          "content": "B: Increase from 2TB (6000IOPS) to 9000IOPS, difference is 1TB(3000IOPS).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 334654,
          "date": "Sat 06 Nov 2021 03:35",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290696,
          "date": "Fri 05 Nov 2021 15:22",
          "username": "wind",
          "content": "B for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289381,
          "date": "Thu 04 Nov 2021 01:09",
          "username": "Kian1",
          "content": "Ans B for me",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281811,
          "date": "Sun 31 Oct 2021 12:54",
          "username": "Ebi",
          "content": "Answer is B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 262911,
          "date": "Sun 31 Oct 2021 06:07",
          "username": "sanjaym",
          "content": "B for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242239,
          "date": "Sun 31 Oct 2021 04:17",
          "username": "gookseang",
          "content": "B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#406",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's data center is connected to the AWS Cloud over a minimally used 10 Gbps AWS Direct Connect connection with a private virtual interface to its virtual private cloud (VPC). The company internet connection is 200 Mbps, and the company has a 150 TB dataset that is created each Friday. The data must be transferred and available in Amazon S3 on Monday morning.<br>Which is the LEAST expensive way to meet the requirements while allowing for data transfer growth?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#406",
          "answers": [
            {
              "choice": "<p>A. Order two 80 TB AWS Snowball appliances. Offload the data to the appliances and ship them to AWS. AWS will copy the data from the Snowball appliances to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a VPC endpoint for Amazon S3. Copy the data to Amazon S3 by using the VPC endpoint, forcing the transfer to use the Direct Connect connection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPC endpoint for Amazon S3. Set up a reverse proxy farm behind a Classic Load Balancer in the VPC.  Copy the data to Amazon S3 using the proxy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a public virtual interface on a Direct Connect connection, and copy the data to Amazon S3 over the connection.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13347,
          "date": "Sat 25 Sep 2021 11:49",
          "username": "donathonwahlbergusafeizz",
          "content": "D<br>A: Too long.<br>B\\C: VPC endpoints are for communications between VPC and S3. You will need a public virtual interface on DC to connect to S3 when the data is on premise. To connect to AWS public endpoints, such as an Amazon Elastic Compute Cloud (Amazon EC2) or Amazon Simple Storage Service (Amazon S3), with dedicated network performance, use a public virtual interface.A public virtual interface allows you to connect to all AWS public IP spaces globally. Direct Connect customers in any Direct Connect location can create public virtual interfaces to receive Amazon’s global IP routes, and they can access publicly routable Amazon services in any AWS Regions (except the AWS China Region).The explanation of B in this comment (and all the comments below) is wrong. S3 Interface Endpoints supports communication from on-prem to S3. <br><br>See : https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/<br><br>However, VPC Endpoints are charged per GB (whichever the data transfer occurs) hence D is still the correct answer.yes, aws now support on prem to s3 by private link which is using the interface endpoints",
          "upvote_count": "4721",
          "selected_answers": ""
        },
        {
          "id": 548097,
          "date": "Tue 15 Feb 2022 22:57",
          "username": "wahlbergusafeizz",
          "content": "The explanation of B in this comment (and all the comments below) is wrong. S3 Interface Endpoints supports communication from on-prem to S3. <br><br>See : https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/<br><br>However, VPC Endpoints are charged per GB (whichever the data transfer occurs) hence D is still the correct answer.yes, aws now support on prem to s3 by private link which is using the interface endpoints",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 598842,
          "date": "Mon 09 May 2022 07:03",
          "username": "feizz",
          "content": "yes, aws now support on prem to s3 by private link which is using the interface endpoints",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 8373,
          "date": "Mon 20 Sep 2021 04:04",
          "username": "Huydpvnmekirrim",
          "content": "A is not practical as Snow Ball takes more than 1 week.<br>B is not valid because Direct Connect can't access VPC Endpoint.<br>C and D are Ok but C is not cost effective because you have to setup a proxy farm.<br><br>D should be correctYeah, D is the correct answerB used to be invalid, but AWS published a solution to access an S3 VPC Endpoint via private DX (the second part of this document):<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/<br><br>So based on that, AWS will support B now.I'm not saying I still wouldn't just do the public VIF (D) to keep things easy, just saying that B is possible now.",
          "upvote_count": "1952",
          "selected_answers": ""
        },
        {
          "id": 10942,
          "date": "Tue 21 Sep 2021 03:32",
          "username": "dpvnme",
          "content": "Yeah, D is the correct answer",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 453435,
          "date": "Sat 06 Nov 2021 23:55",
          "username": "kirrim",
          "content": "B used to be invalid, but AWS published a solution to access an S3 VPC Endpoint via private DX (the second part of this document):<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/<br><br>So based on that, AWS will support B now.I'm not saying I still wouldn't just do the public VIF (D) to keep things easy, just saying that B is possible now.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 648247,
          "date": "Thu 18 Aug 2022 03:41",
          "username": "foureye2004",
          "content": "B,C,D is a valid method:<br>B: https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/ if you create VPC Endpoint for S3 with Interface type<br>C: https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdfwork on 2 type of VPC endpoint.<br>D: it a function of DX. Must be work.<br><br>Now, point to LEAST EXPENSIVE<br>B: Fee of proxy farm.<br>C: Fee of S3 interface endpoint ($/hour) and data processed <br>D: Free (with S3 ingress) <br><br>So, D is the best choice!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 591409,
          "date": "Mon 25 Apr 2022 08:43",
          "username": "jyrajan69Network_1",
          "content": "The question states, a minimally used 10 Gbps AWS Direct Connect connection with a private virtual interface to its virtual private cloud (VPC). Therefore there is an existing virtual interface, so why are we creating another one as stated in answer D.  For me the simpler option is B, unless someone can give a valid reasonPrivate VIF is connected to VPC.  You need Public VIF to connect to S3.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 659166,
          "date": "Sun 04 Sep 2022 12:20",
          "username": "Network_1",
          "content": "Private VIF is connected to VPC.  You need Public VIF to connect to S3.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 583166,
          "date": "Sat 09 Apr 2022 08:52",
          "username": "sophiaabigail",
          "content": "There are few exams as grinding for the candidates as the AWS Solutions Architect Professional exam. The failure rate of the exam is well above 72%. This means that less than 28% of the candidates who take the AWS Solutions Architect Professional exam manage to clear it. Now, this is a daunting number.<br>https://192168l254.com.mx/ES",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 582153,
          "date": "Thu 07 Apr 2022 06:25",
          "username": "SaiKrish123",
          "content": "To connect to s3 using direct connect Public VIF is must",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 514342,
          "date": "Sat 01 Jan 2022 04:57",
          "username": "cldy",
          "content": "D correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513151,
          "date": "Thu 30 Dec 2021 09:38",
          "username": "cldy",
          "content": "D: CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 509120,
          "date": "Sat 25 Dec 2021 14:05",
          "username": "tkanmani76",
          "content": "D is the right choice. https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491237,
          "date": "Wed 01 Dec 2021 05:11",
          "username": "AzureDP900",
          "content": "D is the best answer for given use case",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 464359,
          "date": "Sun 07 Nov 2021 04:18",
          "username": "nsei",
          "content": "Answer is C based on this link<br>https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449913,
          "date": "Sat 06 Nov 2021 08:44",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440633,
          "date": "Sat 06 Nov 2021 06:01",
          "username": "student22",
          "content": "D<br>D - Data can be copied privately using Public VIF on DX.<br>B - VPC endpoints are not accessible through DX.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 436544,
          "date": "Thu 04 Nov 2021 21:33",
          "username": "cloudbruv",
          "content": "The Answer is C.  I really hate AWS for putting questions to a VERY specific solution. See my below link, it literally is the use case they are asking for. <br><br>https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 407572,
          "date": "Thu 04 Nov 2021 20:24",
          "username": "Shran",
          "content": "Answer D<br>Amazon’s primary recommended method is to run multiple VIFs, mixing both public and private.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 397157,
          "date": "Thu 04 Nov 2021 17:56",
          "username": "Pb55",
          "content": "D. <br>https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 376826,
          "date": "Sun 31 Oct 2021 22:49",
          "username": "tekkarttekkarttekkarttekkarttekkartstudent2020",
          "content": "Should be C<br>B : not possible to use Direct Connect with VPC Endpoint : https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/?nc1=h_ls<br>D : Possible to use a Public VIF (https://aws.amazon.com/premiumsupport/knowledge-center/connect-private-network-dx-vif/)but not secure (where is the VPN connexion over it, it is not mentioned in the question), plus you add to the transfer rate an extra cost for the appliance (https://aws.amazon.com/directconnect/pricing/) whereas on C you just pay for the transfer rate of a set of EC2 playing the role of proxy, and the LB allows for future growththe default with C is that it does not state how to connect from the on-prem to the VPC S3, Thus D may be the only viable optionI will vote for D because not satisfied with the term proxy used here to copy some data... \\\"Organizations usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. \\\"It is difficult because in the question it is state that the existing connexion is under-used, why not reuse the existing, plus this \\\"proxy\\\" is a more secured solution to filter the access to S3 and cheaper than adding a new unsecured Public VIF ,and scalable<br>C should be thought over carefullyPlus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging itConnection to VPC through DX is not secure by default. However, connection to S3 can use http or https for security. D is the best option, public VIF via DX.",
          "upvote_count": "112111",
          "selected_answers": ""
        },
        {
          "id": 376829,
          "date": "Mon 01 Nov 2021 08:55",
          "username": "tekkarttekkarttekkarttekkart",
          "content": "the default with C is that it does not state how to connect from the on-prem to the VPC S3, Thus D may be the only viable optionI will vote for D because not satisfied with the term proxy used here to copy some data... \\\"Organizations usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. \\\"It is difficult because in the question it is state that the existing connexion is under-used, why not reuse the existing, plus this \\\"proxy\\\" is a more secured solution to filter the access to S3 and cheaper than adding a new unsecured Public VIF ,and scalable<br>C should be thought over carefullyPlus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging it",
          "upvote_count": "1211",
          "selected_answers": ""
        },
        {
          "id": 376839,
          "date": "Mon 01 Nov 2021 10:03",
          "username": "tekkarttekkarttekkart",
          "content": "I will vote for D because not satisfied with the term proxy used here to copy some data... \\\"Organizations usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. \\\"It is difficult because in the question it is state that the existing connexion is under-used, why not reuse the existing, plus this \\\"proxy\\\" is a more secured solution to filter the access to S3 and cheaper than adding a new unsecured Public VIF ,and scalable<br>C should be thought over carefullyPlus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging it",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 376840,
          "date": "Tue 02 Nov 2021 06:04",
          "username": "tekkarttekkart",
          "content": "It is difficult because in the question it is state that the existing connexion is under-used, why not reuse the existing, plus this \\\"proxy\\\" is a more secured solution to filter the access to S3 and cheaper than adding a new unsecured Public VIF ,and scalable<br>C should be thought over carefullyPlus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging it",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 376842,
          "date": "Tue 02 Nov 2021 19:37",
          "username": "tekkart",
          "content": "Plus the transfers occur during the weekend, so the existing Private VIF connexion is not much used, there is room for leveraging it",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 402920,
          "date": "Thu 04 Nov 2021 18:49",
          "username": "student2020",
          "content": "Connection to VPC through DX is not secure by default. However, connection to S3 can use http or https for security. D is the best option, public VIF via DX.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#407",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has created an account for individual Development teams, resulting in a total of 200 accounts. All accounts have a single virtual private cloud (VPC) in a single region with multiple microservices running in Docker containers that need to communicate with microservices in other accounts. The Security team requirements state that these microservices must not traverse the public internet, and only certain internal services should be allowed to call other individual services. If there is any denied network traffic for a service, the Security team must be notified of any denied requests, including the source IP.<br>How can connectivity be established between service while meeting the security requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#407",
          "answers": [
            {
              "choice": "<p>A. Create a VPC peering connection between the VPCs. Use security groups on the instances to allow traffic from the security group IDs that are permitted to call the microservice. Apply network ACLs and allow traffic from the local VPC and peered VPCs only. Within the task definition in Amazon ECS for each of the microservices, specify a log configuration by using the awslogs driver. Within Amazon CloudWatch Logs, create a metric filter and alarm off of the number of HTTP 403 responses. Create an alarm when the number of messages exceeds a threshold set by the Security team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Ensure that no CIDR ranges are overlapping, and attach a virtual private gateway (VGW) to each VPC.  Provision an IPsec tunnel between each VGW and enable route propagation on the route table. Configure security groups on each service to allow the CIDR ranges of the VPCs in the other accounts. Enable VPC Flow Logs, and use an Amazon CloudWatch Logs subscription filter for rejected traffic. Create an IAM role and allow the Security team to call the AssumeRole action for each account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a transit VPC by using third-party marketplace VPN appliances running on Amazon EC2, dynamically routed VPN connections between the VPN appliance, and the virtual private gateways (VGWs) attached to each VPC within the region. Adjust network ACLs to allow traffic from the local VPC only. Apply security groups to the microservices to allow traffic from the VPN appliances only. Install the awslogs agent on each VPN appliance, and configure logs to forward to Amazon CloudWatch Logs in the security account for the Security team to access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Network Load Balancer (NLB) for each microservice. Attach the NLB to a PrivateLink endpoint service and whitelist the accounts that will be consuming this service. Create an interface endpoint in the consumer VPC and associate a security group that allows only the security group IDs of the services authorized to call the producer service. On the producer services, create security groups for each microservice and allow only the CIDR range of the allowed services. Create VPC Flow Logs on each VPC to capture rejected traffic that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to a security account.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 18862,
          "date": "Sun 03 Oct 2021 09:45",
          "username": "Warrennexamacc",
          "content": "C is not correct as a VPN solution between VPC's would require traffic traversing the internet secure yes but it will traverse the internet. D would be the correct answer providing \\\"only the CIDR range the allowed services\\\" meant only the CIDR range of the producer services as only the ELB would be sending traffic to those services not the consumers directly.C cannot be right. as it mentions to allow traffic from VPN appliance only(that mean they are looking to do NAT that way loacal security group will never see which connections to deny). C is scalable but has issues. I think D is better answer for this.",
          "upvote_count": "194",
          "selected_answers": ""
        },
        {
          "id": 24029,
          "date": "Tue 05 Oct 2021 14:14",
          "username": "examacc",
          "content": "C cannot be right. as it mentions to allow traffic from VPN appliance only(that mean they are looking to do NAT that way loacal security group will never see which connections to deny). C is scalable but has issues. I think D is better answer for this.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 673657,
          "date": "Mon 19 Sep 2022 23:51",
          "username": "joanneli77heany",
          "content": "The answer is C even though it is awful.Today this is TransitGateway to VPC, but prior to that architecture TransitVPC was normal.A transit VPC is EC2 hosting VPN software, so it's EC2-to-EC2 VPN.Yes, that's awful, but that's what drove Transit Gateway architecture into being.I'd be shocked if this question still existed on this old exam.agree.that's 3rd party VPN appliance is equivalent to transit gw .",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 689669,
          "date": "Sat 08 Oct 2022 22:44",
          "username": "heany",
          "content": "agree.that's 3rd party VPN appliance is equivalent to transit gw .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 635816,
          "date": "Sun 24 Jul 2022 02:28",
          "username": "hilft",
          "content": "D.  PrivateLink",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 626407,
          "date": "Sun 03 Jul 2022 06:29",
          "username": "aandc",
          "content": "keyword \\\"PrivateLink\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 557582,
          "date": "Sun 27 Feb 2022 21:01",
          "username": "Ni_yot",
          "content": "D for me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513156,
          "date": "Thu 30 Dec 2021 09:44",
          "username": "cldy",
          "content": "D: CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506348,
          "date": "Tue 21 Dec 2021 19:28",
          "username": "AzureDP900",
          "content": "I will go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449933,
          "date": "Sat 06 Nov 2021 08:47",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 445739,
          "date": "Fri 05 Nov 2021 22:41",
          "username": "Kopa",
          "content": "Im going for D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 440637,
          "date": "Fri 05 Nov 2021 14:02",
          "username": "StelSen",
          "content": "Option-D seems better than other options.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 391551,
          "date": "Wed 03 Nov 2021 22:57",
          "username": "DashL",
          "content": "None of the answers are correct. In the question one of the key items is \\\"If there is any denied network traffic for a service, the Security team must be notified of any denied requests\\\". <br>A - Provides notification, but will hit the VPC peering limit of 125<br>B/C/D- Provides no notification",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 386209,
          "date": "Wed 03 Nov 2021 06:57",
          "username": "01037",
          "content": "D<br><br>A: The maximum quota is 125 peering connections per VPC.  Also too complex.<br>B: Virtual private gateways per Region is 5. Also too complex.<br>C: No obvious difference with B, I think.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 373262,
          "date": "Wed 03 Nov 2021 05:20",
          "username": "nisoshabangu",
          "content": "D is the correct answer, I have implemeted a similar solution in my environment.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362468,
          "date": "Tue 02 Nov 2021 23:32",
          "username": "Radhaghosh",
          "content": "Correct Answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 334659,
          "date": "Tue 02 Nov 2021 15:55",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 305989,
          "date": "Tue 02 Nov 2021 08:29",
          "username": "ksl4u",
          "content": "D is correct",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 289394,
          "date": "Tue 02 Nov 2021 01:58",
          "username": "Kian1",
          "content": "go with D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#408",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a dynamic mission-critical web application that has an SLA of 99.99%. Global application users access the application 24/7. The application is currently hosted on premises and routinely fails to meet its SLA, especially when millions of users access the application concurrently. Remote users complain of latency.<br>How should this application be redesigned to be scalable and allow for automatic failover at the lowest cost?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#408",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Route 53 failover routing with geolocation-based routing. Host the website on automatically scaled Amazon EC2 instances behind an Application Load Balancer with an additional Application Load Balancer and EC2 instances for the application layer in each region. Use a Multi-AZ deployment with MySQL as the data layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Route 53 round robin routing to distribute the load evenly to several regions with health checks. Host the website on automatically scaled Amazon ECS with AWS Fargate technology containers behind a Network Load Balancer, with an additional Network Load Balancer and Fargate containers for the application layer in each region. Use Amazon Aurora replicas for the data layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Route 53 latency-based routing to route to the nearest region with health checks. Host the website in Amazon S3 in each region and use Amazon API Gateway with AWS Lambda for the application layer. Use Amazon DynamoDB global tables as the data layer with Amazon DynamoDB Accelerator (DAX) for caching.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Route 53 geolocation-based routing. Host the website on automatically scaled AWS Fargate containers behind a Network Load Balancer with an additional Network Load Balancer and Fargate containers for the application layer in each region. Use Amazon Aurora Multi-Master for Aurora MySQL as the data layer.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13370,
          "date": "Wed 22 Sep 2021 20:03",
          "username": "donathonppsheinPb55Pb55",
          "content": "C<br>https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/<br>A\\D: Should be latency based routing the ensure latency is at a minimum. Remember the users are spread globally not to specific regions where you can maybe use geo to spread the load across just a few region.<br>B: Similar to A. Lambda has its limitation that cannot handle such concurrent requestWhat about S3 for dynamic website & failed SLA.  How can C be correct?It’s C. <br>Multi region improves SLA due to latency routing<br>S3 hosts static and lambda processes dynamic aspects of website.<br>https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-3/",
          "upvote_count": "37222",
          "selected_answers": ""
        },
        {
          "id": 350826,
          "date": "Wed 20 Oct 2021 22:14",
          "username": "ppshein",
          "content": "Lambda has its limitation that cannot handle such concurrent request",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 397162,
          "date": "Sat 30 Oct 2021 17:52",
          "username": "Pb55Pb55",
          "content": "What about S3 for dynamic website & failed SLA.  How can C be correct?It’s C. <br>Multi region improves SLA due to latency routing<br>S3 hosts static and lambda processes dynamic aspects of website.<br>https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-3/",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 397164,
          "date": "Sun 31 Oct 2021 05:49",
          "username": "Pb55",
          "content": "It’s C. <br>Multi region improves SLA due to latency routing<br>S3 hosts static and lambda processes dynamic aspects of website.<br>https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-3/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 148534,
          "date": "Thu 30 Sep 2021 17:38",
          "username": "fullawsTomPaschenda01037SD13Kelvin",
          "content": "D is correct<br>Below is summary of those discussion<br>A.  RDS 99.95%, no across region strategy<br>B.  Round robin to different region (latency), Aurora 99.99% meet<br>C.  S3 not for dynamic web content, API Gateway & lambda 99.95% <br>D.  ECS, EC2, Fargate, NLB, Aurora 99.99% meet, using geolcation-based routing is enough as regional service is 99.99%C is correct - the SLA problem can be solved by having API Gateway & Lambda in multiple regions and using Route 53 health checks.<br>D cannot be correct because Route 53 Health Check is missing Aurora Multi-Master does not support cross-region, see https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.htmlGood point.<br>SLA rules out A&C. The only problem with D is multi-master Aurora is regional, there is no mention of data sync across regions. C seems correct.Answer D is not correct. Answer C is correct because there is a Route53 with health checks in front.",
          "upvote_count": "355162",
          "selected_answers": ""
        },
        {
          "id": 426256,
          "date": "Wed 03 Nov 2021 11:05",
          "username": "TomPaschenda",
          "content": "C is correct - the SLA problem can be solved by having API Gateway & Lambda in multiple regions and using Route 53 health checks.<br>D cannot be correct because Route 53 Health Check is missing Aurora Multi-Master does not support cross-region, see https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 386218,
          "date": "Fri 22 Oct 2021 23:05",
          "username": "01037",
          "content": "Good point.<br>SLA rules out A&C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 331103,
          "date": "Tue 19 Oct 2021 16:49",
          "username": "SD13",
          "content": "The only problem with D is multi-master Aurora is regional, there is no mention of data sync across regions. C seems correct.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 335032,
          "date": "Wed 20 Oct 2021 15:44",
          "username": "Kelvin",
          "content": "Answer D is not correct. Answer C is correct because there is a Route53 with health checks in front.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 711910,
          "date": "Sat 05 Nov 2022 18:42",
          "username": "sou123454",
          "content": "Will go with D as millions of concurrent request NLB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703579,
          "date": "Tue 25 Oct 2022 07:44",
          "username": "Cloud_noob",
          "content": "C seems correct for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 693866,
          "date": "Thu 13 Oct 2022 14:05",
          "username": "sg0206",
          "content": "C - Will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 544397,
          "date": "Thu 10 Feb 2022 09:02",
          "username": "Ishu_awsguy",
          "content": "Answer is D. <br>The users are worldwide so Geo location based routing is better. <br>Moreover AWS always promotes its services in questions. So aurora multi master and Fargate is better option.<br>Cost for Fargate will be cost efficient.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506355,
          "date": "Tue 21 Dec 2021 19:38",
          "username": "AzureDP900",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498473,
          "date": "Fri 10 Dec 2021 10:21",
          "username": "cldy",
          "content": "C.  Use Amazon Route 53 latency-based routing to route to the nearest region with health checks. Host the website in Amazon S3 in each region and use Amazon API Gateway with AWS Lambda for the application layer. Use Amazon DynamoDB global tables as the data layer with Amazon DynamoDB Accelerator (DAX) for caching.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 452707,
          "date": "Sun 07 Nov 2021 07:36",
          "username": "student22",
          "content": "D <br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449937,
          "date": "Sun 07 Nov 2021 07:16",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448411,
          "date": "Sat 06 Nov 2021 20:33",
          "username": "DonSp",
          "content": "A, C, and D can be eliminated for not fulfilling all requirements.<br>A: SLA not met<br>C: does not allow dynamic web applications, Lambda does not support the required concurrency<br>D: no health checks, Aurora Multi-Master is in one region only<br>B does not look great but fulfills the requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440644,
          "date": "Sat 06 Nov 2021 13:00",
          "username": "StelSen",
          "content": "Option-D looks correct. With Option-C two flaws. Lambda concurrency limitations (https://stackoverflow.com/questions/36766553/aws-lambda-concurrent-request-limit-and-how-to-increase-that) and S3 can't be used for Dynamic Website.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 438837,
          "date": "Fri 05 Nov 2021 17:03",
          "username": "AWS_Noob",
          "content": "D<br><br>Millions of concurrent requests. NLB is needed here, lambda only can handle 10 000 concurrent requests per sec of I remember correctly",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 429226,
          "date": "Thu 04 Nov 2021 01:46",
          "username": "denccc",
          "content": "for me it's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410053,
          "date": "Tue 02 Nov 2021 04:08",
          "username": "Japs",
          "content": "D - Aurora is cheapest and it supports multi region for read-replicas",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 408854,
          "date": "Mon 01 Nov 2021 05:41",
          "username": "DerekKey",
          "content": "Read the question: \\\"redesigned\\\" to be \\\"scalable\\\" and allow for \\\"automatic failover\\\".You can only choose between B and C.  Roud robin is not suitable due to potential latency happening sometime for some of the users. You should use latency-based routing or geolocation-based routing depending on you application/system usage. <br>I would choose C<br>DAX will lower DynamoDB cost and S3 with Lambda is potentially cheaper than Fargate and load balancers. From experience - you have to run at least one container per task no matter their utilization is. Having millions of user accessing web site running in conatiners it would generate a lot of $ for ECS. Instead redesign your app to use S3 and API Gateway.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 407524,
          "date": "Sun 31 Oct 2021 18:25",
          "username": "Akhil254",
          "content": "C Correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#409",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company manages more than 200 separate internet-facing web applications. All of the applications are deployed to AWS in a single AWS Region. The fully qualified domain names (FQDNs) of all of the applications are made available through HTTPS using Application Load Balancers (ALBs). The ALBs are configured to use public SSL/TLS certificates.<br>A Solutions Architect needs to migrate the web applications to a multi-region architecture. All HTTPS services should continue to work without interruption.<br>Which approach meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#409",
          "answers": [
            {
              "choice": "<p>A. Request a certificate for each FQDN using AWS KMS. Associate the certificates with the ALBs in the primary AWS Region. Enable cross-region availability in AWS KMS for the certificates and associate the certificates with the ALBs in the secondary AWS Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Generate the key pairs and certificate requests for each FQDN using AWS KMS. Associate the certificates with the ALBs in both the primary and secondary AWS Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Request a certificate for each FQDN using AWS Certificate Manager. Associate the certificates with the ALBs in both the primary and secondary AWS Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Request certificates for each FQDN in both the primary and secondary AWS Regions using AWS Certificate Manager. Associate the certificates with the corresponding ALBs in each AWS Region.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 8375,
          "date": "Mon 20 Sep 2021 15:12",
          "username": "Huy",
          "content": "A & B mention about KMS which is not valid for SSL/TLS Cert Managment. <br>ACM is regional service and each region must maintain its owns certificate list. So D should be correct in my opinion.",
          "upvote_count": "40",
          "selected_answers": ""
        },
        {
          "id": 24030,
          "date": "Sat 09 Oct 2021 19:59",
          "username": "examacc",
          "content": "Answer is D.  As per AWS FAQ <br>To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 513193,
          "date": "Thu 30 Dec 2021 10:44",
          "username": "cldy",
          "content": "D is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506358,
          "date": "Tue 21 Dec 2021 19:41",
          "username": "AzureDP900",
          "content": "D is right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498475,
          "date": "Fri 10 Dec 2021 10:23",
          "username": "cldy",
          "content": "D.  Request certificates for each FQDN in both the primary and secondary AWS Regions using AWS Certificate Manager. Associate the certificates with the corresponding ALBs in each AWS Region.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 491246,
          "date": "Wed 01 Dec 2021 05:20",
          "username": "AzureDP900",
          "content": "Good explanation, D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449974,
          "date": "Sun 07 Nov 2021 05:01",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 438838,
          "date": "Sat 06 Nov 2021 22:01",
          "username": "AWS_Noob",
          "content": "D.  <br><br>ACM is regional. <br>ACM is for SSL / TLS",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 373274,
          "date": "Tue 02 Nov 2021 07:11",
          "username": "nisoshabangu",
          "content": "D is the correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362476,
          "date": "Sun 31 Oct 2021 17:01",
          "username": "Radhaghosh",
          "content": "ACM is regional service. so for every region certificate to be provisioned. Answer is D<br>Option with KMS is just distraction",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 334670,
          "date": "Sun 31 Oct 2021 16:29",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 292779,
          "date": "Sun 31 Oct 2021 08:11",
          "username": "natpilot",
          "content": "is D. <br>https://docs.aws.amazon.com/acm/latest/userguide/acm-regions.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289428,
          "date": "Sun 31 Oct 2021 05:27",
          "username": "Kian1",
          "content": "going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 284000,
          "date": "Sat 30 Oct 2021 22:35",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 262927,
          "date": "Fri 29 Oct 2021 06:42",
          "username": "sanjaym",
          "content": "It's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242331,
          "date": "Fri 29 Oct 2021 00:18",
          "username": "gookseang",
          "content": "D for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 241923,
          "date": "Thu 28 Oct 2021 13:22",
          "username": "T14102020",
          "content": "Correct answer is D.  Certificate should be in every region.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#410",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company's CIO has asked a Solutions Architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.<br>Which of the following is the MOST reliable approach to meet the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#410",
          "answers": [
            {
              "choice": "<p>A. Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Receive the orders in an Amazon SQS queue and trigger an AWS Lambda function to process them.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Receive the orders using the AWS Step Functions program and trigger an Amazon ECS container to process them.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 16149,
          "date": "Fri 24 Sep 2021 03:13",
          "username": "chaudh",
          "content": "Looks like B, C and D are correct. But I prefer B since SQS provides reliability & Lambda gives scalability.<br>C (an ECS container) & D (EC2 instances) don't give HA & scale.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 13373,
          "date": "Wed 22 Sep 2021 08:34",
          "username": "donathondonathonoatifStelSen",
          "content": "A: It’s no reliable even with auto-scaling, it may not be able to meet sudden demand unless there are very good effort done on a schedule to spin up instances ahead of the demand. Still, it’s difficult to know how much you need.<br>B: SQS suitable and reliable initially. But remember that this is an order processing application SQS is sceptical to duplicates and you would not want duplicates in order processing. You need to choose FIFO SQS which is not in the answer.<br>C: Step Functions replaces SWF but a workflow is not needed here. What we need is a reliable storage like SQS to store the huge surge in orders that may occur during campaigns.<br>D: Kinesis Streams recently was just able to auto scale to demand so this should be it. https://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/<br>https://aws.amazon.com/kinesis/data-streams/faqs/Additional info: Q: How does Amazon Kinesis Data Streams differ from Amazon SQS?<br><br>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).<br>https://aws.amazon.com/kinesis/data-streams/faqs/<br>https://aws.amazon.com/blogs/big-data/unite-real-time-and-batch-analytics-using-the-big-data-lambda-architecture-without-servers/Amazon Kinesis is differentiated from Amazon's Simple Queue Service (SQS) in that Kinesis is used to enable real-time processing of streaming big data. SQS, on the other hand, is used as a message queue to store messages transmitted between distributed application componentsSQS sceptical to duplicates? When I receive order I create a message in SQS and I can process it from Lambda. Where this duplicates issues comes from? Even if duplicates occurs due to app functionality, the issue will be at Kinesis as well. Also FIFO is not required. The order of the ORDER is not an issue as long as the order gets processed. If ordering is an issue, this will be problem in Kinesis as well, I think.",
          "upvote_count": "8162",
          "selected_answers": ""
        },
        {
          "id": 14065,
          "date": "Wed 22 Sep 2021 09:25",
          "username": "donathonoatif",
          "content": "Additional info: Q: How does Amazon Kinesis Data Streams differ from Amazon SQS?<br><br>Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).<br>https://aws.amazon.com/kinesis/data-streams/faqs/<br>https://aws.amazon.com/blogs/big-data/unite-real-time-and-batch-analytics-using-the-big-data-lambda-architecture-without-servers/Amazon Kinesis is differentiated from Amazon's Simple Queue Service (SQS) in that Kinesis is used to enable real-time processing of streaming big data. SQS, on the other hand, is used as a message queue to store messages transmitted between distributed application components",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 117999,
          "date": "Sun 10 Oct 2021 18:28",
          "username": "oatif",
          "content": "Amazon Kinesis is differentiated from Amazon's Simple Queue Service (SQS) in that Kinesis is used to enable real-time processing of streaming big data. SQS, on the other hand, is used as a message queue to store messages transmitted between distributed application components",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 440650,
          "date": "Fri 05 Nov 2021 10:34",
          "username": "StelSen",
          "content": "SQS sceptical to duplicates? When I receive order I create a message in SQS and I can process it from Lambda. Where this duplicates issues comes from? Even if duplicates occurs due to app functionality, the issue will be at Kinesis as well. Also FIFO is not required. The order of the ORDER is not an issue as long as the order gets processed. If ordering is an issue, this will be problem in Kinesis as well, I think.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 706641,
          "date": "Fri 28 Oct 2022 18:57",
          "username": "superuser784",
          "content": "Key here is \\\"loosely coupled\\\", its a common pattern to use a Queue to decouple apps, in this casa SQS is the best choice, so the answer is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 577893,
          "date": "Wed 30 Mar 2022 00:18",
          "username": "jj22222",
          "content": "B looks right",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513196,
          "date": "Thu 30 Dec 2021 10:47",
          "username": "cldy",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491251,
          "date": "Wed 01 Dec 2021 05:23",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449978,
          "date": "Sat 06 Nov 2021 16:03",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445798,
          "date": "Sat 06 Nov 2021 06:15",
          "username": "Kopa",
          "content": "keyword loosely coupled, im going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 428899,
          "date": "Wed 03 Nov 2021 16:40",
          "username": "AWS_Noob",
          "content": "B, major key word there being Loosely coupled",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 426338,
          "date": "Wed 03 Nov 2021 13:48",
          "username": "Kopa",
          "content": "I go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 347295,
          "date": "Tue 02 Nov 2021 16:57",
          "username": "macshild",
          "content": "the answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 335348,
          "date": "Mon 01 Nov 2021 16:59",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 306868,
          "date": "Mon 01 Nov 2021 15:28",
          "username": "Pupu86",
          "content": "I would seek out the keywords such as HA, Sporadic (irregular intervals) and loosely coupled and determine suitable services as SQS, Lamda, Lamda@edge to be used so my answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289429,
          "date": "Mon 01 Nov 2021 12:14",
          "username": "Kian1",
          "content": "Ans B 100%",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 284626,
          "date": "Sat 30 Oct 2021 23:09",
          "username": "LoganIsh",
          "content": "B is the answer the key word is that processing order thus SQS is the real deal here..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283980,
          "date": "Sat 30 Oct 2021 17:07",
          "username": "Ebi",
          "content": "B is my choice<br>For those who picked D, although technically possible but what AWS is evaluating you as well is to pick the right service for the right use case. Kenisis is not the right choice for queues, it is best for streaming",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 283727,
          "date": "Fri 29 Oct 2021 16:32",
          "username": "bnagaraja9099",
          "content": "Step functions are typical use cases for Order processing where it may take different flows and manual workflows. But in the scope of this question to receive and store in async fashion, SQS is good enough for \\\"simple\\\" requirement. <br>I ll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#411",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application written using an in-house software framework. The framework installation takes 30 minutes and is performed with a user data script. Company Developers deploy changes to the application frequently. The framework installation is becoming a bottleneck in this process.<br>Which of the following would speed up this process?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#411",
          "answers": [
            {
              "choice": "<p>A. Create a pipeline to build a custom AMI with the framework installed and use this AMI as a baseline for application deployments.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Employ a user data script to install the framework but compress the installation files to make them smaller.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a pipeline to parallelize the installation tasks and call this pipeline from a user data script.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an AWS OpsWorks cookbook that installs the framework instead of employing user data. Use this cookbook as a base for all deployments.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13377,
          "date": "Thu 23 Sep 2021 21:18",
          "username": "donathonachambokshammousAWSum1StelSen",
          "content": "A<br>B: Does not solve the problem.<br>C\\D: This still does not solve the problem because it still needs 30minutes to install the framework.The bottleneck is inbstalling the framework, so to solve this, pre-built the image with desired framework. (AMI buidling)D solves the problem. \\\"Ops Works can build or update Framework based on a few configurations that can be externalized. More automated way and less error-prone. The most recent way.\\\"I agree with consultsk below.Opsworks is used for Chef and PuppetTechnically D works. But how about 30 mins duration? Can this be done faster with OpsWorks? Option-A seems correct.",
          "upvote_count": "304323",
          "selected_answers": ""
        },
        {
          "id": 163596,
          "date": "Tue 05 Oct 2021 13:45",
          "username": "achambok",
          "content": "The bottleneck is inbstalling the framework, so to solve this, pre-built the image with desired framework. (AMI buidling)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 279960,
          "date": "Fri 15 Oct 2021 06:11",
          "username": "shammousAWSum1StelSen",
          "content": "D solves the problem. \\\"Ops Works can build or update Framework based on a few configurations that can be externalized. More automated way and less error-prone. The most recent way.\\\"I agree with consultsk below.Opsworks is used for Chef and PuppetTechnically D works. But how about 30 mins duration? Can this be done faster with OpsWorks? Option-A seems correct.",
          "upvote_count": "323",
          "selected_answers": ""
        },
        {
          "id": 444758,
          "date": "Thu 04 Nov 2021 11:02",
          "username": "AWSum1",
          "content": "Opsworks is used for Chef and Puppet",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 440651,
          "date": "Wed 03 Nov 2021 03:27",
          "username": "StelSen",
          "content": "Technically D works. But how about 30 mins duration? Can this be done faster with OpsWorks? Option-A seems correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 12455,
          "date": "Mon 20 Sep 2021 20:58",
          "username": "awsec2",
          "content": "ashould be",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 716737,
          "date": "Sat 12 Nov 2022 15:18",
          "username": "DarthYoda",
          "content": "Agree with A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 715076,
          "date": "Thu 10 Nov 2022 10:10",
          "username": "DasguptaS",
          "content": "should be A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 526020,
          "date": "Mon 17 Jan 2022 20:06",
          "username": "Ni_yot",
          "content": "Its A.  if you pre build the image then deploy, its much faster and time is saved",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 517159,
          "date": "Wed 05 Jan 2022 05:14",
          "username": "vinodkp",
          "content": "custom ami is the best solution",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 514396,
          "date": "Sat 01 Jan 2022 07:20",
          "username": "cldy",
          "content": "A correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506361,
          "date": "Tue 21 Dec 2021 19:44",
          "username": "AzureDP900",
          "content": "A is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499063,
          "date": "Sat 11 Dec 2021 04:14",
          "username": "challenger1",
          "content": "My Answer: A<br>Custom AMI",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450885,
          "date": "Sun 07 Nov 2021 04:59",
          "username": "moon2351",
          "content": "Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449993,
          "date": "Thu 04 Nov 2021 16:59",
          "username": "andylogan",
          "content": "It's A - The bottleneck is installing the framework, so to solve this, pre-built the image with desired framework. (AMI building)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 423120,
          "date": "Mon 01 Nov 2021 00:49",
          "username": "Mzehk",
          "content": "A should be the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 386237,
          "date": "Sun 31 Oct 2021 16:22",
          "username": "01037",
          "content": "A<br><br>https://aws.amazon.com/image-builder/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 366415,
          "date": "Fri 29 Oct 2021 22:44",
          "username": "zolthar_z",
          "content": "Even Opsworks can install automatically it will the whole installation process, so it will no differ from the script, with a pre-built image you only need to install the framework one time, so answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362485,
          "date": "Thu 28 Oct 2021 16:48",
          "username": "Radhaghosh",
          "content": "Main phrase \\\"30 Mins\\\" Bottleneck for framework installation <br>Only pre-build AMI is the option.<br>I will go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 349841,
          "date": "Wed 27 Oct 2021 01:06",
          "username": "victordun",
          "content": "A - key point is framework installation, precook it with codepipeline to get the AMI ready. (Precooking an AMI is not ideal if the configs are quick)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343766,
          "date": "Mon 25 Oct 2021 00:52",
          "username": "blackgamer",
          "content": "A is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#412",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to ensure that the workloads for each of its business units have complete autonomy and a minimal blast radius in AWS. The Security team must be able to control access to the resources and services in the account to ensure that particular services are not used by the business units.<br>How can a Solutions Architect achieve the isolation requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#412",
          "answers": [
            {
              "choice": "<p>A. Create individual accounts for each business unit and add the account to an OU in AWS Organizations. Modify the OU to ensure that the particular services are blocked. Federate each account with an IdP, and create separate roles for the business units and the Security team.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create individual accounts for each business unit. Federate each account with an IdP and create separate roles and policies for business units and the Security team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create one shared account for the entire company. Create separate VPCs for each business unit. Create individual IAM policies and resource tags for each business unit. Federate each account with an IdP, and create separate roles for the business units and the Security team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create one shared account for the entire company. Create individual IAM policies and resource tags for each business unit. Federate the account with an IdP, and create separate roles for the business units and the Security team.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11475,
          "date": "Mon 20 Sep 2021 20:01",
          "username": "dpvnme",
          "content": "This question is made for A. ",
          "upvote_count": "30",
          "selected_answers": ""
        },
        {
          "id": 13381,
          "date": "Sun 26 Sep 2021 22:09",
          "username": "donathon",
          "content": "A<br>The best way is to use SCP and individual account.<br>B: This is difficult to manage.<br>C\\D: Does not reduce the blast radius.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 655512,
          "date": "Wed 31 Aug 2022 23:42",
          "username": "epomatti",
          "content": "A no brainer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 649333,
          "date": "Sat 20 Aug 2022 10:30",
          "username": "tracyli",
          "content": "I choose d, because others are all saying that an idp with each accounts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498702,
          "date": "Fri 10 Dec 2021 15:34",
          "username": "cldy",
          "content": "A.  Create individual accounts for each business unit and add the account to an OU in AWS Organizations. Modify the OU to ensure that the particular services are blocked. Federate each account with an IdP, and create separate roles for the business units and the Security team.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493413,
          "date": "Sat 04 Dec 2021 00:42",
          "username": "AzureDP900",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 452713,
          "date": "Sun 07 Nov 2021 14:51",
          "username": "student22",
          "content": "A <br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449995,
          "date": "Sat 06 Nov 2021 11:28",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 407726,
          "date": "Thu 04 Nov 2021 18:49",
          "username": "Shran",
          "content": "In question it is specifying security team wants to control resources and services.. from scp you cannot control resources.. so answer should be B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349844,
          "date": "Thu 04 Nov 2021 08:10",
          "username": "victordun",
          "content": "A - SCP hidden deliberately, OU and multiaccount strategy is the key to reduce blast radius",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 336855,
          "date": "Thu 04 Nov 2021 06:21",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289433,
          "date": "Mon 01 Nov 2021 21:02",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283983,
          "date": "Mon 01 Nov 2021 16:16",
          "username": "Ebi",
          "content": "My answer is A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 283769,
          "date": "Thu 28 Oct 2021 12:33",
          "username": "bnagaraja9099",
          "content": "Poorly written responses. Even without responses all of us know OU with SCP is the best way to handle it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 262936,
          "date": "Wed 27 Oct 2021 03:09",
          "username": "sanjaym",
          "content": "A for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251217,
          "date": "Fri 22 Oct 2021 04:05",
          "username": "Lance_D",
          "content": "Ans A is the most sensible choice here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 245070,
          "date": "Sun 17 Oct 2021 12:32",
          "username": "rscloud",
          "content": "Correct answer is A<br>SCP will help to restrict accwss using deny list strategy",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#413",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating a subset of its application APIs from Amazon EC2 instances to run on a serverless infrastructure. The company has set up Amazon API<br>Gateway, AWS Lambda, and Amazon DynamoDB for the new application. The primary responsibility of the Lambda function is to obtain data from a third-party<br>Software as a Service (SaaS) provider. For consistency, the Lambda function is attached to the same virtual private cloud (VPC) as the original EC2 instances.<br>Test users report an inability to use this newly moved functionality, and the company is receiving 5xx errors from API Gateway. Monitoring reports from the SaaS provider shows that the requests never made it to its systems. The company notices that Amazon CloudWatch Logs are being generated by the Lambda functions. When the same functionality is tested against the EC2 systems, it works as expected.<br>What is causing the issue?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#413",
          "answers": [
            {
              "choice": "<p>A. Lambda is in a subnet that does not have a NAT gateway attached to it to connect to the SaaS provider.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. The end-user application is misconfigured to continue using the endpoint backed by EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The throttle limit set on API Gateway is too low and the requests are not making their way through.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. API Gateway does not have the necessary permissions to invoke Lambda.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 16151,
          "date": "Sun 26 Sep 2021 10:44",
          "username": "chaudhskywalkerGeetarAWSPro24Radhaghoshvbal",
          "content": "My answer is A<br>- Note that testing on EC2 working --> problem comes from Lambda.<br>- If API doesn't have permission to invoke Lambda then Lambda cannot generate CW Logs but in the question, there is the fact that the Lambda was already get invoked and generate CW Logs.<br>- 5xx usually happens when there is a timeout with your call and if there is a delay with your lambda execution. If Lambda is accessing an external resource such as Dynamo DB or an external network call, just make sure Lambda will have ability to do that, otherwise it will be timeout.Agreed with you.. Since Lambda has already generated logs, then its possible that API/Lambda did in fact being triggered.Here there is no mentioned of logs from the NATand thus Is is likely \\\"A\\\".A is correct:<br>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/Because the Lambda logs exist they must be being triggered.<br>This references the NAT Gateway need for Lambda internet access directly. <br><br>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/Hope you read this steps:<br> Create your VPC components<br><br>Create two or more new subnets in your VPC.  During creation, for Name tag, add a name to help you identify which subnet is public and which subnets are private. For example, name one Public Subnet and the other Private Lambda (or Private Lambda 1, Private Lambda 2, and so on, for multiple private subnets).<br>Note: It's a best practice to create multiple private subnets across different Availability Zones for redundancy and so that Lambda can maintain high availability for your function.<br>Create an internet gateway and attach it to your VPC. <br>Create a NAT gateway. During creation, for Subnet, choose the subnet that you want to make public. (For example, Public Subnet if you named it earlier.)<br>Note: For help with testing your NAT gateway after creation, see Testing a NAT Gateway.https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/",
          "upvote_count": "6142211",
          "selected_answers": ""
        },
        {
          "id": 19128,
          "date": "Sun 26 Sep 2021 18:52",
          "username": "skywalker",
          "content": "Agreed with you.. Since Lambda has already generated logs, then its possible that API/Lambda did in fact being triggered.Here there is no mentioned of logs from the NATand thus Is is likely \\\"A\\\".",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 395548,
          "date": "Mon 01 Nov 2021 16:18",
          "username": "Geetar",
          "content": "A is correct:<br>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 51322,
          "date": "Wed 29 Sep 2021 01:21",
          "username": "AWSPro24Radhaghosh",
          "content": "Because the Lambda logs exist they must be being triggered.<br>This references the NAT Gateway need for Lambda internet access directly. <br><br>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/Hope you read this steps:<br> Create your VPC components<br><br>Create two or more new subnets in your VPC.  During creation, for Name tag, add a name to help you identify which subnet is public and which subnets are private. For example, name one Public Subnet and the other Private Lambda (or Private Lambda 1, Private Lambda 2, and so on, for multiple private subnets).<br>Note: It's a best practice to create multiple private subnets across different Availability Zones for redundancy and so that Lambda can maintain high availability for your function.<br>Create an internet gateway and attach it to your VPC. <br>Create a NAT gateway. During creation, for Subnet, choose the subnet that you want to make public. (For example, Public Subnet if you named it earlier.)<br>Note: For help with testing your NAT gateway after creation, see Testing a NAT Gateway.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 362494,
          "date": "Thu 28 Oct 2021 01:42",
          "username": "Radhaghosh",
          "content": "Hope you read this steps:<br> Create your VPC components<br><br>Create two or more new subnets in your VPC.  During creation, for Name tag, add a name to help you identify which subnet is public and which subnets are private. For example, name one Public Subnet and the other Private Lambda (or Private Lambda 1, Private Lambda 2, and so on, for multiple private subnets).<br>Note: It's a best practice to create multiple private subnets across different Availability Zones for redundancy and so that Lambda can maintain high availability for your function.<br>Create an internet gateway and attach it to your VPC. <br>Create a NAT gateway. During creation, for Subnet, choose the subnet that you want to make public. (For example, Public Subnet if you named it earlier.)<br>Note: For help with testing your NAT gateway after creation, see Testing a NAT Gateway.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 508880,
          "date": "Fri 24 Dec 2021 23:22",
          "username": "vbal",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 14633,
          "date": "Fri 24 Sep 2021 09:42",
          "username": "MoonajeeshbdonathonChinmoyMansurtiffanny",
          "content": "Answer \\\"D\\\".<br>A: NAT gateway does not need to be attached to the Lambda subnet! it is only a routing to NAT gateway that allow public access.<br>B: does not make sense.<br>C: this means at least some times it should pass.<br>D: refer to the below link:<br>https://aws.amazon.com/premiumsupport/knowledge-center/500-error-lambda-api-gateway/When it says NAT gateway is not attached to the subnet it means a route to internet via NAT Gateway. The answer should be A.  <br>D --> If API gateway does not have the permission to invoke Lambda, how does lambda function generate cloudwatch logs?!D<br>The reason is we have to explicitly specify the ARN of an IAM role for API Gateway to assume when invoking a Lambda function. If none is specified, resource-based permissions are needed.<br>https://jun711.github.io/aws/aws-api-gateway-invoke-lambda-function-permission/Permission error logged in cloudtrail ..Ans is A<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-function-failures/Answer D make sense after I refer to the link provided by Moon. A satisfying explanation. Thanks.A because When you connect a function to a VPC in your account, the function can't access the internet unless your VPC provides access.<br>https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-internet",
          "upvote_count": "1946312",
          "selected_answers": ""
        },
        {
          "id": 307730,
          "date": "Sun 24 Oct 2021 23:39",
          "username": "ajeeshb",
          "content": "When it says NAT gateway is not attached to the subnet it means a route to internet via NAT Gateway. The answer should be A.  <br>D --> If API gateway does not have the permission to invoke Lambda, how does lambda function generate cloudwatch logs?!",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 15561,
          "date": "Sat 25 Sep 2021 10:49",
          "username": "donathonChinmoy",
          "content": "D<br>The reason is we have to explicitly specify the ARN of an IAM role for API Gateway to assume when invoking a Lambda function. If none is specified, resource-based permissions are needed.<br>https://jun711.github.io/aws/aws-api-gateway-invoke-lambda-function-permission/Permission error logged in cloudtrail ..Ans is A<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-function-failures/",
          "upvote_count": "63",
          "selected_answers": ""
        },
        {
          "id": 61170,
          "date": "Wed 29 Sep 2021 07:37",
          "username": "Chinmoy",
          "content": "Permission error logged in cloudtrail ..Ans is A<br>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-function-failures/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 290607,
          "date": "Fri 22 Oct 2021 15:39",
          "username": "Mansur",
          "content": "Answer D make sense after I refer to the link provided by Moon. A satisfying explanation. Thanks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 328835,
          "date": "Tue 26 Oct 2021 02:05",
          "username": "tiffanny",
          "content": "A because When you connect a function to a VPC in your account, the function can't access the internet unless your VPC provides access.<br>https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-internet",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 701105,
          "date": "Fri 21 Oct 2022 20:45",
          "username": "mrgreatness",
          "content": "D would give 4XX error, 5XX for server , from reading I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 684123,
          "date": "Sat 01 Oct 2022 11:44",
          "username": "Biden",
          "content": "Unclear question, but 2 clear points though:<br>If Lambda is generating logs, then it is being invoked by API GW - hence D is thrown out<br>A is the choice - under the assumption the choice meant ROUTETABLE/ROUTING to NAT vs \\\"attached\\\" to NAT GW",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 655518,
          "date": "Wed 31 Aug 2022 23:48",
          "username": "epomatti",
          "content": "A is correct because Lambda is generating logs, and D is incorrect.<br><br>B and C are dumb.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 648355,
          "date": "Thu 18 Aug 2022 10:18",
          "username": "MikeyJ",
          "content": "I'll say A too.<br><br>To give your function access to the internet, route outbound traffic to a NAT gateway in a public subnet. The NAT gateway has a public IP address and can connect to the internet through the VPC's internet gateway.<br><br>https://docs.aws.amazon.com/lambda/latest/dg/foundation-networking.html#foundation-nw-connecting",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 637650,
          "date": "Wed 27 Jul 2022 00:16",
          "username": "hilft",
          "content": "A.  It doesn't have internet?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 629417,
          "date": "Sun 10 Jul 2022 04:59",
          "username": "jyrajan69Cal88",
          "content": "Nothing here indicates that the resources are in a private subnet, so why do we need NAT? Lambda can generate CW logs because it has the required permission but it API Gateway does not, so answer is DLambda will not generate logs if its not called!!<br>Permission to generate logs is not relevant here<br>The fact that the logs are there means lambda was invoked<br>So A is the correct answer because lambda will fail and cause a 5XX error if it can’t reach the sas service",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 706815,
          "date": "Sat 29 Oct 2022 01:30",
          "username": "Cal88",
          "content": "Lambda will not generate logs if its not called!!<br>Permission to generate logs is not relevant here<br>The fact that the logs are there means lambda was invoked<br>So A is the correct answer because lambda will fail and cause a 5XX error if it can’t reach the sas service",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 628580,
          "date": "Fri 08 Jul 2022 04:09",
          "username": "hilft",
          "content": "D. <br>Lambda and EC2 are in the same VPC.  EC2 can, but Lambda can't.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506364,
          "date": "Tue 21 Dec 2021 19:50",
          "username": "AzureDP900",
          "content": "A Correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 498707,
          "date": "Fri 10 Dec 2021 15:43",
          "username": "cldy",
          "content": "A.  Lambda is in a subnet that does not have a NAT gateway attached to it to connect to the SaaS provider.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 452716,
          "date": "Sat 06 Nov 2021 22:21",
          "username": "student22",
          "content": "A <br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449999,
          "date": "Fri 05 Nov 2021 13:59",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447455,
          "date": "Wed 03 Nov 2021 07:23",
          "username": "nodogoshi",
          "content": "A Correct. It's VPC LAMBDA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 392797,
          "date": "Sat 30 Oct 2021 13:39",
          "username": "nik_aws",
          "content": "A<br>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 379688,
          "date": "Fri 29 Oct 2021 07:30",
          "username": "tvs",
          "content": "Could be lambda related https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/4XX for authentication and throtlle5XX is due to backend",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 366618,
          "date": "Fri 29 Oct 2021 02:22",
          "username": "zolthar_z",
          "content": "The Answer is A<br><br>B and C aren't the response, and D if API gateway can't execute lambda due permissions the lambda function will not generate logs",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#414",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is working with a company that is extremely sensitive to its IT costs and wishes to implement controls that will result in a predictable AWS spend each month.<br>Which combination of steps can help the company control and monitor its monthly AWS usage to achieve a cost that is as close as possible to the target amount?<br>(Choose three.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AEF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#414",
          "answers": [
            {
              "choice": "<p>A. Implement an IAM policy that requires users to specify a 'workload' tag for cost allocation when launching Amazon EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Contact AWS Support and ask that they apply limits to the account so that users are not able to launch more than a certain number of instance types.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Purchase all upfront Reserved Instances that cover 100% of the account's expected Amazon EC2 usage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Place conditions in the users' IAM policies that limit the number of instances they are able to launch.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Define 'workload' as a cost allocation tag in the AWS Billing and Cost Management console.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Set up AWS Budgets to alert and notify when a given workload is expected to exceed a defined cost.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13383,
          "date": "Mon 20 Sep 2021 04:11",
          "username": "donathon",
          "content": "AEF<br>B: not feasible.<br>C: Not everything is applicable to RI. E. g. S3 does not have RI.<br>D: If they chose a very big instance, the bill could still be big.",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 283989,
          "date": "Thu 28 Oct 2021 02:13",
          "username": "Ebi",
          "content": "AEF for sure",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 694930,
          "date": "Fri 14 Oct 2022 19:16",
          "username": "Blair77",
          "content": "It's A E F",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AEF"
        },
        {
          "id": 691739,
          "date": "Tue 11 Oct 2022 06:46",
          "username": "dmscountera",
          "content": "Based on comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AEF"
        },
        {
          "id": 496635,
          "date": "Wed 08 Dec 2021 09:22",
          "username": "cldy",
          "content": "A.  Implement an IAM policy that requires users to specify a ג€˜workloadג€™ tag for cost allocation when launching Amazon EC2 instances.<br>E.  Define ג€˜workloadג€™ as a cost allocation tag in the AWS Billing and Cost Management console.<br>F.  Set up AWS Budgets to alert and notify when a given workload is expected to exceed a defined cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493416,
          "date": "Sat 04 Dec 2021 00:47",
          "username": "AzureDP900",
          "content": "AEF is my answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 452721,
          "date": "Sun 07 Nov 2021 16:06",
          "username": "student22",
          "content": "AEF<br>---",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450000,
          "date": "Fri 05 Nov 2021 14:37",
          "username": "andylogan",
          "content": "It's A E F",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 408883,
          "date": "Sun 31 Oct 2021 00:43",
          "username": "DerekKey",
          "content": "ACF<br>Key word is \\\"predictable spend\\\"<br>A -> will not be able to run a resource that is not taged<br>\\\"Effect\\\": \\\"Deny\\\",<br>\\\"Action\\\": \\\"whatever:youchoose\\\",<br>\\\"Resource\\\": \\\"*\\\",<br>\\\"Condition\\\": {<br>\\\"ForAllValues:StringNotEquals\\\": {<br>\\\"aws:TagKeys\\\": \\\"workload\\\"<br>}}<br>B - impossible per user<br>C -> obvious - it gives you predictable cost<br>D -> impossible<br>E -> it only enables TAGs in Cost&Usage<br>F -> obvious (Filters:Dimension in Budget) - then you can set up alerts based on workloads (tags).",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 336869,
          "date": "Sat 30 Oct 2021 07:53",
          "username": "WhyIronMan",
          "content": "I'll go with A,E,F",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 289435,
          "date": "Sat 30 Oct 2021 03:29",
          "username": "Kian1",
          "content": "only AEF can be answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 262948,
          "date": "Sat 23 Oct 2021 15:47",
          "username": "sanjaymnano2ndkopper2019",
          "content": "I think answer should be CEF. <br><br>A.  Not because - IAM Policy can restrict access to existing EC2 based on tag but cannot enforce to create tag while creating EC2 instance.<br>B.  Not Because - that can be done through IAM policy. no need to got AWS support.<br>C.  Because - There are many other services also needs to be in consideration to reduce cost but reserved instances can also contribute to reduce cost.<br>D.  Not possible.you can use IAM Policy to enforce tagging on EC2 instance creation: https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/I think the same about C, customers wants predictable cost so if you buy RI you will know how much you are paying for 1 or 3 years,CEF",
          "upvote_count": "212",
          "selected_answers": ""
        },
        {
          "id": 710372,
          "date": "Thu 03 Nov 2022 09:52",
          "username": "nano2nd",
          "content": "you can use IAM Policy to enforce tagging on EC2 instance creation: https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 269723,
          "date": "Tue 26 Oct 2021 16:47",
          "username": "kopper2019",
          "content": "I think the same about C, customers wants predictable cost so if you buy RI you will know how much you are paying for 1 or 3 years,CEF",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 245082,
          "date": "Sat 23 Oct 2021 13:35",
          "username": "rscloud",
          "content": "AEF<br>B- Not feasible<br>C- RI is for yearsnot monthly<br>D- Requirement is to put check on monthly budget not on number of instances. 1 big instance = big budget",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242362,
          "date": "Fri 22 Oct 2021 18:05",
          "username": "gookseang",
          "content": "AEF for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 241943,
          "date": "Fri 22 Oct 2021 00:22",
          "username": "T14102020",
          "content": "AEF are correct answers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 239743,
          "date": "Thu 21 Oct 2021 09:08",
          "username": "newme",
          "content": "I think AEF is correct, as long as EC2 is the main service being used.<br>At first I thought what's wrong with BCD. <br>B: You can contact AWS Support to decrease limit.<br>C: Reserved Instances is a good way to lower your cost. Of course if some may say that the company may not use EC2 for long time, but AEF are based on EC2 is the main service the company is using.<br>D: same as B. <br><br>But think it again, the key of the question is \\\"achieve a cost that is as close as possible to the target amount\\\".<br>AEF can alert Solutions Architect and he/she can adjust the resources being used to control cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 228623,
          "date": "Wed 20 Oct 2021 08:06",
          "username": "jackdryan",
          "content": "I'll go with A,E,F",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#415",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large global company wants to migrate a stateless mission-critical application to AWS. The application is based on IBM WebSphere (application and integration middleware), IBM MQ (messaging middleware), and IBM DB2 (database software) on a z/OS operating system.<br>How should the Solutions Architect migrate the application to AWS?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#415",
          "answers": [
            {
              "choice": "<p>A. Re-host WebSphere-based applications on Amazon EC2 behind a load balancer with Auto Scaling. Re-platform the IBM MQ to an Amazon EC2-based MQ. Re-platform the z/OS-based DB2 to Amazon RDS DB2.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Re-host WebSphere-based applications on Amazon EC2 behind a load balancer with Auto Scaling. Re-platform the IBM MQ to an Amazon MQ. Re-platform z/ OS-based DB2 to Amazon EC2-based DB2.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Orchestrate and deploy the application by using AWS Elastic Beanstalk. Re-platform the IBM MQ to Amazon SQS. Re-platform z/OS-based DB2 to Amazon RDS DB2.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the AWS Server Migration Service to migrate the IBM WebSphere and IBM DB2 to an Amazon EC2-based solution. Re-platform the IBM MQ to an Amazon MQ.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13496,
          "date": "Fri 24 Sep 2021 07:34",
          "username": "donathonMoon",
          "content": "B<br>A\\C: RDS only Supports Aurora, PostgreSQL, MySQL, MariaDB, Oracle & MS SQL Server.<br>D: SMS cannot migrate from a z/OS, it can only migrate from VMware or HyperV. It basically replaces VM Import.<br>https://aws.amazon.com/blogs/database/aws-database-migration-service-and-aws-schema-conversion-tool-now-support-ibm-db2-as-a-source/<br>https://aws.amazon.com/quickstart/architecture/ibm-mq/great analysis.",
          "upvote_count": "513",
          "selected_answers": ""
        },
        {
          "id": 14629,
          "date": "Sun 26 Sep 2021 06:28",
          "username": "Moon",
          "content": "great analysis.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 51314,
          "date": "Mon 04 Oct 2021 07:31",
          "username": "AWSPro24",
          "content": "https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach/",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 716747,
          "date": "Sat 12 Nov 2022 15:33",
          "username": "DarthYoda",
          "content": "B seems to be correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 691738,
          "date": "Tue 11 Oct 2022 06:45",
          "username": "dmscountera",
          "content": "Based on comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 689109,
          "date": "Sat 08 Oct 2022 09:31",
          "username": "wassb",
          "content": "mission critical service > HA > ASG",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 500663,
          "date": "Mon 13 Dec 2021 15:48",
          "username": "vbal",
          "content": "Re-platform z/ OS-based DB2 to Amazon EC2-based DB2 --- is it not Re-Host instead Re-Platform?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493417,
          "date": "Sat 04 Dec 2021 00:50",
          "username": "AzureDP900",
          "content": "B right choice",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450001,
          "date": "Sun 07 Nov 2021 02:35",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362504,
          "date": "Fri 05 Nov 2021 10:42",
          "username": "Radhaghosh",
          "content": "B is the only correct answer <br>RDS doesn't support DB2 and z/OS can't be migrated with AWS SMS",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 336870,
          "date": "Wed 03 Nov 2021 02:30",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 289436,
          "date": "Sun 31 Oct 2021 13:47",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283992,
          "date": "Sun 31 Oct 2021 09:32",
          "username": "Ebi",
          "content": "B is my choice",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 262953,
          "date": "Sat 30 Oct 2021 12:15",
          "username": "sanjaym",
          "content": "B for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 245083,
          "date": "Sat 30 Oct 2021 08:34",
          "username": "rscloud",
          "content": "B<br>Only difference in A/B is EC2 DB2. RDS does not support DB2",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 241945,
          "date": "Tue 26 Oct 2021 16:10",
          "username": "T14102020",
          "content": "Correct answer is B.  DB2 is only on EC2.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 228627,
          "date": "Mon 25 Oct 2021 23:44",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 227740,
          "date": "Tue 19 Oct 2021 22:01",
          "username": "Bulti",
          "content": "Answer is B because SMS cannot migrate Z/OS to AWS.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#416",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A media storage application uploads user photos to Amazon S3 for processing. End users are reporting that some uploaded photos are not being processed properly. The Application Developers trace the logs and find that AWS Lambda is experiencing execution issues when thousands of users are on the system simultaneously. Issues are caused by:<br>✑ Limits around concurrent executions.<br>✑ The performance of Amazon DynamoDB when saving data.<br>Which actions can be taken to increase the performance and reliability of the application? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#416",
          "answers": [
            {
              "choice": "<p>A. Evaluate and adjust the read capacity units (RCUs) for the DynamoDB tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Evaluate and adjust the write capacity units (WCUs) for the DynamoDB tables.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add an Amazon ElastiCache layer to increase the performance of Lambda functions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a dead letter queue that will reprocess failed or timed-out Lambda functions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use S3 Transfer Acceleration to provide lower-latency access to end users.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13498,
          "date": "Fri 24 Sep 2021 14:15",
          "username": "donathon",
          "content": "BD<br>A\\C: Read is not the problem here. (when saving data…)<br>B: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.requests<br>D: https://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/c<br>E: Does not solve the problem. Issue does not lies with ingestion, it lies with processing.",
          "upvote_count": "39",
          "selected_answers": ""
        },
        {
          "id": 12320,
          "date": "Wed 22 Sep 2021 13:56",
          "username": "awsec2",
          "content": "my view also BD",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 716748,
          "date": "Sat 12 Nov 2022 15:36",
          "username": "DarthYoda",
          "content": "I'd say B&D in the exam. Reads are not an issue here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 691742,
          "date": "Tue 11 Oct 2022 06:47",
          "username": "dmscountera",
          "content": "Based on comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 577891,
          "date": "Wed 30 Mar 2022 00:14",
          "username": "jj22222",
          "content": "BD is right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 523963,
          "date": "Sat 15 Jan 2022 07:36",
          "username": "RVivek",
          "content": "B is fine . But D dead letter q is only for troubleshooting the error not for reprocessing the message.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493419,
          "date": "Sat 04 Dec 2021 00:52",
          "username": "AzureDP900",
          "content": "BD is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450003,
          "date": "Sun 07 Nov 2021 06:01",
          "username": "andylogan",
          "content": "It's B D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 424361,
          "date": "Sat 06 Nov 2021 20:37",
          "username": "mimadour21698",
          "content": "my Ans : B, D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 423134,
          "date": "Wed 03 Nov 2021 06:42",
          "username": "Mzehk",
          "content": "Agree with B and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362506,
          "date": "Mon 01 Nov 2021 01:37",
          "username": "Radhaghosh",
          "content": "B, D my options",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 336874,
          "date": "Sun 31 Oct 2021 20:10",
          "username": "WhyIronMan",
          "content": "I'll go with B,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289440,
          "date": "Sun 31 Oct 2021 08:09",
          "username": "Kian1",
          "content": "for write Ans BD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 288393,
          "date": "Sun 31 Oct 2021 02:40",
          "username": "AJBA",
          "content": "BD is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 284633,
          "date": "Thu 28 Oct 2021 04:50",
          "username": "LoganIsh",
          "content": "BD is the answer...",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 283993,
          "date": "Wed 27 Oct 2021 21:13",
          "username": "Ebi",
          "content": "BD for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 262954,
          "date": "Wed 27 Oct 2021 09:26",
          "username": "sanjaym",
          "content": "B & D for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#417",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company operates a group of imaging satellites. The satellites stream data to one of the company's ground stations where processing creates about 5 GB of images per minute. This data is added to network-attached storage, where 2 PB of data are already stored.<br>The company runs a website that allows its customers to access and purchase the images over the Internet. This website is also running in the ground station.<br>Usage analysis shows that customers are most likely to access images that have been captured in the last 24 hours.<br>The company would like to migrate the image storage and distribution system to AWS to reduce costs and increase the number of customers that can be served.<br>Which AWS architecture and migration strategy will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#417",
          "answers": [
            {
              "choice": "<p>A. Use multiple AWS Snowball appliances to migrate the existing imagery to Amazon S3. Create a 1-Gb AWS Direct Connect connection from the ground station to AWS, and upload new data to Amazon S3 through the Direct Connect connection. Migrate the data distribution website to Amazon EC2 instances. By using Amazon S3 as an origin, have this website serve the data through Amazon CloudFront by creating signed URLs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a 1-Gb Direct Connect connection from the ground station to AWS. Use the AWS Command Line Interface to copy the existing data and upload new data to Amazon S3 over the Direct Connect connection. Migrate the data distribution website to EC2 instances. By using Amazon S3 as an origin, have this website serve the data through CloudFront by creating signed URLs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use multiple Snowball appliances to migrate the existing images to Amazon S3. Upload new data by regularly using Snowball appliances to upload data from the network-attached storage. Migrate the data distribution website to EC2 instances. By using Amazon S3 as an origin, have this website serve the data through CloudFront by creating signed URLs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use multiple Snowball appliances to migrate the existing images to an Amazon EFS file system. Create a 1-Gb Direct Connect connection from the ground station to AWS, and upload new data by mounting the EFS file system over the Direct Connect connection. Migrate the data distribution website to EC2 instances. By using webservers in EC2 that mount the EFS file system as the origin, have this website serve the data through CloudFront by creating signed URLs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13505,
          "date": "Sat 25 Sep 2021 23:14",
          "username": "donathon",
          "content": "A<br>B: This will take too long.<br>C: The users will not be able to access the data in the last 24 hours this way.<br>D: Although EFS would work and it has higher performance, it’s almost 10x more expensive then S3 and hence does not meet the criteria of reducing cost.",
          "upvote_count": "44",
          "selected_answers": ""
        },
        {
          "id": 10590,
          "date": "Tue 21 Sep 2021 02:22",
          "username": "cptnewmeShanmahi",
          "content": "I believe because only the data/images of the last 24 hours seems to be relevant. It seems to indicate that there is no urgency to migrate the old data (2PB) over to S3 in short time (which could be achieved with Snowball). Instead this can take as long as it needs. By not using Snowball, we can save money. NOTE: \\\"DX\\\" will be setup in both Answers A) and B). Therefore skipping Snowball will reduce the overall cost of Answer B) compared to A).Good point.<br>But cost isn't said to be a problem here, so snowball is a better solution, isn't it?<br>And using 1Gb DX, it really will take a long time, more than 100 days, to upload data...Good point. However, question states, \\\"most likely last 24 hours\\\", so there is a chance of purchasing something older than 24 hours which is part of existing 2 PB data. Everything else, you have explained perfectly fine. Answer A is best option.",
          "upvote_count": "1022",
          "selected_answers": ""
        },
        {
          "id": 239800,
          "date": "Sun 17 Oct 2021 21:16",
          "username": "newme",
          "content": "Good point.<br>But cost isn't said to be a problem here, so snowball is a better solution, isn't it?<br>And using 1Gb DX, it really will take a long time, more than 100 days, to upload data...",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 427133,
          "date": "Fri 05 Nov 2021 10:23",
          "username": "Shanmahi",
          "content": "Good point. However, question states, \\\"most likely last 24 hours\\\", so there is a chance of purchasing something older than 24 hours which is part of existing 2 PB data. Everything else, you have explained perfectly fine. Answer A is best option.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 712318,
          "date": "Sun 06 Nov 2022 14:17",
          "username": "Ni_yot",
          "content": "A looks to be the better ans.S3 a better choice to copy data to",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 693958,
          "date": "Thu 13 Oct 2022 16:06",
          "username": "sg0206",
          "content": "A is the aswer. Send the exisiting data using snowflex appliances and for immediate generating data, use direct connect to upload..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 623533,
          "date": "Tue 28 Jun 2022 01:24",
          "username": "kangtamo",
          "content": "Agree with A: S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 506405,
          "date": "Tue 21 Dec 2021 20:56",
          "username": "AzureDP900",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497792,
          "date": "Thu 09 Dec 2021 15:27",
          "username": "cldy",
          "content": "A.  Use multiple AWS Snowball appliances to migrate the existing imagery to Amazon S3. Create a 1-Gb AWS Direct Connect connection from the ground station to AWS, and upload new data to Amazon S3 through the Direct Connect connection. Migrate the data distribution website to Amazon EC2 instances. By using Amazon S3 as an origin, have this website serve the data through Amazon CloudFront by creating signed URLs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450004,
          "date": "Fri 05 Nov 2021 19:36",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 444692,
          "date": "Fri 05 Nov 2021 15:10",
          "username": "DonSp",
          "content": "The most reasonable solution is A.  B works technically but the 1-Gb Direct Connect connection will largely be taken up by new data coming in at a rate of 5 GB/min. Furthermore, you would not want to transfer 2 PB of achives from the command line but instead use a File Storage gateway.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362507,
          "date": "Fri 05 Nov 2021 07:09",
          "username": "Radhaghosh",
          "content": "Copy 2TB of data via 1 Gb DX will take close to 195 days.<br>So snowball is the best option. Option A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 344181,
          "date": "Mon 01 Nov 2021 06:35",
          "username": "blackgamer",
          "content": "A.  Data is too much to copy over network, snowball is the answer here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 336879,
          "date": "Mon 01 Nov 2021 02:41",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 311094,
          "date": "Fri 29 Oct 2021 17:15",
          "username": "RomanTsainitinz",
          "content": "Answer should be B.  Using Snowball is a very very very bad choice which take a lot of time to waiting device available, delivery / transport data, upload to S3. My bad experience took 1 week to complete all if lucky to have a device available without any waiting.AWS is not looking for your experience with snowball in certification exam. They are looking for ideal answer, which is A. ",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 315309,
          "date": "Sat 30 Oct 2021 16:14",
          "username": "nitinz",
          "content": "AWS is not looking for your experience with snowball in certification exam. They are looking for ideal answer, which is A. ",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 294694,
          "date": "Thu 28 Oct 2021 02:35",
          "username": "gparkgparksarah_t",
          "content": "B. <br>Requirements are<br>\\\"most likely to access images that have been captured in the last 24 hours.\\\"<br><br>Only 24 hours data has a priority. 2PB is not emergent.<br>There is no restriction to send 2PB within 1 month.B is wrong.<br>Have searched best practises on AWS.<br>Over PB, Snowmobile is the one.<br>My apology for confusion.Amazon recommends Snowmobile for migrating more than 10 PB",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 294751,
          "date": "Fri 29 Oct 2021 00:44",
          "username": "gparksarah_t",
          "content": "B is wrong.<br>Have searched best practises on AWS.<br>Over PB, Snowmobile is the one.<br>My apology for confusion.Amazon recommends Snowmobile for migrating more than 10 PB",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 332823,
          "date": "Sun 31 Oct 2021 10:41",
          "username": "sarah_t",
          "content": "Amazon recommends Snowmobile for migrating more than 10 PB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290927,
          "date": "Tue 26 Oct 2021 03:38",
          "username": "wind",
          "content": "B is wrong, the 1Gb DX has 100MB/s actual network bindwidth, this will be used by migrating the existing data and transferring new generated data, it's impossible. A is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289461,
          "date": "Mon 25 Oct 2021 17:22",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 288399,
          "date": "Mon 25 Oct 2021 15:07",
          "username": "AJBA",
          "content": "A should be the answer, It's very important that sometimes welook for the users using AWS recommendation. For PB of data, it should be snowball.",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#418",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics is run, and each execution takes about 4 hours to complete. The statistical analysis is not mission critical to the business, and previous data points are picked up on the next execution if a particular run fails.<br>The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations running full time to ingest and store the streaming data in attached Amazon EBS volumes. On-Demand EC2 instances are launched each night to perform the nightly processing, accessing the stored data from NFS shares on the ingestion servers, and terminating the nightly processing servers when complete. The Reserved Instance reservations are expiring, and the company needs to determine whether to purchase new reservations or implement a new design.<br>Which is the most cost-effective design?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#418",
          "answers": [
            {
              "choice": "<p>A. Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a fleet of On-Demand EC2 instances that launches each night to perform the batch processing of the S3 data and terminates when the processing completes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the ingestion process to use Amazon Kinesis Data Firehouse to save data to Amazon S3. Use AWS Batch to perform nightly processing with a Spot market bid of 50% of the On-Demand price.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the ingestion process to use a fleet of EC2 Reserved Instances behind a Network Load Balancer with 3-year leases. Use Batch with Spot instances with a maximum bid of 50% of the On-Demand price for the nightly processing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use an AWS Lambda function scheduled to run nightly with Amazon CloudWatch Events to query Amazon Redshift to generate the daily statistics.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13508,
          "date": "Thu 23 Sep 2021 22:06",
          "username": "donathonAWSPro24hammadrcher",
          "content": "B<br>A: On demand instances is more expensive.<br>B: Since the analysis is not mission critical and can be restarted, spot instances is cheaper.<br>C: EC2 is more expensive.<br>D: Compared to B Lambda should be cheaper to Batch using spot instances which ultimately still charges based on EC2 pricing. But critical thing is Lambda cannot go beyond 15 minutes of execution time. So it is more for simple processing and in this case it is not. It takes 4 hours.<br>https://www.simform.com/aws-lambda-vs-ec2/<br>https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/<br>https://aws.amazon.com/batch/faqs/?nc=sn&loc=5With the new Spot pricing model you don't set a spot bid only a \\\"Maximum Bid\\\" as in choice C. Thoughts?https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/Application on EC2 takes 4 hours and lambda does not require to take 4 hours. since it is not mission critical it can recollect even it fails. So D is correct.And its ok if the processing keep failing cause its Lambda? Might as well dont implement the solution if its not even working.",
          "upvote_count": "47214",
          "selected_answers": ""
        },
        {
          "id": 51033,
          "date": "Mon 04 Oct 2021 14:16",
          "username": "AWSPro24",
          "content": "With the new Spot pricing model you don't set a spot bid only a \\\"Maximum Bid\\\" as in choice C. Thoughts?https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 70035,
          "date": "Thu 07 Oct 2021 09:15",
          "username": "hammadrcher",
          "content": "Application on EC2 takes 4 hours and lambda does not require to take 4 hours. since it is not mission critical it can recollect even it fails. So D is correct.And its ok if the processing keep failing cause its Lambda? Might as well dont implement the solution if its not even working.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 276556,
          "date": "Sat 23 Oct 2021 20:11",
          "username": "rcher",
          "content": "And its ok if the processing keep failing cause its Lambda? Might as well dont implement the solution if its not even working.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 10310,
          "date": "Sun 19 Sep 2021 19:59",
          "username": "DalianYifangDalianYifangawsdogIsaacTehgaukharAWSum1",
          "content": "Ans is B to meNever mind, Amazon Redshift is the correct ans.Isn't B cheaper than d?do you think lambda can run 4 hours?D.  EC2 takes 4 hours, Redshift is meant for aggregate queries like the one mentioned in the question and as a result can run much faster than 4 hours. This is where real life experience would have come in handy. Ultimately the question is badly worded.Redshift can't be cheaper than Batch on spot? I'm looking at the question and whats jumping out to me is reproducible workloads and cost. <br><br>So on that thought I'm gonna say B",
          "upvote_count": "961712",
          "selected_answers": ""
        },
        {
          "id": 10312,
          "date": "Tue 21 Sep 2021 00:39",
          "username": "DalianYifangawsdogIsaacTehgaukharAWSum1",
          "content": "Never mind, Amazon Redshift is the correct ans.Isn't B cheaper than d?do you think lambda can run 4 hours?D.  EC2 takes 4 hours, Redshift is meant for aggregate queries like the one mentioned in the question and as a result can run much faster than 4 hours. This is where real life experience would have come in handy. Ultimately the question is badly worded.Redshift can't be cheaper than Batch on spot? I'm looking at the question and whats jumping out to me is reproducible workloads and cost. <br><br>So on that thought I'm gonna say B",
          "upvote_count": "61712",
          "selected_answers": ""
        },
        {
          "id": 12902,
          "date": "Thu 23 Sep 2021 17:29",
          "username": "awsdog",
          "content": "Isn't B cheaper than d?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 57170,
          "date": "Thu 07 Oct 2021 03:10",
          "username": "IsaacTehgaukhar",
          "content": "do you think lambda can run 4 hours?D.  EC2 takes 4 hours, Redshift is meant for aggregate queries like the one mentioned in the question and as a result can run much faster than 4 hours. This is where real life experience would have come in handy. Ultimately the question is badly worded.",
          "upvote_count": "71",
          "selected_answers": ""
        },
        {
          "id": 77310,
          "date": "Mon 11 Oct 2021 21:43",
          "username": "gaukhar",
          "content": "D.  EC2 takes 4 hours, Redshift is meant for aggregate queries like the one mentioned in the question and as a result can run much faster than 4 hours. This is where real life experience would have come in handy. Ultimately the question is badly worded.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 444769,
          "date": "Fri 05 Nov 2021 07:06",
          "username": "AWSum1",
          "content": "Redshift can't be cheaper than Batch on spot? I'm looking at the question and whats jumping out to me is reproducible workloads and cost. <br><br>So on that thought I'm gonna say B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 701113,
          "date": "Fri 21 Oct 2022 21:03",
          "username": "mrgreatness",
          "content": "I would say B as it mentions \\\"and previous data points are picked up on the next execution if a particular run fails.\\\" so if no spots instances avail one night its not a critical issue, as about cost saving this solution given in B works for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 680074,
          "date": "Mon 26 Sep 2022 20:49",
          "username": "Ni_yot",
          "content": "B for sure.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 655553,
          "date": "Thu 01 Sep 2022 00:27",
          "username": "epomatti",
          "content": "It's B - Firehose is infinitely cheaper than EC2 and it fits the use case.<br><br>Lambda cannot run for 4h, limit is 15 min.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 623507,
          "date": "Mon 27 Jun 2022 23:52",
          "username": "kangtamo",
          "content": "Agree with B: Spot instance.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 497779,
          "date": "Thu 09 Dec 2021 15:04",
          "username": "cldy",
          "content": "B.  Update the ingestion process to use Amazon Kinesis Data Firehouse to save data to Amazon S3. Use AWS Batch to perform nightly processing with a Spot market bid of 50% of the On-Demand price.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493426,
          "date": "Sat 04 Dec 2021 01:14",
          "username": "AzureDP900",
          "content": "my answer is B, Lambda is only valid for 15 min",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 467704,
          "date": "Sat 06 Nov 2021 22:14",
          "username": "andypham",
          "content": "Definitely B is correct ! Lambda can not execute 4 hours job.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450006,
          "date": "Sat 06 Nov 2021 15:41",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 378860,
          "date": "Thu 04 Nov 2021 14:34",
          "username": "nisoshabangu",
          "content": "Naswe is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 367361,
          "date": "Wed 03 Nov 2021 17:54",
          "username": "zolthar_z",
          "content": "The answer is B, remember that lambda has a timeout for 15minutes. Redshift could improve the process but this exam is based in the given information and they put the 4 Hours time for a reason,",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 342616,
          "date": "Tue 02 Nov 2021 19:30",
          "username": "digimaniac",
          "content": "unfortunately, the answer is D.  this implies that Redshift can do a much better job of aggregating stats than the home brew solution, which takes 4 hours. Redshift is a data warehouse with some adv. data processing horsepower.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 336884,
          "date": "Fri 29 Oct 2021 02:12",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 307155,
          "date": "Thu 28 Oct 2021 03:57",
          "username": "Pupu86",
          "content": "I wish to negate answer B as it says Kinesis Firehouse however it is the only option with spot instance in this non-time sensitive & \\\"re-doable\\\" scenario. So my answer still stick with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 290931,
          "date": "Wed 27 Oct 2021 14:49",
          "username": "wind",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289464,
          "date": "Wed 27 Oct 2021 05:03",
          "username": "Kian1",
          "content": "only B",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#419",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A three-tier web application runs on Amazon EC2 instances. Cron daemons are used to trigger scripts that collect the web server, application, and database logs and send them to a centralized location every hour. Occasionally, scaling events or unplanned outages have caused the instances to stop before the latest logs were collected, and the log files were lost.<br>Which of the following options is the MOST reliable way of collecting and preserving the log files?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#419",
          "answers": [
            {
              "choice": "<p>A. Update the cron jobs to run every 5 minutes instead of every hour to reduce the possibility of log messages being lost in an outage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon CloudWatch Events to trigger Amazon Systems Manager Run Command to invoke the log collection scripts more frequently to reduce the possibility of log messages being lost in an outage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the Amazon CloudWatch Logs agent to stream log messages directly to CloudWatch Logs. Configure the agent with a batch count of 1 to reduce the possibility of log messages being lost in an outage.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon CloudWatch Events to trigger AWS Lambda to SSH into each running instance and invoke the log collection scripts more frequently to reduce the possibility of log messages being lost in an outage.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13510,
          "date": "Tue 28 Sep 2021 18:41",
          "username": "donathonMoonSachinJhaDerekKey",
          "content": "C<br>Adjusting the batch count to 1 means transferring the log to CloudWatch logs after every event instead of 100 event for example to make it closer to real time.<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.htmlAgree...I agree with C but keep a watch on CloudWatch PutEvent limit which could cause throttling and hence loss of logs. See https://aws.amazon.com/about-aws/whats-new/2017/07/cloudwatch-events-increases-rules-and-api-requests-limits/40 KB for HTTP POST requests. PutMetricData can handle 150 transactions per second (TPS), which is the maximum number of operation requests you can make per second without being throttled.<br>You can request a quota increase.",
          "upvote_count": "49113",
          "selected_answers": ""
        },
        {
          "id": 14613,
          "date": "Thu 30 Sep 2021 17:54",
          "username": "Moon",
          "content": "Agree...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 258724,
          "date": "Tue 26 Oct 2021 05:03",
          "username": "SachinJhaDerekKey",
          "content": "I agree with C but keep a watch on CloudWatch PutEvent limit which could cause throttling and hence loss of logs. See https://aws.amazon.com/about-aws/whats-new/2017/07/cloudwatch-events-increases-rules-and-api-requests-limits/40 KB for HTTP POST requests. PutMetricData can handle 150 transactions per second (TPS), which is the maximum number of operation requests you can make per second without being throttled.<br>You can request a quota increase.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 408895,
          "date": "Fri 05 Nov 2021 02:55",
          "username": "DerekKey",
          "content": "40 KB for HTTP POST requests. PutMetricData can handle 150 transactions per second (TPS), which is the maximum number of operation requests you can make per second without being throttled.<br>You can request a quota increase.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 11024,
          "date": "Mon 27 Sep 2021 06:20",
          "username": "dpvnme",
          "content": "C makes more sense",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 506484,
          "date": "Tue 21 Dec 2021 23:20",
          "username": "AzureDP900",
          "content": "I'll go with C.  There is similar question in Neal Davis practice tests with 30 minutes :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496797,
          "date": "Wed 08 Dec 2021 13:11",
          "username": "cldy",
          "content": "C.  Use the Amazon CloudWatch Logs agent to stream log messages directly to CloudWatch Logs. Configure the agent with a batch count of 1 to reduce the possibility of log messages being lost in an outage.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493429,
          "date": "Sat 04 Dec 2021 01:19",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484031,
          "date": "Mon 22 Nov 2021 10:20",
          "username": "acloudguru",
          "content": "hope i can have such simple question in my exam.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450009,
          "date": "Sun 07 Nov 2021 16:56",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 424370,
          "date": "Sat 06 Nov 2021 01:33",
          "username": "mimadour21698",
          "content": "I go for C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362516,
          "date": "Wed 03 Nov 2021 21:26",
          "username": "Radhaghosh",
          "content": "Option C is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 336888,
          "date": "Tue 02 Nov 2021 15:22",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290943,
          "date": "Tue 02 Nov 2021 04:16",
          "username": "wind",
          "content": "B is correct. \\\"You can now create CloudWatch Events rules that use EC2 Run Command to perform actions on EC2 instances or on-premises servers. This opens the door to all sorts of interesting ideas; here are a few that I came up with:<br><br>Final Log Collection – Collect application or system logs from instances that are being shut down (either manually or as a result of a scale-in operation initiated by Auto Scaling).\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289470,
          "date": "Mon 01 Nov 2021 11:22",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 288505,
          "date": "Fri 29 Oct 2021 13:49",
          "username": "AJBA",
          "content": "its sure is B.  Focus on this line \\\" Occasionally, scaling events or unplanned outages have caused the instances to stop before the latest logs were collected, and the log files were lost.\\\" Now read this link<br>https://aws.amazon.com/blogs/aws/ec2-run-command-is-now-a-cloudwatch-events-target/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283997,
          "date": "Fri 29 Oct 2021 04:29",
          "username": "Ebi",
          "content": "C is my choice",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281542,
          "date": "Wed 27 Oct 2021 18:55",
          "username": "certainly",
          "content": "B says Amazon Kinesis Data \\\"Firehouse\\\" instead of \\\"Firehose\\\". I would go forA. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 262961,
          "date": "Wed 27 Oct 2021 15:26",
          "username": "sanjaym",
          "content": "I'll with C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 241960,
          "date": "Mon 25 Oct 2021 20:21",
          "username": "T14102020",
          "content": "C is correct answer. Batch count is 1 means send every event.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#420",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company stores sales transaction data in Amazon DynamoDB tables. To detect anomalous behaviors and respond quickly, all changes to the items stored in the DynamoDB tables must be logged within 30 minutes.<br>Which solution meets the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#420",
          "answers": [
            {
              "choice": "<p>A. Copy the DynamoDB tables into Apache Hive tables on Amazon EMR every hour and analyze them for anomalous behaviors. Send Amazon SNS notifications when anomalous behaviors are detected.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS CloudTrail to capture all the APIs that change the DynamoDB tables. Send SNS notifications when anomalous behaviors are detected using CloudTrail event filtering.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon DynamoDB Streams to capture and send updates to AWS Lambda. Create a Lambda function to output records to Amazon Kinesis Data Streams. Analyze any anomalies with Amazon Kinesis Data Analytics. Send SNS notifications when anomalous behaviors are detected.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use event patterns in Amazon CloudWatch Events to capture DynamoDB API call events with an AWS Lambda function as a target to analyze behavior. Send SNS notifications when anomalous behaviors are detected.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 241966,
          "date": "Wed 06 Oct 2021 17:40",
          "username": "T14102020",
          "content": "Correct answer is C.  DynamoDB Streams is key",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 344338,
          "date": "Mon 18 Oct 2021 16:34",
          "username": "blackgamer",
          "content": "C is correct. <br>https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/#:~:text=DynamoDB%20Streams%20is%20a%20powerful,for%20up%20to%2024%20hours.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 716759,
          "date": "Sat 12 Nov 2022 15:48",
          "username": "DarthYoda",
          "content": "I'd go with C too",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 671478,
          "date": "Sat 17 Sep 2022 13:47",
          "username": "jujumomma",
          "content": "C<br>https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/Streams.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 623165,
          "date": "Mon 27 Jun 2022 11:18",
          "username": "TechXTechX",
          "content": "C is OK, but I wonder that is it too fast with thequestione only require within 30 minutes, while with DynamoDB Streams wehave anh real-time...Missspelling:<br>C is OK, but I wonder that is it too fast with the questione only require within 30 minutes, while with DynamoDB Streams we have a real-time...",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 623166,
          "date": "Mon 27 Jun 2022 11:18",
          "username": "TechX",
          "content": "Missspelling:<br>C is OK, but I wonder that is it too fast with the questione only require within 30 minutes, while with DynamoDB Streams we have a real-time...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514390,
          "date": "Sat 01 Jan 2022 06:44",
          "username": "cldy",
          "content": "C correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493431,
          "date": "Sat 04 Dec 2021 01:20",
          "username": "AzureDP900",
          "content": "C is right Dynamo DB streams is key",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 491121,
          "date": "Wed 01 Dec 2021 02:30",
          "username": "acloudguru",
          "content": "C,Amazon DynamoDB Streamsis designed for this ,this is a simple question, hope i can have it in my exam",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 464456,
          "date": "Sat 06 Nov 2021 18:31",
          "username": "nsei",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450015,
          "date": "Thu 04 Nov 2021 02:21",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447068,
          "date": "Thu 28 Oct 2021 13:28",
          "username": "moon2351",
          "content": "Answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440693,
          "date": "Mon 25 Oct 2021 00:41",
          "username": "student22",
          "content": "Answer is C<br>D would not work without Cloudtrail. If that was the case, it would have been the better solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 396371,
          "date": "Sun 24 Oct 2021 01:14",
          "username": "KittuCheeku",
          "content": "Option: C, DynamoDB Streams + KDS + Lambda function + KDA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 369450,
          "date": "Fri 22 Oct 2021 08:26",
          "username": "Kukkuji",
          "content": "Correct Answer is C, keyword -Amazon DynamoDB Streams to keep the records of changes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362519,
          "date": "Tue 19 Oct 2021 05:17",
          "username": "Radhaghosh",
          "content": "Only correct option is C. <br>DynamoDb Stream to capture DynamoDB update. And Kinesis Data Analytics for anomaly detection (it uses AWS proprietary Random Cut Forest Algorithm)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 336889,
          "date": "Sun 17 Oct 2021 22:41",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 335248,
          "date": "Fri 15 Oct 2021 12:32",
          "username": "ppshein",
          "content": "C is correct one as to detect.<br>D is kinda tricky as not to detect.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#421",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running multiple applications on Amazon EC2. Each application is deployed and managed by multiple business units. All applications are deployed on a single AWS account but on different virtual private clouds (VPCs). The company uses a separate VPC in the same account for test and development purposes.<br>Production applications suffered multiple outages when users accidentally terminated and modified resources that belonged to another business unit. A Solutions<br>Architect has been asked to improve the availability of the company applications while allowing the Developers access to the resources they need.<br>Which option meets the requirements with the LEAST disruption?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#421",
          "answers": [
            {
              "choice": "<p>A. Create an AWS account for each business unit. Move each business unit's instances to its own account and set up a federation to allow users to access their business unit's account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up a federation to allow users to use their corporate credentials, and lock the users down to their own VPC.  Use a network ACL to block each VPC from accessing other VPCs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement a tagging policy based on business units. Create an IAM policy so that each user can terminate instances belonging to their own business units only.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up role-based access for each user and provide limited permissions based on individual roles and the services for which each user is responsible.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13217,
          "date": "Thu 23 Sep 2021 03:19",
          "username": "donathonAWSPro24",
          "content": "C<br>Principal – Control what the person making the request (the principal) is allowed to do based on the tags that are attached to that person's IAM user or role. To do this, use the aws:PrincipalTag/key-name condition key to specify what tags must be attached to the IAM user or role before the request is allowed. https://docs.aws.amazon.com/IAM/latest/UserGuide/access_iam-tags.html<br>A: This would be too disruptive and Organizations should be used instead.<br>B: Question did not say if prod\\dev\\test are in separate VPC or not. It could be separated using business units instead. Hence this is not feasible.<br>D: This is too much effort and disruption.C is correct.Good resource here: https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/",
          "upvote_count": "3411",
          "selected_answers": ""
        },
        {
          "id": 51029,
          "date": "Mon 27 Sep 2021 21:24",
          "username": "AWSPro24",
          "content": "C is correct.Good resource here: https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 64496,
          "date": "Fri 01 Oct 2021 14:25",
          "username": "sb333nqobzaSmart",
          "content": "D is the correct answer. There is no disruption to users by setting up roles and policies. Using least privileges is obviously something that was not setup and really needs to be.<br>C is not correct as it does not cover the scenario. The issue was that people were terminating AND modifying resources of other people. Answer C only addresses terminating. It doesn't completely address the problem, so it's not correct.It's not the resources of other people we are protecting, it's resources of other business units. C is correct. D will actually get out of hand almost immediately because of the roles and permissions we will have to define.Agreed. The first essential step is to apply 'Least Privilege Principal' then optimize further with Tagging-based Policy. Option C is the least disruptive as the users don't have do anything on their end; however, restrictions are not appropriately set. Option D is minimally disruptive as users would have to be notified of the roles they can assume. If not setup properly, there will be additional disruption. However, it does fulfill the requirement.",
          "upvote_count": "1354",
          "selected_answers": ""
        },
        {
          "id": 236413,
          "date": "Mon 18 Oct 2021 16:47",
          "username": "nqobza",
          "content": "It's not the resources of other people we are protecting, it's resources of other business units. C is correct. D will actually get out of hand almost immediately because of the roles and permissions we will have to define.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 69871,
          "date": "Sun 03 Oct 2021 21:19",
          "username": "Smart",
          "content": "Agreed. The first essential step is to apply 'Least Privilege Principal' then optimize further with Tagging-based Policy. Option C is the least disruptive as the users don't have do anything on their end; however, restrictions are not appropriately set. Option D is minimally disruptive as users would have to be notified of the roles they can assume. If not setup properly, there will be additional disruption. However, it does fulfill the requirement.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 701120,
          "date": "Fri 21 Oct 2022 21:17",
          "username": "mrgreatness",
          "content": "100% C - least disruption so no new accounts, and tagging policy works, can use resource tag",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 661461,
          "date": "Tue 06 Sep 2022 18:41",
          "username": "Network_1",
          "content": "I will go with C.  \\\"another business unit\\\" is the key phrase in the statement, 'Production applications suffered multiple outages when users accidentally terminated and modified resources that belonged to another business unit.'",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 543009,
          "date": "Tue 08 Feb 2022 12:29",
          "username": "Ishu_awsguy",
          "content": "C is the correct answer<br>You can then create an IAM policy that allows or denies access to a resource based on that resource's tag. In that policy, you can use tag condition keys to control access to any of the following:<br><br>Resource – Control access to AWS service resources based on the tags on those resources. To do this, use the ResourceTag/key-name condition key to determine whether to allow access to the resource based on the tags that are attached to the resource.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498321,
          "date": "Fri 10 Dec 2021 06:30",
          "username": "cldy",
          "content": "C.  Implement a tagging policy based on business units. Create an IAM policy so that each user can terminate instances belonging to their own business units only.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493432,
          "date": "Sat 04 Dec 2021 01:22",
          "username": "AzureDP900",
          "content": "C is right.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 488631,
          "date": "Sun 28 Nov 2021 02:09",
          "username": "acloudguru",
          "content": "hope i can have this easy one in my exam",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450307,
          "date": "Sun 07 Nov 2021 17:16",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 439470,
          "date": "Sun 07 Nov 2021 12:09",
          "username": "AWS_Noob",
          "content": "C - ABAC",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 437804,
          "date": "Sun 07 Nov 2021 08:29",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349315,
          "date": "Fri 05 Nov 2021 08:49",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343720,
          "date": "Fri 05 Nov 2021 05:31",
          "username": "Waiweng",
          "content": "C it is",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294671,
          "date": "Wed 03 Nov 2021 16:07",
          "username": "gparkgpark",
          "content": "C. <br>It could be a quite burden to control individual developers based on \\\"limited permissions based on individual roles\\\". Supposed that the company has 1000s of employees.D could not be correct.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 294672,
          "date": "Wed 03 Nov 2021 18:49",
          "username": "gpark",
          "content": "D could not be correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289474,
          "date": "Tue 02 Nov 2021 21:38",
          "username": "Kian1",
          "content": "C is the right ans",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 288696,
          "date": "Tue 02 Nov 2021 14:01",
          "username": "AJBA",
          "content": "I go with C<br>https://aws.amazon.com/blogs/security/simplify-granting-access-to-your-aws-resources-by-using-tags-on-aws-iam-users-and-roles/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 283352,
          "date": "Mon 01 Nov 2021 18:41",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#422",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An enterprise runs 103 line-of-business applications on virtual machines in an on-premises data center. Many of the applications are simple PHP, Java, or Ruby web applications, are no longer actively developed, and serve little traffic.<br>Which approach should be used to migrate these applications to AWS with the LOWEST infrastructure costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#422",
          "answers": [
            {
              "choice": "<p>A. Deploy the applications to single-instance AWS Elastic Beanstalk environments without a load balancer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS SMS to create AMIs for each virtual machine and run them in Amazon EC2.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Convert each application to a Docker image and deploy to a small Amazon ECS cluster behind an Application Load Balancer.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use VM Import/Export to create AMIs for each virtual machine and run them in single-instance AWS Elastic Beanstalk environments by configuring a custom image.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 24046,
          "date": "Mon 27 Sep 2021 06:45",
          "username": "examaccAWSPro24AWSPro24",
          "content": "C is right. it has lowest infra costs. Rest all options will need VMs for each service.Hey i found a cool discussion of exactly this use case on Reddit after I wrote my previous comment. https://www.reddit.com/r/aws/comments/5j9r31/one_or_multiple_apps_per_ecs_cluster/I agree with this.If they are all tiny aps and serve nearly no traffic you can host them all on a very small ECS cluster.All 103 of them.A is out because how would you route to 103 separate apps with no LB.  B is out because you'll need 103 EC2 instances which will have a significant cost.D is out because if it's a single-instance environment you're still going to have 103 EB environments which means 103 EC2 instances behind them.ECS is the only option that allows you to have 103 apps and all their runtime environements smushed together in the smallest set of infrastructure just to \\\"keep the lights on\\\".",
          "upvote_count": "36218",
          "selected_answers": ""
        },
        {
          "id": 51021,
          "date": "Thu 30 Sep 2021 06:49",
          "username": "AWSPro24",
          "content": "Hey i found a cool discussion of exactly this use case on Reddit after I wrote my previous comment. https://www.reddit.com/r/aws/comments/5j9r31/one_or_multiple_apps_per_ecs_cluster/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 51020,
          "date": "Wed 29 Sep 2021 20:34",
          "username": "AWSPro24",
          "content": "I agree with this.If they are all tiny aps and serve nearly no traffic you can host them all on a very small ECS cluster.All 103 of them.A is out because how would you route to 103 separate apps with no LB.  B is out because you'll need 103 EC2 instances which will have a significant cost.D is out because if it's a single-instance environment you're still going to have 103 EB environments which means 103 EC2 instances behind them.ECS is the only option that allows you to have 103 apps and all their runtime environements smushed together in the smallest set of infrastructure just to \\\"keep the lights on\\\".",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 16153,
          "date": "Fri 24 Sep 2021 08:11",
          "username": "chaudhSathish1412gpark",
          "content": "C will give lowest infrastructure cost. All other solutions will need 103 instancesNo need 103 Instances, We can use the dynamic port and 1 instance we can run multiple containers(tasks) it's depending on your instance type...This is a fair point.<br>I have left the comment to make people check upvoted count.",
          "upvote_count": "2011",
          "selected_answers": ""
        },
        {
          "id": 670102,
          "date": "Thu 15 Sep 2022 18:16",
          "username": "Sathish1412",
          "content": "No need 103 Instances, We can use the dynamic port and 1 instance we can run multiple containers(tasks) it's depending on your instance type...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 298753,
          "date": "Tue 02 Nov 2021 17:38",
          "username": "gpark",
          "content": "This is a fair point.<br>I have left the comment to make people check upvoted count.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713287,
          "date": "Mon 07 Nov 2022 20:48",
          "username": "Yvarr",
          "content": "C, Though in reality ALB supports only 100 target groups, so it will have to be 2 ALB's. Still probably best option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708771,
          "date": "Tue 01 Nov 2022 00:46",
          "username": "AjayPrajapati",
          "content": "Single instance is too risky. C is good with small container to serve small traffic",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 701143,
          "date": "Fri 21 Oct 2022 22:16",
          "username": "mrgreatness",
          "content": "C it seems asbasic applications only need a run time environment in a container and not a full VM.. as 103 applications what instance type would we choose here?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 635253,
          "date": "Fri 22 Jul 2022 16:59",
          "username": "asfsdfsdf",
          "content": "C is the most correct one.<br>A is the least expensive but will not work since there can be multiple application listening to port 443 or 80 and it will break the deployment.<br>B and D will create 103 EC2 instances its not cost effective",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 614349,
          "date": "Fri 10 Jun 2022 07:55",
          "username": "Alvindo",
          "content": "A is correct as the apps are not developed and you can upgrade the environment from a single instance one to a fully load-balanced production environment.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 611103,
          "date": "Fri 03 Jun 2022 15:34",
          "username": "bobsmith2000",
          "content": "The cheapest one. Fixed small number of instances and numerous hardly used applications",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 546511,
          "date": "Sun 13 Feb 2022 15:55",
          "username": "RVivek",
          "content": "A is LEAST expensive infrastrucreas requested in the quetion",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 506501,
          "date": "Wed 22 Dec 2021 00:12",
          "username": "AzureDP900JohnPi",
          "content": "A.  Deploy the applications to single-instance AWS Elastic Beanstalk environments without a load balancer.Please check Neal Davis questions for explanation.Neal Davis's question is related to \\\"LOWEST operational overhead\\\".",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 686727,
          "date": "Wed 05 Oct 2022 11:03",
          "username": "JohnPi",
          "content": "Neal Davis's question is related to \\\"LOWEST operational overhead\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506491,
          "date": "Tue 21 Dec 2021 23:31",
          "username": "CloudChef",
          "content": "A is right.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496809,
          "date": "Wed 08 Dec 2021 13:26",
          "username": "cldy",
          "content": "A.  Deploy the applications to single-instance AWS Elastic Beanstalk environments without a load balancer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484035,
          "date": "Mon 22 Nov 2021 10:25",
          "username": "acloudguru",
          "content": "hope i can have such simple questions in my exam",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450308,
          "date": "Sat 06 Nov 2021 04:37",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437805,
          "date": "Fri 05 Nov 2021 14:39",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409096,
          "date": "Fri 05 Nov 2021 03:19",
          "username": "DerekKey",
          "content": "C should be the answer - especially with binpack placement strategy<br>A = B = D<br>Single-instance AWS Elastic Beanstalk environments = multiple single EC2 instances<br>Elastic Beanstalk is nothing more but EC2 managed by AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 400692,
          "date": "Thu 04 Nov 2021 01:01",
          "username": "dilprt123",
          "content": "A is right, is cheaper than C where it runs a cluster and ALB.  The key is lowest infra cost & serves little (means no) traffic. So no need of ALB<br>D is not possible as different VMs cannot be run on the same instance<br>B is very expensive for apps that serves no traffic",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#423",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must create a cost-effective backup solution for a company's 500MB source code repository of proprietary and sensitive applications. The repository runs on Linux and backs up daily to tape. Tape backups are stored for 1 year.<br>The current solution is not meeting the company's needs because it is a manual process that is prone to error, expensive to maintain, and does not meet the need for a Recovery Point Objective (RPO) of 1 hour or Recovery Time Objective (RTO) of 2 hours. The new disaster recovery requirement is for backups to be stored offsite and to be able to restore a single file if needed.<br>Which solution meets the customer's needs for RTO, RPO, and disaster recovery with the LEAST effort and expense?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#423",
          "answers": [
            {
              "choice": "<p>A. Replace local tapes with an AWS Storage Gateway virtual tape library to integrate with current backup software. Run backups nightly and store the virtual tapes on Amazon S3 standard storage in US-EAST-1. Use cross-region replication to create a second copy in US-WEST-2. Use Amazon S3 lifecycle policies to perform automatic migration to Amazon Glacier and deletion of expired backups after 1 year.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the local source code repository to synchronize files to an AWS Storage Gateway file Amazon gateway to store backup copies in an Amazon S3 Standard bucket. Enable versioning on the Amazon S3 bucket. Create Amazon S3 lifecycle policies to automatically migrate old versions of objects to Amazon S3 Standard - Infrequent Access, then Amazon Glacier, then delete backups after 1 year.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace the local source code repository storage with a Storage Gateway stored volume. Change the default snapshot frequency to 1 hour. Use Amazon S3 lifecycle policies to archive snapshots to Amazon Glacier and remove old snapshots after 1 year. Use cross-region replication to create a copy of the snapshots in US-WEST-2.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replace the local source code repository storage with a Storage Gateway cached volume. Create a snapshot schedule to take hourly snapshots. Use an Amazon CloudWatch Events schedule expression rule to run an hourly AWS Lambda task to copy snapshots from US-EAST -1 to US-WEST-2.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14443,
          "date": "Thu 23 Sep 2021 11:58",
          "username": "MoonFrank1easytoo7thGuest",
          "content": "Tough question. I do support answer \\\"B\\\".<br>Eventhough there is no cross region replication, but that is not a requirement in the question. The requirement is just an offsite (AWS) disaster recovery. Therefore, a single copy in AWS would make the deal.<br>Also, there is a tricky requirement of restoring a SINGLE FILE! <br>The snapshot of Storage Gateway (cached or stored, or tape) are storing the backup as a whole, and not as files! that mean to restore, you need to build a snapshot, and mount the snapshot into an EC2, then restore the files. Therefore, it needs effort to restore, un-like the (File storage), which store the files as they are in S3 bucket! Where pulling a file is very straight forward.<br>https://d1.awsstatic.com/whitepapers/aws-storage-gateway-file-gateway-for-hybrid-architectures.pdf<br>A: nightly backup, does not meet the requirement. Plus restoration effort.<br>C & D: both are working solutions, and valid too. however restoration effort, and cost is higher than B. <br>Again, it is a tough decision.very good explaination. I think the restore file requirement eliminates C and DMoon is the man.On answer \\\"B\\\" I was concerned about the RTO because of the Glacier Service, but if they use expedited retrievals the information will be available between 1 to 5 minutes. So, for me the correct answer is \\\"B\\\".",
          "upvote_count": "68423",
          "selected_answers": ""
        },
        {
          "id": 21349,
          "date": "Fri 24 Sep 2021 12:53",
          "username": "Frank1",
          "content": "very good explaination. I think the restore file requirement eliminates C and D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 86788,
          "date": "Mon 27 Sep 2021 05:48",
          "username": "easytoo",
          "content": "Moon is the man.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 150512,
          "date": "Thu 07 Oct 2021 22:59",
          "username": "7thGuest",
          "content": "On answer \\\"B\\\" I was concerned about the RTO because of the Glacier Service, but if they use expedited retrievals the information will be available between 1 to 5 minutes. So, for me the correct answer is \\\"B\\\".",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 13220,
          "date": "Mon 20 Sep 2021 02:43",
          "username": "donathondonathonawsprohilft",
          "content": "C<br>In the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS. Because you already have a full copy on premise, you do not need too many versions on S3 and it can go straight to glacier. You will need to mount the snapshot for single file recovery though. https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6<br>A: Run backup nightly: does not meet RPO of 1 hour.<br>B: Does not address the RPO of 1 hour. Versioning needs to be set to hourly. Not the least expense and no cross-region replication.<br>D: Because this is a cache copy, during restore, you will need to download the whole volume from S3 which may exceed the 2 hour RTO.Answer changed to B. <br>https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html<br>A: Run backup nightly: does not meet RPO of 1 hour.<br>B: Although it does not have hourly snapshot, it has versioning configured. This is better for file based recovery. The question only needs the backup to be stored offsite so this actually does satisfy the requirement.<br>C: Because this uses cross region replication, it has 2 copies and double the cost.<br>D: Because this is a cache copy, during restore, you will need to download the whole volume from S3 which may exceed the 2 hour RTO.Thank you so muchI went to B because C didn't mention about hourly backup. It's true that version can handle... Thanks",
          "upvote_count": "112911",
          "selected_answers": ""
        },
        {
          "id": 14047,
          "date": "Tue 21 Sep 2021 20:07",
          "username": "donathon",
          "content": "Answer changed to B. <br>https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html<br>A: Run backup nightly: does not meet RPO of 1 hour.<br>B: Although it does not have hourly snapshot, it has versioning configured. This is better for file based recovery. The question only needs the backup to be stored offsite so this actually does satisfy the requirement.<br>C: Because this uses cross region replication, it has 2 copies and double the cost.<br>D: Because this is a cache copy, during restore, you will need to download the whole volume from S3 which may exceed the 2 hour RTO.",
          "upvote_count": "29",
          "selected_answers": ""
        },
        {
          "id": 13398,
          "date": "Tue 21 Sep 2021 08:21",
          "username": "awspro",
          "content": "Thank you so much",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 636864,
          "date": "Mon 25 Jul 2022 19:37",
          "username": "hilft",
          "content": "I went to B because C didn't mention about hourly backup. It's true that version can handle... Thanks",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709091,
          "date": "Tue 01 Nov 2022 13:07",
          "username": "Jonfernz",
          "content": "\\\"able to restore a single file if needed\\\" = must use file gateway --- Configure the local source code repository to synchronize files to an AWS Storage Gateway file Amazon gateway to store backup copies in an Amazon S3 Standard bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 686736,
          "date": "Wed 05 Oct 2022 11:14",
          "username": "JohnPi",
          "content": "B<br>requirement:\\\"to be able to restore a single file if needed.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 626441,
          "date": "Sun 03 Jul 2022 08:05",
          "username": "aandc",
          "content": "turn to B after reading all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 550818,
          "date": "Sat 19 Feb 2022 10:54",
          "username": "cannottellnamesuperuser784",
          "content": "How can we use Amazon Storage File Gateway for Tape Backup? I believe options are incorrect here.you are not storing the whole backup, in this case the solution the solution will store each file",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 708125,
          "date": "Mon 31 Oct 2022 02:43",
          "username": "superuser784",
          "content": "you are not storing the whole backup, in this case the solution the solution will store each file",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532714,
          "date": "Wed 26 Jan 2022 09:50",
          "username": "shotty1",
          "content": "I selected B because restore of individual files will be easiest with this solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 526161,
          "date": "Mon 17 Jan 2022 23:30",
          "username": "weurseuk",
          "content": "Definitively B : because A :because VT are presented to app bck as iscsi when tape is ejected from app bck, gateway mark data as ro and archive them to glacier on s3, it's not <br>the LEAST amount of work and expense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 522122,
          "date": "Wed 12 Jan 2022 13:03",
          "username": "pititcu667aandc",
          "content": "with the LEAST amount of work and expense? Since they already have a solution that backs up to tape A) makes more sense. I know that B isa better option but it requires work.A -> nightly backup, RPO cannot be meet",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 626442,
          "date": "Sun 03 Jul 2022 08:06",
          "username": "aandc",
          "content": "A -> nightly backup, RPO cannot be meet",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493436,
          "date": "Sat 04 Dec 2021 01:28",
          "username": "AzureDP900",
          "content": "B seems reasonable in this case",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450311,
          "date": "Sun 07 Nov 2021 16:30",
          "username": "andylogan",
          "content": "It's B - versioning",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437806,
          "date": "Sun 07 Nov 2021 14:04",
          "username": "tgv",
          "content": "BBB<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 411390,
          "date": "Fri 05 Nov 2021 23:45",
          "username": "aishvary123",
          "content": "my pick is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 369471,
          "date": "Thu 04 Nov 2021 18:33",
          "username": "Kukkuji",
          "content": "I will go with answer B, file is always available in cache of File Storage Gateway which is synced up with S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351880,
          "date": "Thu 04 Nov 2021 00:50",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343724,
          "date": "Sun 31 Oct 2021 16:47",
          "username": "Waiweng",
          "content": "definitely B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291016,
          "date": "Sat 30 Oct 2021 03:11",
          "username": "wind",
          "content": "B is correct. Note: \\\"restore a single file\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#424",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company CFO recently analyzed the company's AWS monthly bill and identified an opportunity to reduce the cost for AWS Elastic Beanstalk environments in use. The CFO has asked a Solutions Architect to design a highly available solution that will spin up an Elastic Beanstalk environment in the morning and terminate it at the end of the day.<br>The solution should be designed with minimal operational overhead and to minimize costs. It should also be able to handle the increased use of Elastic Beanstalk environments among different teams, and must provide a one-stop scheduler solution for all teams to keep the operational costs low.<br>What design will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#424",
          "answers": [
            {
              "choice": "<p>A. Set up a Linux EC2 Micro instance. Configure an IAM role to allow the start and stop of the Elastic Beanstalk environment and attach it to the instance. Create scripts on the instance to start and stop the Elastic Beanstalk environment. Configure cron jobs on the instance to execute the scripts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Develop AWS Lambda functions to start and stop the Elastic Beanstalk environment. Configure a Lambda execution role granting Elastic Beanstalk environment start/stop permissions, and assign the role to the Lambda functions. Configure cron expression Amazon CloudWatch Events rules to trigger the Lambda functions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Develop an AWS Step Functions state machine with ג€waitג€ as its type to control the start and stop time. Use the activity task to start and stop the Elastic Beanstalk environment. Create a role for Step Functions to allow it to start and stop the Elastic Beanstalk environment. Invoke Step Functions daily.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a time-based Auto Scaling group. In the morning, have the Auto Scaling group scale up an Amazon EC2 instance and put the Elastic Beanstalk environment start command in the EC2 instance user data. At the end of the day, scale down the instance number to 0 to terminate the EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 10721,
          "date": "Mon 20 Sep 2021 01:07",
          "username": "awsec2dpvnmepudakSmartSmart",
          "content": "b.<br>https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/lambda cannot trigger itself. step function is the best choice herein B,'Configure cron expression Amazon CloudWatch Events rules to trigger<br>the Lambda functions.'lambda can run on schedule basis.nvm. interface has changed. now happens through CW Events.",
          "upvote_count": "313821",
          "selected_answers": ""
        },
        {
          "id": 11453,
          "date": "Mon 20 Sep 2021 07:37",
          "username": "dpvnmepudakSmartSmart",
          "content": "lambda cannot trigger itself. step function is the best choice herein B,'Configure cron expression Amazon CloudWatch Events rules to trigger<br>the Lambda functions.'lambda can run on schedule basis.nvm. interface has changed. now happens through CW Events.",
          "upvote_count": "3821",
          "selected_answers": ""
        },
        {
          "id": 13404,
          "date": "Tue 21 Sep 2021 13:04",
          "username": "pudak",
          "content": "in B,'Configure cron expression Amazon CloudWatch Events rules to trigger<br>the Lambda functions.'",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 69913,
          "date": "Tue 28 Sep 2021 11:35",
          "username": "SmartSmart",
          "content": "lambda can run on schedule basis.nvm. interface has changed. now happens through CW Events.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 69917,
          "date": "Thu 30 Sep 2021 07:49",
          "username": "Smart",
          "content": "nvm. interface has changed. now happens through CW Events.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 14048,
          "date": "Tue 21 Sep 2021 13:45",
          "username": "donathontextstudent22tan9",
          "content": "B<br>https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/hi donathon, would you explain why you prefer B better than CAs newme has mentioned below,\\\"Invoke Step Functions daily\\\" sounds like manually.I will choose C as the solution should be \\\"a one-stop scheduler solution for all teams\\\".<br>And Step Functions is eligible since it is:<br>1.it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.<br>2. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and *IT automation*.",
          "upvote_count": "15121",
          "selected_answers": ""
        },
        {
          "id": 24844,
          "date": "Tue 21 Sep 2021 21:03",
          "username": "textstudent22",
          "content": "hi donathon, would you explain why you prefer B better than CAs newme has mentioned below,\\\"Invoke Step Functions daily\\\" sounds like manually.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 461723,
          "date": "Fri 05 Nov 2021 08:52",
          "username": "student22",
          "content": "As newme has mentioned below,\\\"Invoke Step Functions daily\\\" sounds like manually.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 29492,
          "date": "Thu 23 Sep 2021 13:23",
          "username": "tan9",
          "content": "I will choose C as the solution should be \\\"a one-stop scheduler solution for all teams\\\".<br>And Step Functions is eligible since it is:<br>1.it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.<br>2. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and *IT automation*.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 584453,
          "date": "Tue 12 Apr 2022 02:10",
          "username": "lisiuyiu",
          "content": "Agree with B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 506505,
          "date": "Wed 22 Dec 2021 00:31",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493437,
          "date": "Sat 04 Dec 2021 01:29",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 467433,
          "date": "Fri 05 Nov 2021 19:31",
          "username": "Kopa",
          "content": "going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450313,
          "date": "Fri 05 Nov 2021 01:42",
          "username": "andylogan",
          "content": "It's B - cron expression Amazon CloudWatch Events rules",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409109,
          "date": "Thu 04 Nov 2021 11:36",
          "username": "DerekKey",
          "content": "cron expression Amazon CloudWatch Events rules - you have to create 2 - one for increase the other for decrease e.g.<br>\\\"cron(0 6 * * MON-FRI *)\\\"<br>\\\"cron(0 22 * * MON-FRI *)\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351883,
          "date": "Thu 04 Nov 2021 11:16",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343728,
          "date": "Wed 03 Nov 2021 18:12",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291028,
          "date": "Wed 03 Nov 2021 15:39",
          "username": "wind",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290775,
          "date": "Tue 02 Nov 2021 16:35",
          "username": "Mansur",
          "content": "Answer B<br>Ref: https://aws.amazon.com/premiumsupport/knowledge-center/schedule-elastic-beanstalk-stop-restart/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289484,
          "date": "Tue 02 Nov 2021 10:49",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283943,
          "date": "Sun 31 Oct 2021 07:47",
          "username": "Ebi",
          "content": "Answer is B<br>Re \\\"It should also be able to handle the increased use of Elastic Beanstalk environments among different teams\\\", Lambda can go through all EB environments and stop them all",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 263427,
          "date": "Thu 28 Oct 2021 09:23",
          "username": "sanjaym",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242427,
          "date": "Thu 28 Oct 2021 04:19",
          "username": "newme",
          "content": "B. <br>A: additional EC2 cost<br>C: \\\"Invoke Step Functions daily\\\" sounds like manually.<br>D: I think it's the best solution for what the CFO wants to do. But it doesn't meet the requirement of spinning up and terminating environments.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 241986,
          "date": "Wed 27 Oct 2021 23:59",
          "username": "T14102020DarthYoda",
          "content": "Correct answer is D.  Key isAuto Scaling group.No. Why would you do this\\\" put the Elastic Beanstalk environment start command in the EC2 instance user data\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 716896,
          "date": "Sat 12 Nov 2022 20:42",
          "username": "DarthYoda",
          "content": "No. Why would you do this\\\" put the Elastic Beanstalk environment start command in the EC2 instance user data\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#425",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company plans to move regulated and security-sensitive businesses to AWS. The Security team is developing a framework to validate the adoption of AWS best practices and industry-recognized compliance standards. The AWS Management Console is the preferred method for teams to provision resources.<br>Which strategies should a Solutions Architect use to meet the business requirements and continuously assess, audit, and monitor the configurations of AWS resources? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#425",
          "answers": [
            {
              "choice": "<p>A. Use AWS Config rules to periodically audit changes to AWS resources and monitor the compliance of the configuration. Develop AWS Config custom rules using AWS Lambda to establish a test-driven development approach, and further automate the evaluation of configuration changes against the required controls.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon CloudWatch Logs agent to collect all the AWS SDK logs. Search the log data using a pre-defined set of filter patterns that matches mutating API calls. Send notifications using Amazon CloudWatch alarms when unintended changes are performed. Archive log data by using a batch export to Amazon S3 and then Amazon Glacier for a long-term retention and auditability.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS CloudTrail events to assess management activities of all AWS accounts. Ensure that CloudTrail is enabled in all accounts and available AWS services. Enable trails, encrypt CloudTrail event log files with an AWS KMS key, and monitor recorded activities with CloudWatch Logs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the Amazon CloudWatch Events near-real-time capabilities to monitor system events patterns, and trigger AWS Lambda functions to automatically revert non-authorized changes in AWS resources. Also, target Amazon SNS topics to enable notifications and improve the response time of incident responses.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use CloudTrail integration with Amazon SNS to automatically notify unauthorized API activities. Ensure that CloudTrail is enabled in all accounts and available AWS services. Evaluate the usage of Lambda functions to automatically revert non-authorized changes in AWS resources.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14437,
          "date": "Tue 28 Sep 2021 23:28",
          "username": "Moon9Ow30pixepetan9",
          "content": "My answers are \\\"A & C\\\".<br>The key point in the question is that it request to \\\"asses, audit & monitor\\\".Therefore, any answer that contains terminations for instances/services shall be eliminated. <br>So, \\\"D & E\\\": are out because they are taking actions.<br>\\\"B\\\": does not make sense!<br>A: Config rules, are very useful tool for compliancy.<br>C: Cloud Trail is also great tool for auditing.Yes A and C are good. Just tell what is asked and nothing extra. So we can ignore the action answers.Correct - A,C. <br><br>E is incorrect - As cloudtrail can't publish on sns topic on unauthorized usage. Per AWS, \\\"You can be notified when CloudTrail publishes new log files to your Amazon S3 bucket. You manage notifications using Amazon Simple Notification Service (Amazon SNS).\\\" https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.htmlA&C.  I have the same viewpoint to Moon.<br><br>Option C encrypted CloudTrail logs in addition to what option E do, this is considered as a best practice here: https://docs.aws.amazon.com/en_pv/awscloudtrail/latest/userguide/best-practices-security.html",
          "upvote_count": "41512",
          "selected_answers": ""
        },
        {
          "id": 28494,
          "date": "Fri 01 Oct 2021 23:17",
          "username": "9Ow30",
          "content": "Yes A and C are good. Just tell what is asked and nothing extra. So we can ignore the action answers.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 645491,
          "date": "Thu 11 Aug 2022 15:16",
          "username": "pixepe",
          "content": "Correct - A,C. <br><br>E is incorrect - As cloudtrail can't publish on sns topic on unauthorized usage. Per AWS, \\\"You can be notified when CloudTrail publishes new log files to your Amazon S3 bucket. You manage notifications using Amazon Simple Notification Service (Amazon SNS).\\\" https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 29494,
          "date": "Sat 02 Oct 2021 01:48",
          "username": "tan9",
          "content": "A&C.  I have the same viewpoint to Moon.<br><br>Option C encrypted CloudTrail logs in addition to what option E do, this is considered as a best practice here: https://docs.aws.amazon.com/en_pv/awscloudtrail/latest/userguide/best-practices-security.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 13226,
          "date": "Sun 26 Sep 2021 15:25",
          "username": "donathonPacoDereknewmeDerekKeyDashL",
          "content": "AE<br>B\\D: Cloudwatch cannot monitor API changes.<br>C: Both C and E is doable but I feel E is better because it revert changes and hence ensures the environment is always in compliance.<br>https://aws.amazon.com/blogs/security/how-to-audit-your-aws-resources-for-security-compliance-by-using-custom-aws-config-rules/<br>https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-cloudtrail-changes<br>https://aws.amazon.com/blogs/security/how-to-automatically-revert-and-receive-notifications-about-changes-to-your-amazon-vpc-security-groups/i believe A,C.  ans E , no such operation \\\"Evaluate the usage of Lambda functions to automatically revert non-authorized changes in AWS resources\\\". and since aws config already restrict the operation, it can't be any non-authorized changesB\\D: Cloudwatch cannot monitor API changes.<br>Good pointdonathon - you are wrong - Cloudwatch can monitor API changes using CloudTrail integration (you enable it on each trail)Why do you want to revert the changes back (as mentioned in E) when the requirement is only to \\\" continuously assess, audit, and monitor\\\"?",
          "upvote_count": "114113",
          "selected_answers": ""
        },
        {
          "id": 41932,
          "date": "Sun 03 Oct 2021 02:40",
          "username": "PacoDerek",
          "content": "i believe A,C.  ans E , no such operation \\\"Evaluate the usage of Lambda functions to automatically revert non-authorized changes in AWS resources\\\". and since aws config already restrict the operation, it can't be any non-authorized changes",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 242506,
          "date": "Wed 27 Oct 2021 23:11",
          "username": "newme",
          "content": "B\\D: Cloudwatch cannot monitor API changes.<br>Good point",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409115,
          "date": "Wed 03 Nov 2021 13:25",
          "username": "DerekKey",
          "content": "donathon - you are wrong - Cloudwatch can monitor API changes using CloudTrail integration (you enable it on each trail)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 392296,
          "date": "Tue 02 Nov 2021 22:36",
          "username": "DashL",
          "content": "Why do you want to revert the changes back (as mentioned in E) when the requirement is only to \\\" continuously assess, audit, and monitor\\\"?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 701151,
          "date": "Fri 21 Oct 2022 22:28",
          "username": "mrgreatness",
          "content": "A and C 100% -- I built a solution like this.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499212,
          "date": "Sat 11 Dec 2021 09:44",
          "username": "cldy",
          "content": "A.  Use AWS Config rules to periodically audit changes to AWS resources and monitor the compliance of the configuration. Develop AWS Config custom rules using AWS Lambda to establish a test-driven development approach, and further automate the evaluation of configuration changes against the required controls.<br>C.  Use AWS CloudTrail events to assess management activities of all AWS accounts. Ensure that CloudTrail is enabled in all accounts and available AWS services. Enable trails, encrypt CloudTrail event log files with an AWS KMS key, and monitor recorded activities with CloudWatch Logs.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493439,
          "date": "Sat 04 Dec 2021 01:32",
          "username": "AzureDP900",
          "content": "AC is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450315,
          "date": "Sun 07 Nov 2021 03:49",
          "username": "andylogan",
          "content": "It's A C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440746,
          "date": "Fri 05 Nov 2021 08:02",
          "username": "student22",
          "content": "A,C<br><br>Why C instead of D? Because here no requirement to take reactive actions, and C secures the logs better.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409136,
          "date": "Fri 05 Nov 2021 04:50",
          "username": "DerekKey",
          "content": "A nad C<br>Key words - 'continuously' assess, audit, and monitor the 'configurations'<br>A is probably correct - although rules should be triggered on resource configuration change not periodically<br>B is wrong - no explanation needed<br>C is correct - you can search trails history<br>D is wrong - requires CloudTrail trails with CloudWatch integration not mentioned here<br>E is wrong - it says unauthorized API activities - they are not looking for such functionality",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 403397,
          "date": "Wed 03 Nov 2021 05:44",
          "username": "student2020student2020",
          "content": "Answer C has this last statement \\\"and monitor recorded activities with CloudWatch Logs\\\". CloudWatch logs does not record activities from CloudTrail. I think this eliminates C. Edit - After testing in AWS console, you can actually and monitor recorded activities with CloudWatch Logs. This answer is correct. A and C seem to be the best options..",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 403407,
          "date": "Wed 03 Nov 2021 12:21",
          "username": "student2020",
          "content": "Edit - After testing in AWS console, you can actually and monitor recorded activities with CloudWatch Logs. This answer is correct. A and C seem to be the best options..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351886,
          "date": "Tue 02 Nov 2021 16:44",
          "username": "WhyIronMan",
          "content": "I'll go with A,C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343731,
          "date": "Tue 02 Nov 2021 05:27",
          "username": "Waiweng",
          "content": "A and C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 322391,
          "date": "Sun 31 Oct 2021 06:31",
          "username": "alisyech",
          "content": "A & C for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291065,
          "date": "Sun 31 Oct 2021 01:08",
          "username": "wind",
          "content": "go with AC. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289488,
          "date": "Sat 30 Oct 2021 16:42",
          "username": "Kian1",
          "content": "will go with A,C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283947,
          "date": "Sat 30 Oct 2021 06:46",
          "username": "Ebi",
          "content": "AC is my choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263432,
          "date": "Sat 30 Oct 2021 05:24",
          "username": "sanjaym",
          "content": "I'll go with AC.  Initially I thought AE.  C and E both correct but C is more relevant as E is reverting changes which is not in requirement.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 241994,
          "date": "Wed 27 Oct 2021 04:49",
          "username": "T14102020",
          "content": "AC is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#426",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a high-user-volume media-sharing application on premises. It currently hosts about 400 TB of data with millions of video files. The company is migrating this application to AWS to improve reliability and reduce costs.<br>The Solutions Architecture team plans to store the videos in an Amazon S3 bucket and use Amazon CloudFront to distribute videos to users. The company needs to migrate this application to AWS within 10 days with the least amount of downtime possible. The company currently has 1 Gbps connectivity to the Internet with<br>30 percent free capacity.<br>Which of the following solutions would enable the company to migrate the workload to AWS and meet all of the requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#426",
          "answers": [
            {
              "choice": "<p>A. Use a multi-part upload in Amazon S3 client to parallel-upload the data to the Amazon S3 bucket over the Internet. Use the throttling feature to ensure that the Amazon S3 client does not use more than 30 percent of available Internet capacity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Request an AWS Snowmobile with 1 PB capacity to be delivered to the data center. Load the data into Snowmobile and send it back to have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an Amazon S3 client to transfer data from the data center to the Amazon S3 bucket over the Internet. Use the throttling feature to ensure the Amazon S3 client does not use more than 30 percent of available Internet capacity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Request multiple AWS Snowball devices to be delivered to the data center. Load the data concurrently into these devices and send it back. Have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13228,
          "date": "Tue 21 Sep 2021 09:30",
          "username": "donathon",
          "content": "D<br>A\\C: Too slow<br>B: Too expensive. Usually for petabyte workloads.<br>https://aws.amazon.com/snowball/faqs/",
          "upvote_count": "35",
          "selected_answers": ""
        },
        {
          "id": 48546,
          "date": "Wed 29 Sep 2021 19:15",
          "username": "Averageguy",
          "content": "How should I choose between Snowmobile and Snowball?<br><br>To migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball. In addition, you should evaluate the amount of available bandwidth in your network backbone. If you have a high speed backbone with hundreds of Gb/s of spare throughput, then you can use Snowmobile to migrate the large datasets all at once. If you have limited bandwidth on your backbone, you should consider using multiple Snowballs to migrate the data incrementally.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 701163,
          "date": "Fri 21 Oct 2022 22:50",
          "username": "mrgreatness",
          "content": "Okay sorry has to be D, to transfer 400TB over a 1GiB line it would take approx 40 days. So, in my opinion, the questions is bad as it doesn't properly consider the Snowball shipping times , but safest bet here is D.  Final Answer D!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701159,
          "date": "Fri 21 Oct 2022 22:42",
          "username": "mrgreatness",
          "content": "As a default, Snowball uses standard shipping of two- five days. You cannot choose expedited shipping at this time -- from FAQ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701156,
          "date": "Fri 21 Oct 2022 22:37",
          "username": "mrgreatness",
          "content": "D - The Snowball services can typically transfer up to 100 TBs in about a week. So Order 5, we got 10 days here. seems to make sense! However,how long will it take to be delivered and then sent back hmmm",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 637684,
          "date": "Wed 27 Jul 2022 01:50",
          "username": "hilft",
          "content": "Between B and D. <br>D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 593012,
          "date": "Wed 27 Apr 2022 11:34",
          "username": "Weninkamrgreatness",
          "content": "I would go with A, as B & D will most probably not fit in the time frame - the questions states the data should be migrated within 10 days. The delivery & return of the snowball devices will take at least 2 days and you're left with 8 (in best case scenario) to upload the data on/off the devices. According to this documentation 400TB might take ~ 6 days to get uploaded.<br> https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html<br>Utilizing Transfer manager and s3 client multipart parallel uploads seems like a better option for me.<br>https://aws.amazon.com/blogs/developer/introducing-amazon-s3-transfer-manager-in-the-aws-sdk-for-java-2-x/<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.htmlI was thinking exactly the same. Shipping can be 2-5 days, and then sending back..would it make the 10 day timeframe",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 701160,
          "date": "Fri 21 Oct 2022 22:43",
          "username": "mrgreatness",
          "content": "I was thinking exactly the same. Shipping can be 2-5 days, and then sending back..would it make the 10 day timeframe",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 539161,
          "date": "Wed 02 Feb 2022 21:20",
          "username": "Jonfernz",
          "content": "One Snowball = 80TB.  All you need is 5 80TB Snowballs. Much more cost effective than Snowmobile.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499239,
          "date": "Sat 11 Dec 2021 10:09",
          "username": "cldy",
          "content": "D.  Request multiple AWS Snowball devices to be delivered to the data center. Load the data concurrently into these devices and send it back. Have AWS download that data to the Amazon S3 bucket. Sync the new data that was generated while migration was in flight.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450318,
          "date": "Fri 05 Nov 2021 17:36",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 351889,
          "date": "Fri 05 Nov 2021 00:03",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343735,
          "date": "Wed 03 Nov 2021 14:13",
          "username": "Waiweng",
          "content": "Go for Snowball D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 289489,
          "date": "Tue 02 Nov 2021 00:58",
          "username": "Kian1",
          "content": "will go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283956,
          "date": "Mon 01 Nov 2021 19:39",
          "username": "Ebi",
          "content": "Answer is D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 263435,
          "date": "Mon 01 Nov 2021 12:49",
          "username": "sanjaym",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 245136,
          "date": "Wed 27 Oct 2021 20:19",
          "username": "rscloud",
          "content": "D<br>data <10pb",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 241998,
          "date": "Tue 26 Oct 2021 17:30",
          "username": "T14102020",
          "content": "Correct answer is D.  <br>To migrate large datasets of 10PB or more in a single location, you should use Snowmobile. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#427",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has developed a new billing application that will be released in two weeks. Developers are testing the application running on 10 EC2 instances managed by an Auto Scaling group in subnet 172.31.0.0/24 within VPC A with CIDR block 172.31.0.0/16. The Developers noticed connection timeout errors in the application logs while connecting to an Oracle database running on an Amazon EC2 instance in the same region within VPC B with CIDR block 172.50.0.0/16.<br>The IP of the database instance is hard-coded in the application instances.<br>Which recommendations should a Solutions Architect present to the Developers to solve the problem in a secure way with minimal maintenance and overhead?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#427",
          "answers": [
            {
              "choice": "<p>A. Disable the SrcDestCheck attribute for all instances running the application and Oracle Database. Change the default route of VPC A to point ENI of the Oracle Database that has an IP address assigned within the range of 172.50.0.0/16<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create and attach internet gateways for both VPCs. Configure default routes to the internet gateways for both VPCs. Assign an Elastic IP for each Amazon EC2 instance in VPC A<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPC peering connection between the two VPCs and add a route to the routing table of VPC A that points to the IP address range of 172.50.0.0/16<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an additional Amazon EC2 instance for each VPC as a customer gateway; create one virtual private gateway (VGW) for each VPC, configure an end- to-end VPC, and advertise the routes for 172.50.0.0/16<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13229,
          "date": "Sun 26 Sep 2021 15:58",
          "username": "donathon",
          "content": "C<br>A: It does not goes through NAT so this is not the solution.<br>B: It does not need to go through internet. This is not secured.<br>D: This is VPN which is not suitable. Peering should be used.",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 45273,
          "date": "Tue 28 Sep 2021 03:12",
          "username": "amog",
          "content": "Answer is C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 701172,
          "date": "Fri 21 Oct 2022 23:04",
          "username": "mrgreatness",
          "content": "100% C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 699197,
          "date": "Wed 19 Oct 2022 18:53",
          "username": "Blair77",
          "content": "An easy one! C is right!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 635828,
          "date": "Sun 24 Jul 2022 02:49",
          "username": "hilft",
          "content": "most secure.<br>B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493440,
          "date": "Sat 04 Dec 2021 01:36",
          "username": "AzureDP900",
          "content": "C is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450322,
          "date": "Fri 05 Nov 2021 12:01",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447097,
          "date": "Fri 05 Nov 2021 04:34",
          "username": "moon2351",
          "content": "C is Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 423430,
          "date": "Wed 03 Nov 2021 11:18",
          "username": "walkwolf3",
          "content": "Answer C is missing the routes in VPC B. <br><br>You also need to add a route to the routing table of VPC B that points to the IP address of range of 172.31.0.0/24, otherwise, there is no route for return traffic from VPC B to A. <br><br>That's to say, C is most likely answer but not the full solution.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 351890,
          "date": "Fri 29 Oct 2021 15:30",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343736,
          "date": "Fri 22 Oct 2021 11:00",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289491,
          "date": "Tue 19 Oct 2021 15:55",
          "username": "Kian1",
          "content": "will go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283957,
          "date": "Mon 18 Oct 2021 03:48",
          "username": "Ebi",
          "content": "Easy one, answer is C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263436,
          "date": "Sun 17 Oct 2021 09:10",
          "username": "sanjaym",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 262345,
          "date": "Sat 16 Oct 2021 23:13",
          "username": "jayakumarchellamDerekKey",
          "content": "C is wrong - Peering already happened , problem is timeout . autoscale instance required time to route to Database IP addressThere is no such information. Rather opposite.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 409149,
          "date": "Mon 01 Nov 2021 08:34",
          "username": "DerekKey",
          "content": "There is no such information. Rather opposite.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 258940,
          "date": "Fri 15 Oct 2021 19:11",
          "username": "SachinJha",
          "content": "Though C looks appropriate but not sure how that resolves this problem without EIP: \\\"The IP of the database instance is hard-coded\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242000,
          "date": "Fri 15 Oct 2021 04:34",
          "username": "T14102020",
          "content": "C is correct. Key is VPC peering",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#428",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect has been asked to look at a company's Amazon Redshift cluster, which has quickly become an integral part of its technology and supports key business process. The Solutions Architect is to increase the reliability and availability of the cluster and provide options to ensure that if an issue arises, the cluster can either operate or be restored within four hours.<br>Which of the following solution options BEST addresses the business need in the most cost-effective manner?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#428",
          "answers": [
            {
              "choice": "<p>A. Ensure that the Amazon Redshift cluster has been set up to make use of Auto Scaling groups with the nodes in the cluster spread across multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Ensure that the Amazon Redshift cluster creation has been templated using AWS CloudFormation so it can easily be launched in another Availability Zone and data populated from the automated Redshift back-ups stored in Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Kinesis Data Firehose to collect the data ahead of ingestion into Amazon Redshift and create clusters using AWS CloudFormation in another region and stream the data to both clusters.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create two identical Amazon Redshift clusters in different regions (one as the primary, one as the secondary). Use Amazon S3 cross-region replication from the primary to secondary region, which triggers an AWS Lambda function to populate the cluster in the secondary region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14421,
          "date": "Wed 22 Sep 2021 15:09",
          "username": "MoonDashL",
          "content": "I do support answer \\\"B\\\".<br>The question does not request availability on different AZ.<br>The solution can tolerate 4 hours RTO. Therefore, CloudFront for Redshift, and backup stored in S3.<br>https://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/<br>A: Redshift can not work on Multi AZ!<br>C: Kinesis not meant for cross region population!<br>D: is a perfect answer, but not cost effective. More redundant than required!Option C can't be a solution because it is cost effective. But the link provided by Moon (https://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/) brings up a very interesting point: The link outlines a solution where Kinesis Streams can be used to deliver data to a Redshift Cluster either in a different AZ or in a differentdifferent Region.<br>Redshift can be a destination for Kinesis Firehose (https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift).<br>If Kinesis Streams can be cross regional, then why Firehose can't be?<br>However I couldn't find any documentation on how Kinesis (Either Streams or Firehose) can be used for cross region population?",
          "upvote_count": "431",
          "selected_answers": ""
        },
        {
          "id": 392312,
          "date": "Wed 03 Nov 2021 21:17",
          "username": "DashL",
          "content": "Option C can't be a solution because it is cost effective. But the link provided by Moon (https://aws.amazon.com/blogs/big-data/building-multi-az-or-multi-region-amazon-redshift-clusters/) brings up a very interesting point: The link outlines a solution where Kinesis Streams can be used to deliver data to a Redshift Cluster either in a different AZ or in a differentdifferent Region.<br>Redshift can be a destination for Kinesis Firehose (https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html#create-destination-redshift).<br>If Kinesis Streams can be cross regional, then why Firehose can't be?<br>However I couldn't find any documentation on how Kinesis (Either Streams or Firehose) can be used for cross region population?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 15613,
          "date": "Thu 23 Sep 2021 11:45",
          "username": "Marcos",
          "content": "Agree with B. <br><br>https://aws.amazon.com/redshift/faqs/?nc1=h_ls<br><br>Q: What happens to my data warehouse cluster availability and data durability if my data warehouse cluster's Availability Zone (AZ) has an outage?<br>If your Amazon Redshift data warehouse cluster's Availability Zone becomes unavailable, you will not be able to use your cluster until power and network access to the AZ are restored. Your data warehouse cluster's data is preserved so you can start using your Amazon Redshift data warehouse as soon as the AZ becomes available again. In addition, you can also choose to restore any existing snapshots to a new AZ in the same Region. Amazon Redshift will restore your most frequently accessed data first so you can resume queries as quickly as possible.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 701175,
          "date": "Fri 21 Oct 2022 23:11",
          "username": "mrgreatness",
          "content": "first thought B and most comments say B stating my thoughts - most cost effective , s2 backup, and CFN template to build infra quickly if required",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498317,
          "date": "Fri 10 Dec 2021 06:26",
          "username": "cldy",
          "content": "B.  Ensure that the Amazon Redshift cluster creation has been templated using AWS CloudFormation so it can easily be launched in another Availability Zone and data populated from the automated Redshift back-ups stored in Amazon S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493765,
          "date": "Sat 04 Dec 2021 16:27",
          "username": "AzureDP900",
          "content": "I will go with B , It is lowest cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450323,
          "date": "Sat 06 Nov 2021 10:39",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441878,
          "date": "Fri 05 Nov 2021 06:12",
          "username": "nwk",
          "content": "Currently, Amazon Redshift only supports Single-AZ deployments. You can run data warehouse clusters in multiple AZ's by loading data into two Amazon Redshift data warehouse clusters in separate AZs from the same set of Amazon S3 input files. With Redshift Spectrum, you can spin up multiple clusters across AZs and access data in Amazon S3 without having to load it into your cluster. In addition, you can also restore a data warehouse cluster to a different AZ from your data warehouse cluster snapshots.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 437810,
          "date": "Thu 04 Nov 2021 22:38",
          "username": "tgv",
          "content": "BBB<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351892,
          "date": "Wed 03 Nov 2021 04:36",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343737,
          "date": "Mon 01 Nov 2021 13:30",
          "username": "Waiweng",
          "content": "agree with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289493,
          "date": "Sat 30 Oct 2021 08:08",
          "username": "Kian1",
          "content": "go for B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283966,
          "date": "Fri 29 Oct 2021 15:59",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263438,
          "date": "Thu 28 Oct 2021 17:09",
          "username": "sanjaym",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 243327,
          "date": "Thu 21 Oct 2021 22:15",
          "username": "newmenewme",
          "content": "B<br>A: Redshift cluster doesn't support Multi-AZ<br>C/D: Both need to deploy multiple identical Redshift clusters into two AZ/Regions, which double the cost.Only that, how to know 4 fours is enough for CloudFormation to start up new Redshift cluster?",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 243329,
          "date": "Tue 26 Oct 2021 17:00",
          "username": "newme",
          "content": "Only that, how to know 4 fours is enough for CloudFormation to start up new Redshift cluster?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242005,
          "date": "Sat 16 Oct 2021 12:06",
          "username": "T14102020",
          "content": "B is correct as most cost-effective.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 228799,
          "date": "Thu 14 Oct 2021 20:59",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 227978,
          "date": "Wed 13 Oct 2021 18:07",
          "username": "Bulti",
          "content": "Although X is an ideal option it is not cost effective. The best option which is also cost effective is B. ",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#429",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company prefers to limit running Amazon EC2 instances to those that were launched from AMIs pre-approved by the Information Security department. The<br>Development team has an agile continuous integration and deployment process that cannot be stalled by the solution.<br>Which method enforces the required controls with the LEAST impact on the development process? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#429",
          "answers": [
            {
              "choice": "<p>A. Use IAM policies to restrict the ability of users or other automated entities to launch EC2 instances based on a specific set of pre-approved AMIs, such as those tagged in a specific way by Information Security.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use regular scans within Amazon Inspector with a custom assessment template to determine if the EC2 instance that the Amazon Inspector Agent is running on is based upon a pre-approved AMI. If it is not, shut down the instance and inform Information Security by email that this occurred.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Only allow launching of EC2 instances using a centralized DevOps team, which is given work packages via notifications from an internal ticketing system. Users make requests for resources using this ticketing tool, which has manual information security approval steps to ensure that EC2 instances are only launched from approved AMIs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Config rules to spot any launches of EC2 instances based on non-approved AMIs, trigger an AWS Lambda function to automatically terminate the instance, and publish a message to an Amazon SNS topic to inform Information Security that this occurred.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use a scheduled AWS Lambda function to scan through the list of running instances within the virtual private cloud (VPC) and determine if any of these are based on unapproved AMIs. Publish a message to an SNS topic to inform Information Security that this occurred and then shut down the instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13329,
          "date": "Tue 21 Sep 2021 22:26",
          "username": "donathonSD13memesterStelSen",
          "content": "AD<br>A: https://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/<br>B: Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.<br>C: Delays the deployment.<br>D: This ensure that the compliance is enforced.<br>E: Not effective.A will impact the development process.I agree...<br><br>Straight from Jon Bonso exam:<br><br>The option that says: Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.Jon Bonso is also wrong then. Because Option-D will also have disruption. So left with E is only good choice? Both memester and SD13, please tell us ur answer when reply. I am still stick to A D. ",
          "upvote_count": "32212",
          "selected_answers": ""
        },
        {
          "id": 331141,
          "date": "Sun 31 Oct 2021 14:35",
          "username": "SD13memesterStelSen",
          "content": "A will impact the development process.I agree...<br><br>Straight from Jon Bonso exam:<br><br>The option that says: Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.Jon Bonso is also wrong then. Because Option-D will also have disruption. So left with E is only good choice? Both memester and SD13, please tell us ur answer when reply. I am still stick to A D. ",
          "upvote_count": "212",
          "selected_answers": ""
        },
        {
          "id": 405595,
          "date": "Fri 05 Nov 2021 00:22",
          "username": "memesterStelSen",
          "content": "I agree...<br><br>Straight from Jon Bonso exam:<br><br>The option that says: Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.Jon Bonso is also wrong then. Because Option-D will also have disruption. So left with E is only good choice? Both memester and SD13, please tell us ur answer when reply. I am still stick to A D. ",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 454913,
          "date": "Fri 05 Nov 2021 13:15",
          "username": "StelSen",
          "content": "Jon Bonso is also wrong then. Because Option-D will also have disruption. So left with E is only good choice? Both memester and SD13, please tell us ur answer when reply. I am still stick to A D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 10744,
          "date": "Tue 21 Sep 2021 03:10",
          "username": "awsec2dpvnme",
          "content": "a, d<br>https://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/Yes, I think A&D are the best choices here",
          "upvote_count": "112",
          "selected_answers": ""
        },
        {
          "id": 11465,
          "date": "Tue 21 Sep 2021 07:15",
          "username": "dpvnme",
          "content": "Yes, I think A&D are the best choices here",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 709271,
          "date": "Tue 01 Nov 2022 17:20",
          "username": "AjayPrajapatiByrney",
          "content": "E doesn't sound correct. why to have new lamda to scan all VM. remember lambda has limited run time of 15 minute. what if you have ton of VMs.The Lambda doesn't scan the VMs themselves, it scans a *list* of all the VMs and compares each entry with the approved AMI list.That won't take anything like 15 minutes.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 712676,
          "date": "Sun 06 Nov 2022 23:35",
          "username": "Byrney",
          "content": "The Lambda doesn't scan the VMs themselves, it scans a *list* of all the VMs and compares each entry with the approved AMI list.That won't take anything like 15 minutes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701179,
          "date": "Fri 21 Oct 2022 23:22",
          "username": "mrgreatness",
          "content": "Answer is 100% A & D -- you can specify tags in conditions, so have a tag that only allows RunInstance with a specific AMI. Its definitely A & D -- D because Config is perfect solution. I'm 100% certain of this",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 676047,
          "date": "Thu 22 Sep 2022 13:29",
          "username": "JohnPi",
          "content": "D&E Security team is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 670674,
          "date": "Fri 16 Sep 2022 11:53",
          "username": "akash_it",
          "content": "D , E is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 657655,
          "date": "Fri 02 Sep 2022 19:26",
          "username": "epomatti",
          "content": "A will block development.<br><br>Only options that make sense are D and E. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 577918,
          "date": "Wed 30 Mar 2022 01:00",
          "username": "jj22222",
          "content": "AD are right",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 554430,
          "date": "Wed 23 Feb 2022 12:39",
          "username": "futen0326",
          "content": "D E<br><br>If you're already settled on using D why would you also use A? You've already taken care of the launch requirement, now we must solve the potential issue of unapproved AMIs that are may be past launch, but running.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 542869,
          "date": "Tue 08 Feb 2022 06:16",
          "username": "cannottellname",
          "content": "AAAAAA DDDDDD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 507008,
          "date": "Wed 22 Dec 2021 12:09",
          "username": "tkanmani76",
          "content": "A and D - refer link for A - https://aws.amazon.com/premiumsupport/knowledge-center/restrict-launch-tagged-ami/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493768,
          "date": "Sat 04 Dec 2021 16:31",
          "username": "AzureDP900",
          "content": "AD is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450329,
          "date": "Fri 05 Nov 2021 07:56",
          "username": "andylogan",
          "content": "It's A D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409163,
          "date": "Fri 05 Nov 2021 01:32",
          "username": "DerekKey",
          "content": "D&E<br>A is WRONG - since it will stall agile continuous integration and deployment process that cannot be stalled by the solution<br>B - incorrect<br>C - refer to A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 351894,
          "date": "Thu 04 Nov 2021 09:59",
          "username": "WhyIronMan",
          "content": "I'll go with A,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343738,
          "date": "Thu 04 Nov 2021 03:39",
          "username": "Waiweng",
          "content": "will go for A,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 335273,
          "date": "Wed 03 Nov 2021 08:49",
          "username": "ppshein",
          "content": "D, E is the best for me.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#430",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Company had a security event whereby an Amazon S3 bucket with sensitive information was made public. Company policy is to never have public S3 objects, and the Compliance team must be informed immediately when any public objects are identified.<br>How can the presence of a public S3 object be detected, set to trigger alarm notifications, and automatically remediated in the future? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#430",
          "answers": [
            {
              "choice": "<p>A. Turn on object-level logging for Amazon S3. Turn on Amazon S3 event notifications to notify by using an Amazon SNS topic when a PutObject API call is made with a public-read permission.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure an Amazon CloudWatch Events rule that invokes an AWS Lambda function to secure the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the S3 bucket permissions for AWS Trusted Advisor and configure a CloudWatch event to notify by using Amazon SNS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Turn on object-level logging for Amazon S3. Configure a CloudWatch event to notify by using an SNS topic when a PutObject API call with public-read permission is detected in the AWS CloudTrail logs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Schedule a recursive Lambda function to regularly change all object permissions inside the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13331,
          "date": "Wed 22 Sep 2021 19:24",
          "username": "donathonwassbawsenthudonathonMoon",
          "content": "A: There is a possibility that the event may be missed using this method. Amazon S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer. On very rare occasions, events might be lost. https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html<br>B: This actively remediate the public access. https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/<br>C: This is possible but not complete. This Trusted Advisor check doesn't monitor for bucket policies that override bucket ACLs.<br>https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/<br>https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/#security<br>https://willhamill.com/2018/02/19/get-alerts-when-an-s3-bucket-is-made-public-in-your-aws-account<br>D: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html<br>E: Not feasible.Trusted Advisor can check if the S3 bucket is public not objects insidedcloudtrail will take 15min to deliver the logs, my take is A and B<br>https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.htmlAnswer is BD BTW.Good resources... Support \\\"B & D\\\" too.",
          "upvote_count": "2912176",
          "selected_answers": ""
        },
        {
          "id": 689124,
          "date": "Sat 08 Oct 2022 10:08",
          "username": "wassb",
          "content": "Trusted Advisor can check if the S3 bucket is public not objects insided",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 110050,
          "date": "Sun 10 Oct 2021 19:13",
          "username": "awsenthu",
          "content": "cloudtrail will take 15min to deliver the logs, my take is A and B<br>https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 14051,
          "date": "Wed 22 Sep 2021 21:26",
          "username": "donathon",
          "content": "Answer is BD BTW.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 14356,
          "date": "Fri 24 Sep 2021 12:10",
          "username": "Moon",
          "content": "Good resources... Support \\\"B & D\\\" too.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 10753,
          "date": "Wed 22 Sep 2021 13:06",
          "username": "awsec2dpvnme",
          "content": "b,d<br>https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/yes, A is not possible with S3 event notification",
          "upvote_count": "144",
          "selected_answers": ""
        },
        {
          "id": 11467,
          "date": "Wed 22 Sep 2021 15:04",
          "username": "dpvnme",
          "content": "yes, A is not possible with S3 event notification",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 535706,
          "date": "Sat 29 Jan 2022 21:45",
          "username": "niu_tim",
          "content": "A is not correct, the permission can't be detected by event notification. Event format is here:https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-content-structure.html",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 493769,
          "date": "Sat 04 Dec 2021 16:34",
          "username": "AzureDP900",
          "content": "I will go with BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 492868,
          "date": "Fri 03 Dec 2021 01:40",
          "username": "vbal",
          "content": "demand of this 'How can the existence of a public S3 item be recognized' mean D & not C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 492864,
          "date": "Fri 03 Dec 2021 01:32",
          "username": "vbal",
          "content": "B&C ; https://aws.amazon.com/about-aws/whats-new/2018/02/aws-trusted-advisors-s3-bucket-permissions-check-is-now-free/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450332,
          "date": "Thu 04 Nov 2021 19:12",
          "username": "andylogan",
          "content": "It's B D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 382170,
          "date": "Wed 03 Nov 2021 08:49",
          "username": "amithbti416",
          "content": "B and D<br>Amazon CloudWatch Events to detect PutObject and PutObjectAcl API calls in near real time and helps ensure that the objects remain private by making automatic PutObjectAcl calls, when necessary.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 351895,
          "date": "Wed 03 Nov 2021 03:23",
          "username": "WhyIronMan",
          "content": "I'll go with B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 322562,
          "date": "Fri 29 Oct 2021 13:30",
          "username": "ExtHo",
          "content": "Any Thought on A,D<br>many peoples referring as \\\"On very rare occasions, events might be lost\\\"to rule out A on <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html<br><br>I don't found event might be lost <br>Important<br>Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291219,
          "date": "Mon 25 Oct 2021 22:42",
          "username": "AJBA",
          "content": "Definitely BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289503,
          "date": "Mon 25 Oct 2021 00:50",
          "username": "Kian1",
          "content": "will go with B&D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283176,
          "date": "Fri 22 Oct 2021 18:48",
          "username": "Ebi",
          "content": "BD are the right ones",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 263530,
          "date": "Fri 22 Oct 2021 06:00",
          "username": "sanjaym",
          "content": "I'll go with BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242015,
          "date": "Thu 21 Oct 2021 05:10",
          "username": "T14102020",
          "content": "Correct is BD. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 229399,
          "date": "Wed 20 Oct 2021 23:03",
          "username": "jackdryan",
          "content": "I'll go with B,D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229258,
          "date": "Wed 20 Oct 2021 08:56",
          "username": "YouYouYou",
          "content": "https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/<br><br>answer is B&D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#431",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using an Amazon CloudFront distribution to distribute both static and dynamic content from a web application running behind an Application Load<br>Balancer. The web application requires user authorization and session tracking for dynamic content. The CloudFront distribution has a single cache behavior configured to forward the Authorization, Host, and User-Agent HTTP whitelist headers and a session cookie to the origin. All other cache behavior settings are set to their default value.<br>A valid ACM certificate is applied to the CloudFront distribution with a matching CNAME in the distribution settings. The ACM certificate is also applied to the<br>HTTPS listener for the Application Load Balancer. The CloudFront origin protocol policy is set to HTTPS only. Analysis of the cache statistics report shows that the miss rate for this distribution is very high.<br>What can the Solutions Architect do to improve the cache hit rate for this distribution without causing the SSL/TLS handshake between CloudFront and the<br>Application Load Balancer to fail?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#431",
          "answers": [
            {
              "choice": "<p>A. Create two cache behaviors for static and dynamic content. Remove the User-Agent and Host HTTP headers from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section and the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the User-Agent and Authorization HTTP headers from the whitelist headers section of the cache behavior. Then update the cache behavior to use presigned cookies for authorization.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Remove the Host HTTP header from the whitelist headers section and remove the session cookie from the whitelist cookies section for the default cache behavior. Enable automatic object compression and use Lambda@Edge viewer request events for user authorization.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create two cache behaviors for static and dynamic content. Remove the User-Agent HTTP header from the whitelist headers section on both of the cache behaviors. Remove the session cookie from the whitelist cookies section and the Authorization HTTP header from the whitelist headers section for cache behavior configured for static content.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12532,
          "date": "Fri 24 Sep 2021 22:10",
          "username": "HazemYousryMrCarterFrank1uopspopSmartinfb3llman",
          "content": "A - Only session cookie and the Authorization headers to be kept and other headers can be removedthat is incorrectNeed to keep host header as cloudfront and elb is using the SAME ssl certificate.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.htmlsearch \\\"host\\\"Thanks a lot. This explains why A is incorrect. <br>I support D to be the answer, then.^Thanks - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html#ssl-negotitation-failure? This is why A is correct. <br>The article mentioned explicitly states that ONLY if you pass through the Host Header that the certificate must match the domain in the host header. Thus, if you *exclude* the host header, CloudFront does not care about the name in the origin certificate. So don't include the host header.<br><br>\\\"In addition, if you configured CloudFront to forward the Host header to your origin, the origin must respond with a certificate matching the domain in the Host header.\\\"<br>(therefore just exclude the host header)Sorry, upvoted by mistake. <br>You were wrong about this. If you remove the original HOST header, Cloudfront will add it back with the hostname of the origin. Since the HOST header no longer matches with the certificate, SSL handshake will fail at ALB.  So, keeping the original HOST header is a must.",
          "upvote_count": "1542163410",
          "selected_answers": ""
        },
        {
          "id": 394565,
          "date": "Thu 04 Nov 2021 02:22",
          "username": "MrCarter",
          "content": "that is incorrect",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 21718,
          "date": "Mon 27 Sep 2021 17:49",
          "username": "Frank1uopspopSmartinfb3llman",
          "content": "Need to keep host header as cloudfront and elb is using the SAME ssl certificate.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.htmlsearch \\\"host\\\"Thanks a lot. This explains why A is incorrect. <br>I support D to be the answer, then.^Thanks - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html#ssl-negotitation-failure? This is why A is correct. <br>The article mentioned explicitly states that ONLY if you pass through the Host Header that the certificate must match the domain in the host header. Thus, if you *exclude* the host header, CloudFront does not care about the name in the origin certificate. So don't include the host header.<br><br>\\\"In addition, if you configured CloudFront to forward the Host header to your origin, the origin must respond with a certificate matching the domain in the Host header.\\\"<br>(therefore just exclude the host header)Sorry, upvoted by mistake. <br>You were wrong about this. If you remove the original HOST header, Cloudfront will add it back with the hostname of the origin. Since the HOST header no longer matches with the certificate, SSL handshake will fail at ALB.  So, keeping the original HOST header is a must.",
          "upvote_count": "2163410",
          "selected_answers": ""
        },
        {
          "id": 22169,
          "date": "Tue 28 Sep 2021 14:26",
          "username": "uopspop",
          "content": "Thanks a lot. This explains why A is incorrect. <br>I support D to be the answer, then.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 75513,
          "date": "Thu 30 Sep 2021 14:37",
          "username": "Smart",
          "content": "^Thanks - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-502-bad-gateway.html#ssl-negotitation-failure",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 131226,
          "date": "Sat 02 Oct 2021 17:23",
          "username": "infb3llman",
          "content": "? This is why A is correct. <br>The article mentioned explicitly states that ONLY if you pass through the Host Header that the certificate must match the domain in the host header. Thus, if you *exclude* the host header, CloudFront does not care about the name in the origin certificate. So don't include the host header.<br><br>\\\"In addition, if you configured CloudFront to forward the Host header to your origin, the origin must respond with a certificate matching the domain in the Host header.\\\"<br>(therefore just exclude the host header)Sorry, upvoted by mistake. <br>You were wrong about this. If you remove the original HOST header, Cloudfront will add it back with the hostname of the origin. Since the HOST header no longer matches with the certificate, SSL handshake will fail at ALB.  So, keeping the original HOST header is a must.",
          "upvote_count": "410",
          "selected_answers": ""
        },
        {
          "id": 171348,
          "date": "Sat 09 Oct 2021 08:50",
          "username": "b3llman",
          "content": "Sorry, upvoted by mistake. <br>You were wrong about this. If you remove the original HOST header, Cloudfront will add it back with the hostname of the origin. Since the HOST header no longer matches with the certificate, SSL handshake will fail at ALB.  So, keeping the original HOST header is a must.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 44331,
          "date": "Thu 30 Sep 2021 05:31",
          "username": "dummaMrCarter",
          "content": "A is correct https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html the key requirements are to increase cache hit ratio, and not<br>breaking SSL between CloudFront and the ALB.  Breaking up the origin to static and<br>dynamic would help. Application needs session and authorization headers for dynamic<br>content but can be skipped for static content and neither need the user agent or host.nope, D is the correct answer",
          "upvote_count": "102",
          "selected_answers": ""
        },
        {
          "id": 394566,
          "date": "Thu 04 Nov 2021 19:20",
          "username": "MrCarter",
          "content": "nope, D is the correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 716905,
          "date": "Sat 12 Nov 2022 21:07",
          "username": "DarthYoda",
          "content": "D seems to be right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 711846,
          "date": "Sat 05 Nov 2022 16:21",
          "username": "robsonchirara",
          "content": "D - Removing the host header will break the TLS handshake. Static content is probably not being served by the ALB, maybe s3. Therefore no need to send many headers as this is affecting the cache hit ratio.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706203,
          "date": "Fri 28 Oct 2022 08:36",
          "username": "dmscountera",
          "content": "D - Host to not be removed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 655195,
          "date": "Wed 31 Aug 2022 16:40",
          "username": "Sizuma",
          "content": "D IS SURE 100%",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 632990,
          "date": "Mon 18 Jul 2022 13:36",
          "username": "Student1950",
          "content": "I vote for D.  Explanation:<br>Existing configuration is workings with Host Header forwarding - means both CloudFront and ALB are configured with same SSL certificates (same host name definition in SSL cert).<br>If you remove host header, CloudFront will add Custom Origin host (hostname defined in ALB) to the host header (host potion of URL). When this request reaches ALB, the request will be failed at ALB as SSL hostname defined in ALB SSL certificate will not match with host portion of URL hence Host Header is required when we have same SSL certificate deployed on CloudFront and ALB.  This works if ALB has its own SSL certificate matching its own host name definition which means CloudFront, and ALB have different SSL certificates.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 530124,
          "date": "Sun 23 Jan 2022 00:24",
          "username": "jj22222",
          "content": "D looks right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493770,
          "date": "Sat 04 Dec 2021 16:42",
          "username": "AzureDP900",
          "content": "D is right<br>Remove the User-Agent HTTP header from the whitelist headers section on both of the cache behaviors. There is no need to remove Host header.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 490333,
          "date": "Tue 30 Nov 2021 02:56",
          "username": "acloudguru",
          "content": "D, seperate static and dynamic web to increase cache hit",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 450604,
          "date": "Sun 07 Nov 2021 04:39",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 446588,
          "date": "Sat 06 Nov 2021 22:37",
          "username": "Kopa",
          "content": "Going for D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406616,
          "date": "Sat 06 Nov 2021 16:05",
          "username": "Akhil254",
          "content": "D Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 403470,
          "date": "Fri 05 Nov 2021 20:22",
          "username": "student2020student2020",
          "content": "Correct answer is D<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html<br>Create separate cache behaviors for static and dynamic content, and configure CloudFront to forward cookies to your origin only for dynamic content.<br>Host header is required for both cache behaviors not to break the SSL connection with the ALB. User-agent header results in too much variation in each request and therefore lots of cache misses. Removing this header will improve the cache hit ratio.<br>Try to avoid caching based on request headers that have large numbers of unique values.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html",
          "upvote_count": "83",
          "selected_answers": ""
        },
        {
          "id": 403472,
          "date": "Fri 05 Nov 2021 20:25",
          "username": "student2020",
          "content": "User-agent header results in too much variation in each request and therefore lots of cache misses. Removing this header will improve the cache hit ratio.<br>Try to avoid caching based on request headers that have large numbers of unique values.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 362377,
          "date": "Wed 03 Nov 2021 11:52",
          "username": "Radhaghosh",
          "content": "Correct Answer - D<br>Since it's distribution both Static & Dynamic content. You should have two cache behaviors. So Option B & C is eliminated. Now between A & D, Host HTTP headers is required, and you can't remove. So only Valid Option is D<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/understanding-the-cache-key.html",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 356471,
          "date": "Wed 03 Nov 2021 08:25",
          "username": "KnightVictor",
          "content": "going with D, verified in Neal Davis sample questions",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 356184,
          "date": "Mon 01 Nov 2021 21:46",
          "username": "vivektiwari",
          "content": "Correct Answer: D. <br>Removing the host header will result in failed flow between CloudFront and ALB, because they have same certificate.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#432",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An organization has a write-intensive mobile application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB.  The application has scaled well, however, costs have increased exponentially because of higher than anticipated Lambda costs. The application's use is unpredictable, but there has been a steady 20% increase in utilization every month.<br>While monitoring the current Lambda functions, the Solutions Architect notices that the execution-time averages 4.5 minutes. Most of the wait time is the result of a high-latency network call to a 3-TB MySQL database server that is on-premises. A VPN is used to connect to the VPC, so the Lambda functions have been configured with a five-minute timeout.<br>How can the Solutions Architect reduce the cost of the current architecture?<br>A. <br>✑ Replace the VPN with AWS Direct Connect to reduce the network latency to the on-premises MySQL database.<br>✑ Enable local caching in the mobile application to reduce the Lambda function invocation calls.<br>✑ Monitor the Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.<br>✑ Offload the frequently accessed records from DynamoDB to Amazon ElastiCache.<br>B. <br>✑ Replace the VPN with AWS Direct Connect to reduce the network latency to the on-premises MySQL database.<br>✑ Cache the API Gateway results to Amazon CloudFront.<br>✑ Use Amazon EC2 Reserved Instances instead of Lambda.<br>✑ Enable Auto Scaling on EC2, and use Spot Instances during peak times.<br>✑ Enable DynamoDB Auto Scaling to manage target utilization.<br>C. <br>✑ Migrate the MySQL database server into a Multi-AZ Amazon RDS for MySQL.<br>✑ Enable caching of the Amazon API Gateway results in Amazon CloudFront to reduce the number of Lambda function invocations.<br>✑ Monitor the Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.<br>✑ Enable DynamoDB Accelerator for frequently accessed records, and enable the DynamoDB Auto Scaling feature.<br>D. <br>✑ Migrate the MySQL database server into a Multi-AZ Amazon RDS for MySQL.<br>✑ Enable API caching on API Gateway to reduce the number of Lambda function invocations.<br>✑ Continue to monitor the AWS Lambda function performance; gradually adjust the timeout and memory properties to lower values while maintaining an acceptable execution time.<br>✑ Enable Auto Scaling in DynamoDB. <br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#432",
          "answers": []
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 254519,
          "date": "Mon 04 Oct 2021 06:22",
          "username": "BultiNickGRstudent2020",
          "content": "Answer is D.  Because this is a write-intensive application, it makes sense to cache post, put calls in API Gateway. if it was a read intensive application then using CloudFront or even the client-side cache would have helped. So the answer is D. I would agree with D but my only concern is the \\\"The application has scaled well\\\" and in C-D solutions, we are enabling again the Autoscale of DynamoDB.  It doesn't make any sense. I believe is A because it's also the most cost-effective answer.I dont see how implementing a DX solution is cost effective. I think D is more cost effective. There is no mention of which part of the application has scaled well, so implementing auto-scaling on dynamodb is an acceptable answer",
          "upvote_count": "2023",
          "selected_answers": ""
        },
        {
          "id": 385171,
          "date": "Mon 25 Oct 2021 02:44",
          "username": "NickGRstudent2020",
          "content": "I would agree with D but my only concern is the \\\"The application has scaled well\\\" and in C-D solutions, we are enabling again the Autoscale of DynamoDB.  It doesn't make any sense. I believe is A because it's also the most cost-effective answer.I dont see how implementing a DX solution is cost effective. I think D is more cost effective. There is no mention of which part of the application has scaled well, so implementing auto-scaling on dynamodb is an acceptable answer",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 403476,
          "date": "Tue 26 Oct 2021 07:13",
          "username": "student2020",
          "content": "I dont see how implementing a DX solution is cost effective. I think D is more cost effective. There is no mention of which part of the application has scaled well, so implementing auto-scaling on dynamodb is an acceptable answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 267637,
          "date": "Mon 04 Oct 2021 11:03",
          "username": "Ebi",
          "content": "D is my answer",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 679211,
          "date": "Sun 25 Sep 2022 22:33",
          "username": "fanq10examawsfanq10",
          "content": "The answer is A. <br>Here the key word is \\\"reduce the cost of the current architecture\\\".<br>If you migrate 3TB MySQL to RDS, the Database Storage for Multi-AZ cost will cost $700 per month, which will result in a cost increase.Using rds reduce the tco cost of your datacenter for database. It's true that it will add rds cost, but reducing server tco plus dx solution is not cheap.continue the previous post.....<br>If using instance type db.t3.medium costs about $100 per month, plus the Database Storage cost, it can increase the cost by $800 per month, which does not count Data Transfer cost yet. <br>So the Answer is A. ",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 701353,
          "date": "Sat 22 Oct 2022 07:51",
          "username": "examaws",
          "content": "Using rds reduce the tco cost of your datacenter for database. It's true that it will add rds cost, but reducing server tco plus dx solution is not cheap.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 679215,
          "date": "Sun 25 Sep 2022 22:37",
          "username": "fanq10",
          "content": "continue the previous post.....<br>If using instance type db.t3.medium costs about $100 per month, plus the Database Storage cost, it can increase the cost by $800 per month, which does not count Data Transfer cost yet. <br>So the Answer is A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 655198,
          "date": "Wed 31 Aug 2022 16:41",
          "username": "Sizuma",
          "content": "Correct Answer: D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 603293,
          "date": "Wed 18 May 2022 15:00",
          "username": "user0001",
          "content": "A is wrong because write-intensive and this option use elastic cache",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 563025,
          "date": "Tue 08 Mar 2022 05:43",
          "username": "Kuang",
          "content": "D is my answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498612,
          "date": "Fri 10 Dec 2021 13:28",
          "username": "cldy",
          "content": "D. <br>✑ Migrate the MySQL database server to Amazon RDS for MySQL Multi-AZ.<br>✑ Enable API caching on the API Gateway to minimize Lambda function calls.<br>✑ Continue to monitor the performance of the AWS Lambda function; progressively reduce the timeout and memory attributes while keeping an acceptable execution time.<br>✑ Enable DynamoDB's auto scaling.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493773,
          "date": "Sat 04 Dec 2021 16:47",
          "username": "AzureDP900",
          "content": "D is more cost effective for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450605,
          "date": "Thu 04 Nov 2021 12:35",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447883,
          "date": "Wed 03 Nov 2021 23:19",
          "username": "nodogoshi",
          "content": "D. <br>A make cost higher by direct connect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440843,
          "date": "Sat 30 Oct 2021 04:19",
          "username": "student22",
          "content": "Answer is D. <br><br>My main reason for selecting D over C is because C uses DynamoDB Accelerator which adds to the cost but might not be that effective because this is a 'write-intensive' app.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 437817,
          "date": "Fri 29 Oct 2021 11:56",
          "username": "tgv",
          "content": "DDD<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 430230,
          "date": "Thu 28 Oct 2021 21:26",
          "username": "AWS_Noob",
          "content": "D seems the best. Using DX is going to take a while to implement and may not be cheaper than the VPN. <br><br>Caching at API Gateway is preferred.<br><br>This is a tough question, it gave me a headache reading it lol",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 406618,
          "date": "Wed 27 Oct 2021 14:10",
          "username": "Akhil254",
          "content": "D Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 385526,
          "date": "Mon 25 Oct 2021 14:53",
          "username": "rain_wu",
          "content": "I go for D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 354815,
          "date": "Sun 24 Oct 2021 12:24",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344432,
          "date": "Sat 23 Oct 2021 14:45",
          "username": "blackgamer",
          "content": "I will go with D. ",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#433",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a video processing platform. Files are uploaded by users who connect to a web server, which stores them on an Amazon EFS share. This web server is running on a single Amazon EC2 instance. A different group of instances, running in an Auto Scaling group, scans the EFS share directory structure for new files to process and generates new videos (thumbnails, different resolution, compression, etc.) according to the instructions file, which is uploaded along with the video files. A different application running on a group of instances managed by an Auto Scaling group processes the video files and then deletes them from the<br>EFS share. The results are stored in an S3 bucket. Links to the processed video files are emailed to the customer.<br>The company has recently discovered that as they add more instances to the Auto Scaling Group, many files are processed twice, so image processing speed is not improved. The maximum size of these video files is 2GB. <br>What should the Solutions Architect do to improve reliability and reduce the redundant processing of video files?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#433",
          "answers": [
            {
              "choice": "<p>A. Modify the web application to upload the video files directly to Amazon S3. Use Amazon CloudWatch Events to trigger an AWS Lambda function every time a file is uploaded, and have this Lambda function put a message into an Amazon SQS queue. Modify the video processing application to read from SQS queue for new files and use the queue depth metric to scale instances in the video processing Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up a cron job on the web server instance to synchronize the contents of the EFS share into Amazon S3. Trigger an AWS Lambda function every time a file is uploaded to process the video file and store the results in Amazon S3. Using Amazon CloudWatch Events, trigger an Amazon SES job to send an email to the customer containing the link to the processed file.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Rewrite the web application to run directly from Amazon S3 and use Amazon API Gateway to upload the video files to an S3 bucket. Use an S3 trigger to run an AWS Lambda function each time a file is uploaded to process and store new video files in a different bucket. Using CloudWatch Events, trigger an SES job to send an email to the customer containing the link to the processed file.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Rewrite the web application to run from Amazon S3 and upload the video files to an S3 bucket. Each time a new file is uploaded, trigger an AWS Lambda function to put a message in an SQS queue containing the link and the instructions. Modify the video processing application to read from the SQS queue and the S3 bucket. Use the queue depth metric to adjust the size of the Auto Scaling group for video processing instances.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 16158,
          "date": "Wed 22 Sep 2021 08:36",
          "username": "chaudhAICOOb3llmansarah_tAWS2020newme9Ow30SunflyhomeJupi",
          "content": "D is my choice.<br>A & B are incorrect: Web is placed on single EC2 that is not HA, hosting the web on S3 will help to improve the reliability.<br>C: lambda function should not be used ot process the video, it's suitable for short execution.<br>D is best choice and SQS contains link of S3 with instruction is applied a lot in real world.How can run a application from S3? C & D is incorrect because of this.yes, of course, you can. javascript based web app.https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.htmli think lambda can be used to process the video. to wont take more than 15mins to process a video. D is not correct, it may replicate and the req. says clearly that we need to remove the redundancynot remove but reduce.I also vote for D<br>It has all the things as standard practice.<br>Keep data in S3/ pointer in SQS.<br>Process data via lambda on trigger and use SQS queue length as a Auto scaling driver.Lambda has 5 minutes timeout. Video processing is a time-cost process. <br>No way to use Lambda to do de-coding or en-coding of video file (except it's very small likea couple of hundreds MB). <br>D is a better than A in term of HA, Let AWS handle s3 stability than a single instance in A. The Lambda function is not for video processing, it just trigger the video processing. So timeout shouldn't be an issue.",
          "upvote_count": "2833211312",
          "selected_answers": ""
        },
        {
          "id": 148421,
          "date": "Sat 09 Oct 2021 15:45",
          "username": "AICOOb3llmansarah_t",
          "content": "How can run a application from S3? C & D is incorrect because of this.yes, of course, you can. javascript based web app.https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html",
          "upvote_count": "332",
          "selected_answers": ""
        },
        {
          "id": 171366,
          "date": "Tue 12 Oct 2021 22:36",
          "username": "b3llman",
          "content": "yes, of course, you can. javascript based web app.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 333148,
          "date": "Sun 31 Oct 2021 03:03",
          "username": "sarah_t",
          "content": "https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 18440,
          "date": "Wed 22 Sep 2021 13:06",
          "username": "AWS2020newme",
          "content": "i think lambda can be used to process the video. to wont take more than 15mins to process a video. D is not correct, it may replicate and the req. says clearly that we need to remove the redundancynot remove but reduce.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 243618,
          "date": "Mon 18 Oct 2021 18:11",
          "username": "newme",
          "content": "not remove but reduce.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 28851,
          "date": "Sat 25 Sep 2021 19:59",
          "username": "9Ow30",
          "content": "I also vote for D<br>It has all the things as standard practice.<br>Keep data in S3/ pointer in SQS.<br>Process data via lambda on trigger and use SQS queue length as a Auto scaling driver.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 325363,
          "date": "Thu 28 Oct 2021 08:38",
          "username": "SunflyhomeJupi",
          "content": "Lambda has 5 minutes timeout. Video processing is a time-cost process. <br>No way to use Lambda to do de-coding or en-coding of video file (except it's very small likea couple of hundreds MB). <br>D is a better than A in term of HA, Let AWS handle s3 stability than a single instance in A. The Lambda function is not for video processing, it just trigger the video processing. So timeout shouldn't be an issue.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 410942,
          "date": "Thu 04 Nov 2021 00:07",
          "username": "Jupi",
          "content": "The Lambda function is not for video processing, it just trigger the video processing. So timeout shouldn't be an issue.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 22312,
          "date": "Thu 23 Sep 2021 05:59",
          "username": "Frank1superuser784",
          "content": "C is incorrect. API gateway cannot be used to upload the 2GB video file to S3 as \\\"API Gateway supports a reasonable payload size limit of 10MB. \\\"https://sookocheff.com/post/api/uploading-large-payloads-through-api-gateway/<br><br>I support DGood point, I also ruled out the API Gateway because it can last only 29 seconds, and not all internet connections are that fast enough to upload 2GB within that timeframe.",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 708651,
          "date": "Mon 31 Oct 2022 19:22",
          "username": "superuser784",
          "content": "Good point, I also ruled out the API Gateway because it can last only 29 seconds, and not all internet connections are that fast enough to upload 2GB within that timeframe.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717477,
          "date": "Sun 13 Nov 2022 20:28",
          "username": "LrdKanien",
          "content": "B - Solution Architects don't rewrite applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715206,
          "date": "Thu 10 Nov 2022 13:17",
          "username": "resnef",
          "content": "difficult question, but I will choose A, this link helped in the choice: https://aws.amazon.com/blogs/media/processing-user-generated-content-using-aws-lambda-and-ffmpeg/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715008,
          "date": "Thu 10 Nov 2022 08:40",
          "username": "Netaji",
          "content": "SQS avoids the dup messages , if web site is static the \\\" D\\\"<br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703398,
          "date": "Tue 25 Oct 2022 00:29",
          "username": "mrgreatness",
          "content": "It's D for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 507283,
          "date": "Wed 22 Dec 2021 19:01",
          "username": "AzureDP900",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497636,
          "date": "Thu 09 Dec 2021 11:53",
          "username": "cldy",
          "content": "D.  Rewrite the web application to run from Amazon S3 and upload the video files to an S3 bucket. Each time a new file is uploaded, trigger an AWS Lambda function to put a message in an SQS queue containing the link and the instructions. Modify the video processing application to read from the SQS queue and the S3 bucket. Use the queue depth metric to adjust the size of the Auto Scaling group for video processing instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 485501,
          "date": "Wed 24 Nov 2021 00:50",
          "username": "acloudguru",
          "content": "use SQS for the de coupling and S3 to be the storage.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 450610,
          "date": "Sat 06 Nov 2021 19:48",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440801,
          "date": "Fri 05 Nov 2021 14:23",
          "username": "chand0401",
          "content": "B is correct. <br>A & D use SQS, in which redundant processing is possible. The main issue here is avoid redundant processing.<br>C - Using API gateway to upload to S3 is unnecessary<br>B is correct because - here you are avoiding to rewrite the application by simply syncing EFS files to S3 with a cron. There is no requirement to immediately process the files, so using cron is not a bad idea.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 435630,
          "date": "Fri 05 Nov 2021 06:06",
          "username": "tgv",
          "content": "DDD<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406624,
          "date": "Tue 02 Nov 2021 12:01",
          "username": "Akhil254",
          "content": "D Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 368203,
          "date": "Tue 02 Nov 2021 07:42",
          "username": "zolthar_zstudent22student22vinodhg",
          "content": "Is a really hard question, A & D looks good but I will go with D for one reason. Trigger a lambda using S3 events is easiest than configure cloudwatch event.Also, A is not HA - uses a single EC2 instance to host the web site.Agree with DBut you don't know if the website is static, so how sure are you to host it in S3?",
          "upvote_count": "3121",
          "selected_answers": ""
        },
        {
          "id": 440853,
          "date": "Sat 06 Nov 2021 11:34",
          "username": "student22student22",
          "content": "Also, A is not HA - uses a single EC2 instance to host the web site.Agree with D",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 440854,
          "date": "Sat 06 Nov 2021 12:04",
          "username": "student22",
          "content": "Agree with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 398974,
          "date": "Tue 02 Nov 2021 10:32",
          "username": "vinodhg",
          "content": "But you don't know if the website is static, so how sure are you to host it in S3?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 354818,
          "date": "Tue 02 Nov 2021 05:22",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344435,
          "date": "Mon 01 Nov 2021 20:21",
          "username": "blackgamer",
          "content": "Answer is D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343935,
          "date": "Mon 01 Nov 2021 18:48",
          "username": "Waiweng",
          "content": "It's D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#434",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must establish a patching plan for a large mixed fleet of Windows and Linux servers. The patching plan must be implemented securely, be audit-ready, and comply with the company's business requirements.<br>Which option will meet these requirements with MINIMAL effort?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#434",
          "answers": [
            {
              "choice": "<p>A. Install and use an OS-native patching service to manage the update frequency and release approval for all instances. Use AWS Config to verify the OS state on each instance and report on any patch compliance issues.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Systems Manager on all instances to manage patching. Test patches outside of production and then deploy during a maintenance window with the appropriate approval.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS OpsWorks for Chef Automate to run a set of scripts that will iterate through all instances of a given type. Issue the appropriate OS command to get and install updates on each instance, including any required restarts during the maintenance window.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate all applications to AWS OpsWorks and use OpsWorks automatic patching support to keep the OS up-to-date following the initial installation. Use AWS Config to provide audit and compliance reporting.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13338,
          "date": "Tue 21 Sep 2021 09:48",
          "username": "donathonshammousshammousAWSum1RVivek",
          "content": "B<br>Only Systems Manager can patch both OS effectively on AWS and on premise.I'm afraid I'm the only one here feeling that there is something missing: audit and compliance.<br>This would be done by AWS Config mentioned in option D.  SSM can indeed patch both OS but AWS OpsWorks can patch Linux servers directly but have a workaround to patch Windows ones: \\\"The simplest way to ensure that Windows is up to date is to replace your instances regularly, so that they are always running the latest AMI.\\\". Thus, option D will be viable.<br>Ref: https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-os-windows.htmlOpsWorks way of patching would be to replace instances with AWS patched Windows AMIs. Linux servers are directly patched. This would be compliant with the requirement even if SSM is more efficient.The question is more related to Patching. Systems manager is correct in this case. Opsworks is more of a deployment tool then a patching toolI agree with Pb55 <br>B<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html<br>Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.",
          "upvote_count": "232121",
          "selected_answers": ""
        },
        {
          "id": 279924,
          "date": "Tue 12 Oct 2021 05:19",
          "username": "shammousshammousAWSum1RVivek",
          "content": "I'm afraid I'm the only one here feeling that there is something missing: audit and compliance.<br>This would be done by AWS Config mentioned in option D.  SSM can indeed patch both OS but AWS OpsWorks can patch Linux servers directly but have a workaround to patch Windows ones: \\\"The simplest way to ensure that Windows is up to date is to replace your instances regularly, so that they are always running the latest AMI.\\\". Thus, option D will be viable.<br>Ref: https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-os-windows.htmlOpsWorks way of patching would be to replace instances with AWS patched Windows AMIs. Linux servers are directly patched. This would be compliant with the requirement even if SSM is more efficient.The question is more related to Patching. Systems manager is correct in this case. Opsworks is more of a deployment tool then a patching toolI agree with Pb55 <br>B<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html<br>Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.",
          "upvote_count": "2121",
          "selected_answers": ""
        },
        {
          "id": 279925,
          "date": "Tue 12 Oct 2021 12:47",
          "username": "shammous",
          "content": "OpsWorks way of patching would be to replace instances with AWS patched Windows AMIs. Linux servers are directly patched. This would be compliant with the requirement even if SSM is more efficient.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445364,
          "date": "Tue 02 Nov 2021 19:38",
          "username": "AWSum1",
          "content": "The question is more related to Patching. Systems manager is correct in this case. Opsworks is more of a deployment tool then a patching tool",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 532687,
          "date": "Wed 26 Jan 2022 08:42",
          "username": "RVivek",
          "content": "I agree with Pb55 <br>B<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html<br>Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703399,
          "date": "Tue 25 Oct 2022 00:31",
          "username": "mrgreatness",
          "content": "B 100pc use this service all the time for this use case",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 637733,
          "date": "Wed 27 Jul 2022 03:39",
          "username": "hilft",
          "content": "B.  there is a patch manager inside that will do the job.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 538289,
          "date": "Wed 02 Feb 2022 02:21",
          "username": "kyo",
          "content": "SSM is the best for maintaining ec2 instances",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513407,
          "date": "Thu 30 Dec 2021 14:33",
          "username": "cldy",
          "content": "B.  <br>SSM Patch Manager for both patching and compliance.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 506973,
          "date": "Wed 22 Dec 2021 11:31",
          "username": "Ni_yot",
          "content": "B for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499000,
          "date": "Sat 11 Dec 2021 00:16",
          "username": "challenger1",
          "content": "My Answer: B<br>Patching with the least amount of error = Systems Manager",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493781,
          "date": "Sat 04 Dec 2021 16:55",
          "username": "AzureDP900",
          "content": "This is B for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450611,
          "date": "Wed 03 Nov 2021 22:19",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447135,
          "date": "Wed 03 Nov 2021 01:55",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406626,
          "date": "Sun 31 Oct 2021 15:09",
          "username": "Akhil254",
          "content": "B Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 397385,
          "date": "Sun 31 Oct 2021 12:35",
          "username": "Pb55",
          "content": "B<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html<br>Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to. You can also generate patch compliance reports that are sent to an Amazon Simple Storage Service (Amazon S3) bucket of your choice. You can generate one-time reports, or generate reports on a regular schedule. For a single instance, reports include details of all patches for the instance. For a report on all instances, only a summary of how many patches are missing is provided.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 362385,
          "date": "Wed 27 Oct 2021 09:50",
          "username": "Radhaghosh",
          "content": "Correct Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 354823,
          "date": "Sun 24 Oct 2021 23:49",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 354821,
          "date": "Fri 22 Oct 2021 03:28",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343936,
          "date": "Wed 20 Oct 2021 10:43",
          "username": "Waiweng",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 308493,
          "date": "Tue 19 Oct 2021 23:23",
          "username": "Pupu86",
          "content": "with \\\"minimal\\\" effort in mind, I would steer towards option B instead of D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#435",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect must design a highly available, stateless, REST service. The service will require multiple persistent storage layers for service object meta information and the delivery of content. Each request needs to be authenticated and securely processed. There is a requirement to keep costs as low as possible.<br>How can these requirements be met?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#435",
          "answers": [
            {
              "choice": "<p>A. Use AWS Fargate to host a container that runs a self-contained REST service. Set up an Amazon ECS service that is fronted by an Application Load Balancer (ALB). Use a custom authenticator to control access to the API. Store request meta information in Amazon DynamoDB with Auto Scaling and static content in a secured S3 bucket. Make secure signed requests for Amazon S3 objects and proxy the data through the REST service interface.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Fargate to host a container that runs a self-contained REST service. Set up an ECS service that is fronted by a cross-zone ALB.  Use an Amazon Cognito user pool to control access to the API. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon Cognito user pool to control access to the API. Configure the methods to use AWS Lambda proxy integrations, and process each resource with a unique AWS Lambda function. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon API Gateway custom authorizer to control access to the API. Configure the methods to use AWS Lambda custom integrations, and process each resource with a unique Lambda function. Store request meta information in an Amazon ElastiCache Multi-AZ cluster and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13339,
          "date": "Mon 20 Sep 2021 19:11",
          "username": "donathonshammous",
          "content": "C<br>This is definitely a API Gateway question. Also note the questions says “multiple persistent”. ElastiCache is temporary store and hence DynamoDB make more sense.<br>A\\B: Fargate is containerization.Cognito also plays a role here as it's cheap and easy to setup.",
          "upvote_count": "356",
          "selected_answers": ""
        },
        {
          "id": 279927,
          "date": "Sat 23 Oct 2021 10:03",
          "username": "shammous",
          "content": "Cognito also plays a role here as it's cheap and easy to setup.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 14297,
          "date": "Wed 22 Sep 2021 01:27",
          "username": "Moonnewme",
          "content": "Agree with answer \\\"C\\\".<br>The Cognito can be used for authentication. API Gateway for Stateless REST service.<br>A: uses custom authenticator. Self Contained REST service!!!<br>B: Self Contained REST service!!!<br>D: Elasticache is not persistent.Self Contained REST service!!!<br>What's wrong with it?",
          "upvote_count": "141",
          "selected_answers": ""
        },
        {
          "id": 243648,
          "date": "Mon 18 Oct 2021 07:22",
          "username": "newme",
          "content": "Self Contained REST service!!!<br>What's wrong with it?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 610193,
          "date": "Wed 01 Jun 2022 15:44",
          "username": "kangtamo",
          "content": "C looks good to me.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 505049,
          "date": "Sun 19 Dec 2021 20:40",
          "username": "ebase4",
          "content": "C definitely! It's the cheapest option by far",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498481,
          "date": "Fri 10 Dec 2021 10:27",
          "username": "cldy",
          "content": "C.  Set up Amazon API Gateway and create the required API resources and methods. Use an Amazon Cognito user pool to control access to the API. Configure the methods to use AWS Lambda proxy integrations, and process each resource with a unique AWS Lambda function. Store request meta information in DynamoDB with Auto Scaling and static content in a secured S3 bucket. Generate presigned URLs when returning references to content stored in Amazon S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493784,
          "date": "Sat 04 Dec 2021 17:00",
          "username": "AzureDP900",
          "content": "C is right for given scnerio.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450613,
          "date": "Sun 07 Nov 2021 03:20",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447888,
          "date": "Sat 06 Nov 2021 09:05",
          "username": "nodogoshi",
          "content": "Surely C.  Lowest Cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409482,
          "date": "Sat 06 Nov 2021 04:59",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406641,
          "date": "Sat 06 Nov 2021 01:00",
          "username": "Akhil254",
          "content": "C Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 397388,
          "date": "Fri 05 Nov 2021 21:54",
          "username": "Pb55",
          "content": "Has to be C or D and D has no persistent storage. Therefore C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 362389,
          "date": "Tue 02 Nov 2021 09:15",
          "username": "Radhaghosh",
          "content": "Correct Answer C<br>For REST API, AWS Fargate ruled out, which brings Option C & D.  <br>Now Between Option C & D, C is the better option as it Uses Cognito user pool.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344439,
          "date": "Tue 02 Nov 2021 03:17",
          "username": "blackgamer",
          "content": "Agree with C.  Elastic cache is only for the cache purpose, so D Is not correct. DynamoDB is suitable to store object metadata.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343940,
          "date": "Tue 02 Nov 2021 03:03",
          "username": "Waiweng",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 308505,
          "date": "Sat 30 Oct 2021 09:29",
          "username": "Pupu86",
          "content": "https://medium.com/@lakshmanLD/lambda-proxy-vs-lambda-integration-in-aws-api-gateway-3a9397af0e6d",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289532,
          "date": "Thu 28 Oct 2021 16:31",
          "username": "Kian1",
          "content": "I will go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283332,
          "date": "Sun 24 Oct 2021 08:19",
          "username": "Ebi",
          "content": "I go with C",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#436",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company experienced a drastic increase in its monthly AWS spend. This is after Developers accidentally launched Amazon EC2 instances in unexpected regions. The company has established practices around least privileges for Developers and controls access to on-premises resources using Active Directory groups. The company now want to control costs by restricting the level of access that Developers have to the AWS Management Console without impacting their productivity. The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.<br>How can this company achieve these new security requirements while minimizing the administrative burden on the Operations team?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#436",
          "answers": [
            {
              "choice": "<p>A. Set up SAML-based authentication tied to an IAM role that has an AdministrativeAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM user for each Developer and add them to the developer IAM group that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that allows the Developers access to Amazon EC2 only in the required region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up SAML-based authentication tied to an IAM role that has a PowerUserAccess managed policy and a customer managed policy that deny all the Developers access to any AWS services except AWS Service Catalog. Within AWS Service Catalog, create a product containing only the EC2 resources in the approved region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12831,
          "date": "Wed 29 Sep 2021 10:55",
          "username": "donathon",
          "content": "D<br>A: This will grant too much access.<br>B: Should be SAML based due to the AD Group.<br>C: This will block the developer from other access that they may need. Key is “any AWS services”.",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 14300,
          "date": "Wed 29 Sep 2021 14:19",
          "username": "Moon",
          "content": "agree with \\\"donathon\\\", Answer D is the correct one for the same reasons.<br>The tricks here are:<br>- SAML for AD federation and authentication<br>- PowerUserAccess vs AdministrativeAccess. (PowerUSer has less privilege, which is the required once for developers). Admin, has more rights.<br><br>The description of \\\"PowerUser access\\\" given by AWS is “Provides full access to AWS services and resources, but does not allow management of Users and groups.”",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 703402,
          "date": "Tue 25 Oct 2022 00:41",
          "username": "mrgreatness",
          "content": "D - federate access, so just one role which can be assumed and attached the power user policy and then another policy with condition to restrict region. I recreated this. 100pc D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 659045,
          "date": "Sun 04 Sep 2022 09:43",
          "username": "pixepepixepe",
          "content": "C is ruled out as \\\"The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.\\\"Hence D is only correct answer.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 659047,
          "date": "Sun 04 Sep 2022 09:43",
          "username": "pixepe",
          "content": "Hence D is only correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 507293,
          "date": "Wed 22 Dec 2021 19:29",
          "username": "AzureDP900",
          "content": "D is more logical answer based on question --- > without restricting access to other services in any region<br><br>C only giving access to EC2 service and nothing else..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498430,
          "date": "Fri 10 Dec 2021 09:49",
          "username": "cldy",
          "content": "D.  Set up SAML-based authentication tied to an IAM role that has the PowerUserAccess managed policy attached to it. Attach a customer managed policy that denies access to Amazon EC2 in each region except for the one required.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493786,
          "date": "Sat 04 Dec 2021 17:05",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 461947,
          "date": "Sun 07 Nov 2021 16:13",
          "username": "seyik",
          "content": "C. <br>AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata. This helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need. With AWS Service Catalog AppRegistry, organizations can understand the application context of their AWS resources.<br>https://aws.amazon.com/servicecatalog/?aws-service-catalog.sort-by=item.additionalFields.createdDate&aws-service-catalog.sort-order=desc",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450614,
          "date": "Sun 07 Nov 2021 11:04",
          "username": "andylogan",
          "content": "It's D - exclude C because \\\"deny all the Developers access to any AWS services except AWS Service Catalog.\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409486,
          "date": "Sun 07 Nov 2021 06:33",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406640,
          "date": "Wed 03 Nov 2021 12:46",
          "username": "Akhil254",
          "content": "D Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362396,
          "date": "Sun 24 Oct 2021 04:01",
          "username": "RadhaghoshRadhaghosh",
          "content": "Two aspects in this question. The first is \\\"restricting the level of access that Developers have to the AWS Management Console without impacting their productivity\\\" -- This eliminates Option A (as user should have \\\"PowerUserAccess\\\" role.Second point is \\\"The company would also like to allow Developers to launch Amazon EC2 in only one region, without limiting access to other services in any region.\\\" - this eliminates Option C. Now to provide access via on-premises Active Directory groups, you need SAML. So Correct Option is CI am sorry for typo. Correct Option is D",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 362398,
          "date": "Mon 25 Oct 2021 02:30",
          "username": "Radhaghosh",
          "content": "I am sorry for typo. Correct Option is D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343946,
          "date": "Fri 22 Oct 2021 19:58",
          "username": "Waiweng",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289536,
          "date": "Tue 19 Oct 2021 15:24",
          "username": "Kian1",
          "content": "Will go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283334,
          "date": "Mon 18 Oct 2021 07:39",
          "username": "Ebi",
          "content": "D is answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 266522,
          "date": "Sat 16 Oct 2021 10:45",
          "username": "sanjaym",
          "content": "D for sure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242036,
          "date": "Tue 12 Oct 2021 13:43",
          "username": "T14102020",
          "content": "D is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#437",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is finalizing the architecture for its backup solution for applications running on AWS. All of the applications run on AWS and use at least two Availability<br>Zones in each tier.<br>Company policy requires IT to durably store nightly backups for all its data in at least two locations: production and disaster recovery. The locations must be in different geographic regions. The company also needs the backup to be available to restore immediately at the production data center, and within 24 hours at the disaster recovery location. All backup processes must be fully automated.<br>What is the MOST cost-effective backup solution that will meet all requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#437",
          "answers": [
            {
              "choice": "<p>A. Back up all the data to a large Amazon EBS volume attached to the backup media server in the production region. Run automated scripts to snapshot these volumes nightly, and copy these snapshots to the disaster recovery region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Back up all the data to Amazon S3 in the disaster recovery region. Use a lifecycle policy to move this data to Amazon Glacier in the production region immediately. Only the data is replicated; remove the data from the S3 bucket in the disaster recovery region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Back up all the data to Amazon Glacier in the production region. Set up cross-region replication of this data to Amazon Glacier in the disaster recovery region. Set up a lifecycle policy to delete any data older than 60 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Back up all the data to Amazon S3 in the production region. Set up cross-region replication of this S3 bucket to another region and set up a lifecycle policy in the second region to immediately move this data to Amazon Glacier.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12832,
          "date": "Thu 23 Sep 2021 00:40",
          "username": "donathon",
          "content": "D<br>A: Not sustainable. EBS has a 16TiB limit.<br>B: No backup at the production region.<br>C: Glacier does not allow restore immediately.",
          "upvote_count": "38",
          "selected_answers": ""
        },
        {
          "id": 14294,
          "date": "Thu 23 Sep 2021 01:17",
          "username": "Moon",
          "content": "Support \\\"D\\\" answer.<br>S3 in production for immediate recovery. Glacier for Disaster with 24 hours recovery.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 607663,
          "date": "Thu 26 May 2022 15:03",
          "username": "bobsmith2000",
          "content": "no-brainer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 577279,
          "date": "Tue 29 Mar 2022 08:20",
          "username": "Mimek",
          "content": "D meets all requirements and is the most cost efficient.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 534185,
          "date": "Fri 28 Jan 2022 00:34",
          "username": "AMKazi",
          "content": "D: meets all requirements<br>A: Expensive and not needed to back up all data to PROD<br>B: Does not meeting immediate access requirement in PROD <br>C: Same as B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 532242,
          "date": "Tue 25 Jan 2022 16:42",
          "username": "pititcu667",
          "content": "d meets all the checkboxes",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 514009,
          "date": "Fri 31 Dec 2021 10:52",
          "username": "cldy",
          "content": "D is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 507178,
          "date": "Wed 22 Dec 2021 15:43",
          "username": "Ni_yot",
          "content": "D for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493787,
          "date": "Sat 04 Dec 2021 17:09",
          "username": "AzureDP900",
          "content": "D without any doubt",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450616,
          "date": "Sun 07 Nov 2021 06:50",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409493,
          "date": "Fri 05 Nov 2021 23:31",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362405,
          "date": "Wed 03 Nov 2021 20:16",
          "username": "Radhaghosh",
          "content": "Key phrase of the question \\\"the backup to be available to restore immediately at the production data center, and within 24 hours at the disaster recovery location.\\\" and Cost effective solution. So it has to be S3 and S3 Standard (storage tire) for Production backup region. Both the condition available only in Option D. <br>So Correct Answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 344446,
          "date": "Mon 01 Nov 2021 15:20",
          "username": "blackgamer",
          "content": "The answer is D for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 343952,
          "date": "Mon 01 Nov 2021 01:12",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 329223,
          "date": "Fri 29 Oct 2021 02:41",
          "username": "KnightVictor",
          "content": "Should be D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 322386,
          "date": "Fri 29 Oct 2021 00:56",
          "username": "alisyech",
          "content": "i choose D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289542,
          "date": "Wed 27 Oct 2021 22:22",
          "username": "Kian1",
          "content": "Going with D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#438",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an existing on-premises three-tier web application. The Linux web servers serve content from a centralized file share on a NAS server because the content is refreshed several times a day from various sources. The existing infrastructure is not optimized and the company would like to move to AWS in order to gain the ability to scale resources up and down in response to load. On-premises and AWS resources are connected using AWS Direct Connect.<br>How can the company migrate the web infrastructure to AWS without delaying the content refresh process?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#438",
          "answers": [
            {
              "choice": "<p>A. Create a cluster of web server Amazon EC2 instances behind a Classic Load Balancer on AWS. Share an Amazon EBS volume among all instances for the content. Schedule a periodic synchronization of this volume and the NAS server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an on-premises file gateway using AWS Storage Gateway to replace the NAS server and replicate content to AWS. On the AWS side, mount the same Storage Gateway bucket to each web server Amazon EC2 instance to serve the content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Expose an Amazon EFS share to on-premises users to serve as the NAS serve. Mount the same EFS share to the web server Amazon EC2 instances to serve the content.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create web server Amazon EC2 instances on AWS in an Auto Scaling group. Configure a nightly process where the web server instances are updated from the NAS server.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14280,
          "date": "Sat 25 Sep 2021 03:12",
          "username": "Moon",
          "content": "I do support Answer \\\"C\\\".<br>File gateway is limited by performance its gateway instance, whether EC2 or On-premises, Cache will get filled up fast if not properly configured, For large number of EC2 instances EFS scales better. So, bottom line is File Storage gateway is for legacy applications and you have to add cost of large gateway instances before comparing it to same quantity of EFS storage.<br>https://www.reddit.com/r/aws/comments/82pyop/storage_gateway_vs_efs/",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 12841,
          "date": "Tue 21 Sep 2021 02:21",
          "username": "donathonMoonKopaIbranthoviccmm103leeo",
          "content": "B<br>This ensures the content stays on-premise but of cause sacrifice the fact that the EC2 will be accessing slight stale data via the S3 bucket because there is a replication time lag. Key is “without delaying the content refresh process”. I assume the content is on premise and the important fact is EFS is private only, it cannot be exposed to public and hence how would EFS be able to update from those “sources”?<br>A\\C\\D: This will delay the content refresh process. Because of the propagation delay tied to data traveling over long distances, the network latency of an AWS Direct Connect connection between your on-premises data center and your Amazon VPC can be tens of milliseconds. If your file operations are serialized, the latency of the AWS Direct Connect connection directly impacts your read and write throughput. In essence, the volume of data you can read or write during a period of time is bounded by the amount of time it takes for each read and write operation to complete. https://docs.aws.amazon.com/efs/latest/ug/performance-onpremises.htmlC:<br>I believe answer \\\"B\\\" would be good if there is no \\\"frequent updates\\\" from various sources.<br>Also, File Storage Gateway requires physical device to be placed on premises, while the source of updates are from various locations. Therefore, the physical mounting of File Storage Gateway shall be placed on different places \\\"various Sources\\\" - as said in the question.It says \\\"refreshed several times a day from various sources\\\" not \\\"frequent updates\\\". Makes a lot difference.It should be C,<br>As there's Direct Connection between AWS and On-permise, EFS could work here.<br>https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.htmlYes, B. <br>https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedAccessFileShare.htmlSince direct connect already available we can expose EFS to on-premise.<br>From EFS FAQ's<br>Q: How do I access an EFS file system from servers in my on-premises datacenter?<br><br>To access EFS file systems from on-premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC. ",
          "upvote_count": "71418119",
          "selected_answers": ""
        },
        {
          "id": 14293,
          "date": "Sat 25 Sep 2021 04:50",
          "username": "MoonKopa",
          "content": "C:<br>I believe answer \\\"B\\\" would be good if there is no \\\"frequent updates\\\" from various sources.<br>Also, File Storage Gateway requires physical device to be placed on premises, while the source of updates are from various locations. Therefore, the physical mounting of File Storage Gateway shall be placed on different places \\\"various Sources\\\" - as said in the question.It says \\\"refreshed several times a day from various sources\\\" not \\\"frequent updates\\\". Makes a lot difference.",
          "upvote_count": "141",
          "selected_answers": ""
        },
        {
          "id": 476215,
          "date": "Thu 11 Nov 2021 14:04",
          "username": "Kopa",
          "content": "It says \\\"refreshed several times a day from various sources\\\" not \\\"frequent updates\\\". Makes a lot difference.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 14150,
          "date": "Fri 24 Sep 2021 00:31",
          "username": "Ibranthovic",
          "content": "It should be C,<br>As there's Direct Connection between AWS and On-permise, EFS could work here.<br>https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 13479,
          "date": "Tue 21 Sep 2021 02:46",
          "username": "cmm103",
          "content": "Yes, B. <br>https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedAccessFileShare.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13961,
          "date": "Tue 21 Sep 2021 11:53",
          "username": "leeo",
          "content": "Since direct connect already available we can expose EFS to on-premise.<br>From EFS FAQ's<br>Q: How do I access an EFS file system from servers in my on-premises datacenter?<br><br>To access EFS file systems from on-premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC. ",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 686880,
          "date": "Wed 05 Oct 2022 14:44",
          "username": "JohnPi",
          "content": "https://docs.aws.amazon.com/efs/latest/ug/efs-onpremises.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 626568,
          "date": "Sun 03 Jul 2022 15:13",
          "username": "aandc",
          "content": "vote for B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 582770,
          "date": "Fri 08 Apr 2022 10:42",
          "username": "Hasitha99",
          "content": "Storage gateway can store data inS3 , Glacier or EBS.<br>So theansweris B. <br>Ref: https://aws.amazon.com/blogs/storage/cloud-storage-in-minutes-with-aws-storage-gateway/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 569430,
          "date": "Thu 17 Mar 2022 03:01",
          "username": "RVD",
          "content": "Ans: C",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 543818,
          "date": "Wed 09 Feb 2022 15:19",
          "username": "sTeVe86",
          "content": "C's problem is without mentioning how the data transition/migration.<br>B does mention \\\"replicate content to AWS\\\".<br><br>Question asked \\\"transition its online infrastructure to AWS without causing a delay in the process of content refreshment\\\".<br>So Storage Gateway is way to go as my understanding.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 507299,
          "date": "Wed 22 Dec 2021 19:37",
          "username": "AzureDP900",
          "content": "Answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494946,
          "date": "Mon 06 Dec 2021 07:43",
          "username": "cldy",
          "content": "C.  Expose an Amazon EFS share to on-premises users to serve as the NAS serve. Mount the same EFS share to the web server Amazon EC2 instances to serve the content.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493792,
          "date": "Sat 04 Dec 2021 17:23",
          "username": "AzureDP900",
          "content": "C is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484876,
          "date": "Tue 23 Nov 2021 10:05",
          "username": "backfringe",
          "content": "I'd go with C.  with DX EFS can be accessed from on premises",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450617,
          "date": "Sun 07 Nov 2021 16:40",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440867,
          "date": "Sat 06 Nov 2021 08:35",
          "username": "student22",
          "content": "Answer is C<br><br>Why not B? EFS will reflect the changes to files immediately during the day as opposed to File Gateway which could be slower. EFS will also scale up/down nicely. There no cost restriction in the question.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 430243,
          "date": "Thu 04 Nov 2021 13:27",
          "username": "AWS_Noob",
          "content": "Looking at the question , it says a Centralized File share to multiple web servers. EFS does this",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 406639,
          "date": "Tue 02 Nov 2021 15:36",
          "username": "Akhil254",
          "content": "C Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362413,
          "date": "Mon 01 Nov 2021 11:08",
          "username": "Radhaghosh",
          "content": "Important phrase in the question \\\"without delaying the content refresh process\\\". But the phrase \\\"scale resources up and down in response to load\\\" kind of tricks to forcefully think for ELB & Auto scaling. The actual point is to refresh the content without delay you need a shared file server mount point for both On-prem and AWS. which EFS can prove. <br>Correct Answer is C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 343959,
          "date": "Sun 31 Oct 2021 16:23",
          "username": "Waiweng",
          "content": "C for sure",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#439",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has multiple AWS accounts hosting IT applications. An Amazon CloudWatch Logs agent is installed on all Amazon EC2 instances. The company wants to aggregate all security events in a centralized AWS account dedicated to log storage.<br>Security Administrators need to perform near-real-time gathering and correlating of events across multiple AWS accounts.<br>Which solution satisfies these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#439",
          "answers": [
            {
              "choice": "<p>A. Create a Log Audit IAM role in each application AWS account with permissions to view CloudWatch Logs, configure an AWS Lambda function to assume the Log Audit role, and perform an hourly export of CloudWatch Logs data to an Amazon S3 bucket in the logging AWS account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure CloudWatch Logs streams in each application AWS account to forward events to CloudWatch Logs in the logging AWS account. In the logging AWS account, subscribe an Amazon Kinesis Data Firehose stream to Amazon CloudWatch Events, and use the stream to persist log data in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Amazon Kinesis Data Streams in the logging account, subscribe the stream to CloudWatch Logs streams in each application AWS account, configure an Amazon Kinesis Data Firehose delivery stream with the Data Streams as its source, and persist the log data in an Amazon S3 bucket inside the logging AWS account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure CloudWatch Logs agents to publish data to an Amazon Kinesis Data Firehose stream in the logging AWS account, use an AWS Lambda function to read messages from the stream and push messages to Data Firehose, and persist the data in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12842,
          "date": "Tue 05 Oct 2021 05:06",
          "username": "donathonheanyheanypra276LunchTimeppsheinuser0001",
          "content": "C<br>The solution uses Amazon Kinesis Data Streams and a log destination to set up an endpoint in the logging account to receive streamed logs and uses Amazon Kinesis Data Firehose to deliver log data to the Amazon Simple Storage Solution (S3) bucket. Application accounts will subscribe to stream all (or part) of their Amazon CloudWatch logs to a defined destination in the logging account via subscription filters.<br>https://aws.amazon.com/blogs/architecture/central-logging-in-multi-account-environments/<br>A: Does not satisfy “near real time”.Should be B. You will need data stream to send logs cross accounts. But for event, you can send directly to another account.https://medium.com/version-1/centralised-logs-and-alarms-from-multiple-aws-accounts-e8ef02750340from the link below.Forwarding CW events to other accounts can achieve near real time <br>https://aws.amazon.com/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/See this for answer B: <br>https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/The example pra276 provides does not support answer B as the example requires the use of Kinesis in addition to CloudWatch Log Streams.B is incorrect because of CW Event, which is irrelevant.for real-time Kinesis Data Streams",
          "upvote_count": "39111161",
          "selected_answers": ""
        },
        {
          "id": 688351,
          "date": "Fri 07 Oct 2022 07:51",
          "username": "heanyheany",
          "content": "Should be B. You will need data stream to send logs cross accounts. But for event, you can send directly to another account.https://medium.com/version-1/centralised-logs-and-alarms-from-multiple-aws-accounts-e8ef02750340from the link below.Forwarding CW events to other accounts can achieve near real time <br>https://aws.amazon.com/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 729647,
          "date": "Mon 28 Nov 2022 23:10",
          "username": "heany",
          "content": "from the link below.Forwarding CW events to other accounts can achieve near real time <br>https://aws.amazon.com/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 22888,
          "date": "Thu 07 Oct 2021 05:27",
          "username": "pra276LunchTimeppsheinuser0001",
          "content": "See this for answer B: <br>https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/The example pra276 provides does not support answer B as the example requires the use of Kinesis in addition to CloudWatch Log Streams.B is incorrect because of CW Event, which is irrelevant.for real-time Kinesis Data Streams",
          "upvote_count": "1161",
          "selected_answers": ""
        },
        {
          "id": 37220,
          "date": "Sat 09 Oct 2021 14:09",
          "username": "LunchTime",
          "content": "The example pra276 provides does not support answer B as the example requires the use of Kinesis in addition to CloudWatch Log Streams.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345396,
          "date": "Sun 31 Oct 2021 16:32",
          "username": "ppsheinuser0001",
          "content": "B is incorrect because of CW Event, which is irrelevant.for real-time Kinesis Data Streams",
          "upvote_count": "61",
          "selected_answers": ""
        },
        {
          "id": 598217,
          "date": "Sat 07 May 2022 19:06",
          "username": "user0001",
          "content": "for real-time Kinesis Data Streams",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 14276,
          "date": "Wed 06 Oct 2021 12:22",
          "username": "Moon",
          "content": "I support answer \\\"C\\\".<br>C: the solution is having a proper logging steps:<br>CloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)",
          "upvote_count": "28",
          "selected_answers": ""
        },
        {
          "id": 494908,
          "date": "Mon 06 Dec 2021 06:05",
          "username": "cldy",
          "content": "C.  Create Amazon Kinesis Data Streams in the logging account, subscribe the stream to CloudWatch Logs streams in each application AWS account, configure an Amazon Kinesis Data Firehose delivery stream with the Data Streams as its source, and persist the log data in an Amazon S3 bucket inside the logging AWS account.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493796,
          "date": "Sat 04 Dec 2021 17:30",
          "username": "AzureDP900",
          "content": "real time is keyword here and I will go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 484021,
          "date": "Mon 22 Nov 2021 10:14",
          "username": "backfringe",
          "content": "I'd go with C<br>CloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483862,
          "date": "Mon 22 Nov 2021 05:37",
          "username": "fadhilmukh",
          "content": "it's C.  Near real-time = Amazon Kinesis Data Streams",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 450618,
          "date": "Sat 06 Nov 2021 21:56",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409513,
          "date": "Wed 03 Nov 2021 19:48",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362417,
          "date": "Mon 01 Nov 2021 20:02",
          "username": "Radhaghosh",
          "content": "For Centralized login you need Kinesis Data Stream. (using CloudWatch Destination) <br>https://aws.amazon.com/solutions/implementations/centralized-logging/<br><br>Correct Answer is C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 343964,
          "date": "Sun 31 Oct 2021 14:07",
          "username": "Waiweng",
          "content": "Go for C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 289552,
          "date": "Sun 31 Oct 2021 11:12",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 287865,
          "date": "Sun 31 Oct 2021 09:00",
          "username": "LB",
          "content": "https://aws.amazon.com/blogs/architecture/stream-amazon-cloudwatch-logs-to-a-centralized-account-for-audit-and-analysis/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 283342,
          "date": "Sat 30 Oct 2021 10:59",
          "username": "Ebi",
          "content": "Answer is C",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 266550,
          "date": "Fri 29 Oct 2021 22:35",
          "username": "sanjaym",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242394,
          "date": "Fri 29 Oct 2021 04:27",
          "username": "T14102020",
          "content": "Correct answer is C. <br>CloudWatch (application Account) --> Kinesis Data Stream (Logging Account) --> Kinesis Firehose (Logging Account) --> S3 (Logging Account)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229422,
          "date": "Wed 27 Oct 2021 09:28",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 228634,
          "date": "Sat 23 Oct 2021 22:59",
          "username": "Bulti",
          "content": "Correct answer is C.  You will need to create a log destination pointing to the kinesis data stream as the target and then use that destination in the subscription filter of the CloudWatch log streams in the application account to flow the events from the application account to the logging account in near real-time. B is not possible as CloudWatch log stream cannot be a destination of a subscription filter on another CloudWatch log stream.Only supported destination is Kinesis Data Stream or Kinesis Data Firehouse or Lambda.",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#440",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.<br>How can this be accomplished?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#440",
          "answers": [
            {
              "choice": "<p>A. Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12843,
          "date": "Thu 23 Sep 2021 02:47",
          "username": "donathon",
          "content": "B<br>https://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shifting-and-phased-deployments-with-aws-codedeploy/",
          "upvote_count": "30",
          "selected_answers": ""
        },
        {
          "id": 14269,
          "date": "Thu 23 Sep 2021 16:21",
          "username": "Moon",
          "content": "I support answer \\\"B\\\".<br>SAM support CodeDeploy, which can be used for Lambda Core versioning.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 701408,
          "date": "Sat 22 Oct 2022 10:12",
          "username": "SangVu",
          "content": "Vote for B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 542618,
          "date": "Mon 07 Feb 2022 20:04",
          "username": "jj22222",
          "content": "B looks right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 522769,
          "date": "Thu 13 Jan 2022 11:11",
          "username": "pititcu667",
          "content": "SAM is just better.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513346,
          "date": "Thu 30 Dec 2021 13:23",
          "username": "cldy",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499311,
          "date": "Sat 11 Dec 2021 11:12",
          "username": "cldy",
          "content": "B.  Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493798,
          "date": "Sat 04 Dec 2021 17:34",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490391,
          "date": "Tue 30 Nov 2021 04:33",
          "username": "acloudguru",
          "content": "B, SAM is typical solution, simple question, hope I can have it in my exam",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 459390,
          "date": "Fri 05 Nov 2021 02:04",
          "username": "uninit",
          "content": "B<br>https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450712,
          "date": "Wed 03 Nov 2021 16:29",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447899,
          "date": "Tue 02 Nov 2021 05:19",
          "username": "nodogoshi",
          "content": "B.  Why not use SAM?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409521,
          "date": "Mon 01 Nov 2021 15:35",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406645,
          "date": "Fri 29 Oct 2021 17:49",
          "username": "Akhil254",
          "content": "B Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 389359,
          "date": "Fri 29 Oct 2021 03:02",
          "username": "Kopa",
          "content": "B.  That codedeploy is made of.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 362422,
          "date": "Thu 28 Oct 2021 17:22",
          "username": "Radhaghosh",
          "content": "AWS SAM is for Serverless deployment and the method is Canary. So B is most appropriate and valid answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344690,
          "date": "Sat 23 Oct 2021 01:46",
          "username": "blackgamer",
          "content": "IT should be B.  <br>https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#441",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a .NET three-tier web application on AWS. The team currently uses XL storage optimized instances to store and serve the website's image and video files on local instance storage. The company has encountered issues with data loss from replication and instance failures. The Solutions Architect has been asked to redesign this application to improve its reliability while keeping costs low.<br>Which solution will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#441",
          "answers": [
            {
              "choice": "<p>A. Set up a new Amazon EFS share, move all image and video files to this share, and then attach this new drive as a mount point to all existing servers. Create an Elastic Load Balancer with Auto Scaling general purpose instances. Enable Amazon CloudFront to the Elastic Load Balancer. Enable Cost Explorer and use AWS Trusted Advisor checks to continue monitoring the environment for future savings.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Implement Auto Scaling with general purpose instance types and an Elastic Load Balancer. Enable an Amazon CloudFront distribution to Amazon S3 and move images and video files to Amazon S3. Reserve general purpose instances to meet base performance requirements. Use Cost Explorer and AWS Trusted Advisor checks to continue monitoring the environment for future savings.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Move the entire website to Amazon S3 using the S3 website hosting feature. Remove all the web servers and have Amazon S3 communicate directly with the application servers in Amazon VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Elastic Beanstalk to deploy the .NET application. Move all images and video files to Amazon EFS. Create an Amazon CloudFront distribution that points to the EFS share. Reserve the m4.4xl instances needed to meet base performance requirements.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12844,
          "date": "Sun 19 Sep 2021 21:13",
          "username": "donathonMoonKopaDashL",
          "content": "B<br>A: EFS is more than 10 times more expensive than S3 although it has better performance. This is where CloudFront comes in to mitigate the performance impact caused by S3.<br>C: S3 does not suppose server side scripting (.net).<br>D: Cloudfront origin must be S3 or HTTP based. EFS is not HTTP.<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.htmlgreat analysis.Also remaining costly instances stay the same!!Only caveat with this analysis is that\\\"currently uses XL storage optimized instances to store and serve image and video files\\\". If S3 and CloudFront is used, where do the EC2 instances and the load balancer fit in?<br>But Elastic Beanstalk can be used with EFS. Does it matter if the languahe is .NET? So the answer should be D. ",
          "upvote_count": "57411",
          "selected_answers": ""
        },
        {
          "id": 14261,
          "date": "Wed 22 Sep 2021 06:02",
          "username": "Moon",
          "content": "great analysis.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 389376,
          "date": "Fri 22 Oct 2021 23:20",
          "username": "Kopa",
          "content": "Also remaining costly instances stay the same!!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 392410,
          "date": "Sun 24 Oct 2021 19:21",
          "username": "DashL",
          "content": "Only caveat with this analysis is that\\\"currently uses XL storage optimized instances to store and serve image and video files\\\". If S3 and CloudFront is used, where do the EC2 instances and the load balancer fit in?<br>But Elastic Beanstalk can be used with EFS. Does it matter if the languahe is .NET? So the answer should be D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 50228,
          "date": "Thu 23 Sep 2021 06:16",
          "username": "AWSPro24AWS_Noob",
          "content": "This is .NET and talking tiers not microservices so likely on Windows Server EC2 instances.EFS is not great for Windows.It can be connected by SMB but this is not recommended.FSX is preferable where Windows shares are needed. Supports SMB and NTFS natively.Good observation",
          "upvote_count": "62",
          "selected_answers": ""
        },
        {
          "id": 430252,
          "date": "Wed 03 Nov 2021 22:57",
          "username": "AWS_Noob",
          "content": "Good observation",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 685652,
          "date": "Mon 03 Oct 2022 18:12",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513210,
          "date": "Thu 30 Dec 2021 11:00",
          "username": "GeniusMikeLiu",
          "content": "so many duplicated questions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498679,
          "date": "Fri 10 Dec 2021 15:12",
          "username": "cldy",
          "content": "B.  Implement Auto Scaling with general purpose instance types and an Elastic Load Balancer. Enable an Amazon CloudFront distribution to Amazon S3 and move images and video files to Amazon S3. Reserve general purpose instances to meet base performance requirements. Use Cost Explorer and AWS Trusted Advisor checks to continue monitoring the environment for future savings.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493802,
          "date": "Sat 04 Dec 2021 17:36",
          "username": "AzureDP900",
          "content": "B is perfect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 489099,
          "date": "Sun 28 Nov 2021 12:53",
          "username": "acloudguru",
          "content": "such question seems easier than those ones, hope I can get such one in my exam",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450716,
          "date": "Fri 05 Nov 2021 09:33",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409528,
          "date": "Sun 31 Oct 2021 09:18",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406650,
          "date": "Mon 25 Oct 2021 05:15",
          "username": "Akhil254",
          "content": "B COrrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344261,
          "date": "Fri 22 Oct 2021 00:31",
          "username": "Waiweng",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289557,
          "date": "Mon 18 Oct 2021 19:06",
          "username": "Kian1",
          "content": "I am going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283123,
          "date": "Sun 17 Oct 2021 18:41",
          "username": "Ebi",
          "content": "B is my choice",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 275064,
          "date": "Sun 17 Oct 2021 04:00",
          "username": "gookseang",
          "content": "seems B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 266563,
          "date": "Tue 12 Oct 2021 16:35",
          "username": "sanjaym",
          "content": "B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242398,
          "date": "Sat 09 Oct 2021 01:25",
          "username": "T14102020",
          "content": "Correct answer is B.  EFS more expensive then S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229428,
          "date": "Fri 08 Oct 2021 10:41",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#442",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has developed a web application that runs on Amazon EC2 instances in one AWS Region. The company has taken on new business in other countries and must deploy its application into other regions to meet low-latency requirements for its users. The regions can be segregated, and an application running in one region does not need to communicate with instances in other regions.<br>How should the company's Solutions Architect automate the deployment of the application so that it can be MOST efficiently deployed into multiple regions?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#442",
          "answers": [
            {
              "choice": "<p>A. Write a bash script that uses the AWS CLI to query the current state in one region and output a JSON representation. Pass the JSON representation to the AWS CLI, specifying the --region parameter to deploy the application to other regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Write a bash script that uses the AWS CLI to query the current state in one region and output an AWS CloudFormation template. Create a CloudFormation stack from the template by using the AWS CLI, specifying the --region parameter to deploy the application to other regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Write a CloudFormation template describing the application's infrastructure in the resources section. Create a CloudFormation stack from the template by using the AWS CLI, specify multiple regions using the --regions parameter to deploy the application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Write a CloudFormation template describing the application's infrastructure in the Resources section. Use a CloudFormation stack set from an administrator account to launch stack instances that deploy the application to other regions.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12847,
          "date": "Tue 21 Sep 2021 08:37",
          "username": "donathon",
          "content": "D<br>A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that template requires.<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 10778,
          "date": "Mon 20 Sep 2021 07:14",
          "username": "awsec2jimmy_sticksshammous",
          "content": "d:<br><br>https://sanderknape.com/2017/07/cloudformation-stacksets-automated-cross-account-region-deployments/based on your link it is CThere is no --regions parameter. The right parameter is --region. Also, you need to use CFo stackset to deploy stacks to other regions (MOST efficiently)<br>\\\"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\\\"<br>Ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
          "upvote_count": "521",
          "selected_answers": ""
        },
        {
          "id": 123336,
          "date": "Sun 03 Oct 2021 23:25",
          "username": "jimmy_sticksshammous",
          "content": "based on your link it is CThere is no --regions parameter. The right parameter is --region. Also, you need to use CFo stackset to deploy stacks to other regions (MOST efficiently)<br>\\\"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\\\"<br>Ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 279907,
          "date": "Sun 31 Oct 2021 15:51",
          "username": "shammous",
          "content": "There is no --regions parameter. The right parameter is --region. Also, you need to use CFo stackset to deploy stacks to other regions (MOST efficiently)<br>\\\"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.\\\"<br>Ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685654,
          "date": "Mon 03 Oct 2022 18:12",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 542550,
          "date": "Mon 07 Feb 2022 17:52",
          "username": "jj22222",
          "content": "DDDDDDDDDDD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499332,
          "date": "Sat 11 Dec 2021 11:34",
          "username": "cldy",
          "content": "D.  Write a CloudFormation template describing the applicationג€™s infrastructure in the Resources section. Use a CloudFormation stack set from an administrator account to launch stack instances that deploy the application to other regions.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493804,
          "date": "Sat 04 Dec 2021 17:40",
          "username": "AzureDP900",
          "content": "D is right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450721,
          "date": "Sun 07 Nov 2021 16:53",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409532,
          "date": "Thu 04 Nov 2021 16:39",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 344263,
          "date": "Thu 04 Nov 2021 01:15",
          "username": "Waiweng",
          "content": "it's D create cloud formation Stack set first",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 289560,
          "date": "Tue 02 Nov 2021 01:25",
          "username": "Kian1",
          "content": "I will go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283131,
          "date": "Mon 01 Nov 2021 21:33",
          "username": "Ebi",
          "content": "I go with D",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 266564,
          "date": "Tue 26 Oct 2021 12:32",
          "username": "sanjaym",
          "content": "D for sure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 264506,
          "date": "Tue 26 Oct 2021 03:38",
          "username": "halfdeafhilftlalitsrana",
          "content": "Answer is D.  \\\"Stack set\\\" is the key.<br><br>A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template.excellent keywordA stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that template requires.",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 637705,
          "date": "Wed 27 Jul 2022 02:50",
          "username": "hilft",
          "content": "excellent keyword",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 278002,
          "date": "Fri 29 Oct 2021 06:13",
          "username": "lalitsrana",
          "content": "A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that template requires.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 261307,
          "date": "Sun 24 Oct 2021 15:40",
          "username": "elf78",
          "content": "D - quoting to the first paragraph of https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244139,
          "date": "Sun 24 Oct 2021 15:06",
          "username": "newme",
          "content": "D. <br>create-stack doesn't have --regions parameter.<br>create-stack-instances has --regions parameter, but it's to add stack instances to the stack set. So it's first parameter is --stack-set-name.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242401,
          "date": "Sat 23 Oct 2021 17:31",
          "username": "T14102020",
          "content": "Correct answer is D.  We need to create a stack set first to create stack instances using the CloudFormation template associated it. You cannot directly create a stack in different regions without first creating a stack set.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 240341,
          "date": "Thu 21 Oct 2021 17:29",
          "username": "petebear55",
          "content": "c IS THE OLD WAY OF DOING IT .. Thus D. .. https://surevine.com/creating-cloudformation-stacks-in-multiple-aws-regions-with-common-resources/",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#443",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A media company has a 30-TB repository of digital news videos. These videos are stored on tape in an on-premises tape library and referenced by a Media Asset<br>Management (MAM) system. The company wants to enrich the metadata for these videos in an automated fashion and put them into a searchable catalog by using a MAM feature. The company must be able to search based on information in the video, such as objects, scenery items, or people's faces. A catalog is available that contains faces of people who have appeared in the videos that include an image of each person. The company would like to migrate these videos to<br>AWS.<br>The company has a high-speed AWS Direct Connect connection with AWS and would like to move the MAM solution video content directly from its current file system.<br>How can these requirements be met by using the LEAST amount of ongoing management overhead and causing MINIMAL disruption to the existing system?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#443",
          "answers": [
            {
              "choice": "<p>A. Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an AWS Storage Gateway, tape gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the tape gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video in the tape gateway, retrieve the required metadata, and push the metadata into the MAM solution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure a video ingestion stream by using Amazon Kinesis Video Streams. Use the catalog of faces to build a collection in Amazon Rekognition. Stream the videos from the MAM solution into Kinesis Video Streams. Configure Amazon Rekognition to process the streamed videos. Then, use a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Configure the stream to store the videos in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an Amazon EC2 instance that runs the OpenCV libraries. Copy the videos, images, and face catalog from the on-premises library into an Amazon EBS volume mounted on this EC2 instance. Process the videos to retrieve the required metadata, and push the metadata into the MAM solution, while also copying the video files to an Amazon S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13204,
          "date": "Wed 22 Sep 2021 02:12",
          "username": "donathonpra276zijdldfddidxfqpdzfDdssssss",
          "content": "A<br>A\\B: By replacing the physical tape library with file gateway, it has the least amount of management and disruption. It would require us to restore the 30TB to another format (NFS\\SMB style) in order to put it on the file gateway which cannot serve as a VTL. I don’t think it’s possible for Amazon Rekognition to process the video in the tape gateway which is on premise or access the data from S3 directly.<br>A: File gateway supports Linux clients connecting to the gateway using Network File System (NFS) versions 3 and 4.1 for Linux clients, and supports Windows clients connecting to the gateway using Server Message Block (SMB) versions 2 and 3. <br>B: No. You cannot access virtual tape data using Amazon S3 or Amazon S3 Glacier APIs. However, you can use the tape gateway APIs to manage your virtual tape library and your virtual tape shelf. <br>https://aws.amazon.com/storagegateway/faqs/?nc=sn&loc=6<br>C: This is not possible because the data are in tape format.<br>D: This loosk more like the job for Rekognition.Answer is C: Vidoes are stored in tape. I don't see they mentioned tape format etc.(MINIMAL disruption to the existing system), in order to move forward with option C we have to edit MAM software to utilize Kenisis video stream library, so i prefer option AI think the issue with B is that no where does it say that Rek can ingest S3 Tape Archives. It can consume files in S3, but no mention of Tape gateway storage. Its a virtual tape library and when ejected from your backup application they are archived to glacier.",
          "upvote_count": "31811",
          "selected_answers": ""
        },
        {
          "id": 23116,
          "date": "Sat 25 Sep 2021 17:09",
          "username": "pra276zijdldfddidxfqpdzf",
          "content": "Answer is C: Vidoes are stored in tape. I don't see they mentioned tape format etc.(MINIMAL disruption to the existing system), in order to move forward with option C we have to edit MAM software to utilize Kenisis video stream library, so i prefer option A",
          "upvote_count": "81",
          "selected_answers": ""
        },
        {
          "id": 213968,
          "date": "Thu 14 Oct 2021 23:57",
          "username": "zijdldfddidxfqpdzf",
          "content": "(MINIMAL disruption to the existing system), in order to move forward with option C we have to edit MAM software to utilize Kenisis video stream library, so i prefer option A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 604171,
          "date": "Fri 20 May 2022 01:19",
          "username": "Ddssssss",
          "content": "I think the issue with B is that no where does it say that Rek can ingest S3 Tape Archives. It can consume files in S3, but no mention of Tape gateway storage. Its a virtual tape library and when ejected from your backup application they are archived to glacier.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 22335,
          "date": "Thu 23 Sep 2021 05:53",
          "username": "Frank1AWSPro24MultiAZAWSPro24JoeyleeWillCloud",
          "content": "A is wrong. Analysis stored video using AWS Rekognition is an async process. Therefore you need additional component to inject result to the MAM solution. Example here uses SQS and SNS to get the recognition result (https://docs.aws.amazon.com/rekognition/latest/dg/video.html)<br><br>C is correct as it is almost the same as described here: https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.htmlSorry, meant to include this link to a description of how the async API works.Says you can use SNS / SQS OR a Lambda function.While C is possible, it is definitely not \\\"LEAST amount of ongoing management overhead\\\" with so many moving parts.I'm not so sure you are correct.The following says you can use a Lambda function to get the metadata.You use SNS/SQS to get the state of the analysis process.<br>https://aws.amazon.com/rekognition/faqs/#Video_Analytics<br><br>In addition, why would we convert stored video into streaming video when Rek has the ability to analyze stored video https://docs.aws.amazon.com/rekognition/latest/dg/video.html The question says \\\"move the MAM solution video content directly from its current file system\\\"file-to-file/object seems more direct than streaming the files in through Kinesis.<br>https://docs.aws.amazon.com/rekognition/latest/dg/video.htmlhttps://docs.aws.amazon.com/rekognition/latest/dg/video.html<br>Rekognition is very picky on video size & format. Better use streamingIn your link: <br>With Amazon Rekognition Video, you can detect labels, faces, people, celebrities, and adult (suggestive and explicit) content in videos that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.",
          "upvote_count": "1511111",
          "selected_answers": ""
        },
        {
          "id": 50207,
          "date": "Mon 27 Sep 2021 08:18",
          "username": "AWSPro24",
          "content": "Sorry, meant to include this link to a description of how the async API works.Says you can use SNS / SQS OR a Lambda function.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 143913,
          "date": "Sat 09 Oct 2021 10:38",
          "username": "MultiAZ",
          "content": "While C is possible, it is definitely not \\\"LEAST amount of ongoing management overhead\\\" with so many moving parts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 50204,
          "date": "Mon 27 Sep 2021 03:02",
          "username": "AWSPro24JoeyleeWillCloud",
          "content": "I'm not so sure you are correct.The following says you can use a Lambda function to get the metadata.You use SNS/SQS to get the state of the analysis process.<br>https://aws.amazon.com/rekognition/faqs/#Video_Analytics<br><br>In addition, why would we convert stored video into streaming video when Rek has the ability to analyze stored video https://docs.aws.amazon.com/rekognition/latest/dg/video.html The question says \\\"move the MAM solution video content directly from its current file system\\\"file-to-file/object seems more direct than streaming the files in through Kinesis.<br>https://docs.aws.amazon.com/rekognition/latest/dg/video.htmlhttps://docs.aws.amazon.com/rekognition/latest/dg/video.html<br>Rekognition is very picky on video size & format. Better use streamingIn your link: <br>With Amazon Rekognition Video, you can detect labels, faces, people, celebrities, and adult (suggestive and explicit) content in videos that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 75590,
          "date": "Mon 27 Sep 2021 18:39",
          "username": "JoeyleeWillCloud",
          "content": "https://docs.aws.amazon.com/rekognition/latest/dg/video.html<br>Rekognition is very picky on video size & format. Better use streamingIn your link: <br>With Amazon Rekognition Video, you can detect labels, faces, people, celebrities, and adult (suggestive and explicit) content in videos that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 448177,
          "date": "Fri 29 Oct 2021 09:28",
          "username": "WillCloud",
          "content": "In your link: <br>With Amazon Rekognition Video, you can detect labels, faces, people, celebrities, and adult (suggestive and explicit) content in videos that are stored in an Amazon Simple Storage Service (Amazon S3) bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685655,
          "date": "Mon 03 Oct 2022 18:13",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 668547,
          "date": "Wed 14 Sep 2022 04:07",
          "username": "bihani",
          "content": "I don't understand why people are going with B, Answer of this question is definitely A. You can't directly fetch the media files from your tape gateway in real-time since this is backed up using Glacier.People are getting confused with the tape word in question. This problem can't be solved using tape gateway and will need file gateway.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 657070,
          "date": "Fri 02 Sep 2022 08:46",
          "username": "kadevkadev",
          "content": "Source for Rekognition is S3 or Kinesis<br>=> B wrong, Rekognition can not rertrive video on tape gateway <br>=> A and C is right, but \\\"LEAST amount of ongoing management\\\" => pick A, to re-use MAM feature of existing systemTape drive <=== extract by MMA ==> put to file gateway ==> S3",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 657074,
          "date": "Fri 02 Sep 2022 08:48",
          "username": "kadev",
          "content": "Tape drive <=== extract by MMA ==> put to file gateway ==> S3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 655204,
          "date": "Wed 31 Aug 2022 16:47",
          "username": "Sizuma",
          "content": "C IS CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 616184,
          "date": "Tue 14 Jun 2022 13:38",
          "username": "Ddssssss",
          "content": "My take is that the MAM tool and the company don't \\\"NEED\\\" the archives to be on tape or virtual tape. They are just on tape now as it was a legacy long term storage plan. They simply need the files in S3 and the MAM tools to absorb the metadata from REK. No need for B.  Answer A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 588602,
          "date": "Wed 20 Apr 2022 13:34",
          "username": "mirnuj_atom",
          "content": "Why not B? The MAM software in the question is already designed to work with the existing tape catalog, introducing the File Gateway from the and A would force us to re-write the catalog concept for the MAM app.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 519878,
          "date": "Sun 09 Jan 2022 04:09",
          "username": "frankzeng",
          "content": "B.  These movies are archived on tape in an on-premises tape library<br>and accessed using a Media Asset Management (MAM) system. It means after moving to AWS, still use MAM to access the tapes. Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without changing existing backup workflows",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 504438,
          "date": "Sat 18 Dec 2021 21:06",
          "username": "vbalvbalCal88",
          "content": "No One really talking about B here ...its shocking.. wht am I missing...? the following picture suggests B is absolutely correct. https://aws.amazon.com/storagegateway/vtl/I am confused with A after reading more about Media Asset Management. This is ridiculously hard..How could someone be tested on something like Media Asset Management provides you the files...???You don’t need to know about MAM to answer the question since its not relevant<br>The question is on how would you extract the meta data from the videos.<br>Putting MAM there is just to distract you and make you think its a hard question.<br>This question is mainly about storage gateways , Rekognition and Kinesis Video Stream<br>So think about it , if you need to extract faces , objects etc from videos what’s the best way? AWS Rekognition<br>So that will eliminate answer D for sure <br>Now what’s the best way to copy your videos to AWS to be used with Rekognition?<br>If you know about storage gateways you will know that when using a tape virtual gateway your backup will be sent to glacier and stored in something called tape archive so Rekognition can’t read a tape , it reads media (ex mp4 , jpg etc) so that will eliminate B as well<br>C can be done and will work but remember in the question its says minimal change<br>So the correct answer is A as most people in the comment are saying",
          "upvote_count": "211",
          "selected_answers": ""
        },
        {
          "id": 504445,
          "date": "Sat 18 Dec 2021 21:16",
          "username": "vbalCal88",
          "content": "I am confused with A after reading more about Media Asset Management. This is ridiculously hard..How could someone be tested on something like Media Asset Management provides you the files...???You don’t need to know about MAM to answer the question since its not relevant<br>The question is on how would you extract the meta data from the videos.<br>Putting MAM there is just to distract you and make you think its a hard question.<br>This question is mainly about storage gateways , Rekognition and Kinesis Video Stream<br>So think about it , if you need to extract faces , objects etc from videos what’s the best way? AWS Rekognition<br>So that will eliminate answer D for sure <br>Now what’s the best way to copy your videos to AWS to be used with Rekognition?<br>If you know about storage gateways you will know that when using a tape virtual gateway your backup will be sent to glacier and stored in something called tape archive so Rekognition can’t read a tape , it reads media (ex mp4 , jpg etc) so that will eliminate B as well<br>C can be done and will work but remember in the question its says minimal change<br>So the correct answer is A as most people in the comment are saying",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 707376,
          "date": "Sat 29 Oct 2022 19:58",
          "username": "Cal88",
          "content": "You don’t need to know about MAM to answer the question since its not relevant<br>The question is on how would you extract the meta data from the videos.<br>Putting MAM there is just to distract you and make you think its a hard question.<br>This question is mainly about storage gateways , Rekognition and Kinesis Video Stream<br>So think about it , if you need to extract faces , objects etc from videos what’s the best way? AWS Rekognition<br>So that will eliminate answer D for sure <br>Now what’s the best way to copy your videos to AWS to be used with Rekognition?<br>If you know about storage gateways you will know that when using a tape virtual gateway your backup will be sent to glacier and stored in something called tape archive so Rekognition can’t read a tape , it reads media (ex mp4 , jpg etc) so that will eliminate B as well<br>C can be done and will work but remember in the question its says minimal change<br>So the correct answer is A as most people in the comment are saying",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496587,
          "date": "Wed 08 Dec 2021 07:27",
          "username": "cldy",
          "content": "A.  Set up an AWS Storage Gateway, file gateway appliance on-premises. Use the MAM solution to extract the videos from the current archive and push them into the file gateway. Use the catalog of faces to build a collection in Amazon Rekognition. Build an AWS Lambda function that invokes the Rekognition Javascript SDK to have Rekognition pull the video from the Amazon S3 files backing the file gateway, retrieve the required metadata, and push the metadata into the MAM solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493807,
          "date": "Sat 04 Dec 2021 17:44",
          "username": "AzureDP900",
          "content": "archived on tape in an on-premises tape library is keyword, I will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483807,
          "date": "Mon 22 Nov 2021 03:01",
          "username": "acloudguru",
          "content": "B, there is no need to use Tape at all",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 455787,
          "date": "Thu 04 Nov 2021 20:43",
          "username": "StelSen",
          "content": "I chose A.  Although Ans A & C seems good solution. The Kinesis Video Stream usecase is for mostly live streaming (such as Camera). Not sure streaming a file is a good use case. Those who support Option-C, can you please paste the architecture link where Kinesis Video Streams process from Video File?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450723,
          "date": "Tue 02 Nov 2021 20:07",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409550,
          "date": "Fri 29 Oct 2021 09:01",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 406655,
          "date": "Thu 28 Oct 2021 22:18",
          "username": "Akhil254",
          "content": "A Correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#444",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning the migration of several lab environments used for software testing. An assortment of custom tooling is used to manage the test runs for each lab. The labs use immutable infrastructure for the software test runs, and the results are stored in a highly available SQL database cluster. Although completely rewriting the custom tooling is out of scope for the migration project, the company would like to optimize workloads during the migration.<br>Which application migration strategy meets this requirement?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#444",
          "answers": [
            {
              "choice": "<p>A. Re-host<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Re-platform<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Re-factor/re-architect<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Retire<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13800,
          "date": "Sat 25 Sep 2021 13:24",
          "username": "donathon",
          "content": "B<br>A: You cannot optimize workloads if you lift and shift.<br>C: You cannot modify the custom tooling.<br>D: This is not even an option.<br>Here you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren’t otherwise changing the core architecture of the application.<br>https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 11529,
          "date": "Sat 25 Sep 2021 07:58",
          "username": "dpvnme",
          "content": "B. <br>https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 716926,
          "date": "Sat 12 Nov 2022 22:00",
          "username": "DarthYoda",
          "content": "B.  Re-platform.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 685656,
          "date": "Mon 03 Oct 2022 18:13",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 496518,
          "date": "Wed 08 Dec 2021 05:12",
          "username": "cldy",
          "content": "B.  Re-platform",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450725,
          "date": "Sun 07 Nov 2021 01:32",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437818,
          "date": "Mon 01 Nov 2021 16:20",
          "username": "tgv",
          "content": "BBB<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409555,
          "date": "Sat 30 Oct 2021 17:00",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344269,
          "date": "Mon 25 Oct 2021 13:37",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291380,
          "date": "Tue 19 Oct 2021 08:35",
          "username": "Mansur",
          "content": "Agree with B<br><br>Ref: https://cloud.netapp.com/blog/aws-migration-strategy-the-6-rs-in-depth",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 289568,
          "date": "Sun 10 Oct 2021 15:40",
          "username": "Kian1",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283141,
          "date": "Sun 10 Oct 2021 11:11",
          "username": "Ebi",
          "content": "B is my choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 266569,
          "date": "Fri 08 Oct 2021 06:26",
          "username": "sanjaym",
          "content": "B for sure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242407,
          "date": "Thu 07 Oct 2021 05:55",
          "username": "T14102020",
          "content": "B is correct answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 229437,
          "date": "Mon 04 Oct 2021 15:00",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 228715,
          "date": "Mon 04 Oct 2021 04:23",
          "username": "Bulti",
          "content": "B is the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 136923,
          "date": "Sat 02 Oct 2021 14:24",
          "username": "noisonnoiton",
          "content": "B acceptable",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#445",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is implementing a multi-account strategy; however, the Management team has expressed concerns that services like DNS may become overly complex. The company needs a solution that allows private DNS to be shared among virtual private clouds (VPCs) in different accounts. The company will have approximately 50 accounts in total.<br>What solution would create the LEAST complex DNS architecture and ensure that each VPC can resolve all AWS resources?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#445",
          "answers": [
            {
              "choice": "<p>A. Create a shared services VPC in a central account, and create a VPC peering connection from the shared services VPC to each of the VPCs in the other accounts. Within Amazon Route 53, create a privately hosted zone in the shared services VPC and resource record sets for the domain and subdomains. Programmatically associate other VPCs with the hosted zone.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a VPC peering connection among the VPCs in all accounts. Set the VPC attributes enableDnsHostnames and enableDnsSupport to ג€trueג€ for each VPC.  Create an Amazon Route 53 private zone for each VPC.  Create resource record sets for the domain and subdomains. Programmatically associate the hosted zones in each VPC with the other VPCs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a shared services VPC in a central account. Create a VPC peering connection from the VPCs in other accounts to the shared services VPC.  Create an Amazon Route 53 privately hosted zone in the shared services VPC with resource record sets for the domain and subdomains. Allow UDP and TCP port 53 over the VPC peering connections.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set the VPC attributes enableDnsHostnames and enableDnsSupport to ג€falseג€ in every VPC.  Create an AWS Direct Connect connection with a private virtual interface. Allow UDP and TCP port 53 over the virtual interface. Use the on-premises DNS servers to resolve the IP addresses in each VPC on AWS.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13801,
          "date": "Wed 22 Sep 2021 13:05",
          "username": "donathondonathonvirtualDashLMoonSmart",
          "content": "A <br>B: enableDnsHostnames: Indicates whether instances with public IP addresses get corresponding public DNS hostnames. If this attribute is true, instances in the VPC get public DNS hostnames, but only if the enableDnsSupport attribute is also set to true. enableDnsSupport: Indicates whether the DNS resolution is supported. This is not needed.<br>C: Do it from the central account is less complex and faster.<br>D: This is not recommended and not the least complex solution. This will be difficult to maintain too. I don’t think it’s even possible.In this setup you want to query Route 53 private hosted zone resolution across multiple accounts, and VPC’s from your resources on-premises. In this design setup you will use a shared services VPC to accomplish this. At the same time, you also want to conditionally forward queries for on-premises domains from the VPCs to the on-premises DNS resolver. These VPCs are inter-connected using a hub and spoke topology. Each of the spoke VPCs belongs to a different account, and they are managed by their respective accounts.<br>When a Route 53 private hosted zone needs to be resolved in multiple VPCs and AWS accounts as described earlier, the most reliable pattern is to share the private hosted zone between accounts and associate it to each VPC that needs it. Although it’s possible to use Route 53 Resolver forwarding to solve this use case, this introduces additional costs, possible inter-Availability Zone dependencies, and complexity, which directly associating zones avoids.<br>https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/Thanks for your explanation.First of all, the solution doesn't need hybrid architecture.<br>A much better solution is available using DNS Resolver (which doesn't even need VPC peering): https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/<br>However, with the given options, the most logical answer is Agood analysis.<br>I do support \\\"A\\\" also.On side note: enableDnsHostnames & enableDnsSupport is required for Private Hosted Zone",
          "upvote_count": "26172214",
          "selected_answers": ""
        },
        {
          "id": 13802,
          "date": "Wed 22 Sep 2021 14:01",
          "username": "donathonvirtualDashL",
          "content": "In this setup you want to query Route 53 private hosted zone resolution across multiple accounts, and VPC’s from your resources on-premises. In this design setup you will use a shared services VPC to accomplish this. At the same time, you also want to conditionally forward queries for on-premises domains from the VPCs to the on-premises DNS resolver. These VPCs are inter-connected using a hub and spoke topology. Each of the spoke VPCs belongs to a different account, and they are managed by their respective accounts.<br>When a Route 53 private hosted zone needs to be resolved in multiple VPCs and AWS accounts as described earlier, the most reliable pattern is to share the private hosted zone between accounts and associate it to each VPC that needs it. Although it’s possible to use Route 53 Resolver forwarding to solve this use case, this introduces additional costs, possible inter-Availability Zone dependencies, and complexity, which directly associating zones avoids.<br>https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/Thanks for your explanation.First of all, the solution doesn't need hybrid architecture.<br>A much better solution is available using DNS Resolver (which doesn't even need VPC peering): https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/<br>However, with the given options, the most logical answer is A",
          "upvote_count": "1722",
          "selected_answers": ""
        },
        {
          "id": 60735,
          "date": "Tue 28 Sep 2021 22:21",
          "username": "virtual",
          "content": "Thanks for your explanation.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 392425,
          "date": "Wed 27 Oct 2021 07:28",
          "username": "DashL",
          "content": "First of all, the solution doesn't need hybrid architecture.<br>A much better solution is available using DNS Resolver (which doesn't even need VPC peering): https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/<br>However, with the given options, the most logical answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 14160,
          "date": "Thu 23 Sep 2021 03:51",
          "username": "Moon",
          "content": "good analysis.<br>I do support \\\"A\\\" also.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 70157,
          "date": "Wed 29 Sep 2021 00:21",
          "username": "Smart",
          "content": "On side note: enableDnsHostnames & enableDnsSupport is required for Private Hosted Zone",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 685657,
          "date": "Mon 03 Oct 2022 18:14",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 493835,
          "date": "Sat 04 Dec 2021 18:11",
          "username": "AzureDP900",
          "content": "A is perfect based on https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/ provided by donathon",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450736,
          "date": "Mon 01 Nov 2021 19:57",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409556,
          "date": "Thu 28 Oct 2021 07:50",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 344270,
          "date": "Sun 24 Oct 2021 01:13",
          "username": "Waiweng",
          "content": "A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 294773,
          "date": "Fri 22 Oct 2021 22:49",
          "username": "gparkstudent2020",
          "content": "A<br>---<br>B would be correct with there is the limited number of VPC like 2.<br>More than 3 VPC scenarios will need Transit Gateway.<br>It would be a burden to VPC peering all the VPCs.VPC peering/TGW is not even required to shared a PHZ with different AWS accounts",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 404884,
          "date": "Wed 27 Oct 2021 08:56",
          "username": "student2020",
          "content": "VPC peering/TGW is not even required to shared a PHZ with different AWS accounts",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289570,
          "date": "Wed 20 Oct 2021 13:47",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283146,
          "date": "Tue 19 Oct 2021 08:34",
          "username": "Ebi",
          "content": "My answer is A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242410,
          "date": "Sun 17 Oct 2021 03:30",
          "username": "T14102020",
          "content": "Correct answer is A.  Use shared VPC services",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229440,
          "date": "Sat 16 Oct 2021 12:11",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 228752,
          "date": "Thu 14 Oct 2021 14:20",
          "username": "Bulti",
          "content": "Answer is A.  You need to programmatically associate the VPC in another account to the private hosted zone in a central account. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html<br>C is similar to A in terms of how VPCs are peered but the association of VPC in each account to the Route 3 private hosted zone is incorrectly described.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 149470,
          "date": "Sun 10 Oct 2021 12:27",
          "username": "fullaws",
          "content": "A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 136934,
          "date": "Sat 09 Oct 2021 07:14",
          "username": "noisonnoiton",
          "content": "A Acceptable<br>associate other VPCs with the hosted zone",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 134244,
          "date": "Thu 07 Oct 2021 03:43",
          "username": "NikkyDicky",
          "content": "A most likely",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 128657,
          "date": "Thu 07 Oct 2021 01:03",
          "username": "ricoyaoipindado2020",
          "content": "Still not quite understand why C is incorrect?https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html<br>In order to use the DNS it should be done like A",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 182640,
          "date": "Mon 11 Oct 2021 08:32",
          "username": "ipindado2020",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html<br>In order to use the DNS it should be done like A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 97182,
          "date": "Sun 03 Oct 2021 11:22",
          "username": "JohnyGaddarJohnyGaddar",
          "content": "a -correct<br>c -important step : associating VPCs with the hosted zone is missing<br>b -no need for peering among all vpcs<br>d -on-premise dns will complicate architecturehttps://aws.amazon.com/blogs/security/how-to-centralize-dns-management-in-a-multi-account-environment/",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 97185,
          "date": "Wed 06 Oct 2021 01:21",
          "username": "JohnyGaddar",
          "content": "https://aws.amazon.com/blogs/security/how-to-centralize-dns-management-in-a-multi-account-environment/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#446",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has asked a Solutions Architect to design a secure content management solution that can be accessed by API calls by external customer applications.<br>The company requires that a customer administrator must be able to submit an API call and roll back changes to existing files sent to the content management solution, as needed.<br>What is the MOST secure deployment design that meets all solution requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#446",
          "answers": [
            {
              "choice": "<p>A. Use Amazon S3 for object storage with versioning and bucket access logging enabled, and an IAM role and access policy for each customer application. Encrypt objects using SSE-KMS. Develop the content management application to use a separate AWS KMS key for each customer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon WorkDocs for object storage. Leverage WorkDocs encryption, user access management, and version control. Use AWS CloudTrail to log all SDK actions and create reports of hourly access by using the Amazon CloudWatch dashboard. Enable a revert function in the SDK based on a static Amazon S3 webpage that shows the output of the CloudWatch dashboard.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon EFS for object storage, using encryption at rest for the Amazon EFS volume and a customer managed key stored in AWS KMS. Use IAM roles and Amazon EFS access policies to specify separate encryption keys for each customer application. Deploy the content management application to store all new versions as new files in Amazon EFS and use a control API to revert a specific file to a previous version.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon S3 for object storage with versioning and enable S3 bucket access logging. Use an IAM role and access policy for each customer application. Encrypt objects using client-side encryption, and distribute an encryption key to all customers when accessing the content management application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13207,
          "date": "Tue 21 Sep 2021 21:40",
          "username": "donathonAWSPro24sb333AWSPro24chaudhshammous",
          "content": "B<br>Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents. You can use Amazon WorkDocs to retire legacy file share infrastructure by moving file shares to the cloud. Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications.<br>https://aws.amazon.com/workdocs/B sounds good until you get to the last sentance which to me just sounds like nonesense.Use Cloudtrail to log SDK actions to create hourly reports, and use an S3 webpage to enable the revert function in the SDK?Just sounds like a bunch of wrong stuff put in to disqualify this answer.There doesn't appear to be an API available in Workdocs to perform roll back of changes. https://docs.aws.amazon.com/workdocs/latest/APIReference/Welcome.htmlCheck page 116 https://docs.aws.amazon.com/workdocs/latest/APIReference/workdocs-api.pdf . UpdateDocumentVersion Method should do the trick.I'm not a developer but this looks possible.WorkkDocs is not for Object Storage I thinkA is the right answer. You are not considering the rollback requirement which is satisfied by S3 versioning.",
          "upvote_count": "24143461",
          "selected_answers": ""
        },
        {
          "id": 50044,
          "date": "Wed 29 Sep 2021 20:29",
          "username": "AWSPro24",
          "content": "B sounds good until you get to the last sentance which to me just sounds like nonesense.Use Cloudtrail to log SDK actions to create hourly reports, and use an S3 webpage to enable the revert function in the SDK?Just sounds like a bunch of wrong stuff put in to disqualify this answer.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 46386,
          "date": "Mon 27 Sep 2021 11:27",
          "username": "sb333AWSPro24",
          "content": "There doesn't appear to be an API available in Workdocs to perform roll back of changes. https://docs.aws.amazon.com/workdocs/latest/APIReference/Welcome.htmlCheck page 116 https://docs.aws.amazon.com/workdocs/latest/APIReference/workdocs-api.pdf . UpdateDocumentVersion Method should do the trick.I'm not a developer but this looks possible.",
          "upvote_count": "34",
          "selected_answers": ""
        },
        {
          "id": 50040,
          "date": "Wed 29 Sep 2021 15:33",
          "username": "AWSPro24",
          "content": "Check page 116 https://docs.aws.amazon.com/workdocs/latest/APIReference/workdocs-api.pdf . UpdateDocumentVersion Method should do the trick.I'm not a developer but this looks possible.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 16167,
          "date": "Fri 24 Sep 2021 09:47",
          "username": "chaudh",
          "content": "WorkkDocs is not for Object Storage I think",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 279911,
          "date": "Sat 23 Oct 2021 04:19",
          "username": "shammous",
          "content": "A is the right answer. You are not considering the rollback requirement which is satisfied by S3 versioning.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 12328,
          "date": "Mon 20 Sep 2021 07:37",
          "username": "awsec2",
          "content": "Ais my view",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 716787,
          "date": "Sat 12 Nov 2022 16:40",
          "username": "Lorrendo",
          "content": "SSE-KMS = One KMS key per customer (most SECURE)<br>Rollback = S3 versioning<br>Against B: 1) Default encryption for all customers (not MOST secuire) 2) The last part of the description sounds made up",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 716783,
          "date": "Sat 12 Nov 2022 16:31",
          "username": "Lorrendo",
          "content": "It's A; keyword is MOST SECURE, and with A you have one key per user, in B this is not specified",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 709460,
          "date": "Tue 01 Nov 2022 23:43",
          "username": "AjayPrajapati",
          "content": "can not be B - work docs is not content management solution (CMS). work doc is to share documents with other. <br>A supports versioning",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704144,
          "date": "Tue 25 Oct 2022 21:56",
          "username": "mrgreatness",
          "content": "The answer is A.  B is 100pc incorrect people, trust me. Workdocs is not for object storage. S3 with versioning can be used as CMS. A is 1000pc correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 685660,
          "date": "Mon 03 Oct 2022 18:16",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 668589,
          "date": "Wed 14 Sep 2022 06:09",
          "username": "zaypaak",
          "content": "B - WorkDocs is ready built CMS with API capabilities and easy rollback",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 655206,
          "date": "Wed 31 Aug 2022 16:49",
          "username": "Sizuma",
          "content": "A RIGHT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 606116,
          "date": "Mon 23 May 2022 15:25",
          "username": "ssSsEclipse",
          "content": "A is the best option here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 551548,
          "date": "Sun 20 Feb 2022 06:27",
          "username": "peddyua",
          "content": "Seems like it's one of those AWS questions where they promote their services, why do you need to develop an application from scratch when they already have a service for it, and it's AWS WorkDocs",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 530544,
          "date": "Sun 23 Jan 2022 14:05",
          "username": "lulz111",
          "content": "\\\"A business has hired a Solutions Architect to create a secure content management system that can be accessed through API calls from external customer apps.\\\" - The first sentence of the question. Check out https://aws.amazon.com/workdocs/?amazon-workdocs-whats-new.sort-by=item.additionalFields.postDateTime&amazon-workdocs-whats-new.sort-order=desc.<br>\\\"Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service.\\\"<br>\\\"Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications.\\\"<br>\\\"“We started using Amazon WorkDocs to securely store and share files such as queries, datasets, and reports. The ability to view historical versions and \\\"... I think its B given this. Also most questions typically are looking for us to sell the AWS way of doing things with whatever shiny service they are currently peddling.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494744,
          "date": "Mon 06 Dec 2021 01:03",
          "username": "vbal",
          "content": "Doesn't SSE-KMS generally mean Default AWS Managed Key. SSE-KMS CMK is generally used term to specify customer specific keys???",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493847,
          "date": "Sat 04 Dec 2021 18:23",
          "username": "AzureDP900",
          "content": "A right choice",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 454418,
          "date": "Sun 07 Nov 2021 14:16",
          "username": "student22",
          "content": "A <br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450737,
          "date": "Sat 06 Nov 2021 13:49",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445413,
          "date": "Mon 01 Nov 2021 13:43",
          "username": "AWSum1",
          "content": "As much as I want to agree that the answer is B, I'm leaning towards A <br><br>Workdocs is content storage. Yes the question is about content management, but based on the answer s provided, it talks about OBJECT storage. So thay being said I will say the answer is <br><br>A",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#447",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has released a new version of a website to target an audience in Asia and South America. The website's media assets are hosted on Amazon S3 and have an Amazon CloudFront distribution to improve end-user performance. However, users are having a poor login experience, the authentication service is only available in the us-east-1 AWS Region.<br>How can the Solutions Architect improve the login experience and maintain high security and performance with minimal management overhead?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#447",
          "answers": [
            {
              "choice": "<p>A. Replicate the setup in each new geography and use Amazon Route 53 geo-based routing to route traffic to the AWS Region closest to the users.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Amazon Route 53 weighted routing policy to route traffic to the CloudFront distribution. Use CloudFront cached HTTP methods to improve the user login experience.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Lambda@Edge attached to the CloudFront viewer request trigger to authenticate and authorize users by maintaining a secure cookie token with a session expiry to improve the user experience in multiple geographies.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replicate the setup in each geography and use Network Load Balancers to route traffic to the authentication service running in the closest region to users.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 14040,
          "date": "Sun 26 Sep 2021 00:30",
          "username": "donathonLeoChukirrim",
          "content": "C<br>There are several benefits to using Lambda@Edge for authorization operations. First, performance is improved by running the authorization function using Lambda@Edge closest to the viewer, reducing latency and response time to the viewer request. The load on your origin servers is also reduced by offloading CPU-intensive operations such as verification of JSON Web Token (JWT) signatures. Finally, there are security benefits such as filtering out unauthorized requests before they reach your origin infrastructure.<br>https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-how-to-use-lambdaedge-and-json-web-tokens-to-enhance-web-application-security/very detail explanation, thank youAnother link specifically about using Lambda@Edge with cookies:<br><br>https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",
          "upvote_count": "4111",
          "selected_answers": ""
        },
        {
          "id": 205650,
          "date": "Thu 21 Oct 2021 13:32",
          "username": "LeoChu",
          "content": "very detail explanation, thank you",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 455191,
          "date": "Sat 06 Nov 2021 12:38",
          "username": "kirrim",
          "content": "Another link specifically about using Lambda@Edge with cookies:<br><br>https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 81492,
          "date": "Mon 04 Oct 2021 08:04",
          "username": "JWCJAWS1600",
          "content": "To those proposing C, the issue is with user login/authentication, NOT authorization.I agree Lambda@Edge would help with authorization.I'm not sure how it helps with authentication, which would require some type of directory, preferably close to the user, to validate user credentials.A would accomplish this.https://medium.com/monstar-lab-bangladesh-engineering/configure-basic-authentication-for-cloudfront-using-lambda-edge-c23ce46216d7<br>Here is teh document showing how to use lambda edge for authENTICATION",
          "upvote_count": "54",
          "selected_answers": ""
        },
        {
          "id": 98607,
          "date": "Wed 06 Oct 2021 16:23",
          "username": "JAWS1600",
          "content": "https://medium.com/monstar-lab-bangladesh-engineering/configure-basic-authentication-for-cloudfront-using-lambda-edge-c23ce46216d7<br>Here is teh document showing how to use lambda edge for authENTICATION",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 685662,
          "date": "Mon 03 Oct 2022 18:19",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 598055,
          "date": "Sat 07 May 2022 11:33",
          "username": "tartarus23",
          "content": "C.  Lambda@Edge enables modifying and servicing requests to and from the CloudFront so that the authorization process is offloaded to it instead of waiting to reach the AWS servers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 493850,
          "date": "Sat 04 Dec 2021 18:28",
          "username": "AzureDP900",
          "content": "c is perfect answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 482502,
          "date": "Sat 20 Nov 2021 13:08",
          "username": "SivaDorai76",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/lambdaedge-design-best-practices/ The link has details on Authentication which can be done using Lambda@Edge and more.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450739,
          "date": "Thu 04 Nov 2021 21:31",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409563,
          "date": "Thu 04 Nov 2021 12:42",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344274,
          "date": "Tue 02 Nov 2021 11:50",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293287,
          "date": "Tue 02 Nov 2021 06:39",
          "username": "kiev",
          "content": "Full House says C is the answer. Very clear explanation from Donathon",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 289575,
          "date": "Sun 31 Oct 2021 08:54",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 283163,
          "date": "Fri 29 Oct 2021 09:19",
          "username": "Ebi",
          "content": "C is the answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 266581,
          "date": "Thu 28 Oct 2021 12:09",
          "username": "sanjaym",
          "content": "C for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242431,
          "date": "Wed 27 Oct 2021 15:42",
          "username": "T14102020",
          "content": "Correct answer is C. Lambda@Edge",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229444,
          "date": "Sun 24 Oct 2021 05:04",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 228798,
          "date": "Sat 23 Oct 2021 01:23",
          "username": "Bulti",
          "content": "C is the answer. A Lambda@Edgefunction can also make network calls to external resources to confirm user credentials. Assuming authentication happens only in us-east-1 region, Lamba@Edge can atleast validate the JWT Token signature to determine if the user can directly access the content if the signature is valid. If the JWT token signature is invalid then it can redirect the user to the Authentication service in us-east-1 thereby improving the performance of the system.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 191489,
          "date": "Thu 21 Oct 2021 12:07",
          "username": "AlwaysLearning2020",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#448",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a standard three-tier architecture using two Availability Zones. During the company's off season, users report that the website is not working. The<br>Solutions Architect finds that no changes have been made to the environment recently, the website is reachable, and it is possible to log in. However, when the<br>Solutions Architect selects the `find a store near you` function, the maps provided on the site by a third-party RESTful API call do not work about 50% of the time after refreshing the page. The outbound API calls are made through Amazon EC2 NAT instances.<br>What is the MOST likely reason for this failure and how can it be mitigated in the future?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#448",
          "answers": [
            {
              "choice": "<p>A. The network ACL for one subnet is blocking outbound web traffic. Open the network ACL and prevent administration from making future changes through IAM.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The fault is in the third-party environment. Contact the third party that provides the maps and request a fix that will provide better uptime.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. One NAT instance has become overloaded. Replace both EC2 NAT instances with a larger-sized instance and make sure to account for growth when making the new instance size.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. One of the NAT instances failed. Recommend replacing the EC2 NAT instances with a NAT gateway.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13209,
          "date": "Fri 24 Sep 2021 02:59",
          "username": "donathonJohnyGaddarsarah_t",
          "content": "D<br>A: Network ACL is stateless and hence return traffic must be explicitly allowed by rules. If outbound is not allowed then how can the webpage load 100% of the time?<br>B: This is not possible since it only fails 50% of the time which means only 1 AZ is affected.<br>C: Unlikely to cause exactly 50% failure. API calls should not be load intensive.<br>D: Assuming NAT instance HA is not configured. https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/logic for A seems incorrect , it mentions outbound calls(call to restapi) are restricted via NACL which is possible. The correct reason is that there has no changes made in the environment , so A cannot be the optionA failed instance is not a \\\"change\\\" though, as changes are (more or less) intentional.",
          "upvote_count": "3252",
          "selected_answers": ""
        },
        {
          "id": 97134,
          "date": "Sat 02 Oct 2021 02:33",
          "username": "JohnyGaddarsarah_t",
          "content": "logic for A seems incorrect , it mentions outbound calls(call to restapi) are restricted via NACL which is possible. The correct reason is that there has no changes made in the environment , so A cannot be the optionA failed instance is not a \\\"change\\\" though, as changes are (more or less) intentional.",
          "upvote_count": "52",
          "selected_answers": ""
        },
        {
          "id": 333257,
          "date": "Mon 01 Nov 2021 20:13",
          "username": "sarah_t",
          "content": "A failed instance is not a \\\"change\\\" though, as changes are (more or less) intentional.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 11061,
          "date": "Thu 23 Sep 2021 03:17",
          "username": "awsec2",
          "content": "why not d",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 685664,
          "date": "Mon 03 Oct 2022 18:20",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 527879,
          "date": "Wed 19 Jan 2022 21:08",
          "username": "Ni_yot",
          "content": "D forme.the NAT instances are deployed in 2 AZs so when one fails thats 50% of the routing down. So best to replace the NAT with a Gateway. Why not use a GW in the first place anyways",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 499077,
          "date": "Sat 11 Dec 2021 04:45",
          "username": "challenger1",
          "content": "My Answer: D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493856,
          "date": "Sat 04 Dec 2021 18:34",
          "username": "AzureDP900",
          "content": "D is perfect . Thanks donathon for detail explanation !",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450742,
          "date": "Thu 04 Nov 2021 18:34",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409568,
          "date": "Wed 03 Nov 2021 19:02",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344275,
          "date": "Wed 03 Nov 2021 14:26",
          "username": "Waiweng",
          "content": "it's D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 309651,
          "date": "Sat 30 Oct 2021 13:12",
          "username": "Pupu86",
          "content": "can't be B as question has also indicated as \\\"off-peak\\\" season so no way the NAT instance would be overloaded.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293290,
          "date": "Sat 30 Oct 2021 01:05",
          "username": "kiev",
          "content": "D for me is the answer and the replacement with Nat Gatway will solve any elasticity issue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289578,
          "date": "Sat 23 Oct 2021 23:16",
          "username": "Kian1",
          "content": "D is my choice",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 287523,
          "date": "Sat 23 Oct 2021 22:15",
          "username": "LoganIsh",
          "content": "D is the right choice to replacing NAT gateway",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283168,
          "date": "Fri 22 Oct 2021 17:56",
          "username": "Ebi",
          "content": "Definitely D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 267416,
          "date": "Fri 22 Oct 2021 14:15",
          "username": "sanjaym",
          "content": "D for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242433,
          "date": "Thu 21 Oct 2021 03:25",
          "username": "T14102020",
          "content": "D is Correct. NAT Gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 238649,
          "date": "Wed 20 Oct 2021 06:54",
          "username": "MeepMeep",
          "content": "DDDDDDDD",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#449",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.<br>Which would enable the collection of this data MOST cost effectively?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#449",
          "answers": [
            {
              "choice": "<p>A. Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13211,
          "date": "Thu 30 Sep 2021 23:54",
          "username": "donathonTechGuru",
          "content": "A<br>A: Agent is needed because network dependencies are needed.<br>B: CloudWatch is not suitable.<br>C\\D: Agentless is only for VMWare environment where network dependencies are not needed.<br>AWS Application Discovery Service collects and presents data to enable enterprise customers to understand the configuration, usage, and behavior of servers in their IT environments. Server data is retained in the Application Discovery Service where it can be tagged and grouped into applications to help organize AWS migration planning. Collected data can be exported for analysis in Excel or other cloud migration analysis tools. <br>AWS Application Discovery Service supports agent-based and agentless modes of operation. With the agentless discovery, VMware customers collect VM configuration and performance profiles without deploying the AWS Application Discovery Agent on each host, which accelerates data collection. Customers in a non-VMware environment or that need additional information, like network dependencies and information about running processes, may install the Application Discovery Agent on servers and virtual machines (VMs) to collect data.Awesome explanation donathon!!",
          "upvote_count": "761",
          "selected_answers": ""
        },
        {
          "id": 17595,
          "date": "Fri 01 Oct 2021 11:46",
          "username": "TechGuru",
          "content": "Awesome explanation donathon!!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 10805,
          "date": "Mon 20 Sep 2021 22:03",
          "username": "awsec2dpvnme",
          "content": "A is right oneThe question doesn't mention the VMs are VMware so we need agent here. A",
          "upvote_count": "82",
          "selected_answers": ""
        },
        {
          "id": 11628,
          "date": "Wed 22 Sep 2021 10:13",
          "username": "dpvnme",
          "content": "The question doesn't mention the VMs are VMware so we need agent here. A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 716932,
          "date": "Sat 12 Nov 2022 22:15",
          "username": "DarthYoda",
          "content": "Most definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 716931,
          "date": "Sat 12 Nov 2022 22:15",
          "username": "DarthYoda",
          "content": "Most definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 709336,
          "date": "Tue 01 Nov 2022 18:48",
          "username": "superuser784",
          "content": "I did not know about AWS Application Discovery Service and my answer was B, because CloudWatch Agent can do this too, but after reviewing the AWS Application Discovery Service Documentation, this service does exactly what is requested. Final Answer A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 685665,
          "date": "Mon 03 Oct 2022 18:22",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 602716,
          "date": "Tue 17 May 2022 01:11",
          "username": "user0001",
          "content": "A ,this is because you need info about each process running on the VMs<br>https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497712,
          "date": "Thu 09 Dec 2021 13:28",
          "username": "cldy",
          "content": "A.  Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450743,
          "date": "Sat 06 Nov 2021 11:19",
          "username": "andylogan",
          "content": "It's A - Agentless for VMware",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409570,
          "date": "Sat 06 Nov 2021 10:30",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344277,
          "date": "Wed 03 Nov 2021 05:08",
          "username": "Waiweng",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 323033,
          "date": "Tue 02 Nov 2021 23:40",
          "username": "ExtHo",
          "content": "Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:<br><br>- Agentless discovery can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware Center.<br><br>- Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers.<br><br>So A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289581,
          "date": "Tue 02 Nov 2021 21:27",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283170,
          "date": "Tue 02 Nov 2021 07:37",
          "username": "Ebi",
          "content": "A is answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 267419,
          "date": "Sun 31 Oct 2021 06:55",
          "username": "sanjaym",
          "content": "A for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242438,
          "date": "Thu 28 Oct 2021 23:10",
          "username": "T14102020",
          "content": "A is correct answer. Application Discovery Service agent is key",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 229446,
          "date": "Thu 28 Oct 2021 10:32",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#450",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An Administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:<br><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0029700001.png\" class=\"in-exam-image\"><br>Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the Administrator address this problem?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#450",
          "answers": [
            {
              "choice": "<p>A. Add s3:CreateBucket with ג€Allowג€ effect to the SCP.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Instruct the Developers to add Amazon S3 permissions to their IAM entities.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Remove the SCP from account 1111-1111-1111.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 424557,
          "date": "Sat 02 Oct 2021 16:08",
          "username": "walkwolf3joe16",
          "content": "C<br><br>A.  It will give other people access of creating S3 bucket.<br>B.  It doesn't comply with organization's rule by removing accournt from OU. And it won't work either.<br>C.  Add required access to Developers only, not affecting others, right option.<br>D.  Provide people to change cloudtrail, which should be prohibited.C is right.<br>However A's explanation is incorrect - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html<br>\\\"SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions.\\\"",
          "upvote_count": "94",
          "selected_answers": ""
        },
        {
          "id": 451967,
          "date": "Mon 01 Nov 2021 17:11",
          "username": "joe16",
          "content": "C is right.<br>However A's explanation is incorrect - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html<br>\\\"SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions.\\\"",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 344278,
          "date": "Sun 26 Sep 2021 16:17",
          "username": "Waiweng",
          "content": "go with C",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 704149,
          "date": "Tue 25 Oct 2022 22:13",
          "username": "mrgreatness",
          "content": "100pc it is C.  If you don't understand why I suggest studying IAM and Orgs more.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685667,
          "date": "Mon 03 Oct 2022 18:23",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 577907,
          "date": "Wed 30 Mar 2022 00:44",
          "username": "jj22222",
          "content": "C looks ok",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 507001,
          "date": "Wed 22 Dec 2021 12:02",
          "username": "tkanmani76",
          "content": "Answer - C - The below passage clarifies why its C. <br>SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 493869,
          "date": "Sat 04 Dec 2021 18:45",
          "username": "AzureDP900",
          "content": "C seems perfect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 489854,
          "date": "Mon 29 Nov 2021 13:09",
          "username": "ryu10_09",
          "content": "why C ? I cannot add a permission to my user explicitly. if i do not have it then i need to ask someone to add it for like an admin. so C is ruled out here as well",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450766,
          "date": "Thu 21 Oct 2021 12:44",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445418,
          "date": "Thu 14 Oct 2021 13:15",
          "username": "AWSum1AWSum1",
          "content": "B seems the best <br><br>The SCP will continue to allow until it reaches and explicit Deny.Changing to C, <br><br>Taking this link into consideration , C is correct <br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 445422,
          "date": "Tue 19 Oct 2021 04:38",
          "username": "AWSum1",
          "content": "Changing to C, <br><br>Taking this link into consideration , C is correct <br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 423617,
          "date": "Fri 01 Oct 2021 16:15",
          "username": "tekkartAWS_Noob",
          "content": "I would say answer B. <br>See this page : https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html<br>-> Permissions work as the intersection from SCP of decreasing levels : root OU, child OU, account<br>-> Explicit Deny > Explicit Allow > Implicit Deny > Implicit Allow<br>where > means \\\"takes precedence over\\\".<br>If in this (OU-level SCP) where Explicit Allow on All Resources, S3 actions cannot be performed, it means that there must be an Explicit Deny on the (Root-OU Level).<br>Then to troubleshoot, the 2 options would be : <br>- to remove this Explicit Deny from the Root-OU SCP we assume there is (not proposed in the answers)<br>- or remove the OU dependency of the 1111-1111-1111 account for the Root-OU SCP not to apply anymore. This will have the impact that this Child-OU SCP will not apply anymore either, the only left will be Account-Level-IAM-Policy , assuming that she allows S3 actions<br><br>Answer BI tend to agree <br><br>The deny on the ou is blocking",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 430269,
          "date": "Sun 03 Oct 2021 19:03",
          "username": "AWS_Noob",
          "content": "I tend to agree <br><br>The deny on the ou is blocking",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409574,
          "date": "Thu 30 Sep 2021 11:56",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 337511,
          "date": "Thu 23 Sep 2021 16:26",
          "username": "CarisB",
          "content": "C is the only one which makes sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 329054,
          "date": "Tue 21 Sep 2021 17:04",
          "username": "certainly",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 328792,
          "date": "Mon 20 Sep 2021 03:32",
          "username": "gsw",
          "content": "\\\"SCPs don't affect resource-based policies directly.\\\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 310378,
          "date": "Mon 20 Sep 2021 01:43",
          "username": "Nguyenhau",
          "content": "I go with C",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#451",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company that provides wireless services needs a solution to store and analyze log files about user activities. Currently, log files are delivered daily to Amazon<br>Linux on an Amazon EC2 instance. A batch script is run once a day to aggregate data used for analysis by a third-party tool. The data pushed to the third-party tool is used to generate a visualization for end users. The batch script is cumbersome to maintain, and it takes several hours to deliver the ever-increasing data volumes to the third-party tool. The company wants to lower costs, and is open to considering a new tool that minimizes development effort and lowers administrative overhead. The company wants to build a more agile solution that can store and perform the analysis in near-real time, with minimal overhead. The solution needs to be cost effective and scalable to meet the company's end-user base growth.<br>Which solution meets the company's requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#451",
          "answers": [
            {
              "choice": "<p>A. Develop a Python script to capture the data from Amazon EC2 in real time and store the data in Amazon S3. Use a copy command to copy data from Amazon S3 to Amazon Redshift. Connect a business intelligence tool running on Amazon EC2 to Amazon Redshift and create the visualizations.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Amazon Kinesis agent running on an EC2 instance in an Auto Scaling group to collect and send the data to an Amazon Kinesis Data Firehose delivery stream. The Kinesis Data Firehose delivery stream will deliver the data directly to Amazon ES. Use Kibana to visualize the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an in-memory caching application running on an Amazon EBS-optimized EC2 instance to capture the log data in near real-time. Install an Amazon ES cluster on the same EC2 instance to store the log files as they are delivered to Amazon EC2 in near real-time. Install a Kibana plugin to create the visualizations.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon Kinesis agent running on an EC2 instance to collect and send the data to an Amazon Kinesis Data Firehose delivery stream. The Kinesis Data Firehose delivery stream will deliver the data to Amazon S3. Use an AWS Lambda function to deliver the data from Amazon S3 to Amazon ES. Use Kibana to visualize the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13215,
          "date": "Fri 24 Sep 2021 03:57",
          "username": "donathonStelSen",
          "content": "B<br>https://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html<br>A: Not real time.<br>C: Not sure if in-memory caching will help in terms of logs and not DB? Anyway installing ES cluster on the same EC2 instance means the ES is not managed and will increase overhead.<br>D: ES already has direct integration with Firehose, there is no need to transfer to S3 first unless it needs to retain the data for long period of time for analysis<br>https://aws.amazon.com/elasticsearch-service/?nc=sn&loc=1Just adding one more link to support Option B.  https://aws.amazon.com/blogs/big-data/ingest-streaming-data-into-amazon-elasticsearch-service-within-the-privacy-of-your-vpc-with-amazon-kinesis-data-firehose/",
          "upvote_count": "471",
          "selected_answers": ""
        },
        {
          "id": 455850,
          "date": "Sun 07 Nov 2021 00:26",
          "username": "StelSen",
          "content": "Just adding one more link to support Option B.  https://aws.amazon.com/blogs/big-data/ingest-streaming-data-into-amazon-elasticsearch-service-within-the-privacy-of-your-vpc-with-amazon-kinesis-data-firehose/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 11162,
          "date": "Mon 20 Sep 2021 19:52",
          "username": "Xiaoyao2000",
          "content": "Should be B?",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 716936,
          "date": "Sat 12 Nov 2022 22:23",
          "username": "DarthYoda",
          "content": "I'd say B too.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 685669,
          "date": "Mon 03 Oct 2022 18:26",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 497783,
          "date": "Thu 09 Dec 2021 15:13",
          "username": "cldy",
          "content": "B.  Use an Amazon Kinesis agent running on an EC2 instance in an Auto Scaling group to collect and send the data to an Amazon Kinesis Data Firehose delivery stream. The Kinesis Data Firehose delivery stream will deliver the data directly to Amazon ES. Use Kibana to visualize the data.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493874,
          "date": "Sat 04 Dec 2021 18:50",
          "username": "AzureDP900",
          "content": "There is no need of Lambda function that is the reason B is right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 457110,
          "date": "Sun 07 Nov 2021 07:31",
          "username": "kirrim",
          "content": "Would use OpenSearch these days instead of ElasticSearch, otherwise still B is the correct answer.<br><br>https://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450786,
          "date": "Sat 06 Nov 2021 22:48",
          "username": "andylogan",
          "content": "it's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 450692,
          "date": "Sat 06 Nov 2021 19:27",
          "username": "awsbob2021",
          "content": "B Kinesis to fire hose to s3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409580,
          "date": "Tue 02 Nov 2021 08:53",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 408780,
          "date": "Sun 31 Oct 2021 05:31",
          "username": "nodogoshi",
          "content": "D.  Firehosedont supplementaryefs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406675,
          "date": "Fri 29 Oct 2021 17:57",
          "username": "Akhil254",
          "content": "B Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 349095,
          "date": "Thu 28 Oct 2021 23:43",
          "username": "macshild",
          "content": "The answer is B because Kinesis Firehose is for near real time streaming of data from cloudwatch, IOT, EC2 etc and it outputs data to splunk/http endpoints, redshift, S3 and elastic seacrch",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344359,
          "date": "Thu 28 Oct 2021 11:39",
          "username": "Waiweng",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 292586,
          "date": "Thu 28 Oct 2021 09:17",
          "username": "AJBA",
          "content": "B <br>https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 289583,
          "date": "Wed 27 Oct 2021 12:21",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 283172,
          "date": "Mon 25 Oct 2021 22:53",
          "username": "Ebi",
          "content": "I go with B, there is no need to have a Lambda function",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#452",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to move a web application to AWS. The application stores session information locally on each web server, which will make auto scaling difficult.<br>As part of the migration, the application will be rewritten to decouple the session data from the web servers. The company requires low latency, scalability, and availability.<br>Which service will meet the requirements for storing the session information in the MOST cost-effective way?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#452",
          "answers": [
            {
              "choice": "<p>A. Amazon ElastiCache with the Memcached engine<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon S3<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon RDS MySQL<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon ElastiCache with the Redis engine<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13785,
          "date": "Thu 23 Sep 2021 05:44",
          "username": "donathon",
          "content": "D<br>While Key/Value data stores are known to be extremely fast and provide sub-millisecond latency, the added network latency and added cost are the drawbacks. An added benefit of leveraging Key/Value stores is that they can also be utilized to cache any data, not just HTTP sessions, which can help boost the overall performance of your applications.<br>A vs D: ElastiCache offerings for In-Memory key/value stores include ElastiCache for Redis, which can support replication, and ElastiCache for Memcached which does not support replication.<br>https://aws.amazon.com/caching/session-management/",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 310061,
          "date": "Sun 10 Oct 2021 12:48",
          "username": "Pupu86StelSenJonfernz",
          "content": "I think they key aspect to choosing whether it is Memcached (A) or Redis (D) is to understand the key difference between both. <br><br>Memcache & Redis offers low latency & scalability however if you talk about availability (1 of the pillar of good framework architecture), you must know that Redis is the only one that offers read replicas in the event of a node failure while Memcached doesn't have the ability to replicate a failed node (which means downtime)Agree. And possible to loose all user sessions from that node in this case. So Redis is better.plus Memcached isn't easy to scale. every time you want to upgrade, you cant do it directly. you'd need to create a new cluster.",
          "upvote_count": "1011",
          "selected_answers": ""
        },
        {
          "id": 455855,
          "date": "Wed 03 Nov 2021 13:35",
          "username": "StelSenJonfernz",
          "content": "Agree. And possible to loose all user sessions from that node in this case. So Redis is better.plus Memcached isn't easy to scale. every time you want to upgrade, you cant do it directly. you'd need to create a new cluster.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 688980,
          "date": "Sat 08 Oct 2022 04:06",
          "username": "Jonfernz",
          "content": "plus Memcached isn't easy to scale. every time you want to upgrade, you cant do it directly. you'd need to create a new cluster.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716940,
          "date": "Sat 12 Nov 2022 22:27",
          "username": "DarthYoda",
          "content": "I'd say D.  S3 doesn't really qualify for \\\"scalability\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 716938,
          "date": "Sat 12 Nov 2022 22:26",
          "username": "DarthYoda",
          "content": "I'd say D.  S3 doesn't really qualify for \\\"scalability\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 704152,
          "date": "Tue 25 Oct 2022 22:22",
          "username": "mrgreatness",
          "content": "D for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685672,
          "date": "Mon 03 Oct 2022 18:28",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 576110,
          "date": "Sun 27 Mar 2022 12:19",
          "username": "Mechanicashii007",
          "content": "Why not B? S3 is more cost-effective, is it not useful when it comes to storing sessions information with high-performance needs?S3 cannot support millisecond latency that is required for session management. In-memory caching solution is the way to go",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 696686,
          "date": "Mon 17 Oct 2022 02:34",
          "username": "ashii007",
          "content": "S3 cannot support millisecond latency that is required for session management. In-memory caching solution is the way to go",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495747,
          "date": "Tue 07 Dec 2021 08:44",
          "username": "cldy",
          "content": "D.  Amazon ElastiCache with the Redis engine",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 450790,
          "date": "Tue 02 Nov 2021 09:03",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445659,
          "date": "Tue 02 Nov 2021 06:34",
          "username": "DonSp",
          "content": "A in real life, probably D for the exam.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441133,
          "date": "Mon 25 Oct 2021 14:34",
          "username": "student22",
          "content": "Answer is D<br><br>Redis: <br>Building real-time apps across versatile use cases like gaming, geospatial service, caching, session stores, or queuing, with advanced data structures, replication, and point-in-time snapshot support. <br><br>Memcached: <br>Building a simple, scalable caching layer for your data-intensive apps. <br><br>https://aws.amazon.com/elasticache/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437819,
          "date": "Thu 21 Oct 2021 16:48",
          "username": "tgv",
          "content": "DDD<br>---",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409583,
          "date": "Thu 21 Oct 2021 06:19",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 408782,
          "date": "Mon 18 Oct 2021 01:59",
          "username": "nodogoshi",
          "content": "A COSTBEST. redis PERFORMANCE UNDERGONE",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344361,
          "date": "Wed 13 Oct 2021 07:41",
          "username": "Waiweng",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 322363,
          "date": "Tue 12 Oct 2021 01:02",
          "username": "alisyech",
          "content": "sure for D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 282605,
          "date": "Fri 08 Oct 2021 18:16",
          "username": "Ebi",
          "content": "I go with D",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#453",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an Amazon EC2 deployment that has the following architecture:<br>✑ An application tier that contains 8 m4.xlarge instances<br>✑ A Classic Load Balancer<br>✑ Amazon S3 as a persistent data store<br>After one of the EC2 instances fails, users report very slow processing of their requests. A Solutions Architect must recommend design changes to maximize system reliability. The solution must minimize costs.<br>What should the Solutions Architect recommend?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#453",
          "answers": [
            {
              "choice": "<p>A. Migrate the existing EC2 instances to a serverless deployment using AWS Lambda functions<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Change the Classic Load Balancer to an Application Load Balancer<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace the application tier with m4.large instances in an Auto Scaling group<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replace the application tier with 4 m4.2xlarge instances<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 46406,
          "date": "Wed 29 Sep 2021 20:47",
          "username": "sb333Smart",
          "content": "C looks to be the best answer IMO. This is a capacity issue. Losing one EC2 instance resulted in reduced performance. What's needed is the benefits of an Auto Scaling group. You can scale out when needed and scale in as well (during off-peak hours, for example). Also, changing the size of the server can help with costs. You can scale out and in with smaller increments of capacity at a time. So I don't see an issue with the instance size change.<br><br>Nothing in this question points to an issue with CLB.  Yes, AWS is encouraging people to move to ALB, but CLB is not the cause of the issue here. Having an ALB would not have avoided the issue being experienced.Agreed. The main problem is capacity that is fixed with ASG. It also helps reduce instance cost. <br>A (Invalid): CLB cannot trigger Lambda functions - ALB can. Otherwise, good choice. <br>B & D(Invalid): Doesn't resolve capacity issue or auto-healing (achieved by ASG)",
          "upvote_count": "303",
          "selected_answers": ""
        },
        {
          "id": 76459,
          "date": "Tue 05 Oct 2021 16:14",
          "username": "Smart",
          "content": "Agreed. The main problem is capacity that is fixed with ASG. It also helps reduce instance cost. <br>A (Invalid): CLB cannot trigger Lambda functions - ALB can. Otherwise, good choice. <br>B & D(Invalid): Doesn't resolve capacity issue or auto-healing (achieved by ASG)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 12707,
          "date": "Thu 23 Sep 2021 23:50",
          "username": "donathonAWSPro24shammousjohnnsmithawsgcpazureawsproawspro",
          "content": "By default, connection draining is enabled for Application Load Balancers but must be enabled for Classic Load Balancers. When Connection Draining is enabled and configured, the process of deregistering an instance from an Elastic Load Balancer gains an additional step. For the duration of the configured timeout, the load balancer will allow existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. During this time, the API will report the status of the instance as InService, along with a message stating that “Instance deregistration currently in progress.” Once the timeout is reached, any remaining connections will be forcibly closed.<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html<br>https://aws.amazon.com/blogs/aws/elb-connection-draining-remove-instances-from-service-with-care/I think donathon is feeding people bad answers.Either he's not knowledgeable or works for AWS and is trying to spread disinformation. How is connection draining related to the fact that a single failed instances causes degraded performance?You say there is no indication it is a load issue but how can it not be a load issue when only a single failure causes performance degradation.That sounds like the definition of a load issue to me and so Autoscaling is what is needed to adjust to both instance failures and fluctuating demand. Autoscaling also automatically replaces failed instances.. which would solve your single failure issue as well. https://aws.amazon.com/ec2/autoscaling/faqs/#Replacing_Impaired_Instances<br><br>C should be the answer.I have read all donathon answers in Topic 2 and most are correct with great analysis. It's unfair to accuse people like that as we are all still learning and preparing for the exam. Instead, you might just criticize wrong answers and provide your preferred one based on your own analysis and references.When an instance breaks, it is dead. All connections to that instance break. What is the use of draining? How can \\\"the load balancer will allow existing, in-flight requests made to an instance to complete\\\"?Wow, amazing! <br>Your comments are very helpful.So, Ans is B?Ah~ got it.<br>ELB + auto scaling <br>yeah. thank you",
          "upvote_count": "2145321212",
          "selected_answers": ""
        },
        {
          "id": 49764,
          "date": "Thu 30 Sep 2021 13:03",
          "username": "AWSPro24shammousjohnnsmith",
          "content": "I think donathon is feeding people bad answers.Either he's not knowledgeable or works for AWS and is trying to spread disinformation. How is connection draining related to the fact that a single failed instances causes degraded performance?You say there is no indication it is a load issue but how can it not be a load issue when only a single failure causes performance degradation.That sounds like the definition of a load issue to me and so Autoscaling is what is needed to adjust to both instance failures and fluctuating demand. Autoscaling also automatically replaces failed instances.. which would solve your single failure issue as well. https://aws.amazon.com/ec2/autoscaling/faqs/#Replacing_Impaired_Instances<br><br>C should be the answer.I have read all donathon answers in Topic 2 and most are correct with great analysis. It's unfair to accuse people like that as we are all still learning and preparing for the exam. Instead, you might just criticize wrong answers and provide your preferred one based on your own analysis and references.When an instance breaks, it is dead. All connections to that instance break. What is the use of draining? How can \\\"the load balancer will allow existing, in-flight requests made to an instance to complete\\\"?",
          "upvote_count": "45321",
          "selected_answers": ""
        },
        {
          "id": 279842,
          "date": "Sat 23 Oct 2021 08:26",
          "username": "shammousjohnnsmith",
          "content": "I have read all donathon answers in Topic 2 and most are correct with great analysis. It's unfair to accuse people like that as we are all still learning and preparing for the exam. Instead, you might just criticize wrong answers and provide your preferred one based on your own analysis and references.When an instance breaks, it is dead. All connections to that instance break. What is the use of draining? How can \\\"the load balancer will allow existing, in-flight requests made to an instance to complete\\\"?",
          "upvote_count": "321",
          "selected_answers": ""
        },
        {
          "id": 552340,
          "date": "Sun 20 Feb 2022 23:43",
          "username": "johnnsmith",
          "content": "When an instance breaks, it is dead. All connections to that instance break. What is the use of draining? How can \\\"the load balancer will allow existing, in-flight requests made to an instance to complete\\\"?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13148,
          "date": "Fri 24 Sep 2021 01:36",
          "username": "awsgcpazure",
          "content": "Wow, amazing! <br>Your comments are very helpful.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 13550,
          "date": "Fri 24 Sep 2021 13:23",
          "username": "awspro",
          "content": "So, Ans is B?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13553,
          "date": "Fri 24 Sep 2021 22:55",
          "username": "awspro",
          "content": "Ah~ got it.<br>ELB + auto scaling <br>yeah. thank you",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 716945,
          "date": "Sat 12 Nov 2022 22:30",
          "username": "DarthYoda",
          "content": "C looks good. Q says \\\"maximize system reliability.\\\", the issue is with one of the instances failing, and we know autoscaling will immediately remediate this when one instance is lost",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 685675,
          "date": "Mon 03 Oct 2022 18:34",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 679284,
          "date": "Mon 26 Sep 2022 01:50",
          "username": "tomosabc1",
          "content": "@sb333's explanation clearly shows that the answer should be C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 655208,
          "date": "Wed 31 Aug 2022 16:55",
          "username": "Sizuma",
          "content": "B SURE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 653872,
          "date": "Tue 30 Aug 2022 08:46",
          "username": "GuoxianAum",
          "content": "what i do not get for C is how come a m4.large is more capable than m4.xlarge? I meant, I have chosen B.  Then again, B isn't the best, either. This entire question has a lot of assumptions in it. If there is a capacity issue by loosing a m4.xlarge, by replacing the entire fleet of m4.xlarge with something less capable, wouldn't cause similar problem, too?I guess the keyword is AutoScaling.. even when the instance type is lower in spec but it can scale up automatically",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 715244,
          "date": "Thu 10 Nov 2022 14:06",
          "username": "Aum",
          "content": "I guess the keyword is AutoScaling.. even when the instance type is lower in spec but it can scale up automatically",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 652983,
          "date": "Sun 28 Aug 2022 16:34",
          "username": "astalavista1",
          "content": "The scaling issue has nothing to do with intermittent session issues or users being logged off after an instance fails.<br>The answer has to be C, not B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 493879,
          "date": "Sat 04 Dec 2021 19:03",
          "username": "AzureDP900",
          "content": "requests processing slow don't have any relation with ALB vs Classic LB, it is capacity issue so C is right answer",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 451147,
          "date": "Sat 06 Nov 2021 00:24",
          "username": "andylogan",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445423,
          "date": "Fri 05 Nov 2021 20:48",
          "username": "AWSum1",
          "content": "C <br><br>Think of it this way. <br><br>If 2 instances fail, processing is gonna get slower. The CLB will still direct traffic to the instances. If it was latency or SNI needed than, an ALB would satisfy the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437821,
          "date": "Tue 02 Nov 2021 11:07",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409599,
          "date": "Sun 31 Oct 2021 02:57",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 406918,
          "date": "Fri 29 Oct 2021 20:04",
          "username": "kuroro",
          "content": "I think the answer is C. <br>Because the use of Classic Load Balancer is a pre-requisite, which means the application is using EC2-Classic Network instead of VPC, hence this rule out the use of ALB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406678,
          "date": "Fri 29 Oct 2021 09:34",
          "username": "Akhil254",
          "content": "B,C correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 392679,
          "date": "Thu 28 Oct 2021 01:59",
          "username": "Kopa",
          "content": "key word reduce cost and improve reability, both direct me to go for C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 363548,
          "date": "Tue 26 Oct 2021 20:28",
          "username": "ibrahimsow",
          "content": "C is the anwer. A classic Load Balancer has the connection draining feature. So they point here is Auto Scaling.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#454",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An on-premises application will be migrated to the cloud. The application consists of a single Elasticsearch virtual machine with data source feeds from local systems that will not be migrated, and a Java web application on Apache Tomcat running on three virtual machines. The Elasticsearch server currently uses 1 TB of storage out of 16 TB available storage, and the web application is updated every 4 months. Multiple users access the web application from the Internet. There is a 10Gbit AWS Direct Connect connection established, and the application can be migrated over a scheduled 48-hour change window.<br>Which strategy will have the LEAST impact on the Operations staff after the migration?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#454",
          "answers": [
            {
              "choice": "<p>A. Create an Elasticsearch server on Amazon EC2 right-sized with 2 TB of Amazon EBS and a public AWS Elastic Beanstalk environment for the web application. Pause the data sources, export the Elasticsearch index from on premises, and import into the EC2 Elasticsearch server. Move data source feeds to the new Elasticsearch server and move users to the web application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon ES cluster for Elasticsearch and a public AWS Elastic Beanstalk environment for the web application. Use AWS DMS to replicate Elasticsearch data. When replication has finished, move data source feeds to the new Amazon ES cluster endpoint and move users to the new web application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the AWS SMS to replicate the virtual machines into AWS. When the migration is complete, pause the data source feeds and start the migrated Elasticsearch and web application instances. Place the web application instances behind a public Elastic Load Balancer. Move the data source feeds to the new Elasticsearch server and move users to the new web Application Load Balancer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon ES cluster for Elasticsearch and a public AWS Elastic Beanstalk environment for the web application. Pause the data source feeds, export the Elasticsearch index from on premises, and import into the Amazon ES cluster. Move the data source feeds to the new Amazon ES cluster endpoint and move users to the new web application.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 34842,
          "date": "Tue 21 Sep 2021 20:07",
          "username": "arunkumarSmartDashLDeathFrmAbv",
          "content": "LEAST impcat on the Operations staff after the migration.<br><br>A.  Need to be managed by Operations staff<br>B.  ES data can not be used as source for DMS. It can be used as target.<br>C.  Need to be managed by Operations staff<br>D.  Company has 10GB DX, so migration can be done done within 48 hours. <br><br>Answer is DPerfect!Who will manage the Elastic Beanstalk environment?<br>Since operations staff manages VMs, they can manage EC2. But learning about Elastic Beanstalk??Elastic Beanstalk is a managed service by AWS, it's meant for developers so that they can easily deploy their app to AWS without having near to zero knowledge of AWS on how to manage stuffs,because AWS will do the management of beanstalk",
          "upvote_count": "48212",
          "selected_answers": ""
        },
        {
          "id": 76469,
          "date": "Fri 24 Sep 2021 22:02",
          "username": "Smart",
          "content": "Perfect!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 393240,
          "date": "Tue 02 Nov 2021 15:58",
          "username": "DashLDeathFrmAbv",
          "content": "Who will manage the Elastic Beanstalk environment?<br>Since operations staff manages VMs, they can manage EC2. But learning about Elastic Beanstalk??Elastic Beanstalk is a managed service by AWS, it's meant for developers so that they can easily deploy their app to AWS without having near to zero knowledge of AWS on how to manage stuffs,because AWS will do the management of beanstalk",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 403591,
          "date": "Wed 03 Nov 2021 07:41",
          "username": "DeathFrmAbv",
          "content": "Elastic Beanstalk is a managed service by AWS, it's meant for developers so that they can easily deploy their app to AWS without having near to zero knowledge of AWS on how to manage stuffs,because AWS will do the management of beanstalk",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 26953,
          "date": "Tue 21 Sep 2021 10:23",
          "username": "johannes756",
          "content": "Can't be B as AWS DMS supports Elasticsearch as a target, not a source",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 685676,
          "date": "Mon 03 Oct 2022 18:38",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 670594,
          "date": "Fri 16 Sep 2022 09:58",
          "username": "Dionenonly",
          "content": "ES not supported as source in DMS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 655210,
          "date": "Wed 31 Aug 2022 16:56",
          "username": "Sizuma",
          "content": "B FOR SURE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496397,
          "date": "Wed 08 Dec 2021 00:39",
          "username": "vbaltomosabc1",
          "content": "D - It doesn't talk about ES current 1TB data migration to AWS just an Index...???A valid point. I am confused with the same question.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 679302,
          "date": "Mon 26 Sep 2022 02:18",
          "username": "tomosabc1",
          "content": "A valid point. I am confused with the same question.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 451148,
          "date": "Sun 07 Nov 2021 12:52",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409603,
          "date": "Sat 06 Nov 2021 21:00",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 406680,
          "date": "Wed 03 Nov 2021 14:50",
          "username": "Akhil254",
          "content": "D Correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344992,
          "date": "Tue 02 Nov 2021 02:46",
          "username": "blackgamer",
          "content": "I will go with D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344396,
          "date": "Sun 31 Oct 2021 17:34",
          "username": "Waiweng",
          "content": "It's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 333686,
          "date": "Fri 29 Oct 2021 02:59",
          "username": "Amitv2706",
          "content": "Which strategy will have the LEAST impact on the Operations staff after the migration?<br><br>A- Will require management of EC2 nodes for ES post migration<br>B- ES cant be source for DMS as it can be only a target<br>C- ec2 VMs will be required to manage post migration<br>D- Correct Answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 308818,
          "date": "Fri 29 Oct 2021 01:40",
          "username": "ajeeshb",
          "content": "Answer: D. <br>Index snapshots are a popular way to migrate from a self-managed Elasticsearch cluster to Amazon Elasticsearch Service.<br>Refer: https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/migration.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 297070,
          "date": "Wed 27 Oct 2021 07:07",
          "username": "rosebank",
          "content": "so, which is the correct answer in the exam, B or D?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290297,
          "date": "Tue 26 Oct 2021 23:49",
          "username": "Kian1",
          "content": "will go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 285100,
          "date": "Sun 24 Oct 2021 09:24",
          "username": "bnagaraja9099",
          "content": "No to A & C: No ES cluster. No DMS source for ES. That leaves D.  The time window of 48 hours aligns with 1TB size and 10 GB band width. I ll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282629,
          "date": "Sat 23 Oct 2021 04:16",
          "username": "Ebi",
          "content": "This is easy, minimum overhead for staff means AWS ES, so answer is either B or D<br>DMS does not support ES as source, so B ruled out <br>Answer is D",
          "upvote_count": "7",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#455",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's application is increasingly popular and experiencing latency because of high volume reads on the database server.<br>The service has the following properties:<br>✑ A highly available REST API hosted in one region using Application Load Balancer (ALB) with auto scaling.<br>✑ A MySQL database hosted on an Amazon EC2 instance in a single Availability Zone.<br>The company wants to reduce latency, increase in-region database read performance, and have multi-region disaster recovery capabilities that can perform a live recovery automatically without any data or performance loss (HA/DR).<br>Which deployment strategy will meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#455",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudFormation StackSets to deploy the API layer in two regions. Migrate the database to an Amazon Aurora with MySQL database cluster with multiple read replicas in one region and a read replica in a different region than the source database cluster. Use Amazon Route 53 health checks to trigger a DNS failover to the standby region if the health checks to the primary load balancer fail. In the event of Route 53 failover, promote the cross-region database replica to be the master and build out new read replicas in the standby region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. In the event of failure, use Amazon Route 53 health checks on the database to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS CloudFormation StackSets to deploy the API layer in two regions. Add the database to an Auto Scaling group. Add a read replica to the database in the second region. Use Amazon Route 53 health checks on the database to trigger a DNS failover to the standby region if the health checks in the primary region fail. Promote the cross-region database replica to be the master and build out new read replicas in the standby region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon ElastiCache for Redis Multi-AZ with an automatic failover to cache the database read queries. Use AWS OpsWorks to deploy the API layer, cache layer, and existing database layer in two regions. Use Amazon Route 53 health checks on the ALB to trigger a DNS failover to the standby region if the health checks in the primary region fail. Back up the MySQL database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12715,
          "date": "Tue 21 Sep 2021 00:02",
          "username": "donathonPacoDerek",
          "content": "A<br>B: Not multi-region.<br>C: Adding database to Auto Scaling group? Is that possible?<br>D: Live recovery is required.support A<br>B/D: B also 2 regions. but B heath check on Database is quite weird, and both request \\\"a live recover automatically\\\" ,both B/D using a manual job of \\\"copy and restore\\\"",
          "upvote_count": "304",
          "selected_answers": ""
        },
        {
          "id": 42527,
          "date": "Wed 22 Sep 2021 11:26",
          "username": "PacoDerek",
          "content": "support A<br>B/D: B also 2 regions. but B heath check on Database is quite weird, and both request \\\"a live recover automatically\\\" ,both B/D using a manual job of \\\"copy and restore\\\"",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 282631,
          "date": "Thu 28 Oct 2021 04:05",
          "username": "Ebi",
          "content": "No doubt answer is A",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 716947,
          "date": "Sat 12 Nov 2022 22:42",
          "username": "DarthYoda",
          "content": "Without a doubt, A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 685678,
          "date": "Mon 03 Oct 2022 18:41",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 626453,
          "date": "Sun 03 Jul 2022 08:47",
          "username": "aandc",
          "content": "only A has a cross-region database replica setup",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 561454,
          "date": "Sat 05 Mar 2022 14:56",
          "username": "Ni_yot",
          "content": "Yah it is A.  Multi AZs with Aurora and Standy can easily be promoted. Then Cloudformation can be used to build out a new environment",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 527606,
          "date": "Wed 19 Jan 2022 15:43",
          "username": "Yecine11y",
          "content": "think it's A<br>https://aws.amazon.com/blogs/database/deploy-multi-region-amazon-aurora-applications-with-a-failover-blueprint/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493881,
          "date": "Sat 04 Dec 2021 19:21",
          "username": "AzureDP900",
          "content": "A is right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 451151,
          "date": "Thu 04 Nov 2021 02:06",
          "username": "andylogan",
          "content": "It's A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448681,
          "date": "Thu 04 Nov 2021 00:20",
          "username": "Kopa",
          "content": "Aurora and Route 53 failover goes to A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409610,
          "date": "Sun 31 Oct 2021 14:13",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344405,
          "date": "Sat 30 Oct 2021 17:20",
          "username": "Waiweng",
          "content": "It's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290302,
          "date": "Fri 29 Oct 2021 16:05",
          "username": "Kian1",
          "content": "will go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281592,
          "date": "Wed 27 Oct 2021 21:07",
          "username": "kopper2019",
          "content": "A, Opsworks options are out and the one scaling the MySQL instance no go",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 267970,
          "date": "Mon 25 Oct 2021 22:34",
          "username": "sanjaym",
          "content": "Go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242469,
          "date": "Thu 21 Oct 2021 14:13",
          "username": "T14102020",
          "content": "Answer is A.  Aurora",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230024,
          "date": "Mon 18 Oct 2021 18:56",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#456",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a three-tier application in AWS. Users report that the application performance can vary greatly depending on the time of day and functionality being accessed.<br>The application includes the following components:<br>✑ Eight t2.large front-end web servers that serve static content and proxy dynamic content from the application tier.<br>✑ Four t2.large application servers.<br>✑ One db.m4.large Amazon RDS MySQL Multi-AZ DB instance.<br>Operations has determined that the web and application tiers are network constrained.<br>Which of the following is a cost effective way to improve application performance? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#456",
          "answers": [
            {
              "choice": "<p>A. Replace web and app tiers with t2.xlarge instances<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Auto Scaling and m4.large instances for the web and application tiers<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Convert the MySQL RDS instance to a self-managed MySQL cluster on Amazon EC2<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudFront distribution to cache content<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Increase the size of the Amazon RDS instance to db.m4.xlarge<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 310252,
          "date": "Mon 20 Sep 2021 09:03",
          "username": "ItsmeP",
          "content": "B: use autoscaling, and that allow for increasing and reducing of utilization, plus the performance of M4 is better. <br>D: using CloudFront, reduce the network local utilization, thus support the solution.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 344409,
          "date": "Mon 04 Oct 2021 09:21",
          "username": "Waiweng",
          "content": "It's BD",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 685679,
          "date": "Mon 03 Oct 2022 18:42",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 636278,
          "date": "Mon 25 Jul 2022 00:51",
          "username": "hilft",
          "content": "Autoscaling + Cloudfront",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 539640,
          "date": "Thu 03 Feb 2022 12:24",
          "username": "HellGate",
          "content": "\\\"The web and application layers have been identified as network limited by operations.\\\"=> what does this mean?Need more strong instance type? or ???",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493885,
          "date": "Sat 04 Dec 2021 19:31",
          "username": "AzureDP900",
          "content": "B,D is prefect answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 451152,
          "date": "Tue 26 Oct 2021 13:33",
          "username": "andylogan",
          "content": "It's B D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409616,
          "date": "Sat 16 Oct 2021 20:30",
          "username": "WhyIronMan",
          "content": "I'll go with B, D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 353407,
          "date": "Tue 12 Oct 2021 19:48",
          "username": "victordun",
          "content": "should be BD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 310325,
          "date": "Tue 21 Sep 2021 04:15",
          "username": "Nguyenhau",
          "content": "i go with BD",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#457",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An online retailer needs to regularly process large product catalogs, which are handled in batches. These are sent out to be processed by people using the<br>Amazon Mechanical Turk service, but the retailer has asked its Solutions Architect to design a workflow orchestration system that allows it to handle multiple concurrent Mechanical Turk operations, deal with the result assessment process, and reprocess failures.<br>Which of the following options gives the retailer the ability to interrogate the state of every workflow with the LEAST amount of implementation effort?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#457",
          "answers": [
            {
              "choice": "<p>A. Trigger Amazon CloudWatch alarms based upon message visibility in multiple Amazon SQS queues (one queue per workflow stage) and send messages via Amazon SNS to trigger AWS Lambda functions to process the next step. Use Amazon ES and Kibana to visualize Lambda processing logs to see the workflow states.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Hold workflow information in an Amazon RDS instance with AWS Lambda functions polling RDS for status changes. Worker Lambda functions then process the next workflow steps. Amazon QuickSight will visualize workflow states directly out of Amazon RDS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Build the workflow in AWS Step Functions, using it to orchestrate multiple concurrent workflows. The status of each workflow can be visualized in the AWS Management Console, and historical data can be written to Amazon S3 and visualized using Amazon QuickSight.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon SWF to create a workflow that handles a single batch of catalog records with multiple worker tasks to extract the data, transform it, and send it through Mechanical Turk. Use Amazon ES and Kibana to visualize AWS Lambda processing logs to see the workflow states.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 22055,
          "date": "Wed 22 Sep 2021 15:20",
          "username": "huhupainewme",
          "content": "I would go for D, Use case #2: Processing large product catalogs using Amazon Mechanical Turk. https://aws.amazon.com/swf/faqs/The FAQ itself becomes a question. Have to go for D. ",
          "upvote_count": "466",
          "selected_answers": ""
        },
        {
          "id": 245525,
          "date": "Thu 21 Oct 2021 17:25",
          "username": "newme",
          "content": "The FAQ itself becomes a question. Have to go for D. ",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 12719,
          "date": "Tue 21 Sep 2021 04:50",
          "username": "donathonMultiAZsg0206hailiangconsultskconsultskDashL",
          "content": "C<br>AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Instead of writing a Decider program, you define state machines in JSON. AWS customers should consider using Step Functions for new applications. If Step Functions does not fit your needs, then you should consider Amazon Simple Workflow (SWF). Amazon SWF provides you complete control over your orchestration logic, but increases the complexity of developing applications. You may write decider programs in the programming language of your choice, or you may use the Flow framework to use programming constructs that structure asynchronous interactions for you. AWS will continue to provide the Amazon SWF service, Flow framework, and support all Amazon SWF customers.<br>https://aws.amazon.com/swf/faqs/Step Functions do not support Mechanical Turk. You need SWF, so Dit supports mechanical turkit can use lambda to call mechanical turk, does not need to \\\"support\\\" it per sayyour assumptions are incorrect and may check the link: https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3<br>C is correct.Steps Functions does not support Mechanical Turk.",
          "upvote_count": "261814116",
          "selected_answers": ""
        },
        {
          "id": 143799,
          "date": "Wed 06 Oct 2021 10:52",
          "username": "MultiAZsg0206hailiangconsultsk",
          "content": "Step Functions do not support Mechanical Turk. You need SWF, so Dit supports mechanical turkit can use lambda to call mechanical turk, does not need to \\\"support\\\" it per sayyour assumptions are incorrect and may check the link: https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3",
          "upvote_count": "18141",
          "selected_answers": ""
        },
        {
          "id": 695080,
          "date": "Sat 15 Oct 2022 03:09",
          "username": "sg0206",
          "content": "it supports mechanical turk",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 174839,
          "date": "Sat 09 Oct 2021 13:33",
          "username": "hailiang",
          "content": "it can use lambda to call mechanical turk, does not need to \\\"support\\\" it per say",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 252643,
          "date": "Sat 23 Oct 2021 05:00",
          "username": "consultsk",
          "content": "your assumptions are incorrect and may check the link: https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 252641,
          "date": "Fri 22 Oct 2021 07:34",
          "username": "consultskDashL",
          "content": "https://blog.mturk.com/tutorial-using-mturk-together-with-aws-lambda-c91d414496d3<br>C is correct.Steps Functions does not support Mechanical Turk.",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 393263,
          "date": "Mon 01 Nov 2021 14:48",
          "username": "DashL",
          "content": "Steps Functions does not support Mechanical Turk.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 695079,
          "date": "Sat 15 Oct 2022 03:08",
          "username": "sg0206wofu",
          "content": "C is the correct answer, we can implement human approval feature in step function.. SWF is old and not serverless feature.Step Functions does not work with Mechanical Turk",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 716088,
          "date": "Fri 11 Nov 2022 14:41",
          "username": "wofu",
          "content": "Step Functions does not work with Mechanical Turk",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685682,
          "date": "Mon 03 Oct 2022 18:47",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 643106,
          "date": "Fri 05 Aug 2022 23:42",
          "username": "Mr_nobody79",
          "content": "It's D 100%. Mechanical Turk = Human intervention = SWF. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 637701,
          "date": "Wed 27 Jul 2022 02:44",
          "username": "hilft",
          "content": "D.  keyword here is SWF. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 553171,
          "date": "Mon 21 Feb 2022 20:32",
          "username": "azure_kai",
          "content": "Mechanical Turk supports integration with SWF, not Step functions",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 546190,
          "date": "Sun 13 Feb 2022 03:01",
          "username": "futen0326",
          "content": "D IS CORRECT, MECHANICAL TURK DOES NOT SUPPORT STEP FUNCTIONS. SWF IS USED WHERE STEP FUNCTIONS ARE NOT SUPPORTED. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 542543,
          "date": "Mon 07 Feb 2022 17:39",
          "username": "jj22222",
          "content": "C.  Build the workflow in AWS Step Functions, using it to orchestrate multiple concurrent workflows. The status of each workflow can be visualized in the AWS Management Console, and historical data can be written to Amazon S3 and visualized using Amazon QuickSight.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 539701,
          "date": "Thu 03 Feb 2022 13:53",
          "username": "zoliv",
          "content": "D.  Mechanical Turk works well with SWF, and that is the only few cases where both services complement one another.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 499300,
          "date": "Sat 11 Dec 2021 11:03",
          "username": "cldy",
          "content": "D.  Use Amazon SWF to create a workflow that handles a single batch of catalog records with multiple worker tasks to extract the data, transform it, and send it through Mechanical Turk. Use Amazon ES and Kibana to visualize AWS Lambda processing logs to see the workflow states.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496823,
          "date": "Wed 08 Dec 2021 13:54",
          "username": "wem",
          "content": "D is correct<br>or miss it on the test",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495479,
          "date": "Tue 07 Dec 2021 01:10",
          "username": "tkanmani76tkanmani76",
          "content": "Option C<br> When should I use Amazon SWF vs. AWS Step Functions?<br><br>AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Instead of writing a Decider program, you define state machines in JSON. AWS customers should consider using Step Functions for new applications. If Step Functions does not fit your needs, then you should consider Amazon Simple Workflow (SWF). Amazon SWF provides you complete control over your orchestration logic, but increases the complexity of developing applications. You may write decider programs in the programming language of your choice, or you may use the Flow framework to use programming constructs that structure asynchronous interactions for you. AWS will continue to provide the Amazon SWF service, Flow framework, and support all Amazon SWF customers.Go for D - Use case #2: Processing large product catalogs using Amazon Mechanical Turk. Under SWF FAQ.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 515495,
          "date": "Mon 03 Jan 2022 09:28",
          "username": "tkanmani76",
          "content": "Go for D - Use case #2: Processing large product catalogs using Amazon Mechanical Turk. Under SWF FAQ.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493886,
          "date": "Sat 04 Dec 2021 19:35",
          "username": "AzureDP900",
          "content": "D is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490371,
          "date": "Tue 30 Nov 2021 04:00",
          "username": "acloudguru",
          "content": "C,typical step function question and easy one,hope i can have it in my exam",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 455886,
          "date": "Sat 06 Nov 2021 11:20",
          "username": "StelSen",
          "content": "I will choose D.  I won't debate SWF/Step. Rather I use another technique to prove D is correct. Management asked SA to design a workflow which can handle multiple concurrent Mech Turk Operations. PLEASE NOTE.  It will be a single workflow with concurrent operations. Ans C says build multiple concurrent workflow, No need. Ans D, says single batch with multiple worker. One more Tips: AWS Mechanical Turk is just a outsourcing service performed by another human via marketplace. Isn't the good reason to use SWF?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 451153,
          "date": "Fri 05 Nov 2021 05:48",
          "username": "andylogan",
          "content": "It's D",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#458",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An organization has two Amazon EC2 instances:<br>✑ The first is running an ordering application and an inventory application.<br>The second is running a queuing system.<br><img src=\"https://www.examtopics.com/https://examtopics.com/assets/media/exam-media/04241/0030200002.png\" class=\"in-exam-image\"><br>During certain times of the year, several thousand orders are placed per second. Some orders were lost when the queuing system was down. Also, the organization's inventory application has the incorrect quantity of products because some orders were processed twice.<br>What should be done to ensure that the applications can handle the increasing number of orders?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#458",
          "answers": [
            {
              "choice": "<p>A. Put the ordering and inventory applications into their own AWS Lambda functions. Have the ordering application write the messages into an Amazon SQS FIFO queue.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Put the ordering and inventory applications into their own Amazon ECS containers, and create an Auto Scaling group for each application. Then, deploy the message queuing server in multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Put the ordering and inventory applications into their own Amazon EC2 instances, and create an Auto Scaling group for each application. Use Amazon SQS standard queues for the incoming orders, and implement idempotency in the inventory application.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Put the ordering and inventory applications into their own Amazon EC2 instances. Write the incoming orders to an Amazon Kinesis data stream. Configure AWS Lambda to poll the stream and update the inventory application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 317590,
          "date": "Sat 02 Oct 2021 23:25",
          "username": "nitinz",
          "content": "Correct answer is C. ",
          "upvote_count": "28",
          "selected_answers": ""
        },
        {
          "id": 341896,
          "date": "Sun 03 Oct 2021 13:56",
          "username": "Juan21hbrandstudent22user0001Eric0909user0001StelSen",
          "content": "Correct answer is A.  With FIFO you avoid the problem of duplicate processing in the queue.Changing to C<br>I originally thought it was A but seeing this <br><br>\\\"FIFO queues are different.The ordering imposes a real throughput limit – currently 300 requests per second per queue...Fortunately, our conversations with customers have told us that FIFO applications are generally lower-throughput—10 messages per second or lower.\\\"<br><br>changes mine to C.  Sure it can go to 3,000/s with batching but this is not mentioned.Along with this they mention in the question that the orders are in several thousands per second. <br><br>https://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/Answer is C <br>SQS + Idempotencythere is no such thing called Idempotency, it made up hereAgree! I was rocketMQ prudcut support before.A is rightTo me A is immediate failure. Moving an application from EC2 to lambda without knowing what was the application stack is not a good choice. Although lambda can run simple PHP web application with the help of API GATEWAY, there is no mention of API GAteway, So A failed in this case.",
          "upvote_count": "15651112",
          "selected_answers": ""
        },
        {
          "id": 421888,
          "date": "Fri 22 Oct 2021 13:10",
          "username": "hbrand",
          "content": "Changing to C<br>I originally thought it was A but seeing this <br><br>\\\"FIFO queues are different.The ordering imposes a real throughput limit – currently 300 requests per second per queue...Fortunately, our conversations with customers have told us that FIFO applications are generally lower-throughput—10 messages per second or lower.\\\"<br><br>changes mine to C.  Sure it can go to 3,000/s with batching but this is not mentioned.Along with this they mention in the question that the orders are in several thousands per second. <br><br>https://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 441145,
          "date": "Wed 27 Oct 2021 03:08",
          "username": "student22user0001Eric0909",
          "content": "Answer is C <br>SQS + Idempotencythere is no such thing called Idempotency, it made up hereAgree! I was rocketMQ prudcut support before.",
          "upvote_count": "511",
          "selected_answers": ""
        },
        {
          "id": 598595,
          "date": "Sun 08 May 2022 17:06",
          "username": "user0001",
          "content": "there is no such thing called Idempotency, it made up here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 641282,
          "date": "Tue 02 Aug 2022 15:25",
          "username": "Eric0909",
          "content": "Agree! I was rocketMQ prudcut support before.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 598596,
          "date": "Sun 08 May 2022 17:07",
          "username": "user0001",
          "content": "A is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 455889,
          "date": "Wed 03 Nov 2021 13:13",
          "username": "StelSen",
          "content": "To me A is immediate failure. Moving an application from EC2 to lambda without knowing what was the application stack is not a good choice. Although lambda can run simple PHP web application with the help of API GATEWAY, there is no mention of API GAteway, So A failed in this case.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 717564,
          "date": "Sun 13 Nov 2022 23:55",
          "username": "mrgreatness",
          "content": "several thousand per second so C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717562,
          "date": "Sun 13 Nov 2022 23:53",
          "username": "mrgreatness",
          "content": "C: FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the 5-minute deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715196,
          "date": "Thu 10 Nov 2022 13:03",
          "username": "Netaji",
          "content": "there is a difference between FIFO and a standard queue of SQS -- https://jayendrapatil.com/aws-sqs-standard-vs-fifo-queue/<br>so the answer should be \\\"A\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709863,
          "date": "Wed 02 Nov 2022 15:30",
          "username": "superuser784",
          "content": "Could be A, but we do not know how long the total process could take (maybe more than 15 min) and the SQS FiFo is not for this use case, the keyword here is \\\"Idempotency\\\" which means you have to build your application making sure you do not process the same message twice. so the correct option is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695101,
          "date": "Sat 15 Oct 2022 03:43",
          "username": "sg0206",
          "content": "A is the correct answer - <br>FIFO Queues<br>High Throughput: By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). When you batch 10 messages per operation (maximum), FIFO queues can support up to 3,000 messages per second. If you require higher throughput, you can enable high throughput mode for FIFO on the Amazon SQS console, which will support up to 30,000 messages per second with batching, or up to 3,000 messages per second without batching.<br>Exactly-Once Processing: A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren't introduced into the queue.<br><br>First-In-First-Out Delivery: The order in which messages are sent and received is strictly preserved (i.e. First-In-First-Out).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685683,
          "date": "Mon 03 Oct 2022 18:48",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 681528,
          "date": "Wed 28 Sep 2022 10:41",
          "username": "li_qiyang",
          "content": "How about This?<br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 670613,
          "date": "Fri 16 Sep 2022 10:15",
          "username": "Dionenonly",
          "content": "Keyword: Process twice<br>SQS prevents duplicate processes<br>Answer: C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 576100,
          "date": "Sun 27 Mar 2022 11:53",
          "username": "Mechanic",
          "content": "Answer is C. <br>Key concepts:<br>1. Hight availability<br>2. Thousands of requests/s<br>3. Duplication in processes<br>EC2 ASG & SQS solve all of that.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 571758,
          "date": "Sun 20 Mar 2022 18:20",
          "username": "bfal",
          "content": "Correct answer is C<br>C addresses the issues highlighted in the question. One of queueing message going down, and losing orders, and the last one is duplicate processor certain orders.<br>Amazon SQS address the issue of messaging<br>Implement idempotency in the application resolves the issue of processing duplicate order - aka At-least-once delivery <br>(Design your applications to be idempotent (they should not be affected adversely when processing the same message more than once).https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html#standard-queues-at-least-once-delivery",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 551665,
          "date": "Sun 20 Feb 2022 10:21",
          "username": "Alexey79",
          "content": "Why D:<br>Can hadle more than 300 messages w/o loosing them.<br>Will treat idempotency problem.<br>It’s AWS sudgested topology.<br><br>Why NOT A:<br>“Put the ordering and inventory applications into their own AWS Lambda” … Lambda doesn’t support hosting application, API Gateway is required for that and it’s not mentioned.<br><br>A is wrong. --SQS FIFO queues can only support 300 send, receive, or delete operations per second. and the question satats many thousand orders /second<br><br>Why NOT C:<br>no one mentioned about \\\"idempotency\\\" term from C - this one requires a control database to check the value for duplicates. I do not see this in C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 547514,
          "date": "Tue 15 Feb 2022 04:54",
          "username": "jyrajan69",
          "content": "All answering C , focused on the high number of orders,but there is an issue with duplication, standard queues cannot handle this. Only FIFO can handle this part, so unless there is an answer to this, the answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 547340,
          "date": "Mon 14 Feb 2022 21:18",
          "username": "robsonchirara",
          "content": "Decoupling systems and growing demand. Definitely SQS and EC2 ASG.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 530561,
          "date": "Sun 23 Jan 2022 14:47",
          "username": "RVivek",
          "content": "Answer C<br>A is wrong.--SQS FIFO Que can handle only 300 messages per second and the question satats many thousand orders /second",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 495787,
          "date": "Tue 07 Dec 2021 09:45",
          "username": "cldy",
          "content": "C.  Put the ordering and inventory applications into their own Amazon EC2 instances, and create an Auto Scaling group for each application. Use Amazon SQS standard queues for the incoming orders, and implement idempotency in the inventory application.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#459",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating its on-premises build artifact server to an AWS solution. The current system consists of an Apache HTTP server that serves artifacts to clients on the local network, restricted by the perimeter firewall. The artifact consumers are largely build automation scripts that download artifacts via anonymous<br>HTTP, which the company will be unable to modify within its migration timetable.<br>The company decides to move the solution to Amazon S3 static website hosting. The artifact consumers will be migrated to Amazon EC2 instances located within both public and private subnets in a virtual private cloud (VPC).<br>Which solution will permit the artifact consumers to download artifacts without modifying the existing automation scripts?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#459",
          "answers": [
            {
              "choice": "<p>A. Create a NAT gateway within a public subnet of the VPC.  Add a default route pointing to the NAT gateway into the route table associated with the subnets containing consumers. Configure the bucket policy to allow the s3:ListBucket and s3:GetObject actions using the condition IpAddress and the condition key aws:SourceIp matching the elastic IP address of the NAT gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition StringEquals and the condition key aws:sourceVpce matching the identification of the VPC endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM role and instance profile for Amazon EC2 and attach it to the instances that consume build artifacts. Configure the bucket policy to allow the s3:ListBucket and s3:GetObjects actions for the principal matching the IAM role created.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition IpAddress and the condition key aws:SourceIp matching the VPC CIDR block.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12721,
          "date": "Sat 25 Sep 2021 01:56",
          "username": "donathonJAWS1600PacoDerek",
          "content": "B<br>A: While this will work, this is still going through public and because the traffic is HTTP, it is not encrypted so this cannot be a good solution.<br>B: This uses privatelink and hence is better since you cannot change the script to download via HTTP.<br>C: The EC2 needs network connectivity to S3 bucket.<br>D: How would this work when the actual access should be from the VPC endpoint and not the EC2 instance itself?Requirement does not ask for secured solution ( https) . It requires HTTP.something more for D: You cannot use an IAM policy or bucket policy to allow access from a VPC IPv4 CIDR range (the private IPv4 address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Therefore, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC endpoint. This applies to IAM policies for users and roles, and any bucket policies. If a statement includes the aws:SourceIp condition, the value fails to match any provided IP address or range.<br>https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
          "upvote_count": "3029",
          "selected_answers": ""
        },
        {
          "id": 94614,
          "date": "Tue 05 Oct 2021 17:47",
          "username": "JAWS1600",
          "content": "Requirement does not ask for secured solution ( https) . It requires HTTP.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 42555,
          "date": "Sat 25 Sep 2021 04:25",
          "username": "PacoDerek",
          "content": "something more for D: You cannot use an IAM policy or bucket policy to allow access from a VPC IPv4 CIDR range (the private IPv4 address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Therefore, you cannot use the aws:SourceIp condition in your IAM policies for requests to Amazon S3 through a VPC endpoint. This applies to IAM policies for users and roles, and any bucket policies. If a statement includes the aws:SourceIp condition, the value fails to match any provided IP address or range.<br>https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 105589,
          "date": "Mon 11 Oct 2021 02:47",
          "username": "meenu2225",
          "content": "Correct option is B<br>Remember the original setup is: Apache HTTP server that serves artifacts to clients on the local network, restricted by the perimeter firewall. Which mean the comms cannot be on internet it has to be either in private subnet or via endpoint service. Which means A & C are out of euqation because in both the traffic is via internet. Leaving only B and D. <br>Out of these B makes more sense.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 713164,
          "date": "Mon 07 Nov 2022 17:39",
          "username": "alnadan",
          "content": "B is the correct ans. <br>https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 698846,
          "date": "Wed 19 Oct 2022 10:56",
          "username": "bandaot",
          "content": "Why so many people select B, based on this https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3<br>AWS PrivateLink doesn't support webside endpoints and user must change to use the endpoint-specific DNS names. Even thought A also has the problem for the EC2 in public subnet and maybe not that secure to go thought the public internet, but it's still the only possible solution.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 685685,
          "date": "Mon 03 Oct 2022 18:53",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 577523,
          "date": "Tue 29 Mar 2022 12:58",
          "username": "jj22222",
          "content": "B.  Create a VPC endpoint and add it to the route table associated with subnets containing consumers. Configure the bucket policy to allow s3:ListBucket and s3:GetObject actions using the condition StringEquals and the condition key aws:sourceVpce matching the identification of the VPC endpoint.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 569773,
          "date": "Thu 17 Mar 2022 15:15",
          "username": "ozan11",
          "content": "it's c.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 535110,
          "date": "Sat 29 Jan 2022 03:52",
          "username": "frankzeng",
          "content": "A.  Need to use HTTP. The public subnet can go through NAT gateway",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 530550,
          "date": "Sun 23 Jan 2022 14:22",
          "username": "lulz111",
          "content": "Maybe im misreading it, but this feels like another badly written question to me. The consumers currently make HTTP calls to get the artifacts, and we are asking to not change that. Yet there is no mention of anything other than using S3, which by default doesnt support HTTP. I would expect the real answer to make reference to static website hosting in S3 tbh.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493895,
          "date": "Sat 04 Dec 2021 19:48",
          "username": "AzureDP900",
          "content": "B is right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 451158,
          "date": "Sun 07 Nov 2021 14:02",
          "username": "andylogan",
          "content": "It's B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413091,
          "date": "Fri 05 Nov 2021 10:37",
          "username": "DerekKey",
          "content": "A wrong - \\\"aws:SourceIp matching the elastic IP address of the NAT gateway\\\" will not serve instances in public subnets<br>B OK - aws:sourceVpce <br>C wrong - no access to S3 from private subnets<br>D wrong - with Vpce instead of aws:SourceIp you have to use aws:VpcSourceIp",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 409705,
          "date": "Wed 03 Nov 2021 19:53",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 344490,
          "date": "Sat 30 Oct 2021 06:52",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 290315,
          "date": "Mon 25 Oct 2021 08:42",
          "username": "Kian1",
          "content": "will go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282655,
          "date": "Sat 23 Oct 2021 04:03",
          "username": "Ebi",
          "content": "Answer is B, you don't need to update scripts:<br><br>\\\"If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.\\\"",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 276642,
          "date": "Fri 22 Oct 2021 09:12",
          "username": "rcher",
          "content": "B. <br><br>Why not D<br><br>If the request comes from a host that uses an Amazon VPC endpoint, then the aws:SourceIp key is not available. You should instead use a VPC-specific key such as aws:VpcSourceIp.<br><br>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-sourceip",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#460",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A group of research institutions and hospitals are in a partnership to study 2 PBs of genomic data. The institute that owns the data stores it in an Amazon S3 bucket and updates it regularly. The institute would like to give all of the organizations in the partnership read access to the data. All members of the partnership are extremely cost-conscious, and the institute that owns the account with the S3 bucket is concerned about covering the costs for requests and data transfers from Amazon S3.<br>Which solution allows for secure datasharing without causing the institute that owns the bucket to assume all the costs for S3 requests and data transfers?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#460",
          "answers": [
            {
              "choice": "<p>A. Ensure that all organizations in the partnership have AWS accounts. In the account with the S3 bucket, create a cross-account role for each account in the partnership that allows read access to the data. Have the organizations assume and use that read role when accessing the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Ensure that all organizations in the partnership have AWS accounts. Create a bucket policy on the bucket that owns the data. The policy should allow the accounts in the partnership read access to the bucket. Enable Requester Pays on the bucket. Have the organizations use their AWS credentials when accessing the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Ensure that all organizations in the partnership have AWS accounts. Configure buckets in each of the accounts with a bucket policy that allows the institute that owns the data the ability to write to the bucket. Periodically sync the data from the institute's account to the other organizations. Have the organizations use their AWS credentials when accessing the data using their accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Ensure that all organizations in the partnership have AWS accounts. In the account with the S3 bucket, create a cross-account role for each account in the partnership that allows read access to the data. Enable Requester Pays on the bucket. Have the organizations assume and use that read role when accessing the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12725,
          "date": "Thu 23 Sep 2021 01:19",
          "username": "donathonAJ41185",
          "content": "B<br>In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket. A bucket owner, however, can configure a bucket to be a Requester Pays bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data. If you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed.<br>A\\D: When the requester assumes an AWS Identity and Access Management (IAM) role prior to making their request, the account to which the role belongs is charged for the request.<br>C: This would incur additional cost of storing the data.Agree with the explaination",
          "upvote_count": "512",
          "selected_answers": ""
        },
        {
          "id": 282067,
          "date": "Wed 27 Oct 2021 19:19",
          "username": "AJ41185",
          "content": "Agree with the explaination",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 80943,
          "date": "Tue 28 Sep 2021 00:16",
          "username": "fw",
          "content": "B. <br>D doesn't work as if another account use cross-account role created under the bucket owner account, the bucket owner account is charged for the request.",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 685689,
          "date": "Mon 03 Oct 2022 18:56",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 669059,
          "date": "Wed 14 Sep 2022 15:37",
          "username": "bihani",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 495961,
          "date": "Tue 07 Dec 2021 13:24",
          "username": "cldy",
          "content": "B.  Ensure that all organizations in the partnership have AWS accounts. Create a bucket policy on the bucket that owns the data. The policy should allow the accounts in the partnership read access to the bucket. Enable Requester Pays on the bucket. Have the organizations use their AWS credentials when accessing the data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493896,
          "date": "Sat 04 Dec 2021 19:53",
          "username": "AzureDP900",
          "content": "requester pay the price , my answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483084,
          "date": "Sun 21 Nov 2021 08:39",
          "username": "acloudguru",
          "content": "B. <br>To enable Requester Pays for an S3 bucket",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 462141,
          "date": "Sun 07 Nov 2021 01:56",
          "username": "seyik",
          "content": "B. <br>To enable Requester Pays for an S3 bucket<br><br>Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.<br><br>In the Buckets list, choose the name of the bucket that you want to enable Requester Pays for.<br><br>Choose Properties.<br><br>Under Requester pays, choose Edit.<br><br>Choose Enable, and choose Save changes.<br><br>Amazon S3 enables Requester Pays for your bucket and displays your Bucket overview. Under Requester pays, you see Enabled.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysExamples.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 451160,
          "date": "Sat 06 Nov 2021 09:00",
          "username": "andylogan",
          "content": "It's B - If requester pays is enabled then the request has to be authenticated and cannot assume a role to access the S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409706,
          "date": "Fri 05 Nov 2021 09:23",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406693,
          "date": "Thu 04 Nov 2021 09:39",
          "username": "Akhil254",
          "content": "B correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 378137,
          "date": "Wed 03 Nov 2021 05:02",
          "username": "Amitv2706",
          "content": "Answer is B. <br><br>D -Cant be answer as the role owning account will have to pay for the requests which will defeat the purpose of transferring data transfer cost to requester account.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344493,
          "date": "Sun 31 Oct 2021 23:02",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 312155,
          "date": "Sat 30 Oct 2021 04:18",
          "username": "Pupu86",
          "content": "Answer is B.  Option D defeats the purpose of creating individual AWS accounts if there is no policy to mandates its use.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290321,
          "date": "Thu 28 Oct 2021 03:44",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282653,
          "date": "Thu 28 Oct 2021 03:06",
          "username": "Ebi",
          "content": "B is my choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 268010,
          "date": "Wed 27 Oct 2021 03:19",
          "username": "sanjaym",
          "content": "B for sure.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#461",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company currently uses a single 1 Gbps AWS Direct Connect connection to establish connectivity between an AWS Region and its data center. The company has five Amazon VPCs, all of which are connected to the data center using the same Direct Connect connection. The Network team is worried about the single point of failure and is interested in improving the redundancy of the connections to AWS while keeping costs to a minimum.<br>Which solution would improve the redundancy of the connection to AWS while meeting the cost requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#461",
          "answers": [
            {
              "choice": "<p>A. Provision another 1 Gbps Direct Connect connection and create new VIFs to each of the VPCs. Configure the VIFs in a load balancing fashion using BGP.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up VPN tunnels from the data center to each VPC.  Terminate each VPN tunnel at the virtual private gateway (VGW) of the respective VPC and set up BGP for route management.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up a new point-to-point Multiprotocol Label Switching (MPLS) connection to the AWS Region that's being used. Configure BGP to use this new circuit as passive, so that no traffic flows through this unless the AWS Direct Connect fails.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a public VIF on the Direct Connect connection and set up a VPN tunnel which will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF.  Use BGP to handle the failover to the VPN connection.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12726,
          "date": "Tue 28 Sep 2021 13:07",
          "username": "donathon",
          "content": "B<br>A: This is too costly.<br>C: How will this help when direct connect is the issue?<br>D; There is still a single point of failure and Direct Connect cannot be set to public. It is not publicily accessible.",
          "upvote_count": "28",
          "selected_answers": ""
        },
        {
          "id": 13896,
          "date": "Tue 28 Sep 2021 16:16",
          "username": "Moon",
          "content": "I agree with \\\"B\\\".<br>A: is costly.<br>C: MPLS can not be used without dedicated link. Also, AWS does not support it.<br>D: using Public VIF over the same direct connect will be helpful, as it is not adding extra physical redundancy. Also, Public VIFs are not used to connect on-prim to VPCs. it is used to connect on-prim to AWS public services like S3, DynamoDB. ..etc.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 685694,
          "date": "Mon 03 Oct 2022 19:04",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 652167,
          "date": "Fri 26 Aug 2022 11:40",
          "username": "Sumit_Kumar",
          "content": "D: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498325,
          "date": "Fri 10 Dec 2021 06:36",
          "username": "cldy",
          "content": "B.  Set up VPN tunnels from the data center to each VPC.  Terminate each VPN tunnel at the virtual private gateway (VGW) of the respective VPC and set up BGP for route management.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493898,
          "date": "Sat 04 Dec 2021 19:56",
          "username": "AzureDP900",
          "content": "B is right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447665,
          "date": "Sat 06 Nov 2021 03:57",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 445657,
          "date": "Fri 05 Nov 2021 01:03",
          "username": "student22",
          "content": "B<br>redundency + minimum cost",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409722,
          "date": "Wed 03 Nov 2021 15:41",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 344583,
          "date": "Mon 01 Nov 2021 03:34",
          "username": "Waiweng",
          "content": "B is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 312162,
          "date": "Sat 30 Oct 2021 11:45",
          "username": "Pupu86",
          "content": "Option A - too costly to subscribe to another 1Gbps DX link<br>Option C - doesn't make sense<br>Option D - Public VIF still make use of the same DX link but only for public AWS resource connections such as S3 DynamoDB etc..<br><br>So Answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282643,
          "date": "Sat 30 Oct 2021 06:33",
          "username": "Ebi",
          "content": "B is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 268014,
          "date": "Thu 28 Oct 2021 03:23",
          "username": "sanjaym",
          "content": "I'll with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 244894,
          "date": "Sun 24 Oct 2021 06:10",
          "username": "PAUGURUAquavk",
          "content": "D as stated here: https://docs.aws.amazon.com/directconnect/latest/UserGuide/remote_regions.html<br>\\\"You can create a Direct Connect gateway in any public Region. Use it to connect your AWS Direct Connect connection over a private virtual interface to VPCs in your account that are located in different Regions or to a transit gateway.<br>Alternatively, you can create a public virtual interface for your AWS Direct Connect connection and then establish a VPN connection to your VPC in the remote Region. \\\"in case ofDirect connect would still remain a single point of failure. B is correct",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 252491,
          "date": "Tue 26 Oct 2021 02:20",
          "username": "Aquavk",
          "content": "in case ofDirect connect would still remain a single point of failure. B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242498,
          "date": "Sat 23 Oct 2021 07:34",
          "username": "T14102020",
          "content": "Correct answer is B.  Not D because Public VIF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230035,
          "date": "Thu 21 Oct 2021 03:12",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 229910,
          "date": "Tue 12 Oct 2021 23:37",
          "username": "YouYouYoumikhailwang",
          "content": "the answer goes between A and B other answers are wrong <br>additional 1 GB per month is not too coasty around 216 $ while 5 vpns are 180 $ so b is more cost effective checked.5 vpns are 400$, additional 1G direct connect is around 1200.<br>https://aws.amazon.com/vpn/pricing/<br>https://aws.amazon.com/directconnect/pricing/#:~:text=AWS%20Direct%20Connect%20data%20transfer,per%20GB%20in%20all%20locations.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 547843,
          "date": "Tue 15 Feb 2022 16:38",
          "username": "mikhailwang",
          "content": "5 vpns are 400$, additional 1G direct connect is around 1200.<br>https://aws.amazon.com/vpn/pricing/<br>https://aws.amazon.com/directconnect/pricing/#:~:text=AWS%20Direct%20Connect%20data%20transfer,per%20GB%20in%20all%20locations.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#462",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company currently uses Amazon EBS and Amazon RDS for storage purposes. The company intends to use a pilot light approach for disaster recovery in a different AWS Region. The company has an RTO of 6 hours and an RPO of 24 hours.<br>Which solution would achieve the requirements with MINIMAL cost?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#462",
          "answers": [
            {
              "choice": "<p>A. Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-active failover configuration. Use Amazon EC2 in an Auto Scaling group configured in the same way as in the primary region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon ECS to handle long-running tasks to create daily EBS and RDS snapshots, and copy to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use EBS and RDS cross-region snapshot copy capability to create snapshots in the disaster recovery region. Use Amazon Route 53 with active-active failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12727,
          "date": "Tue 21 Sep 2021 02:52",
          "username": "donathonPacoDerekeasytooeasytooIbranthovicrb39TiredDadamithbti416shammous",
          "content": "A<br>B\\D: Too costly and not pilot light.<br>C: ECS should not be used.D<br>sorry, this time may be u and Moon were wrong.<br>EBS and RDS both support CRR.<br>https://amazonaws-china.com/about-aws/whats-new/2013/06/11/amazon-announces-faster-cross-region-ebs-snapshot-copy/<br>BTW,no matter how, using Lambda or ECS , bring extra fee, and there is no way to copy snapshot to another region since Lambda&ECS can not span across regionHi PacoDerek you can use Lambda to copy snapshots across regions. <br>https://stackoverflow.com/questions/41726536/aws-lambda-copy-ec2-snapshot-automatically-between-regionshttps://stackoverflow.com/questions/58922949/aws-lambda-copy-monthly-snapshots-to-another-regionSo you are prefering using Active-actice to reduce cost ?<br>A is the right answerD is wrong - it implies active-active so both environments are working, and pilot light means active-passiveDepending on the Amazon Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete. https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_CopySnapshot.htmlhttps://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/Why ECS should not be used? ECS would be cheaper as there is a requirement for cost effectiveness. \\\"AWS Lambda is optimized for simple and quick functions to execute. Larger and more complex functions create execution complexity (and significant execution cost) to the user. Amazon ECS, on the other hand, can be used with any reasonable size and complexity container.\\\" So ECS can better handle \\\"long-running job\\\" like taking RDS and EBS snapshots. So C is a more suitable answer.",
          "upvote_count": "2273221111",
          "selected_answers": ""
        },
        {
          "id": 42564,
          "date": "Thu 23 Sep 2021 04:32",
          "username": "PacoDerekeasytooeasytooIbranthovicrb39",
          "content": "D<br>sorry, this time may be u and Moon were wrong.<br>EBS and RDS both support CRR.<br>https://amazonaws-china.com/about-aws/whats-new/2013/06/11/amazon-announces-faster-cross-region-ebs-snapshot-copy/<br>BTW,no matter how, using Lambda or ECS , bring extra fee, and there is no way to copy snapshot to another region since Lambda&ECS can not span across regionHi PacoDerek you can use Lambda to copy snapshots across regions. <br>https://stackoverflow.com/questions/41726536/aws-lambda-copy-ec2-snapshot-automatically-between-regionshttps://stackoverflow.com/questions/58922949/aws-lambda-copy-monthly-snapshots-to-another-regionSo you are prefering using Active-actice to reduce cost ?<br>A is the right answerD is wrong - it implies active-active so both environments are working, and pilot light means active-passive",
          "upvote_count": "73221",
          "selected_answers": ""
        },
        {
          "id": 88550,
          "date": "Mon 27 Sep 2021 11:30",
          "username": "easytooeasytoo",
          "content": "Hi PacoDerek you can use Lambda to copy snapshots across regions. <br>https://stackoverflow.com/questions/41726536/aws-lambda-copy-ec2-snapshot-automatically-between-regionshttps://stackoverflow.com/questions/58922949/aws-lambda-copy-monthly-snapshots-to-another-region",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 88553,
          "date": "Mon 27 Sep 2021 12:50",
          "username": "easytoo",
          "content": "https://stackoverflow.com/questions/58922949/aws-lambda-copy-monthly-snapshots-to-another-region",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 94148,
          "date": "Wed 29 Sep 2021 12:54",
          "username": "Ibranthovic",
          "content": "So you are prefering using Active-actice to reduce cost ?<br>A is the right answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 501398,
          "date": "Tue 14 Dec 2021 14:58",
          "username": "rb39",
          "content": "D is wrong - it implies active-active so both environments are working, and pilot light means active-passive",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413822,
          "date": "Sat 06 Nov 2021 18:35",
          "username": "TiredDad",
          "content": "Depending on the Amazon Regions involved and the amount of data to be copied, a cross-Region snapshot copy can take hours to complete. https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 383189,
          "date": "Tue 02 Nov 2021 05:16",
          "username": "amithbti416",
          "content": "https://aws.amazon.com/blogs/database/cross-region-automatic-disaster-recovery-on-amazon-rds-for-oracle-database-using-db-snapshots-and-aws-lambda/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 279806,
          "date": "Mon 25 Oct 2021 09:17",
          "username": "shammous",
          "content": "Why ECS should not be used? ECS would be cheaper as there is a requirement for cost effectiveness. \\\"AWS Lambda is optimized for simple and quick functions to execute. Larger and more complex functions create execution complexity (and significant execution cost) to the user. Amazon ECS, on the other hand, can be used with any reasonable size and complexity container.\\\" So ECS can better handle \\\"long-running job\\\" like taking RDS and EBS snapshots. So C is a more suitable answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13895,
          "date": "Tue 21 Sep 2021 10:23",
          "username": "Moonvirtualb3llman",
          "content": "I would go with \\\"A\\\".<br>B: it is not pilot, as it has working nodes in DR region.<br>C: comparing Lambda to ECS snapshot job, it is better to use Lambda.<br>D: EBS does not have auto regional replication!!Agree with youEBS does have auto regional replication. It can be defined under Snapshot Lifecycle Policy.",
          "upvote_count": "1113",
          "selected_answers": ""
        },
        {
          "id": 61119,
          "date": "Fri 24 Sep 2021 05:26",
          "username": "virtual",
          "content": "Agree with you",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 183504,
          "date": "Fri 15 Oct 2021 03:51",
          "username": "b3llman",
          "content": "EBS does have auto regional replication. It can be defined under Snapshot Lifecycle Policy.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 717518,
          "date": "Sun 13 Nov 2022 21:52",
          "username": "LrdKanien",
          "content": "A is best answer due to pilot and cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716809,
          "date": "Sat 12 Nov 2022 17:44",
          "username": "Lorrendo",
          "content": "It's actually D.  R53 Active-Active with 0 instances on the DR region will result in a pilot-light configuration. A B and C have \\\"daily\\\" snapshot copy that won't meet the RPO/RTO requirements. D does not specify how often the snapshots are created/copied on the DR region, and the minimum interval is 1 hour, which will meet RPO and RTO requirements.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 711155,
          "date": "Fri 04 Nov 2022 14:07",
          "username": "resnefresnef",
          "content": "trick of the question is \\\"pilot light\\\", which makes A better than Danswer is A",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 711157,
          "date": "Fri 04 Nov 2022 14:09",
          "username": "resnef",
          "content": "answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 694466,
          "date": "Fri 14 Oct 2022 05:28",
          "username": "SVJS",
          "content": "A. <br>A is correct , we need active-passive failover configuration in Route 53.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685696,
          "date": "Mon 03 Oct 2022 19:09",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 665527,
          "date": "Sat 10 Sep 2022 18:25",
          "username": "engmohhamed",
          "content": "i thinkit is easy to implementEBS cross-region rather than lambda, actually today i use AWS backup service<br><br>but \\\"'D\\\" answer has active-active which is not the pilot light strategy as active-active is the strategy of multi-site <br><br>https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/<br><br>So, Answer A is best choice in this case",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 659174,
          "date": "Sun 04 Sep 2022 12:39",
          "username": "pixepe",
          "content": "As it's pilot light approach on backup region, anything which has active-active doesn't make sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 636718,
          "date": "Mon 25 Jul 2022 15:04",
          "username": "Enigmaaaaaa",
          "content": "Only A is correct.<br>How D can be correct? its active-active are you going to shift traffic to an instance which does not exists? also in the question states \\\"pilot light\\\" this is clearly a active-passive deployment. another point is that you cannot just cross-region copy snapshots by itself you need to have a lambda or a script that will do it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 636028,
          "date": "Sun 24 Jul 2022 13:54",
          "username": "KiraguJohn",
          "content": "Why i think A is wrong...The company's RTO is six hours and its RPO is twenty-four hours.<br>A talks about a daily backup which means once after 24 hrs. What if the system fails at 23rd hour. Will you meet RTO of 6 hours? I prefer D although its Active-Active the autoscaling is set to 0.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 610476,
          "date": "Thu 02 Jun 2022 09:24",
          "username": "bobsmith2000",
          "content": "D is perfect, if it's not \\\"active-active failover\\\".<br>So it's A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 503821,
          "date": "Fri 17 Dec 2021 18:39",
          "username": "vbal",
          "content": "EBS & EFS don't have cross-region replication in-built; you can use AWS Backup for that.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 497324,
          "date": "Thu 09 Dec 2021 05:43",
          "username": "CloudChef",
          "content": "A is correct. Pilot light.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496528,
          "date": "Wed 08 Dec 2021 05:38",
          "username": "cldy",
          "content": "A.  Use AWS Lambda to create daily EBS and RDS snapshots, and copy them to the disaster recovery region. Use Amazon Route 53 with active-passive failover configuration. Use Amazon EC2 in an Auto Scaling group with the capacity set to 0 in the disaster recovery region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483206,
          "date": "Sun 21 Nov 2021 12:37",
          "username": "acloudguru",
          "content": "https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-iii-pilot-light-and-warm-standby/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 434166,
          "date": "Sun 07 Nov 2021 04:34",
          "username": "student22student22",
          "content": "D. <br>It's still pilot light because ASG is 0.Changing to A because lambda is a better solution to copy EBS snapshots.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 455128,
          "date": "Sun 07 Nov 2021 09:54",
          "username": "student22",
          "content": "Changing to A because lambda is a better solution to copy EBS snapshots.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#463",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to cost-effectively persist small data records (up to 1 KiB) for up to 30 days. The data is read rarely. When reading the data, a 5-minute delay is acceptable.<br>Which of the following solutions achieve this goal? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#463",
          "answers": [
            {
              "choice": "<p>A. Use Amazon S3 to collect multiple records in one S3 object. Use a lifecycle configuration to move data to Amazon Glacier immediately after write. Use expedited retrievals when reading the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Write the records to Amazon Kinesis Data Firehose and configure Kinesis Data Firehose to deliver the data to Amazon S3 after 5 minutes. Set an expiration action at 30 days on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an AWS Lambda function invoked via Amazon API Gateway to collect data for 5 minutes. Write data to Amazon S3 just before the Lambda execution stops.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Write the records to Amazon DynamoDB configured with a Time To Live (TTL) of 30 days. Read data using the GetItem or BatchGetItem call.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Write the records to an Amazon ElastiCache for Redis. Configure the Redis append-only file (AOF) persistence logs to write to Amazon S3. Recover from the log if the ElastiCache instance has failed.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12729,
          "date": "Tue 21 Sep 2021 16:37",
          "username": "donathongookseangkirrim",
          "content": "BD<br>A: After 30 days the data should be deleted instead of storing it.<br>B: When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired.<br>C: Does not address the 30 days deletion.<br>D: https://aws.amazon.com/blogs/aws/new-manage-dynamodb-items-using-time-to-live-ttl/<br>E: This is for cache and not suitable for this use case.<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.htmlyou talk too muchA: storing data for 90 days in Glacier still costs half as much as storing in S3 Standard for 30 days, this is the least expensive option<br>B: valid, although it would cost twice as much as A if you used S3 Stanadard<br>C: agree on no lifecycle out of the data, also Lambda execution time charges for 5 mins would be very high<br>D: is technically valid but storage cost alone for DynamoDB would be 10x option B and 20x option A<br>E: sizing the memory cache large enough to not overwrite data for 30 days would make this the most expensive option of all",
          "upvote_count": "3261",
          "selected_answers": ""
        },
        {
          "id": 275107,
          "date": "Tue 26 Oct 2021 14:20",
          "username": "gookseang",
          "content": "you talk too much",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 457243,
          "date": "Sat 06 Nov 2021 09:08",
          "username": "kirrim",
          "content": "A: storing data for 90 days in Glacier still costs half as much as storing in S3 Standard for 30 days, this is the least expensive option<br>B: valid, although it would cost twice as much as A if you used S3 Stanadard<br>C: agree on no lifecycle out of the data, also Lambda execution time charges for 5 mins would be very high<br>D: is technically valid but storage cost alone for DynamoDB would be 10x option B and 20x option A<br>E: sizing the memory cache large enough to not overwrite data for 30 days would make this the most expensive option of all",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 11093,
          "date": "Mon 20 Sep 2021 01:04",
          "username": "awsec2dpvnme",
          "content": "b,dmostlybd here",
          "upvote_count": "92",
          "selected_answers": ""
        },
        {
          "id": 11701,
          "date": "Tue 21 Sep 2021 10:47",
          "username": "dpvnme",
          "content": "bd here",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 694471,
          "date": "Fri 14 Oct 2022 05:37",
          "username": "SVJS",
          "content": "B&D. <br>Key is to delete the data after 30 days. There should not be any cost for the data after 30 days.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685697,
          "date": "Mon 03 Oct 2022 19:11",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 655221,
          "date": "Wed 31 Aug 2022 17:21",
          "username": "Sizuma",
          "content": "AB CORRECT",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 641541,
          "date": "Wed 03 Aug 2022 05:05",
          "username": "MarkChoi",
          "content": "A is cheaper than B. <br>S3 cost is 0.023$ / GB, S3Glacier cost is 0.004$ / GB. <br>So,even if S3Glacier kept paying 90 days, they pay 0.012$ /GB.  So they pay half of S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 641536,
          "date": "Wed 03 Aug 2022 04:49",
          "username": "MarkChoi",
          "content": "\\\"after 5 minutes\\\"?????<br>what a dull action~~<br>why are they doing that????",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 607430,
          "date": "Thu 26 May 2022 02:11",
          "username": "necsk",
          "content": "will go with ab, accessed seldom,so why you store them in dynamo or redis?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496021,
          "date": "Tue 07 Dec 2021 14:04",
          "username": "cldy",
          "content": "B.  Write the records to Amazon Kinesis Data Firehose and configure Kinesis Data Firehose to deliver the data to Amazon S3 after 5 minutes. Set an expiration action at 30 days on the S3 bucket.<br>D.  Write the records to Amazon DynamoDB configured with a Time To Live (TTL) of 30 days. Read data using the GetItem or BatchGetItem call.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493924,
          "date": "Sat 04 Dec 2021 20:49",
          "username": "AzureDP900",
          "content": "I will go with BD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483123,
          "date": "Sun 21 Nov 2021 09:45",
          "username": "acloudguru",
          "content": "A: After 30 days the data should be deleted instead of storing it.<br>B: When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a delay between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired.<br>C: Does not address the 30 days deletion.<br>D: https://aws.amazon.com/blogs/aws/new-manage-dynamodb-items-using-time-to-live-ttl/<br>E: This is for cache and not suitable for this use case.<br>https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-expire-general-considerations.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 414193,
          "date": "Fri 05 Nov 2021 05:27",
          "username": "bad_syntaxRVivek",
          "content": "A & D - Reluctantly<br>The question says the data should persist for up-to 30 days. Removal of the data there-after is not a requirement, only an assumption.<br>A vs B<br>A: Terrible option. But it meets the criteria technically. Even if the data is stored for 90 days,or forever for that matter, and even when there is a cost to retrieve.<br>B: Incorrect. Question says a 5 minute delay is acceptable. However answer B wants to use Kinesis Data Firehose to write to S3 after 5 minutes. Where is the data supposed to be read from? Kinesis Data Firehose or S3? Kinesis Data Firehose only has data younger than 5min, and reading from S3, data younger than 5min is not available for at least 5min.Data will be read from S3. New data will be available only after 5 minutes.5 minute delay is acceptable",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 539275,
          "date": "Thu 03 Feb 2022 01:40",
          "username": "RVivek",
          "content": "Data will be read from S3. New data will be available only after 5 minutes.5 minute delay is acceptable",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409733,
          "date": "Thu 04 Nov 2021 11:11",
          "username": "WhyIronMan",
          "content": "I'll go with B,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345024,
          "date": "Tue 02 Nov 2021 07:22",
          "username": "Waiweng",
          "content": "B,D mostly",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 290676,
          "date": "Fri 29 Oct 2021 19:05",
          "username": "Kian1",
          "content": "will go with BD",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 282432,
          "date": "Wed 27 Oct 2021 00:57",
          "username": "Ebi",
          "content": "Answer is BD",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 268022,
          "date": "Tue 26 Oct 2021 13:59",
          "username": "sanjaym",
          "content": "I'll go with BD. ",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#464",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Development team is deploying new APIs as serverless applications within a company. The team is currently using the AWS Management Console to provision<br>Amazon API Gateway, AWS Lambda, and Amazon DynamoDB resources. A Solutions Architect has been tasked with automating the future deployments of these serverless APIs.<br>How can this be accomplished?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#464",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudFormation with a Lambda-backed custom resource to provision API Gateway. Use the AWS::DynamoDB::Table and AWS::Lambda::Function resources to create the Amazon DynamoDB table and Lambda functions. Write a script to automate the deployment of the CloudFormation template.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use the AWS Serverless Application Model to define the resources. Upload a YAML template and application files to the code repository. Use AWS CodePipeline to connect to the code repository and to create an action to build using AWS CodeBuild. Use the AWS CloudFormation deployment provider in CodePipeline to deploy the solution.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS CloudFormation to define the serverless application. Implement versioning on the Lambda functions and create aliases to point to the versions. When deploying, configure weights to implement shifting traffic to the newest version, and gradually update the weights as traffic moves over.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Commit the application code to the AWS CodeCommit code repository. Use AWS CodePipeline and connect to the CodeCommit code repository. Use AWS CodeBuild to build and deploy the Lambda functions using AWS CodeDeploy. Specify the deployment preference type in CodeDeploy to gradually shift traffic over to the new version.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12732,
          "date": "Mon 20 Sep 2021 20:16",
          "username": "donathonSD13zolthar_z",
          "content": "B<br>https://aws-quickstart.s3.amazonaws.com/quickstart-trek10-serverless-enterprise-cicd/doc/serverless-cicd-for-the-enterprise-on-the-aws-cloud.pdf <br>https://aws.amazon.com/quickstart/architecture/serverless-cicd-for-enterprise/Option B don't provide a gradual deployment option. Correct Answer is Dthat is wrong, https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "2622",
          "selected_answers": ""
        },
        {
          "id": 330640,
          "date": "Thu 21 Oct 2021 07:05",
          "username": "SD13zolthar_z",
          "content": "Option B don't provide a gradual deployment option. Correct Answer is Dthat is wrong, https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 368799,
          "date": "Thu 04 Nov 2021 15:28",
          "username": "zolthar_z",
          "content": "that is wrong, https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 12070,
          "date": "Mon 20 Sep 2021 04:09",
          "username": "huhupaiarunkumarVrushaliD01037chaudhsarah_t",
          "content": "I would go for D, SAM only works in CLI and the team is currently using the AWS Management Console to provision Amazon API Gateway, AWS Lambda, and Amazon DynamoDB resources.Codedeploy will not deploy Lambda functions. So the answer is B. check this out - https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/<br>I will go for DActually it's still sam.<br>CodeDeploy only controls the traffic.Because team is using Console --> need to improve for future automation deployment and not using Console any more. B is my choice.There is no requirement to use the console in the future.",
          "upvote_count": "6421102",
          "selected_answers": ""
        },
        {
          "id": 34904,
          "date": "Mon 27 Sep 2021 10:06",
          "username": "arunkumarVrushaliD01037",
          "content": "Codedeploy will not deploy Lambda functions. So the answer is B. check this out - https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/<br>I will go for DActually it's still sam.<br>CodeDeploy only controls the traffic.",
          "upvote_count": "421",
          "selected_answers": ""
        },
        {
          "id": 93828,
          "date": "Wed 29 Sep 2021 18:48",
          "username": "VrushaliD01037",
          "content": "check this out - https://aws.amazon.com/blogs/compute/implementing-safe-aws-lambda-deployments-with-aws-codedeploy/<br>I will go for DActually it's still sam.<br>CodeDeploy only controls the traffic.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 342534,
          "date": "Sat 30 Oct 2021 00:29",
          "username": "01037",
          "content": "Actually it's still sam.<br>CodeDeploy only controls the traffic.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 16185,
          "date": "Sun 26 Sep 2021 03:13",
          "username": "chaudh",
          "content": "Because team is using Console --> need to improve for future automation deployment and not using Console any more. B is my choice.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 333329,
          "date": "Mon 25 Oct 2021 02:35",
          "username": "sarah_t",
          "content": "There is no requirement to use the console in the future.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 685700,
          "date": "Mon 03 Oct 2022 19:13",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 658026,
          "date": "Sat 03 Sep 2022 05:22",
          "username": "AYANtheGLADIATOR",
          "content": "B is the answer here is the link to end the discussion.<br><br>https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 544965,
          "date": "Fri 11 Feb 2022 03:15",
          "username": "peddyua",
          "content": "B.  SAM for serveles is recommended by AWS <br>https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 527663,
          "date": "Wed 19 Jan 2022 16:28",
          "username": "tkanmani76",
          "content": "B - If you want to deploy your AWS SAM application gradually rather than all at once, you can specify deployment configurations that AWS CodeDeploy provides.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493926,
          "date": "Sat 04 Dec 2021 20:54",
          "username": "AzureDP900",
          "content": "B is right!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 448753,
          "date": "Fri 05 Nov 2021 05:33",
          "username": "Kopa",
          "content": "going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409735,
          "date": "Fri 05 Nov 2021 00:07",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 345028,
          "date": "Tue 02 Nov 2021 21:03",
          "username": "Waiweng",
          "content": "It's definitely B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 342536,
          "date": "Mon 01 Nov 2021 08:41",
          "username": "01037MrCarter",
          "content": "B. <br>D (not correct), CodeDeploy can NOT be used to deploy lambdaYour statement is just wrong. CodeDeploy can be used to deploy lambda. But the answer is still B",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 394910,
          "date": "Thu 04 Nov 2021 22:58",
          "username": "MrCarter",
          "content": "Your statement is just wrong. CodeDeploy can be used to deploy lambda. But the answer is still B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293693,
          "date": "Tue 19 Oct 2021 09:32",
          "username": "kiev",
          "content": "My answer is B.  Serverless application points to Sam.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290677,
          "date": "Sun 17 Oct 2021 00:37",
          "username": "Kian1",
          "content": "will go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282441,
          "date": "Tue 12 Oct 2021 22:01",
          "username": "Ebi",
          "content": "I go with B too",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 268029,
          "date": "Tue 12 Oct 2021 19:48",
          "username": "sanjaym",
          "content": "B for sure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242523,
          "date": "Mon 11 Oct 2021 15:17",
          "username": "T14102020",
          "content": "Answer is B.  SAM",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230043,
          "date": "Tue 05 Oct 2021 17:36",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#465",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>The company Security team requires that all data uploaded into an Amazon S3 bucket must be encrypted. The encryption keys must be highly available and the company must be able to control access on a per-user basis, with different users having access to different encryption keys.<br>Which of the following architectures will meet these requirements? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#465",
          "answers": [
            {
              "choice": "<p>A. Use Amazon S3 server-side encryption with Amazon S3-managed keys. Allow Amazon S3 to generate an AWS/S3 master key, and use IAM to control access to the data keys that are generated.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon S3 server-side encryption with AWS KMS-managed keys, create multiple customer master keys, and use key policies to control access to them.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon S3 server-side encryption with customer-managed keys, and use AWS CloudHSM to manage the keys. Use CloudHSM client software to control access to the keys that are generated.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon S3 server-side encryption with customer-managed keys, and use two AWS CloudHSM instances configured in high-availability mode to manage the keys. Use the CloudHSM client software to control access to the keys that are generated.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use Amazon S3 server-side encryption with customer-managed keys, and use two AWS CloudHSM instances configured in high-availability mode to manage the keys. Use IAM to control access to the keys that are generated in CloudHSM.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13891,
          "date": "Wed 22 Sep 2021 12:41",
          "username": "Moonshammous",
          "content": "My preference \\\"B\\\" & \\\"D\\\".<br>A: customer can not control the keys!<br>B: AWS-KMS managed keys, allow the user to create Master keys, and control them. It is high available as it is a managed service by AWS.<br>C:CloudHSM can be high available by including a second instance in different AZ.<br>D: Meet the requirement of management and high availability.<br>E: Managing the keys by CloudHSM client, not IAM user!!CloudHSM instance? CloudHSM is a service so D and E are ruled out.<br>B and C would be good options.",
          "upvote_count": "2810",
          "selected_answers": ""
        },
        {
          "id": 279817,
          "date": "Thu 14 Oct 2021 02:20",
          "username": "shammous",
          "content": "CloudHSM instance? CloudHSM is a service so D and E are ruled out.<br>B and C would be good options.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 11233,
          "date": "Sun 19 Sep 2021 21:16",
          "username": "Xiaoyao2000",
          "content": "I would choose b d",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 709776,
          "date": "Wed 02 Nov 2022 12:53",
          "username": "tomosabc1tomosabc1tomosabc1tomosabc1",
          "content": "I have to say that this is a really bad question with several inaccurate wordings.<br><br>Firstly, the \\\"KMS-managed keys\\\" in option B actually refers to KMS keys stored in KMS(i.e. SSE-KMS), customer managed keys or AWS managed keys. Otherwise B can't be right, because the key policy of AWS managed keys cannot be changed. So to make B a valid answer, \\\"KMS-managed keys\\\" have to refer to customer managed keys, which is the same as CDE, but using a completely different wording. B have to be right answer, otherwise there aren't 2 correct answers for this question.\\\"You have three mutually exclusive options, depending on how you choose to manage the encryption keys.<br><br>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)<br>.....<br>Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS)<br>.... Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region.<br>...<br>Server-Side Encryption with Customer-Provided Keys (SSE-C)\\\"<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.htmlSecondly, \\\"CloudHSM instances\\\" and \\\"CloudHSM instances configured in high availability mode\\\" is not a real thing.<br><br>\\\"High availability is provided automatically when you have at least two HSMs in your CloudHSM Cluster. No additional configuration is required. In the event an HSM in your cluster fails, it will be replaced automatically, and all clients will be updated to reflect the new configuration without interrupting any processing\\\"<br><br>\\\"Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones.\\\"<br><br>https://aws.amazon.com/cloudhsm/faqs/Compared with C, D looks more like a correct answer, because C makes no mention of HA, which is not enabled by default(We have to have two HSMs in CloudHSM Cluster to make it HA)",
          "upvote_count": "1211",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 709777,
          "date": "Wed 02 Nov 2022 12:53",
          "username": "tomosabc1",
          "content": "\\\"You have three mutually exclusive options, depending on how you choose to manage the encryption keys.<br><br>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)<br>.....<br>Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS)<br>.... Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region.<br>...<br>Server-Side Encryption with Customer-Provided Keys (SSE-C)\\\"<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 709780,
          "date": "Wed 02 Nov 2022 12:56",
          "username": "tomosabc1",
          "content": "Secondly, \\\"CloudHSM instances\\\" and \\\"CloudHSM instances configured in high availability mode\\\" is not a real thing.<br><br>\\\"High availability is provided automatically when you have at least two HSMs in your CloudHSM Cluster. No additional configuration is required. In the event an HSM in your cluster fails, it will be replaced automatically, and all clients will be updated to reflect the new configuration without interrupting any processing\\\"<br><br>\\\"Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones.\\\"<br><br>https://aws.amazon.com/cloudhsm/faqs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709788,
          "date": "Wed 02 Nov 2022 12:59",
          "username": "tomosabc1",
          "content": "Compared with C, D looks more like a correct answer, because C makes no mention of HA, which is not enabled by default(We have to have two HSMs in CloudHSM Cluster to make it HA)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 693581,
          "date": "Thu 13 Oct 2022 06:07",
          "username": "caveman712",
          "content": "Don't know why the community disregards E.  <br>You can create KMS key with CloudHSM-backed custom-key store. The keys can then be managed by regular IAM and key policies. https://docs.aws.amazon.com/kms/latest/developerguide/manage-cmk-keystore.html<br><br>No need to use CloudHSM client, because KMS connects to CloudHSM on our behalf. https://docs.aws.amazon.com/kms/latest/developerguide/disconnect-keystore.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 685702,
          "date": "Mon 03 Oct 2022 19:17",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 676760,
          "date": "Fri 23 Sep 2022 06:38",
          "username": "JohnPi",
          "content": "In AWS CloudHSM, use any of the following to manage keys on the HSMs in your cluster:<br><br>- PKCS #11 library<br>- JCE provider<br>- CNG and KSP providers<br>- key_mgmt_util",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 611752,
          "date": "Sun 05 Jun 2022 11:33",
          "username": "KengL",
          "content": "CD<br>B is wrong as multiple doesn't mean each user has unique key",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 607664,
          "date": "Thu 26 May 2022 15:04",
          "username": "bobsmith2000bobsmith2000",
          "content": "A doesn't fit the bill at all.<br>C is wrong because we need more than 1 HSM instance in a cluster (https://docs.aws.amazon.com/cloudhsm/latest/userguide/clusters.html)<br>E is wrong because CloudHSM doesn't support IAM (https://docs.aws.amazon.com/cloudhsm/latest/userguide/hsm-users.html)<br>What we have left? B and D. <br>Now referring to the <br>https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html,<br>it becomes clear, that we can use either a \\\"Customer managed key\\\" or a \\\"AWS managed key\\\" which in turn creates \\\"Data Keys\\\".<br>Taking this into account, B doesn't make sence at all!<br>B) \\\"Use Amazon S3 server-side encryption with AWS KMS-managed keys, create multiple customer master keys\\\".<br>If we choose \\\"AWS KMS-managed key\\\" there's no such thing as \\\"customer master keys\\\". Moreover according to the link above: \\\"However, you cannot manage these AWS KMS-managed keys, rotate them, or change their key policies.\\\" which violates the stipulations.<br>So B is wrong as well.<br>The only answer which makes sence and fits the stipulation as D. <br>Correct me if I'm wrong.Second run.<br>C doesn't mention number of instances, so it might work.<br>With B it's not possible to change key policy.<br><br>So for me it's CD",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 613123,
          "date": "Wed 08 Jun 2022 09:02",
          "username": "bobsmith2000",
          "content": "Second run.<br>C doesn't mention number of instances, so it might work.<br>With B it's not possible to change key policy.<br><br>So for me it's CD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 523038,
          "date": "Thu 13 Jan 2022 20:27",
          "username": "pititcu667",
          "content": "e is wrong because with IAM you can only do:<br> \\\"cloudhsm:DescribeClusters\\\",<br> \\\"cloudhsm:DescribeBackups\\\",<br> \\\"cloudhsm:CreateCluster\\\",<br> \\\"cloudhsm:CreateHsm\\\",<br> \\\"cloudhsm:RestoreBackup\\\",<br> \\\"cloudhsm:CopyBackupToRegion\\\",<br> \\\"cloudhsm:InitializeCluster\\\",<br> \\\"cloudhsm:ListTags\\\",<br> \\\"cloudhsm:TagResource\\\",<br> \\\"cloudhsm:UntagResource\\\",",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 513998,
          "date": "Fri 31 Dec 2021 10:41",
          "username": "cldy",
          "content": "B and D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 507171,
          "date": "Wed 22 Dec 2021 15:38",
          "username": "Ni_yotNi_yot",
          "content": "B & C for me.As well as creating 2 instances for HA, you will also need to manage keys using CloudHSM software.Not IAMignore thatB and D",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 507173,
          "date": "Wed 22 Dec 2021 15:39",
          "username": "Ni_yot",
          "content": "ignore thatB and D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 493931,
          "date": "Sat 04 Dec 2021 20:57",
          "username": "AzureDP900",
          "content": "Should be B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490507,
          "date": "Tue 30 Nov 2021 09:01",
          "username": "acloudguru",
          "content": "KMS 99.9%, HSM 99.95%, if B is ok, why need two HSM in D or E?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 488292,
          "date": "Sat 27 Nov 2021 17:45",
          "username": "Sachhi",
          "content": "B and D, not E as CloudHSM can be used only with HSM client<br><br>Q: How do I set up a high availability (HA) configuration?<br><br>High availability is provided automatically when you have at least two HSMs in your CloudHSM Cluster. No additional configuration is required. In the event an HSM in your cluster fails, it will be replaced automatically, and all clients will be updated to reflect the new configuration without interrupting any processing. Additional HSMs can be added to the cluster via the AWS API or SDK, increasing availability without interrupting your application.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 456069,
          "date": "Sat 06 Nov 2021 16:25",
          "username": "StelSen",
          "content": "Today I tried to create Cloud HSM and there is an option to choose subnet (upto 3 as I am in SGP region). But I am able to choose only one region. So I can create HSM with 1 instance. Although Cloud HSM is managed service. Creating HSM with 1 won't give HA.  So, I would choose D as one of the answer over C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 440731,
          "date": "Thu 04 Nov 2021 12:55",
          "username": "nisoshabangusashenka",
          "content": "B and C, changed to C because AWS Cloud HSM is a highly available managed service.Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones. You can learn more about recommended best practices in our online documentation.<br>https://aws.amazon.com/cloudhsm/faqs/",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 476137,
          "date": "Thu 11 Nov 2021 11:38",
          "username": "sashenka",
          "content": "Please note it is your responsibility to architect your cluster for high availability. AWS strongly recommends that you use CloudHSM Clusters with two or more HSMs in separate Availability Zones. You can learn more about recommended best practices in our online documentation.<br>https://aws.amazon.com/cloudhsm/faqs/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 413210,
          "date": "Wed 03 Nov 2021 17:06",
          "username": "mericovjoe16",
          "content": "Question for those who choose D as answer: did you try to create two AWS CloudHSM instances in HA mode? :-D the only option in console is to create a CloudHSM cluster :-) \\\"It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. CloudHSM also enables you to scale quickly by adding and removing HSM capacity on-demand, with no up-front costs.\\\"\\\"You can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region). You can place the HSMs in different Availability Zones in an AWS Region. Adding more HSMs to a cluster provides higher performance. Spreading clusters across Availability Zones provides redundancy and high availability.\\\"<br>This means you need to configure 2 instances at least to keep the cluster highly available.<br>I would go with B and D. ",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 452101,
          "date": "Fri 05 Nov 2021 03:26",
          "username": "joe16",
          "content": "\\\"You can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region). You can place the HSMs in different Availability Zones in an AWS Region. Adding more HSMs to a cluster provides higher performance. Spreading clusters across Availability Zones provides redundancy and high availability.\\\"<br>This means you need to configure 2 instances at least to keep the cluster highly available.<br>I would go with B and D. ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#466",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a public-facing application that uses a Java-based web service via a RESTful API. It is hosted on Apache Tomcat on a single server in a data center that runs consistently at 30% CPU utilization. Use of the API is expected to increase by 10 times with a new product launch. The business wants to migrate the application to AWS with no disruption, and needs it to scale to meet demand.<br>The company has already decided to use Amazon Route 53 and CNAME records to redirect traffic. How can these requirements be met with the LEAST amount of effort?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#466",
          "answers": [
            {
              "choice": "<p>A. Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Lift and shift the Apache server to the cloud using AWS SMS. Then switch the application to direct web service traffic to the new instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Docker image and migrate the image to Amazon ECS. Then change the application code to direct web service queries to the ECS container.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Modify the application to call the web service via Amazon API Gateway. Then create a new AWS Lambda Java function to run the Java web service code. After testing, change API Gateway to use the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13888,
          "date": "Thu 23 Sep 2021 12:51",
          "username": "Moon",
          "content": "I prefer answer \\\"A\\\".<br>A: using EB meets the requirement of least amount of effort.<br>B: does not meet the scaling (10 times capacity) requirement<br>C: alot of changes.<br>D: this solution can't be considered least amount of effort. ( but it is good for no disruption though!).",
          "upvote_count": "36",
          "selected_answers": ""
        },
        {
          "id": 23178,
          "date": "Fri 24 Sep 2021 13:44",
          "username": "uopspopuopspop",
          "content": "A<br>We already decide to use Route53 CNAME to redirect the traffic. Therefore, it already provides no disruption when switching to the ElastiBeanstalk.<br><br>D: it does not use the Route53 CNAME to redirect at all. In addition, it's too much work, including modifying existing code to call API Gateway, converting app to Lambda.Moreover, when using ElastiBeanstalk, if your domain name does not include region, you MUST use CNAME record. <br><br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-beanstalk-environment.html#routing-to-beanstalk-environment-create-resource-record-set",
          "upvote_count": "72",
          "selected_answers": ""
        },
        {
          "id": 23180,
          "date": "Sat 25 Sep 2021 06:12",
          "username": "uopspop",
          "content": "Moreover, when using ElastiBeanstalk, if your domain name does not include region, you MUST use CNAME record. <br><br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-beanstalk-environment.html#routing-to-beanstalk-environment-create-resource-record-set",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 685703,
          "date": "Mon 03 Oct 2022 19:19",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 603610,
          "date": "Thu 19 May 2022 07:00",
          "username": "bobsmith2000",
          "content": "THE LEAST work possible.<br>It's AWS Elastic Beanstalk.<br>Associate level question.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 497604,
          "date": "Thu 09 Dec 2021 11:27",
          "username": "cldy",
          "content": "A.  Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493933,
          "date": "Sat 04 Dec 2021 21:02",
          "username": "AzureDP900",
          "content": "A seems perfect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409740,
          "date": "Sat 06 Nov 2021 17:56",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345033,
          "date": "Fri 05 Nov 2021 16:07",
          "username": "Waiweng",
          "content": "should be A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 333331,
          "date": "Thu 04 Nov 2021 10:01",
          "username": "sarah_tsarah_t",
          "content": "Everyone agrees on B.  <br>I think the second is C: \\\"CloudHSM is standards-compliant and enables you to export all of your keys to most other commercially-available HSMs, subject to your configurations. It is a fully-managed service that automates time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups. \\\"<br>https://aws.amazon.com/cloudhsm/this belongs to the previous question. no idea how it ended up here",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 333335,
          "date": "Fri 05 Nov 2021 01:29",
          "username": "sarah_t",
          "content": "this belongs to the previous question. no idea how it ended up here",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 331170,
          "date": "Mon 01 Nov 2021 02:17",
          "username": "anandbabu",
          "content": "i will go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 293699,
          "date": "Thu 28 Oct 2021 12:21",
          "username": "kiev",
          "content": "A is the undisputed king.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290680,
          "date": "Mon 25 Oct 2021 07:53",
          "username": "Kian1",
          "content": "will go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282457,
          "date": "Mon 25 Oct 2021 00:21",
          "username": "Ebi",
          "content": "API Gateway uses alias not CNAME<br>I go with A as well although beanstalk is not my preference",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242532,
          "date": "Sat 23 Oct 2021 02:34",
          "username": "T14102020",
          "content": "Correct answer is A. ElastiBeanstalk and if your domain name does not include region, you MUST use CNAME record.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 233166,
          "date": "Wed 20 Oct 2021 04:14",
          "username": "shwanjaff",
          "content": "it shown D in other places",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230047,
          "date": "Sun 17 Oct 2021 18:13",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 229774,
          "date": "Wed 13 Oct 2021 20:11",
          "username": "Bulti",
          "content": "Answer is A. - Mainly because auto scaling is included and CNAME record is being used to direct traffic to the Elastic Beanstalk env.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#467",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS for production and development workloads. Each business unit has its own AWS account for production, and a separate AWS account to develop and deploy its applications. The Information Security department has introduced new security policies that limit access for terminating certain Amazon<br>EC2 instances in all accounts to a small group of individuals from the Security team.<br>How can the Solutions Architect meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#467",
          "answers": [
            {
              "choice": "<p>A. Create a new IAM policy that allows access to those EC2 instances only for the Security team. Apply this policy to the AWS Organizations master account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new tag-based IAM policy that allows access to these EC2 instances only for the Security team. Tag the instances appropriately, and apply this policy in each account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an organizational unit under AWS Organizations. Move all the accounts into this organizational unit and use SCP to apply a whitelist policy to allow access to these EC2 instances for the Security team only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up SAML federation for all accounts in AWS. Configure SAML so that it checks for the service API call before authenticating the user. Block SAML from authenticating API calls if anyone other than the Security team accesses these instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13883,
          "date": "Mon 20 Sep 2021 02:23",
          "username": "MoonSmartSmartexamacc9Ow30exergengNickGR",
          "content": "I prefer answer \\\"B\\\".<br>A: applying policy to master account does not mean Security Team!<br>B: using tags on EC2s. Then use an IAM policy restrictions/rules on these taged instances.<br>C: Organizational unit is used to limit the access, but not to provide privileges.<br>D: SAML is used for federation with on premise, which is not the case here!Agreed. B vs. C: SCPs are similar to IAM permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). <br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html#scp-effects-on-permissionsRevisiting this question. There is no mention of cross-account access. I am not sure if this should be automatically assumed. I wonder if option D can be an option (it doesn't have to have on-prem setup). Even in that, identities are authenticated & federated then authorizedbased on role; here it seems to be happening other way around.this way you cannot limit root from controlling the targeted instancesCan you explain what you mean?<br>We can do something like this, right?<br>https://aws.amazon.com/premiumsupport/knowledge-center/restrict-ec2-iam/IAM user is different from aws account.<br>Not using IAM role/AWS organization ,it is not possible to manage access privileges of other aws accounts.<br>choose CReference:<br>https://aws.amazon.com/premiumsupport/knowledge-center/iam-ec2-resource-tags/",
          "upvote_count": "50612211",
          "selected_answers": ""
        },
        {
          "id": 70252,
          "date": "Thu 30 Sep 2021 20:05",
          "username": "SmartSmart",
          "content": "Agreed. B vs. C: SCPs are similar to IAM permission policies and use almost the same syntax. However, an SCP never grants permissions. Instead, SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). <br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html#scp-effects-on-permissionsRevisiting this question. There is no mention of cross-account access. I am not sure if this should be automatically assumed. I wonder if option D can be an option (it doesn't have to have on-prem setup). Even in that, identities are authenticated & federated then authorizedbased on role; here it seems to be happening other way around.",
          "upvote_count": "61",
          "selected_answers": ""
        },
        {
          "id": 76534,
          "date": "Sat 02 Oct 2021 18:44",
          "username": "Smart",
          "content": "Revisiting this question. There is no mention of cross-account access. I am not sure if this should be automatically assumed. I wonder if option D can be an option (it doesn't have to have on-prem setup). Even in that, identities are authenticated & federated then authorizedbased on role; here it seems to be happening other way around.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 30764,
          "date": "Tue 21 Sep 2021 09:51",
          "username": "examacc9Ow30exergeng",
          "content": "this way you cannot limit root from controlling the targeted instancesCan you explain what you mean?<br>We can do something like this, right?<br>https://aws.amazon.com/premiumsupport/knowledge-center/restrict-ec2-iam/IAM user is different from aws account.<br>Not using IAM role/AWS organization ,it is not possible to manage access privileges of other aws accounts.<br>choose C",
          "upvote_count": "221",
          "selected_answers": ""
        },
        {
          "id": 31565,
          "date": "Fri 24 Sep 2021 05:08",
          "username": "9Ow30exergeng",
          "content": "Can you explain what you mean?<br>We can do something like this, right?<br>https://aws.amazon.com/premiumsupport/knowledge-center/restrict-ec2-iam/IAM user is different from aws account.<br>Not using IAM role/AWS organization ,it is not possible to manage access privileges of other aws accounts.<br>choose C",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 185274,
          "date": "Tue 19 Oct 2021 09:31",
          "username": "exergeng",
          "content": "IAM user is different from aws account.<br>Not using IAM role/AWS organization ,it is not possible to manage access privileges of other aws accounts.<br>choose C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 385392,
          "date": "Mon 01 Nov 2021 13:06",
          "username": "NickGR",
          "content": "Reference:<br>https://aws.amazon.com/premiumsupport/knowledge-center/iam-ec2-resource-tags/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 522632,
          "date": "Thu 13 Jan 2022 05:50",
          "username": "RVivek",
          "content": "It seems the answers are not forthis question. the correct set of answers <br><br> A.  Modify the application to call the web service via Amazon API Gateway Then create a new AWS Lambda Java function to run the Java web service code After testing change API Gateway to use the Lambda function<br> B.  Lift and shift the Apache server to the cloud using AWS SMS Then switch the application to direct web service traffic to the new instance<br> C.  Create a Docker image and migrate the image to Amazon ECS Then change the application code to direct web service queries to the ECS container<br> D.  Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling Then switch the application to use the new web service<br><br>Answer: D",
          "upvote_count": "37",
          "selected_answers": ""
        },
        {
          "id": 716826,
          "date": "Sat 12 Nov 2022 18:14",
          "username": "Lorrendo",
          "content": "IAM Policy + Tag -> ABAC model<br>https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 710768,
          "date": "Thu 03 Nov 2022 20:57",
          "username": "AjayPrajapati",
          "content": "B mentions about applying the policy to all account which make sense vs applying to master account in A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 703153,
          "date": "Mon 24 Oct 2022 17:09",
          "username": "kharakbeer",
          "content": "B is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 702783,
          "date": "Mon 24 Oct 2022 09:24",
          "username": "Yashar1691",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 685704,
          "date": "Mon 03 Oct 2022 19:22",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 653158,
          "date": "Sun 28 Aug 2022 23:09",
          "username": "Rocketeer",
          "content": "Answer seems to be C - https://aws.amazon.com/blogs/security/how-to-use-service-control-policies-to-set-permission-guardrails-across-accounts-in-your-aws-organization/#:~:text=For%20example%2C%20you%20can%20use,used%20for%20your%20central%20administrators.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 587121,
          "date": "Sun 17 Apr 2022 10:46",
          "username": "LiamNguser0001",
          "content": "The Answer are corresponding the incorrect qestion. Here're the ans:<br><br>•\tA.  Use AWS Elastic Beanstalk to deploy the Java web service and enable Auto Scaling. Then switch the application to use the new web service.<br>•\tB.  Lift and shift the Apache server to the cloud using AWS SMS. Then switch the application to direct web service traffic to the new instance.<br>•\tC.  Create a Docker image and migrate the image to Amazon ECS. Then change the application code to direct web service queries to the ECS container.<br>•\tD.  Modify the application to call the web service via Amazon API Gateway. Then create a new AWS Lambda Java function to run the Java web service code. After testing, change API Gateway to use the Lambda function.<br><br>Correct Answer should be : A(least effort)i agree with you , answers are not related to this question",
          "upvote_count": "51",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 597785,
          "date": "Fri 06 May 2022 17:52",
          "username": "user0001",
          "content": "i agree with you , answers are not related to this question",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 527311,
          "date": "Wed 19 Jan 2022 09:14",
          "username": "sTeVe86",
          "content": "To me: this question doesn't match with the answers, didn't make any scenes.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 523427,
          "date": "Fri 14 Jan 2022 09:21",
          "username": "GeniusMikeLiu",
          "content": "what's the main point about this question? I am confused after read.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 520369,
          "date": "Sun 09 Jan 2022 18:27",
          "username": "Duke_YUBigbearcn",
          "content": "I don't understand this question and answers at all. The question is about to \\\"The organization needs a smooth transfer of the program to AWS and the ability for the application to scale in response to demand\\\". Why the answers are about IAM and security team? Couldn't Auto Scaling groups and Route 53 and CNAME records satisfy the requirement?They made mistake. This is question 466 and the answer option is 467.",
          "upvote_count": "43",
          "selected_answers": ""
        },
        {
          "id": 521904,
          "date": "Wed 12 Jan 2022 05:08",
          "username": "Bigbearcn",
          "content": "They made mistake. This is question 466 and the answer option is 467.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 455666,
          "date": "Sat 06 Nov 2021 19:45",
          "username": "student22",
          "content": "B<br>---<br>Not D because we need to limit access to ec2 instances, not ec2 service.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409741,
          "date": "Tue 02 Nov 2021 17:10",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406454,
          "date": "Tue 02 Nov 2021 03:02",
          "username": "neta1o",
          "content": "Why B, couldn't tags be easily modified to thwart the security effort?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345035,
          "date": "Sun 31 Oct 2021 22:42",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 333823,
          "date": "Sun 31 Oct 2021 16:58",
          "username": "Amitv2706",
          "content": "Correct answer is B<br><br>SCP alone are never sufficient to grant permissions to accounts in organization. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions.<br><br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#468",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is moving a business-critical, multi-tier application to AWS. The architecture consists of a desktop client application and server infrastructure. The server infrastructure resides in an on-premises data center that frequently fails to maintain the application uptime SLA of 99.95%. A Solutions Architect must re- architect the application to ensure that it can meet or exceed the SLA. <br>The application contains a PostgreSQL database running on a single virtual machine. The business logic and presentation layers are load balanced between multiple virtual machines. Remote users complain about slow load times while using this latency-sensitive application.<br>Which of the following will meet the availability requirements with little change to the application while improving user experience and minimizing costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#468",
          "answers": [
            {
              "choice": "<p>A. Migrate the database to a PostgreSQL database in Amazon EC2. Host the application and presentation layers in automatically scaled Amazon ECS containers behind an Application Load Balancer. Allocate an Amazon WorkSpaces WorkSpace for each end user to improve the user experience.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the database to an Amazon RDS Aurora PostgreSQL configuration. Host the application and presentation layers in an Auto Scaling configuration on Amazon EC2 instances behind an Application Load Balancer. Use Amazon AppStream 2.0 to improve the user experience.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the database to an Amazon RDS PostgreSQL Multi-AZ configuration. Host the application and presentation layers in automatically scaled AWS Fargate containers behind a Network Load Balancer. Use Amazon ElastiCache to improve the user experience.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the database to an Amazon Redshift cluster with at least two nodes. Combine and host the application and presentation layers in automatically scaled Amazon ECS containers behind an Application Load Balancer. Use Amazon CloudFront to improve the user experience.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12552,
          "date": "Thu 23 Sep 2021 15:12",
          "username": "donathonheany",
          "content": "B<br>A: Workspace is expensive and a single instance of EC2 hosting the DB would not improve the availability.<br>B: B is the best answer. Aurora would improve availability that can replicate to multiple AZ (6 copies). Auto scaling would improve the performance together with a ALB.  AppStream is like Citrix that deliver hosted Apps to users.<br>C: Fargate is managed ECS. Elasticache only improve DB access performance but not the total user experience. This does not address the slowness that users faced.<br>D: Redshift is dataware house and not a SQL based database.B is wrong. should be C. <br>1.B didn't mention Multi-AZ of aurora. Aurora is not Multi-AZ by default.Single AZ cannot achieve SLA required ( https://aws.amazon.com/rds/aurora/sla/ )<br>2. Appstream is a good product but you need a lot of changes . you need to build the image of the app and deployed it appstream",
          "upvote_count": "371",
          "selected_answers": ""
        },
        {
          "id": 690716,
          "date": "Mon 10 Oct 2022 07:09",
          "username": "heany",
          "content": "B is wrong. should be C. <br>1.B didn't mention Multi-AZ of aurora. Aurora is not Multi-AZ by default.Single AZ cannot achieve SLA required ( https://aws.amazon.com/rds/aurora/sla/ )<br>2. Appstream is a good product but you need a lot of changes . you need to build the image of the app and deployed it appstream",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13875,
          "date": "Sun 26 Sep 2021 15:48",
          "username": "Moon",
          "content": "Answer \\\"B\\\" meets the requirements.<br>A: using containers, is a big change to app. Maintaining EC2 is a challenge and could affect SLA. <br>B: using RDS, is auto maintenance with high SLA.  using autoscaling on EC2 allow more availability/performance. Appstream is good virtual solution.<br>C: lots of re-works.<br>D: there is no DB, only Datawarehouse.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 710770,
          "date": "Thu 03 Nov 2022 21:06",
          "username": "AjayPrajapati",
          "content": "B improves SLA from all layers. Desktop app can be supported by App stream<br>C - elastic cache needs code changes. Does not talk about desktop app",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 704082,
          "date": "Tue 25 Oct 2022 19:54",
          "username": "Vinafec",
          "content": "Going with C; B does not meet uptime SLA: https://aws.amazon.com/appstream2/sla/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695205,
          "date": "Sat 15 Oct 2022 07:21",
          "username": "Dionenonly",
          "content": "Don't confuse Desktop Client Application to \\\"Desktop\\\".<br>Desktop = Workspace<br>Desktop Client Application = Appstream",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 692960,
          "date": "Wed 12 Oct 2022 12:45",
          "username": "Blair77Blair77",
          "content": "-\\\"A Solutions Architect must re- architect the application to ensure that it can meet or exceed the SLA. \\\"<br>Fargate SLA: 99.99%<br>RDS SLA: 99.95%<br>Aurora SLA: 99.99<br>AppStream SLA: 99.9%<br><br>AppStream does not meet the 99.95% SLA.  <br>I'll go with C. And by the way, AuroraDB and AppStream do not minimize costs !!",
          "upvote_count": "21",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692963,
          "date": "Wed 12 Oct 2022 12:53",
          "username": "Blair77",
          "content": "And by the way, AuroraDB and AppStream do not minimize costs !!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 685705,
          "date": "Mon 03 Oct 2022 19:25",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 493935,
          "date": "Sat 04 Dec 2021 21:09",
          "username": "AzureDP900",
          "content": "B is right, other answers doesn't make any sense as mentioned by Ebi.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 433169,
          "date": "Fri 05 Nov 2021 23:29",
          "username": "denccc",
          "content": "Little change... so B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413142,
          "date": "Thu 04 Nov 2021 13:11",
          "username": "DerekKey",
          "content": "B ok<br>C wrong - \\\"multiple virtual machines\\\" must be converted to Docker it is AGAINST \\\"little change to the application\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409744,
          "date": "Wed 03 Nov 2021 03:07",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345042,
          "date": "Tue 02 Nov 2021 21:40",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 323258,
          "date": "Thu 28 Oct 2021 09:35",
          "username": "ExtHo",
          "content": "Key of answer is desktop client applications so its AppStream =B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 298086,
          "date": "Wed 27 Oct 2021 06:19",
          "username": "areke",
          "content": "Answer is B: AppStream is built for virtualdesktop application",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290686,
          "date": "Sun 24 Oct 2021 06:49",
          "username": "Kian1",
          "content": "will go with B only Bmakes sense",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282470,
          "date": "Sat 23 Oct 2021 19:13",
          "username": "Ebi",
          "content": "I go with B, all other answer don't make any sense",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 282169,
          "date": "Fri 22 Oct 2021 01:01",
          "username": "Trap_D0_rwindBlair77selva",
          "content": "The answer is definitely C -- read the question carefully, the company is having trouble meeting it's SLA REQUIREMENTS. Hence you need multi-AZ deployments. While Aurora multi-AZ + Elasticache is probably overkill, it will definitely help you meet an SLA for a HIGHLY LATENCY SENSITIVE application. Everyone saying B is better because of Appstream isn't reading the question requirements, nevermind that B does no indicated a multi-AZ deployment which will not ensure a higher SLA. <br>If you read the question, the answer is definitely C. C is wrong. SLA of Aurora is higher than RDS PostgreSQL.B is correct.SLA needed: 99.95%<br>AppStream SLA: 99.9%<br>C is good.Aurora is muti-az by default. \\\"Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. Aurora stores these copies regardless of whether the instances in the DB cluster span multiple Availability Zones.\\\"",
          "upvote_count": "4111",
          "selected_answers": ""
        },
        {
          "id": 291442,
          "date": "Tue 26 Oct 2021 02:38",
          "username": "windBlair77",
          "content": "C is wrong. SLA of Aurora is higher than RDS PostgreSQL.B is correct.SLA needed: 99.95%<br>AppStream SLA: 99.9%<br>C is good.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 693373,
          "date": "Wed 12 Oct 2022 22:51",
          "username": "Blair77",
          "content": "SLA needed: 99.95%<br>AppStream SLA: 99.9%<br>C is good.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291425,
          "date": "Mon 25 Oct 2021 11:12",
          "username": "selva",
          "content": "Aurora is muti-az by default. \\\"Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. Aurora stores these copies regardless of whether the instances in the DB cluster span multiple Availability Zones.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#469",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to run a software package that has a license that must be run on the same physical host for the duration of its use. The software package is only going to be used for 90 days. The company requires patching and restarting of all instances every 30 days.<br>How can these requirements be met using AWS?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#469",
          "answers": [
            {
              "choice": "<p>A. Run a dedicated instance with auto-placement disabled.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Run the instance on a dedicated host with Host Affinity set to Host.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Run an On-Demand Instance with a Reserved Instance to ensure consistent placement.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Run the instance on a licensed host with termination set for 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12553,
          "date": "Wed 22 Sep 2021 11:18",
          "username": "donathonStelSen",
          "content": "Host Affinity is configured at the instance level. It establishes a launch relationship between an instance and a Dedicated Host. (This set which host the instance can run on)<br>Auto-placement allows you to manage whether instances that you launch are launched onto a specific host, or onto any available host that has matching configurations. Auto-placement must be configured at the host level. (This sets which instance the host can run.)<br>When affinity is set to Host, an instance launched onto a specific host always restarts on the same host if stopped. This applies to both targeted and untargeted launches.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-dedicated-hosts-work.htmlVery well explained.",
          "upvote_count": "391",
          "selected_answers": ""
        },
        {
          "id": 456072,
          "date": "Fri 05 Nov 2021 14:13",
          "username": "StelSen",
          "content": "Very well explained.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716963,
          "date": "Sat 12 Nov 2022 23:48",
          "username": "DarthYoda",
          "content": "Dedicated host, so B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 710061,
          "date": "Wed 02 Nov 2022 21:35",
          "username": "superuser784",
          "content": "I hope to have several of these straightforward questions on the real test!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 685707,
          "date": "Mon 03 Oct 2022 19:27",
          "username": "dmscountera",
          "content": "Based on all comments",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 562665,
          "date": "Mon 07 Mar 2022 15:04",
          "username": "Fuccon",
          "content": "Why is it C instead of A?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494356,
          "date": "Sun 05 Dec 2021 14:24",
          "username": "cldy",
          "content": "B.  Run the instance on a dedicated host with Host Affinity set to Host.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493937,
          "date": "Sat 04 Dec 2021 21:11",
          "username": "AzureDP900",
          "content": "B is right answer based on requirement.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447684,
          "date": "Tue 02 Nov 2021 08:33",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409745,
          "date": "Sat 30 Oct 2021 02:55",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345043,
          "date": "Thu 28 Oct 2021 10:43",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293717,
          "date": "Wed 27 Oct 2021 14:41",
          "username": "kiev",
          "content": "Full House B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 290689,
          "date": "Sun 24 Oct 2021 17:54",
          "username": "Kian1",
          "content": "B host affinity",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282481,
          "date": "Sat 16 Oct 2021 15:32",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 269361,
          "date": "Mon 11 Oct 2021 09:36",
          "username": "kopper2019",
          "content": "B for sure<br><br>When affinity is set to Host, an instance launched onto a specific host always restarts on the same host if stopped. This applies to both targeted and untargeted launches.<br><br>When affinity is set to Off, and you stop and restart the instance, it can be restarted on any available host. However, it tries to launch back onto the last Dedicated Host on which it ran (on a best-effort basis).",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 268112,
          "date": "Sun 10 Oct 2021 19:04",
          "username": "sanjaym",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242550,
          "date": "Sun 03 Oct 2021 09:05",
          "username": "T14102020",
          "content": "Correct answer is B.  Host Affinity",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230054,
          "date": "Sat 02 Oct 2021 00:12",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#470",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A bank is designing an online customer service portal where customers can chat with customer service agents. The portal is required to maintain a 15-minute<br>RPO or RTO in case of a regional disaster. Banking regulations require that all customer service chat transcripts must be preserved on durable storage for at least<br>7 years, chat conversations must be encrypted in-flight, and transcripts must be encrypted at rest. The Data Loss Prevention team requires that data at rest must be encrypted using a key that the team controls, rotates, and revokes.<br>Which design meets these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#470",
          "answers": [
            {
              "choice": "<p>A. The chat application logs each chat message into Amazon CloudWatch Logs. A scheduled AWS Lambda function invokes a CloudWatch Logs CreateExportTask every 5 minutes to export chat transcripts to Amazon S3. The S3 bucket is configured for cross-region replication to the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The chat application logs each chat message into two different Amazon CloudWatch Logs groups in two different regions, with the same AWS KMS key applied. Both CloudWatch Logs groups are configured to export logs into an Amazon Glacier vault with a 7-year vault lock policy with a KMS key specified.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The chat application logs each chat message into Amazon CloudWatch Logs. A subscription filter on the CloudWatch Logs group feeds into an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Kinesis Data Firehose.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. The chat application logs each chat message into Amazon CloudWatch Logs.The CloudWatch Logs group is configured to export logs into an Amazon Glacier vault with a 7-year vault lock policy. Glacier cross-region replication mirrors chat archives to the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Amazon Glacier vault.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 76547,
          "date": "Mon 27 Sep 2021 02:39",
          "username": "Smartheanysam422shammousDerekKey",
          "content": "A & D (Invalid): Cross-Region Replication won't support 15-min RTO.<br>B (Invalid): KMS Keys are region-specific. <br>C (Valid): Issue about data delivery failure, Cloudwatch Logs itself is durable storage that can retain logs indefinitely.cloudwatch log store (archive) is 0.03 usd /gb, glacier instant retrieval is 0.004 usd/gb.I don't believe people will use cloudwatch logs for long term ( 7 years ) storage. the cost difference is order of magnitudedo we have integration of cloud watch logs to kinesis?Indeed, CloudWatch logs can be retained for up to 10 years and one day!correction to your B - KMS customer managed keys can be replicated between regions.",
          "upvote_count": "222122",
          "selected_answers": ""
        },
        {
          "id": 644323,
          "date": "Tue 09 Aug 2022 05:18",
          "username": "heany",
          "content": "cloudwatch log store (archive) is 0.03 usd /gb, glacier instant retrieval is 0.004 usd/gb.I don't believe people will use cloudwatch logs for long term ( 7 years ) storage. the cost difference is order of magnitude",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 87539,
          "date": "Wed 29 Sep 2021 02:19",
          "username": "sam422",
          "content": "do we have integration of cloud watch logs to kinesis?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 280477,
          "date": "Tue 26 Oct 2021 10:16",
          "username": "shammous",
          "content": "Indeed, CloudWatch logs can be retained for up to 10 years and one day!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 413153,
          "date": "Fri 05 Nov 2021 14:24",
          "username": "DerekKey",
          "content": "correction to your B - KMS customer managed keys can be replicated between regions.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 12555,
          "date": "Wed 22 Sep 2021 16:54",
          "username": "donathonBlueGreendonathonShawn1chaudhAWSPro24Frank1examaccPacoDerek",
          "content": "A<br>A: Creates an export task, which allows you to efficiently export data from a log group to an Amazon S3 bucket. Separate KMS keys are needed because only 1 key (used by S3) will be shared across to the other region.<br>B: The application is in one region, how would it be able to export the logs to another cloudwatch in other region? S3 should be used.<br>C: If data delivery to your Amazon S3 bucket fails, Amazon Kinesis Data Firehose retries to deliver data every 5 seconds for up to a maximum period of 24 hours. If the issue continues beyond the 24-hour maximum retention period, it discards the data.<br>D: Glacier cannot achieve the 15minute RTO.Amazon CloudWatch Log retention – By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group.https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateExportTask.html<br>https://aws.amazon.com/kinesis/data-firehose/faqs/<br>https://aws.amazon.com/blogs/devops/ensuring-security-of-your-code-in-a-cross-regioncross-account-deployment-solution/the first link says \\\"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.\\\" Will this make answer A invalid?Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.To be clear I think you are pointing out that the CRR bucket will be the one encrypted with KMS because the same-region bucket can't support the export.Glacier expedited retrieval can restore within 5 minutes. Did I miss anything here?for glacier you will need to do client side encryption as server side encryption in glacier is only with aws managed keyshttps://docs.aws.amazon.com/amazonglacier/latest/dev/DataEncryption.html<br>Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS. If you prefer to manage your own keys, you can also use client-side encryption before storing data in S3 Glacier.",
          "upvote_count": "2124391425",
          "selected_answers": ""
        },
        {
          "id": 567108,
          "date": "Sun 13 Mar 2022 18:53",
          "username": "BlueGreen",
          "content": "Amazon CloudWatch Log retention – By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 13781,
          "date": "Thu 23 Sep 2021 02:24",
          "username": "donathonShawn1",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CreateExportTask.html<br>https://aws.amazon.com/kinesis/data-firehose/faqs/<br>https://aws.amazon.com/blogs/devops/ensuring-security-of-your-code-in-a-cross-regioncross-account-deployment-solution/the first link says \\\"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.\\\" Will this make answer A invalid?",
          "upvote_count": "43",
          "selected_answers": ""
        },
        {
          "id": 138634,
          "date": "Mon 11 Oct 2021 07:14",
          "username": "Shawn1",
          "content": "the first link says \\\"Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.\\\" Will this make answer A invalid?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 16189,
          "date": "Thu 23 Sep 2021 12:13",
          "username": "chaudhAWSPro24",
          "content": "Exporting to S3 buckets that are encrypted with AES-256 is supported. Exporting to S3 buckets encrypted with SSE-KMS is not supported.To be clear I think you are pointing out that the CRR bucket will be the one encrypted with KMS because the same-region bucket can't support the export.",
          "upvote_count": "91",
          "selected_answers": ""
        },
        {
          "id": 47456,
          "date": "Fri 24 Sep 2021 18:06",
          "username": "AWSPro24",
          "content": "To be clear I think you are pointing out that the CRR bucket will be the one encrypted with KMS because the same-region bucket can't support the export.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 22542,
          "date": "Thu 23 Sep 2021 18:27",
          "username": "Frank1examaccPacoDerek",
          "content": "Glacier expedited retrieval can restore within 5 minutes. Did I miss anything here?for glacier you will need to do client side encryption as server side encryption in glacier is only with aws managed keyshttps://docs.aws.amazon.com/amazonglacier/latest/dev/DataEncryption.html<br>Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS. If you prefer to manage your own keys, you can also use client-side encryption before storing data in S3 Glacier.",
          "upvote_count": "425",
          "selected_answers": ""
        },
        {
          "id": 24542,
          "date": "Thu 23 Sep 2021 21:57",
          "username": "examaccPacoDerek",
          "content": "for glacier you will need to do client side encryption as server side encryption in glacier is only with aws managed keyshttps://docs.aws.amazon.com/amazonglacier/latest/dev/DataEncryption.html<br>Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS. If you prefer to manage your own keys, you can also use client-side encryption before storing data in S3 Glacier.",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 42744,
          "date": "Fri 24 Sep 2021 03:03",
          "username": "PacoDerek",
          "content": "https://docs.aws.amazon.com/amazonglacier/latest/dev/DataEncryption.html<br>Data at rest stored in S3 Glacier is automatically server-side encrypted using 256-bit Advanced Encryption Standard (AES-256) with keys maintained by AWS. If you prefer to manage your own keys, you can also use client-side encryption before storing data in S3 Glacier.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 685874,
          "date": "Tue 04 Oct 2022 04:19",
          "username": "caveman712et22s",
          "content": "A is not correct because CW Logs cannot export log data to Amazon S3 buckets that are encrypted by AWS KMS<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.htmlCloudWatch logs CAN be exported to S3 buckets encrypted by AWS KMS.<br>In the link you provided, it literally says in the first paragraph: \\\"Exporting log data to S3 buckets that are encrypted by AWS KMS is supported.\\\"<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html#:~:text=Exporting%20log%20data%20to%20S3%20buckets%20that%20are%20encrypted%20by%20AWS%20KMS%20is%20supported.",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 716561,
          "date": "Sat 12 Nov 2022 09:31",
          "username": "et22s",
          "content": "CloudWatch logs CAN be exported to S3 buckets encrypted by AWS KMS.<br>In the link you provided, it literally says in the first paragraph: \\\"Exporting log data to S3 buckets that are encrypted by AWS KMS is supported.\\\"<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html#:~:text=Exporting%20log%20data%20to%20S3%20buckets%20that%20are%20encrypted%20by%20AWS%20KMS%20is%20supported.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 562100,
          "date": "Sun 06 Mar 2022 16:36",
          "username": "asfsdfsdf",
          "content": "I will go with C, using elimination:<br>D - in order to create an export will take about 12 hours - \\\"Log data can take up to 12 hours to become available for export. For near real-time analysis of log data, see Analyzing log data with CloudWatch Logs Insights or Real-time processing of log data with subscriptions instead.\\\"<br>for A - Lambda time is 5 Minutes + CRR can take up to 15 minutes (or more) <br>For B - KMS keys are regional - so its invalid.<br>so the only valid answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 556875,
          "date": "Sat 26 Feb 2022 18:39",
          "username": "johnnsmith",
          "content": "C is correct. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.htmlLog data can take up to 12 hours to become available for export. For near real-time analysis of log data, see Analyzing log data with CloudWatch Logs Insights or Real-time processing of log data with subscriptions instead. Only C meets the RPO/RTO requirements.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 546181,
          "date": "Sun 13 Feb 2022 02:31",
          "username": "RVivek",
          "content": "A is the answer.<br>B &D are wrong since The Data Loss Prevention team requires that data at rest must be encrypted using a key that the team controls, rotates, and revokes.<br>Glacier Vault cannot modify files, therefore, keys cannot be rotated.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 544625,
          "date": "Thu 10 Feb 2022 15:46",
          "username": "RVivek",
          "content": "Answer is A<br>B & DGlacier Vault will not allow encryption using KMS amanged keys<br>C:If data delivery to your Amazon S3 bucket fails, Amazon Kinesis Data Firehose retries to deliver data every 5 seconds for up to a maximum period of 24 hours. If the issue continues beyond the 24-hour maximum retention period, it discards the data",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513786,
          "date": "Fri 31 Dec 2021 01:17",
          "username": "peddyua",
          "content": "To Support C<br>https://docs.aws.amazon.com/firehose/latest/dev/security-best-practices.html<br>Data at rest and data in transit can be encrypted in Kinesis Data Firehose<br>https://docs.aws.amazon.com/firehose/latest/dev/encryption.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496591,
          "date": "Wed 08 Dec 2021 07:34",
          "username": "cldy",
          "content": "C.  The chat application logs each chat message into Amazon CloudWatch Logs. A subscription filter on the CloudWatch Logs group feeds into an Amazon Kinesis Data Firehose which streams the chat messages into an Amazon S3 bucket in the backup region. Separate AWS KMS keys are specified for the CloudWatch Logs group and the Kinesis Data Firehose.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483804,
          "date": "Mon 22 Nov 2021 02:57",
          "username": "acloudguru",
          "content": "For C, kinese data fire hose does not support encryption. So C is wrong.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 455674,
          "date": "Sun 07 Nov 2021 12:24",
          "username": "student22",
          "content": "C <br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449620,
          "date": "Sun 07 Nov 2021 09:47",
          "username": "Bigbearcn",
          "content": "B and D are wrong because CloudWatch logs cannot be exported to glacier directly. It can be exported to S3 and then move to glacier afterwards.<br>A is wrong because exporting log data to Amazon S3 buckets that are encrypted by AWS KMS is not supported.<br><br>Only option is C. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 448853,
          "date": "Sun 07 Nov 2021 04:12",
          "username": "Kopa",
          "content": "Im going for C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437858,
          "date": "Sun 07 Nov 2021 02:06",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413177,
          "date": "Sat 06 Nov 2021 16:03",
          "username": "DerekKey",
          "content": "A wrong - CreateExportTask can not export to S3 buckets encrypted with SSE-KMS<br>B & D wrong - you can not specify a key for Glacier<br>C correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409748,
          "date": "Wed 03 Nov 2021 20:22",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 406844,
          "date": "Wed 03 Nov 2021 12:03",
          "username": "Akhil254",
          "content": "C COrrect",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#471",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company currently runs a secure application on Amazon EC2 that takes files from on-premises locations through AWS Direct Connect, processes them, and uploads them to a single Amazon S3 bucket. The application uses HTTPS for encryption in transit to Amazon S3, and S3 server-side encryption to encrypt at rest.<br>Which of the following changes should the Solutions Architect recommend to make this solution more secure without impeding application's performance?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#471",
          "answers": [
            {
              "choice": "<p>A. Add a NAT gateway. Update the security groups on the EC2 instance to allow access to and from the S3 IP range only. Configure an S3 bucket policy that allows communication from the NAT gateway's Elastic IP address only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add a VPC endpoint. Configure endpoint policies on the VPC endpoint to allow access to the required Amazon S3 buckets only. Implement an S3 bucket policy that allows communication from the VPC's source IP range only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add a NAT gateway. Update the security groups on the EC2 instance to allow access to and from the S3 IP range only. Configure an S3 bucket policy that allows communication from the source public IP address of the on-premises network only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add a VPC endpoint. Configure endpoint policies on the VPC endpoint to allow access to the required S3 buckets only. Implement an S3 bucket policy that allows communication from the VPC endpoint only.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 715027,
          "date": "Thu 10 Nov 2022 09:07",
          "username": "janvandermerwer",
          "content": "D - Via process of elimination. And allows for more security than other options.<br><br>A , C-whitelist S3 ip range -- seems a bit overkill<br>B - Allows access from all IPs in the VPC - Potentially overly permissive.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 712548,
          "date": "Sun 06 Nov 2022 19:12",
          "username": "resnef",
          "content": "Agreed with D too, We cannot use aws:SourceIp for VPCE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 624987,
          "date": "Thu 30 Jun 2022 04:14",
          "username": "TechX",
          "content": "It's D, no doubt",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 522630,
          "date": "Thu 13 Jan 2022 05:42",
          "username": "m0h3nwahlbergusa",
          "content": "Ans D. Agreed.",
          "upvote_count": "41",
          "selected_answers": ""
        },
        {
          "id": 533707,
          "date": "Thu 27 Jan 2022 13:02",
          "username": "wahlbergusa",
          "content": "Agreed.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#472",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>As a part of building large applications in the AWS Cloud, the Solutions Architect is required to implement the perimeter security protection. Applications running on AWS have the following endpoints:<br>✑ Application Load Balancer<br>✑ Amazon API Gateway regional endpoint<br>✑ Elastic IP address-based EC2 instances.<br>✑ Amazon S3 hosted websites.<br>✑ Classic Load Balancer<br>The Solutions Architect must design a solution to protect all of the listed web front ends and provide the following security capabilities:<br>✑ DDoS protection<br>✑ SQL injection protection<br>✑ IP address whitelist/blacklist<br>✑ HTTP flood protection<br>✑ Bad bot scraper protection<br>How should the Solutions Architect design the solution?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#472",
          "answers": [
            {
              "choice": "<p>A. Deploy AWS WAF and AWS Shield Advanced on all web endpoints. Add AWS WAF rules to enforce the company's requirements.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy Amazon CloudFront in front of all the endpoints. The CloudFront distribution provides perimeter protection. Add AWS Lambda-based automation to provide additional security.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy Amazon CloudFront in front of all the endpoints. Deploy AWS WAF and AWS Shield Advanced. Add AWS WAF rules to enforce the company's requirements. Use AWS Lambda to automate and enhance the security posture.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Secure the endpoints by using network ACLs and security groups and adding rules to enforce the company's requirements. Use AWS Lambda to automatically update the rules.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13783,
          "date": "Mon 27 Sep 2021 05:19",
          "username": "donathon",
          "content": "C<br>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.",
          "upvote_count": "25",
          "selected_answers": ""
        },
        {
          "id": 11591,
          "date": "Mon 20 Sep 2021 09:20",
          "username": "Leedpvnme",
          "content": "C is the correct answer. CloudFront also can solve a partial DDoS attack.yep, cloudfront helps",
          "upvote_count": "134",
          "selected_answers": ""
        },
        {
          "id": 11730,
          "date": "Thu 23 Sep 2021 05:05",
          "username": "dpvnme",
          "content": "yep, cloudfront helps",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 637004,
          "date": "Tue 26 Jul 2022 02:03",
          "username": "hilft",
          "content": "A vs. C<br>C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 596143,
          "date": "Mon 02 May 2022 19:25",
          "username": "tartarus23",
          "content": "C.  Services such as Cloudfront, WAF and Shield Advanced address the security and anti-DDoS protection required for the architecture specifications of the given company.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 518882,
          "date": "Fri 07 Jan 2022 11:10",
          "username": "lucesarano",
          "content": "It is true that a CLB is not supported with WAF but it does indeed support a CloudFront distribution. Hence C is the correct answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493944,
          "date": "Sat 04 Dec 2021 21:26",
          "username": "AzureDP900",
          "content": "C is right and this is simple question",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409778,
          "date": "Wed 03 Nov 2021 23:57",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345076,
          "date": "Mon 01 Nov 2021 18:31",
          "username": "Waiweng",
          "content": "It's C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290735,
          "date": "Mon 01 Nov 2021 08:24",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 287481,
          "date": "Sun 31 Oct 2021 14:06",
          "username": "Ebi",
          "content": "Answer is C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 268204,
          "date": "Sun 31 Oct 2021 13:16",
          "username": "sanjaym",
          "content": "I'll go with C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242568,
          "date": "Sat 23 Oct 2021 07:54",
          "username": "T14102020",
          "content": "Correct answer is C.  All of features",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230057,
          "date": "Thu 21 Oct 2021 15:04",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229889,
          "date": "Fri 15 Oct 2021 05:46",
          "username": "Bulti",
          "content": "C is correct. Although A seems right, not protecting the S3 hosted website using CloudFront and AWS WAF with it is not a good security posture for that web endpoint. So C is the correct answer.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 226825,
          "date": "Mon 11 Oct 2021 22:01",
          "username": "lostri",
          "content": "Answer is C because CLB does not support WAF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 193426,
          "date": "Sun 10 Oct 2021 21:51",
          "username": "pdddddwassbiamgk",
          "content": "What exactly are you going to use the Lambda for in answer C? Answer seems OK, but Lambda does not fit...There are several benefits to using Lambda (Lambda@Edge) for authorization operations like filtering out unauthorized requests before they reach your origin infrastructure.AWS Lambda can check the third-party IP reputation lists hourly for new ranges to block.",
          "upvote_count": "313",
          "selected_answers": ""
        },
        {
          "id": 689545,
          "date": "Sat 08 Oct 2022 19:32",
          "username": "wassb",
          "content": "There are several benefits to using Lambda (Lambda@Edge) for authorization operations like filtering out unauthorized requests before they reach your origin infrastructure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 212544,
          "date": "Mon 11 Oct 2021 14:41",
          "username": "iamgk",
          "content": "AWS Lambda can check the third-party IP reputation lists hourly for new ranges to block.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 181990,
          "date": "Sat 09 Oct 2021 06:13",
          "username": "df1228",
          "content": "I support answer \\\"C\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#473",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has more than 100 AWS accounts, with one VPC per account, that need outbound HTTPS connectivity to the internet. The current design contains one NAT gateway per Availability Zone (AZ) in each VPC.  To reduce costs and obtain information about outbound traffic, management has asked for a new architecture for internet access.<br>Which solution will meet the current needs, and continue to grow as new accounts are provisioned, while reducing costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#473",
          "answers": [
            {
              "choice": "<p>A. Create a transit VPC across two AZs using a third-party routing appliance. Create a VPN connection to each VPC.  Default route internet traffic to the transit VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create multiple hosted-private AWS Direct Connect VIFs, one per account, each with a Direct Connect gateway. Default route internet traffic back to an on- premises router to route to the internet.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a central VPC for outbound internet traffic. Use VPC peering to default route to a set of redundant NAT gateway in the central VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a proxy fleet in a central VPC account. Create an AWS PrivateLink endpoint service in the central VPC.  Use PrivateLink interface for internet connectivity through the proxy fleet.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13869,
          "date": "Sat 25 Sep 2021 07:12",
          "username": "Moonmanhmalucdonathonchaudhwahlbergusa",
          "content": "Answer is: \\\"D\\\"<br>user proxy fleet over PrivateLink. As explained in this AWS website:<br>https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/<br>A: does not provide a full solution, only showing transit VPC, and VPN but without the exiting solution to internet. Also, it is a costly solution.<br>B: This would work, but it will send traffic to on-premise, and the question does not show that the company is having on-premise network!<br>C: can not work, because VPC-Peering can not be used for transit traffic over Net Gateway.best answerMoon is correct.<br>D<br>A: You can use VPC peering, there is no need to use VPN.<br>B: This will create unnecessary load on the Direct Connect and your on premise internet connection.<br>C: You cannot route traffic to a NAT gateway through a VPC peering connection, a Site-to-Site VPN connection, or AWS Direct Connect. A NAT gateway cannot be used by resources on the other side of these connections. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html<br>D: https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/Thank you, D is an answer.No one explained why A is incorrect properly. So let me do that. The method in A is valid, however it is far more expensive than D.  Cause VPN traffic between VPCs traverses internet which is AWS to Internet traffic. And that is more expensive then D. ",
          "upvote_count": "5221042",
          "selected_answers": ""
        },
        {
          "id": 14084,
          "date": "Sun 26 Sep 2021 08:18",
          "username": "manhmalucdonathonchaudhwahlbergusa",
          "content": "best answerMoon is correct.<br>D<br>A: You can use VPC peering, there is no need to use VPN.<br>B: This will create unnecessary load on the Direct Connect and your on premise internet connection.<br>C: You cannot route traffic to a NAT gateway through a VPC peering connection, a Site-to-Site VPN connection, or AWS Direct Connect. A NAT gateway cannot be used by resources on the other side of these connections. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html<br>D: https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/Thank you, D is an answer.No one explained why A is incorrect properly. So let me do that. The method in A is valid, however it is far more expensive than D.  Cause VPN traffic between VPCs traverses internet which is AWS to Internet traffic. And that is more expensive then D. ",
          "upvote_count": "21042",
          "selected_answers": ""
        },
        {
          "id": 14188,
          "date": "Tue 28 Sep 2021 01:30",
          "username": "donathonchaudhwahlbergusa",
          "content": "Moon is correct.<br>D<br>A: You can use VPC peering, there is no need to use VPN.<br>B: This will create unnecessary load on the Direct Connect and your on premise internet connection.<br>C: You cannot route traffic to a NAT gateway through a VPC peering connection, a Site-to-Site VPN connection, or AWS Direct Connect. A NAT gateway cannot be used by resources on the other side of these connections. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html<br>D: https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/Thank you, D is an answer.No one explained why A is incorrect properly. So let me do that. The method in A is valid, however it is far more expensive than D.  Cause VPN traffic between VPCs traverses internet which is AWS to Internet traffic. And that is more expensive then D. ",
          "upvote_count": "1042",
          "selected_answers": ""
        },
        {
          "id": 16197,
          "date": "Mon 04 Oct 2021 00:30",
          "username": "chaudhwahlbergusa",
          "content": "Thank you, D is an answer.No one explained why A is incorrect properly. So let me do that. The method in A is valid, however it is far more expensive than D.  Cause VPN traffic between VPCs traverses internet which is AWS to Internet traffic. And that is more expensive then D. ",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 546322,
          "date": "Sun 13 Feb 2022 09:38",
          "username": "wahlbergusa",
          "content": "No one explained why A is incorrect properly. So let me do that. The method in A is valid, however it is far more expensive than D.  Cause VPN traffic between VPCs traverses internet which is AWS to Internet traffic. And that is more expensive then D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 544296,
          "date": "Thu 10 Feb 2022 05:22",
          "username": "RVivekRVivek",
          "content": "Answer is A. Transit VPC with VPN connection to each VPC and roting table entry complete solution<br>B: No need to send internet traffic to on-premise route<br>C: NAT gateway cannot be shared using VPC-peering<br>D:PrivateLink end point should be used on each VPC that tries to access the central VPC.  If Central VPC has a PrivateLinK that is not enoughmy comment about PrivateLink end point on each VPC is wrong . so the answeris D. <br>Though A can work ,D is easier to setup, sacle<br> and add new VPCs",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 544310,
          "date": "Thu 10 Feb 2022 06:08",
          "username": "RVivek",
          "content": "my comment about PrivateLink end point on each VPC is wrong . so the answeris D. <br>Though A can work ,D is easier to setup, sacle<br> and add new VPCs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513791,
          "date": "Fri 31 Dec 2021 01:51",
          "username": "tkanmani76",
          "content": "Answer - D - <br>Why not C ? Currently there are 100 accounts which could grow in future. However there is a 125 max VPC peering limit which will become a bottleneck. And obviously it will be too messy.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499120,
          "date": "Sat 11 Dec 2021 05:04",
          "username": "cldy",
          "content": "D.  Create a proxy fleet in a central VPC account. Create an AWS PrivateLink endpoint service in the central VPC.  Use PrivateLink interface for internet connectivity through the proxy fleet.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493946,
          "date": "Sat 04 Dec 2021 21:28",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 437860,
          "date": "Sat 06 Nov 2021 00:38",
          "username": "tgv",
          "content": "DDD<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409783,
          "date": "Thu 04 Nov 2021 16:15",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345080,
          "date": "Wed 03 Nov 2021 15:37",
          "username": "Waiweng",
          "content": "It's D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 342417,
          "date": "Wed 03 Nov 2021 07:27",
          "username": "01037",
          "content": "D. <br>I think A also works, but it needs more work",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 333993,
          "date": "Tue 02 Nov 2021 08:38",
          "username": "Amitv2706",
          "content": "Answer is D. <br><br>Why not C?<br>You cannot route traffic to a NAT gateway through a VPC peering connection, a Site-to-Site VPN connection, or AWS Direct Connect. A NAT gateway cannot be used by resources on the other side of these connections.<br>https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 307130,
          "date": "Sat 30 Oct 2021 22:10",
          "username": "ajeeshb",
          "content": "Yes, the answer should be D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290737,
          "date": "Sat 30 Oct 2021 13:36",
          "username": "Kian1",
          "content": "D a proxy fleet",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281633,
          "date": "Fri 29 Oct 2021 01:31",
          "username": "Ebi",
          "content": "I go with D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242571,
          "date": "Tue 26 Oct 2021 14:48",
          "username": "T14102020",
          "content": "Correct answer is D. <br>NAT-Gateway can not be used through VPC peering",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230059,
          "date": "Fri 22 Oct 2021 02:52",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 229892,
          "date": "Fri 22 Oct 2021 02:07",
          "username": "Bulti",
          "content": "D is correct. C is incorrect due to the limitations of using NAT Gateway via VPC-to-VPC peering.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 149683,
          "date": "Tue 19 Oct 2021 06:52",
          "username": "fullaws",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#474",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an e-commerce platform with front-end and e-commerce tiers. Both tiers run on LAMP stacks with the front-end instances running behind a load balancing appliance that has a virtual offering on AWS. Currently, the Operations team uses SSH to log in to the instances to maintain patches and address other concerns. The platform has recently been the target of multiple attacks, including:<br>✑ A DDoS attack.<br>✑ An SQL injection attack.<br>✑ Several successful dictionary attacks on SSH accounts on the web servers.<br>The company wants to improve the security of the e-commerce platform by migrating to AWS. The company's Solutions Architects have decided to use the following approach:<br>✑ Code review the existing application and fix any SQL injection issues.<br>✑ Migrate the web application to AWS and leverage the latest AWS Linux AMI to address initial security patching.<br>✑ Install AWS Systems Manager to manage patching and allow the system administrators to run commands on all instances, as needed. all<br>What additional steps will address<br>of the identified attack types while providing high availability and minimizing risk?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#474",
          "answers": [
            {
              "choice": "<p>A. Enable SSH access to the Amazon EC2 instances using a security group that limits access to specific IPs. Migrate on-premises MySQL to Amazon RDS Multi- AZ. Install the third-party load balancer from the AWS Marketplace and migrate the existing rules to the load balancer's AWS instances. Enable AWS Shield Standard for DDoS protection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Disable SSH access to the Amazon EC2 instances. Migrate on-premises MySQL to Amazon RDS Multi-AZ. Leverage an Elastic Load Balancer to spread the load and enable AWS Shield Advanced for protection. Add an Amazon CloudFront distribution in front of the website. Enable AWS WAF on the distribution to manage the rules.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable SSH access to the Amazon EC2 instances through a bastion host secured by limiting access to specific IP addresses. Migrate on-premises MySQL to a self-managed EC2 instance. Leverage an AWS Elastic Load Balancer to spread the load and enable AWS Shield Standard for DDoS protection. Add an Amazon CloudFront distribution in front of the website.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Disable SSH access to the EC2 instances. Migrate on-premises MySQL to Amazon RDS Single-AZ. Leverage an AWS Elastic Load Balancer to spread the load. Add an Amazon CloudFront distribution in front of the website. Enable AWS WAF on the distribution to manage the rules.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12558,
          "date": "Mon 27 Sep 2021 10:20",
          "username": "donathonAsds",
          "content": "B<br>A: Does not need third party load balancer.<br>C: SSH should be disabled and commands run from System Manager. SQL needs to be more highly available and not on a single EC2 instance.<br>D: DB should be multi-AZ. DDOS protection needs Shield.Agree on B<br>Additionally and to be more precise : CF gives protection over DDOS through Shield, D only lacks Multi-AZ hence",
          "upvote_count": "214",
          "selected_answers": ""
        },
        {
          "id": 61903,
          "date": "Wed 29 Sep 2021 02:59",
          "username": "Asds",
          "content": "Agree on B<br>Additionally and to be more precise : CF gives protection over DDOS through Shield, D only lacks Multi-AZ hence",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 636285,
          "date": "Mon 25 Jul 2022 01:04",
          "username": "hilft",
          "content": "WAF and SHIELD. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626421,
          "date": "Sun 03 Jul 2022 07:23",
          "username": "aandc",
          "content": "AWS Systems Manager -> can disable SSH<br>D does not mention DDOS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 580490,
          "date": "Mon 04 Apr 2022 01:54",
          "username": "bfal",
          "content": "still on why B is wrong<br>Leverage an Elastic Load Balancer to spread the load ? load of what ? Amazon RDS multi-AZ, would you put LB in front of these, if this doable? or just use DNS to lb?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 580482,
          "date": "Mon 04 Apr 2022 01:32",
          "username": "bfalbfal",
          "content": "How else would you connect to the instance if you disableSSH access ? how would you disable SSH access? Correct answer is C. <br>B is wrong, so AWS WAF will be used to \\\"manage\\\" rules on the distribution??? which rules?I take it back, you can access through AWS session manager, but I still think connecting through the bastion host has mitigated the risk, so C is still correct in my view",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 580486,
          "date": "Mon 04 Apr 2022 01:42",
          "username": "bfal",
          "content": "I take it back, you can access through AWS session manager, but I still think connecting through the bastion host has mitigated the risk, so C is still correct in my view",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 580481,
          "date": "Mon 04 Apr 2022 01:24",
          "username": "bfal",
          "content": "C is correct. Why would you want to disabled ssh access? Best practice is to connect through a bastion host, so do that, and whitelist.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 558326,
          "date": "Mon 28 Feb 2022 21:30",
          "username": "Ni_yot",
          "content": "B for me. Shield advance for DDOS protection and disabling ssh give more protection.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 501201,
          "date": "Tue 14 Dec 2021 09:11",
          "username": "KiraguJohnAkaAka4",
          "content": "I will go with B but how will they maintain the patches if they cannot use SSH to login to the instances?With System Manager? I think it's mentioned in the question, it's one of the actions that they have already taken.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 505132,
          "date": "Sun 19 Dec 2021 23:41",
          "username": "AkaAka4",
          "content": "With System Manager? I think it's mentioned in the question, it's one of the actions that they have already taken.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493947,
          "date": "Sat 04 Dec 2021 21:30",
          "username": "AzureDP900",
          "content": "B is right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413055,
          "date": "Wed 03 Nov 2021 09:05",
          "username": "DeathFrmAbv",
          "content": "This one was way too predictable, only B has both HIGH AVAILABILITY AND ADDITIONAL SECURITY",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409786,
          "date": "Wed 03 Nov 2021 00:22",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 365993,
          "date": "Sun 24 Oct 2021 04:36",
          "username": "ss160700memester",
          "content": "You cannot simply disable SSH.Yeah you can? Just close port 22 and then use session manager to connect instead of SSH",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 405703,
          "date": "Fri 29 Oct 2021 08:17",
          "username": "memester",
          "content": "Yeah you can? Just close port 22 and then use session manager to connect instead of SSH",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 345083,
          "date": "Sat 23 Oct 2021 04:29",
          "username": "Waiweng",
          "content": "It's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 341074,
          "date": "Tue 19 Oct 2021 11:36",
          "username": "01037",
          "content": "B is the correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290742,
          "date": "Wed 13 Oct 2021 10:27",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281636,
          "date": "Mon 11 Oct 2021 10:52",
          "username": "Ebi",
          "content": "B<br>D could be an answer as well, only issue is Single AZ",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 247671,
          "date": "Sun 10 Oct 2021 13:15",
          "username": "binhdx",
          "content": "B forsure",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#475",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a High Performance Computing (HPC) cluster in its on-premises data center, which runs thousands of jobs in parallel for one week every month, processing petabytes of images. The images are stored on a network file server, which is replicated to a disaster recovery site. The on-premises data center has reached capacity and has started to spread the jobs out over the course of the month in order to better utilize the cluster, causing a delay in the job completion.<br>The company has asked its Solutions Architect to design a cost-effective solution on AWS to scale beyond the current capacity of 5,000 cores and 10 petabytes of data. The solution must require the least amount of management overhead and maintain the current level of durability.<br>Which solution will meet the company's requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#475",
          "answers": [
            {
              "choice": "<p>A. Create a container in the Amazon Elastic Container Registry with the executable file for the job. Use Amazon ECS with Spot Fleet in Auto Scaling groups. Store the raw data in Amazon EBS SC1 volumes and write the output to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon EMR cluster with a combination of On Demand and Reserved Instance Task Nodes that will use Spark to pull data from Amazon S3. Use Amazon DynamoDB to maintain a list of jobs that need to be processed by the Amazon EMR cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the raw data in Amazon S3, and use AWS Batch with Managed Compute Environments to create Spot Fleets. Submit jobs to AWS Batch Job Queues to pull down objects from Amazon S3 onto Amazon EBS volumes for temporary storage to be processed, and then write the results back to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Submit the list of jobs to be processed to an Amazon SQS to queue the jobs that need to be processed. Create a diversified cluster of Amazon EC2 worker instances using Spot Fleet that will automatically scale based on the queue depth. Use Amazon EFS to store all the data sharing it across all instances in the cluster.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 710110,
          "date": "Wed 02 Nov 2022 23:25",
          "username": "superuser784",
          "content": "AWS Batch is more suitable for this case, AWS EMR if mainly for BigData and ML processing.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 687358,
          "date": "Thu 06 Oct 2022 02:57",
          "username": "ToanVN1988",
          "content": "For cost effective, C is best choice",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 652508,
          "date": "Sat 27 Aug 2022 10:02",
          "username": "aqiao",
          "content": "emr is not for HPC scenario",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 625550,
          "date": "Fri 01 Jul 2022 05:17",
          "username": "TechX",
          "content": "It's B.  The question said that \\\"The solution must be as low-maintenance as possible while maintaining the existing degree of durability\\\"<br>With B, we can maintain durability and also can safe cost with combination of on-demand and reverse instance. Other choices are using Spot Instance which is not satisfy the requirement.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 577915,
          "date": "Wed 30 Mar 2022 00:52",
          "username": "jj22222",
          "content": "more cost effective than B",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 574207,
          "date": "Thu 24 Mar 2022 10:13",
          "username": "gorodetskywassb",
          "content": "Answer is B, <br>A,C,D are using Spot instances, there is guarantee for a cluster with more then 5000 cores with Spot instancesB is not cost effective because you use Reserved Instance one week every month ...",
          "upvote_count": "43",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 689572,
          "date": "Sat 08 Oct 2022 19:57",
          "username": "wassb",
          "content": "B is not cost effective because you use Reserved Instance one week every month ...",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 536191,
          "date": "Sun 30 Jan 2022 14:46",
          "username": "HellGateJonfernz",
          "content": "My answer is B. <br>EBS seems not proper storage choice in C. that's how hpc/batch processing works. you'd have a fleet of ec2 instances processing data stored in a single file storage.",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 689704,
          "date": "Sun 09 Oct 2022 00:25",
          "username": "Jonfernz",
          "content": "that's how hpc/batch processing works. you'd have a fleet of ec2 instances processing data stored in a single file storage.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 521587,
          "date": "Tue 11 Jan 2022 15:46",
          "username": "pititcu667",
          "content": "I would go for c",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 521586,
          "date": "Tue 11 Jan 2022 15:45",
          "username": "pititcu667",
          "content": "I would go for c",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 513796,
          "date": "Fri 31 Dec 2021 02:08",
          "username": "peddyua",
          "content": "C<br>https://aws.amazon.com/blogs/industries/building-a-scalable-image-processing-pipeline-for-image-based-transcriptomics/<br>https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/batch-based-architecture.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 513790,
          "date": "Fri 31 Dec 2021 01:33",
          "username": "tkanmani76lavywahlbergusa",
          "content": "C looks more cost effective over B. They are not asking for cost efficient, it's looking for durability and less maintenance\\\"...with developing a cost-effective solution on AWS...\\\"",
          "upvote_count": "326",
          "selected_answers": ""
        },
        {
          "id": 543159,
          "date": "Tue 08 Feb 2022 16:33",
          "username": "lavywahlbergusa",
          "content": "They are not asking for cost efficient, it's looking for durability and less maintenance\\\"...with developing a cost-effective solution on AWS...\\\"",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 546019,
          "date": "Sat 12 Feb 2022 18:26",
          "username": "wahlbergusa",
          "content": "\\\"...with developing a cost-effective solution on AWS...\\\"",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#476",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company has many business units. Each business unit has multiple AWS accounts for different purposes. The CIO of the company sees that each business unit has data that would be useful to share with other parts of the company. In total, there are about 10 PB of data that needs to be shared with users in<br>1,000 AWS accounts. The data is proprietary, so some of it should only be available to users with specific job types. Some of the data is used for throughput of intensive workloads, such as simulations. The number of accounts changes frequently because of new initiatives, acquisitions, and divestitures.<br>A Solutions Architect has been asked to design a system that will allow for sharing data for use in AWS with all of the employees in the company.<br>Which approach will allow for secure data sharing in scalable way?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#476",
          "answers": [
            {
              "choice": "<p>A. Store the data in a single Amazon S3 bucket. Create an IAM role for every combination of job type and business unit that allows for appropriate read/write access based on object prefixes in the S3 bucket. The roles should have trust policies that allow the business unit's AWS accounts to assume their roles. Use IAM in each business unit's AWS account to prevent them from assuming roles for a different job type. Users get credentials to access the data by using AssumeRole from their business unit's AWS account. Users can then use those credentials with an S3 client.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the data in a single Amazon S3 bucket. Write a bucket policy that uses conditions to grant read and write access where appropriate, based on each user's business unit and job type. Determine the business unit with the AWS account accessing the bucket and the job type with a prefix in the IAM user's name. Users can access data by using IAM credentials from their business unit's AWS account with an S3 client.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the data in a series of Amazon S3 buckets. Create an application running in Amazon EC2 that is integrated with the company's identity provider (IdP) that authenticates users and allows them to download or upload data through the application. The application uses the business unit and job type information in the IdP to control what users can upload and download through the application. The users can access the data through the application's API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the data in a series of Amazon S3 buckets. Create an AWS STS token vending machine that is integrated with the company's identity provider (IdP). When a user logs in, have the token vending machine attach an IAM policy that assumes the role that limits the user's access and/or upload only the data the user is authorized to access. Users can get credentials by authenticating to the token vending machine's website or API and then use those credentials with an S3 client.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 76579,
          "date": "Tue 28 Sep 2021 12:45",
          "username": "SmartSD13pixepe",
          "content": "I am gonna go with D.  <br>I just don't feel comfortable with putting all kinds of data in a single bucket. With options A & B, there is going to be lot of editing of IAM Roles & Bucket policy as add or remove more accounts. Option C is good but what about auditability at CloudTrail if application API is used for data access. At this business level, AD or AWS SSO is must.Token vending machine can be single point of failure.. Going with CTVM can be implemented with LAMBDA - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-saas-tenant-isolation-for-amazon-s3-by-using-an-aws-lambda-token-vending-machine.html<br><br>So Option D is not single point of failure",
          "upvote_count": "2321",
          "selected_answers": ""
        },
        {
          "id": 330656,
          "date": "Wed 27 Oct 2021 22:13",
          "username": "SD13pixepe",
          "content": "Token vending machine can be single point of failure.. Going with CTVM can be implemented with LAMBDA - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-saas-tenant-isolation-for-amazon-s3-by-using-an-aws-lambda-token-vending-machine.html<br><br>So Option D is not single point of failure",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 709598,
          "date": "Wed 02 Nov 2022 04:15",
          "username": "pixepe",
          "content": "TVM can be implemented with LAMBDA - https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-saas-tenant-isolation-for-amazon-s3-by-using-an-aws-lambda-token-vending-machine.html<br><br>So Option D is not single point of failure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 12694,
          "date": "Tue 21 Sep 2021 19:12",
          "username": "donathonkadevKonnondirectconnectSadioManeJapstiffanny",
          "content": "B<br>A: Not scalable. Remember this company has 1000 accounts.<br>C: How would users be easily access the files using the application’s API?<br>D: STS are used for web based identity like Google or Facebook and not used for IDP. https://aws.amazon.com/blogs/mobile/simplifying-token-vending-machine-deployment-with-aws-cloudformation/\\\"Store the data in a single Amazon S3 bucket\\\", Nope, exclude Answer with single bucketWith SAML, STS can work with IdP. <br>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.htmlI will go with D.  A,B store the data in a single S3 bucket. Note that the questions says petabytes of data. A single S3 bucket can only store 5 terabytes of data, so a series of buckets will be needed.The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes.A bucket can contain unlimited amounts of dataMax IAM user to attch is 20",
          "upvote_count": "14182341",
          "selected_answers": ""
        },
        {
          "id": 651143,
          "date": "Wed 24 Aug 2022 09:37",
          "username": "kadev",
          "content": "\\\"Store the data in a single Amazon S3 bucket\\\", Nope, exclude Answer with single bucket",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 80633,
          "date": "Sat 02 Oct 2021 12:29",
          "username": "Konnon",
          "content": "With SAML, STS can work with IdP. <br>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 151925,
          "date": "Tue 12 Oct 2021 08:44",
          "username": "directconnectSadioManeJaps",
          "content": "I will go with D.  A,B store the data in a single S3 bucket. Note that the questions says petabytes of data. A single S3 bucket can only store 5 terabytes of data, so a series of buckets will be needed.The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes.A bucket can contain unlimited amounts of data",
          "upvote_count": "234",
          "selected_answers": ""
        },
        {
          "id": 177646,
          "date": "Thu 14 Oct 2021 09:45",
          "username": "SadioMane",
          "content": "The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 412371,
          "date": "Mon 01 Nov 2021 11:14",
          "username": "Japs",
          "content": "A bucket can contain unlimited amounts of data",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 414021,
          "date": "Fri 05 Nov 2021 14:25",
          "username": "tiffanny",
          "content": "Max IAM user to attch is 20",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 681478,
          "date": "Wed 28 Sep 2022 09:34",
          "username": "tomosabc1",
          "content": "The answer is D. <br>Thanks for bobsmith2000's explanation.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 608173,
          "date": "Fri 27 May 2022 20:47",
          "username": "redipaCal88",
          "content": "Max S3 bucket size is 5TB.  A and B cannot be correct because they want to store 10PB in a SINGLE bucket.A bucket has unlimited storage , the 5 TB max is for each object",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 707883,
          "date": "Sun 30 Oct 2022 16:15",
          "username": "Cal88",
          "content": "A bucket has unlimited storage , the 5 TB max is for each object",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 600759,
          "date": "Thu 12 May 2022 19:52",
          "username": "bobsmith2000",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-saas-tenant-isolation-for-amazon-s3-by-using-an-aws-lambda-token-vending-machine.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 598772,
          "date": "Mon 09 May 2022 02:07",
          "username": "user0001",
          "content": "A , because The roles should have trust policies that allow the business unit's AWS accounts to assume their roles<br>you can not share s3 without trust relation",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 567731,
          "date": "Mon 14 Mar 2022 16:18",
          "username": "syscao",
          "content": "B is perfect. key word is prefix<br>D too many policies to manage",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 549061,
          "date": "Thu 17 Feb 2022 02:22",
          "username": "jyrajan69",
          "content": "Based on number of accounts and the size of Data and the following link (https://aws.amazon.com/blogs/apn/isolating-saas-tenants-with-dynamically-generated-iam-policies/), my answer for this will be D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 484025,
          "date": "Mon 22 Nov 2021 10:15",
          "username": "Kopa",
          "content": "If you see the question is about role so the proper answer seems D, also B is lot more complex and not so scalable.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 446710,
          "date": "Sun 07 Nov 2021 11:55",
          "username": "AWSum1",
          "content": "D <br><br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-saas-tenant-isolation-for-amazon-s3-by-using-an-aws-lambda-token-vending-machine.html",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 413758,
          "date": "Fri 05 Nov 2021 11:41",
          "username": "DerekKey",
          "content": "10 PB of data - users in 1,000 AWS accounts<br>A wrong - \\\"an IAM role for every combination of job type and business unit\\\"<br>B wrong - \\\"write a bucket policy that uses conditions to grant read and write access where appropriate, based on each user's business unit and job type\\\" & \\\"with a prefix in the IAM user's name\\\"<br>C wrong - a single point of failure and bottleneck<br>D correct - I don't see any other way to manage such a huge environment BTW. https://aws.amazon.com/blogs/apn/isolating-saas-tenants-with-dynamically-generated-iam-policies/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 413068,
          "date": "Fri 05 Nov 2021 11:06",
          "username": "DeathFrmAbv",
          "content": "Going with D as this is the most scalable solution",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409796,
          "date": "Sat 30 Oct 2021 06:31",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406849,
          "date": "Thu 28 Oct 2021 01:24",
          "username": "Akhil254",
          "content": "B Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 314749,
          "date": "Wed 27 Oct 2021 08:57",
          "username": "Pupu86",
          "content": "The question has clearly indicated the permutations of BU and AWS accounts and the key objectives is scalability & clarity of purpose for various amount of data shared (10PB). Having a different S3 buckets resolve the clarify of purpose for various data type shared. Crafting a STS solutions mitigates the permutations of user accounts * IAM roles = scalability",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290900,
          "date": "Wed 27 Oct 2021 07:39",
          "username": "Kian1",
          "content": "going with D, STS API",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 287490,
          "date": "Tue 26 Oct 2021 23:37",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#477",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company's security policy states that privileges and network permissions must be configured according to best practice, using least privilege.<br>A Solutions Architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.<br>What steps are required after the deployment to meet the requirements? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#477",
          "answers": [
            {
              "choice": "<p>A. Create tasks using the bridge network mode.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create tasks using the awsvpc network mode.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Apply security groups to the tasks, and use IAM roles for tasks to access other resources.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11570,
          "date": "Sun 19 Sep 2021 22:13",
          "username": "huhupai",
          "content": "I would go for B, E.  <br>https://amazonaws-china.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/<br>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 12695,
          "date": "Tue 21 Sep 2021 03:12",
          "username": "donathondonathon",
          "content": "Agree BE: With the default bridge network mode, containers on an instance are connected to each other using the docker0 bridge. This means you cannot address these containers with the IP address allocated by Docker (it’s allocated from a pool of locally scoped addresses), nor can you enforce finely grained network ACLs and firewall rules. Instead, containers are addressable in your VPC by the combination of the IP address of the primary elastic network interface of the instance, and the host port to which they are mapped (either via static or dynamic port mapping). Also, because a single elastic network interface is shared by multiple containers, it can be difficult to create easily understandable network policies for each container. The awsvpc networking mode addresses these issues by provisioning elastic network interfaces on a per-task basis. Hence, containers no longer share or contend use these resources.",
          "upvote_count": "1410",
          "selected_answers": ""
        },
        {
          "id": 13784,
          "date": "Tue 28 Sep 2021 17:57",
          "username": "donathon",
          "content": ": With the default bridge network mode, containers on an instance are connected to each other using the docker0 bridge. This means you cannot address these containers with the IP address allocated by Docker (it’s allocated from a pool of locally scoped addresses), nor can you enforce finely grained network ACLs and firewall rules. Instead, containers are addressable in your VPC by the combination of the IP address of the primary elastic network interface of the instance, and the host port to which they are mapped (either via static or dynamic port mapping). Also, because a single elastic network interface is shared by multiple containers, it can be difficult to create easily understandable network policies for each container. The awsvpc networking mode addresses these issues by provisioning elastic network interfaces on a per-task basis. Hence, containers no longer share or contend use these resources.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 553016,
          "date": "Mon 21 Feb 2022 17:33",
          "username": "Ni_yot",
          "content": "B and E looks good",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493955,
          "date": "Sat 04 Dec 2021 21:50",
          "username": "AzureDP900",
          "content": "I agree with B,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 447894,
          "date": "Sat 30 Oct 2021 18:09",
          "username": "moon2351",
          "content": "Answer is B&E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409798,
          "date": "Fri 29 Oct 2021 04:13",
          "username": "WhyIronMan",
          "content": "I'll go with B,E",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 290932,
          "date": "Thu 28 Oct 2021 18:13",
          "username": "Kian1",
          "content": "only BE. ..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282402,
          "date": "Sat 23 Oct 2021 21:25",
          "username": "Ebi",
          "content": "BE are correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244102,
          "date": "Wed 20 Oct 2021 12:59",
          "username": "petebear55",
          "content": "BE AND E. .. GOOD GUESS BY ME ;)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242712,
          "date": "Tue 19 Oct 2021 15:19",
          "username": "T14102020",
          "content": "Correct answer BE.  Bridge and role",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230162,
          "date": "Tue 19 Oct 2021 10:49",
          "username": "Bulti",
          "content": "B & E is the right answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230068,
          "date": "Mon 18 Oct 2021 07:28",
          "username": "jackdryan",
          "content": "I'll go with B,E",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 149742,
          "date": "Sat 16 Oct 2021 21:39",
          "username": "fullaws",
          "content": "B and E, awsvpc & task role (not task execution role)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 138194,
          "date": "Fri 15 Oct 2021 23:34",
          "username": "noisonnoiton",
          "content": "B,E acceptable<br>Amazon EC2 Container Service Task Role",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 134460,
          "date": "Tue 12 Oct 2021 02:12",
          "username": "NikkyDicky",
          "content": "BE for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 110332,
          "date": "Sat 09 Oct 2021 13:15",
          "username": "mat2020",
          "content": "Agree B & E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 51070,
          "date": "Sat 02 Oct 2021 20:35",
          "username": "amog",
          "content": "Should be B,E as the link below",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#478",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating its marketing website and content management system from an on-premises data center to AWS. The company wants the AWS application to be deployed in a VPC with Amazon EC2 instances used for the web servers and an Amazon RDS instance for the database.<br>The company has a runbook document that describes the installation process of the on-premises system. The company would like to base the AWS system on the processes referenced in the runbook document. The runbook document describes the installation and configuration of the operating systems, network settings, the website, and content management system software on the servers. After the migration is complete, the company wants to be able to make changes quickly to take advantage of other AWS features.<br>How can the application and environment be deployed and automated in AWS, while allowing for future changes?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#478",
          "answers": [
            {
              "choice": "<p>A. Update the runbook to describe how to create the VPC, the EC2 instances, and the RDS instance for the application by using the AWS Console. Make sure that the rest of the steps in the runbook are updated to reflect any changes that may come from the AWS migration.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Write a Python script that uses the AWS API to create the VPC, the EC2 instances, and the RDS instance for the application. Write shell scripts that implement the rest of the steps in the runbook. Have the Python script copy and run the shell scripts on the newly created instances to complete the installation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Write an AWS CloudFormation template that creates the VPC, the EC2 instances, and the RDS instance for the application. Ensure that the rest of the steps in the runbook are updated to reflect any changes that may come from the AWS migration.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Write an AWS CloudFormation template that creates the VPC, the EC2 instances, and the RDS instance for the application. Include EC2 user data in the AWS CloudFormation template to install and configure the software.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12696,
          "date": "Fri 24 Sep 2021 09:26",
          "username": "donathon",
          "content": "D<br>A: This is not automated.<br>B: This cannot be easily re-used for future.<br>C: The runbook is still considered manual configuration. Only D is fully automated.",
          "upvote_count": "24",
          "selected_answers": ""
        },
        {
          "id": 13831,
          "date": "Mon 27 Sep 2021 16:43",
          "username": "Moon",
          "content": "I do support answer \\\"D\\\".<br>The runbook is having the \\\"Website, and content management software on the servers\\\". That means, the installation and configuration is required in the CloudFormation.<br>C: The runbook does not need to have migration configuration, but the current configuration.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 709073,
          "date": "Tue 01 Nov 2022 12:39",
          "username": "Blair77",
          "content": "DDD for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 533255,
          "date": "Wed 26 Jan 2022 21:18",
          "username": "Ni_yot",
          "content": "D for me. full automation achieved here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499004,
          "date": "Sat 11 Dec 2021 00:35",
          "username": "challenger1",
          "content": "My answer: D<br>D is the only fully automated option",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493958,
          "date": "Sat 04 Dec 2021 21:54",
          "username": "AzureDP900",
          "content": "agree with D as others commented",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 409804,
          "date": "Sun 07 Nov 2021 07:54",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290938,
          "date": "Sat 06 Nov 2021 04:48",
          "username": "Kian1",
          "content": "Going with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282403,
          "date": "Sat 06 Nov 2021 01:38",
          "username": "Ebi",
          "content": "D is the answer",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 268244,
          "date": "Wed 03 Nov 2021 11:04",
          "username": "sanjaym",
          "content": "D without doubt.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 251473,
          "date": "Wed 03 Nov 2021 06:49",
          "username": "01037",
          "content": "D. <br>Runbook doesn't help with automation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242810,
          "date": "Wed 03 Nov 2021 00:26",
          "username": "T14102020",
          "content": "D is correct answer.CloudFormation andinclude EC2 user data",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230169,
          "date": "Sat 30 Oct 2021 16:21",
          "username": "Bulti",
          "content": "D is the right answer. Always use userdata to configure the instances as per the steps defined in the runbook.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230069,
          "date": "Fri 22 Oct 2021 21:38",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 152090,
          "date": "Mon 18 Oct 2021 00:39",
          "username": "gd1",
          "content": "Dedicated circuit takes 3 months to 5 months. D can not be the answer. - C looks logical.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 149780,
          "date": "Sun 17 Oct 2021 17:10",
          "username": "fullaws",
          "content": "D is corrcet",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 134462,
          "date": "Sat 16 Oct 2021 08:40",
          "username": "NikkyDicky",
          "content": "D makes sense",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#479",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is adding a new approved external vendor that only supports IPv6 connectivity. The company's backend systems sit in the private subnet of an<br>Amazon VPC.  The company uses a NAT gateway to allow these systems to communicate with external vendors over IPv4. Company policy requires systems that communicate with external vendors to use a security group that limits access to only approved external vendors. The virtual private cloud (VPC) uses the default network ACL.<br>The Systems Operator successfully assigns IPv6 addresses to each of the backend systems. The Systems Operator also updates the outbound security group to include the IPv6 CIDR of the external vendor (destination). The systems within the VPC are able to ping one another successfully over IPv6. However, these systems are unable to communicate with the external vendor.<br>What changes are required to enable communication with the external vendor?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#479",
          "answers": [
            {
              "choice": "<p>A. Create an IPv6 NAT instance. Add a route for destination 0.0.0.0/0 pointing to the NAT instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable IPv6 on the NAT gateway. Add a route for destination ::/0 pointing to the NAT gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable IPv6 on the internet gateway. Add a route for destination 0.0.0.0/0 pointing to the IGW.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an egress-only internet gateway. Add a route for destination ::/0 pointing to the gateway.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12697,
          "date": "Fri 24 Sep 2021 19:44",
          "username": "donathon",
          "content": "D<br>NAT gateways are not supported for IPv6 traffic—use an egress-only internet gateway instead.<br>https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html",
          "upvote_count": "30",
          "selected_answers": ""
        },
        {
          "id": 11742,
          "date": "Wed 22 Sep 2021 20:11",
          "username": "dpvnme",
          "content": "D.  ipv6 address are public, no need for NAT",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 637693,
          "date": "Wed 27 Jul 2022 02:08",
          "username": "hilft",
          "content": "D.  ipv6 are public no need for NAT. EGRESS ONLY.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 613069,
          "date": "Wed 08 Jun 2022 06:50",
          "username": "Anhdd",
          "content": "D, easy one. Long question but look at the answer will immediately take the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 499293,
          "date": "Sat 11 Dec 2021 10:55",
          "username": "cldy",
          "content": "D.  Create an egress-only internet gateway. Add a route for destination ::/0 pointing to the gateway.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493961,
          "date": "Sat 04 Dec 2021 21:57",
          "username": "AzureDP900",
          "content": "D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 490364,
          "date": "Tue 30 Nov 2021 03:53",
          "username": "acloudguru",
          "content": "hope i can have such simple question in my exam",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 409811,
          "date": "Thu 04 Nov 2021 05:13",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 314781,
          "date": "Wed 03 Nov 2021 23:14",
          "username": "Pupu86",
          "content": "Pertaining to outbound traffic for IPv6, it is always related to a egress-only internet gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290946,
          "date": "Fri 22 Oct 2021 16:04",
          "username": "Kian1",
          "content": "D is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282404,
          "date": "Tue 19 Oct 2021 06:27",
          "username": "Ebi",
          "content": "D is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 268245,
          "date": "Mon 18 Oct 2021 01:48",
          "username": "sanjaym",
          "content": "D.  No second thought.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242811,
          "date": "Wed 13 Oct 2021 06:42",
          "username": "T14102020",
          "content": "D is correct. egress-only internet gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230172,
          "date": "Mon 04 Oct 2021 22:40",
          "username": "Bulti",
          "content": "D is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230072,
          "date": "Sun 03 Oct 2021 07:40",
          "username": "jackdryan",
          "content": "I'll go with D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 182725,
          "date": "Fri 01 Oct 2021 03:33",
          "username": "ipindado2020",
          "content": "D for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 149754,
          "date": "Fri 01 Oct 2021 00:14",
          "username": "fullaws",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#480",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.<br>Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#480",
          "answers": [
            {
              "choice": "<p>A. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12702,
          "date": "Mon 20 Sep 2021 22:35",
          "username": "donathonchallenger1",
          "content": "B<br>A\\C\\D: Would not solve the problem as the bottleneck is on the DB.  Amazon ELB is able to handle the vast majority of use cases for our customers without requiring \\\"pre-warming\\\" (configuring the load balancer to have the appropriate level of capacity based on expected traffic). In certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic, we recommend that you contact us to have your load balancer \\\"pre-warmed\\\". We will then configure the load balancer to have the appropriate level of capacity based on the traffic that you expect. We will need to know the start and end dates of your tests or expected flash traffic, the expected request rate per second and the total size of the typical request/response that you will be testing.\\\"The bottleneck is on the DB\\\" - well said.",
          "upvote_count": "341",
          "selected_answers": ""
        },
        {
          "id": 504491,
          "date": "Sat 18 Dec 2021 23:28",
          "username": "challenger1",
          "content": "\\\"The bottleneck is on the DB\\\" - well said.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13811,
          "date": "Mon 20 Sep 2021 23:11",
          "username": "Moon",
          "content": "I support \\\"B\\\" answer.<br>A: is not appropriate as the pre-warming ELB requires to contact AWS, and that is recommended if the traffic is expecting to have sudden increase in 5 minutes duration.<br>C: not practical.<br>D: does not add much enhancement. Plus the question never talked about snapshots!",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 635781,
          "date": "Sun 24 Jul 2022 00:49",
          "username": "hilft",
          "content": "I'm also going for B. <br><br>with the LEAST amount of performance degradation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 626398,
          "date": "Sun 03 Jul 2022 05:49",
          "username": "aandc",
          "content": "D is based on metric, still could have performance degradation. \\\"LEAST amount of performance degradation\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 537839,
          "date": "Tue 01 Feb 2022 13:29",
          "username": "HellGate",
          "content": "C<br>B and D need too much manual works and there is still possibility of failure by unexpected events.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 534540,
          "date": "Fri 28 Jan 2022 09:36",
          "username": "zoliv",
          "content": "B; month-end reporting == READ REPLICAS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 521959,
          "date": "Wed 12 Jan 2022 08:27",
          "username": "pititcu667",
          "content": "Which of the following actions would enable the database to manage the month-end load with the LEAST amount of performance degradation?For me this is key moving all the reads to a read replica will be highly efficient .",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 513819,
          "date": "Fri 31 Dec 2021 02:57",
          "username": "peddyua",
          "content": "B is an overkill, solution needs to solve the issue for the last 3 days that's why C seems more relevant, you can test it in stage to know how much time it takes to switch EBS so then you can schedule it ahead of time. Plus read replica comes with a cost and the rest of the time it won't do anything.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 513785,
          "date": "Fri 31 Dec 2021 01:08",
          "username": "tkanmani76",
          "content": "Option C is right - Refer https://aws.amazon.com/blogs/storage/automating-amazon-ebs-volume-resizing-with-aws-step-functions-and-aws-systems-manager/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493964,
          "date": "Sat 04 Dec 2021 22:01",
          "username": "AzureDP900",
          "content": "last three days of each month owing to month-end reporting is key here in this question, I will go with B.  C doesn't make any sense to me",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 456293,
          "date": "Sun 07 Nov 2021 15:04",
          "username": "StelSen",
          "content": "Both B & C technically will work and satisfies the requirement. But I will choose C.  Because we already know high volumes are just last 3 days. So, I will execute the lambda on 26th 11pm to change it to IOPS and run the lambda again on 1st 6am of the month to revert to original. Just bare in mind, adding read replicas comes with heavy cost and no use to do this just for 3 days high usage.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 435672,
          "date": "Thu 04 Nov 2021 06:09",
          "username": "tgv",
          "content": "CCC:<br>---<br>\\\"You can now increase volume size, adjust performance, or change the volume type while the volume is in use. You can continue to use your application while the change takes effect.<br><br>Spiking Demand – You are running a relational database on a Provisioned IOPS volume that is set to handle a moderate amount of traffic during the month, with a 10x spike in traffic during the final three days of each month due to month-end processing. You can use Elastic Volumes to dial up the provisioning in order to handle the spike, and then dial it down afterward.\\\" --> https://aws.amazon.com/blogs/aws/amazon-ebs-update-new-elastic-volumes-change-everything/<br>----<br>Option C refers to the attached EBS volumes (not EC2 instances)<br>While option B might work as well, I don't see the need of creating several additional read replicas to be used just in the final three days of the month. AUrora MySQL with Auto-Scaling Read Replicas would have been nice",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 434448,
          "date": "Tue 02 Nov 2021 01:48",
          "username": "student22student22",
          "content": "C<br>This will handle the spike.Changing to B",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 455683,
          "date": "Sat 06 Nov 2021 01:07",
          "username": "student22",
          "content": "Changing to B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 429939,
          "date": "Mon 01 Nov 2021 10:10",
          "username": "Kopa",
          "content": "after reading all comments i choose C as from the link<br>https://aws.amazon.com/blogs/aws/amazon-ebs-update-new-elastic-volumes-change-everything/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413770,
          "date": "Fri 29 Oct 2021 08:49",
          "username": "DerekKeyTiredDadStelSen",
          "content": "C correct - Amazon EBS Elastic Volumes - By using Amazon CloudWatch with AWS Lambda you can automate volume changes to meet the changing needs of your applications.But that seems like a reactive approach and would have atleast some impact on the performance. We want LEAST impact on the performance and hence read replicas should be the solutionIt can be proactive. We already know last 3 days are high vol. So, I will execute the lambda on 26th 11pm to change it to IOPS and run the lambda again on 1st of the month to decrease. Just bare in mind, adding read replicas comes with heavy cost and no use to do this just for 3 days high usage.",
          "upvote_count": "312",
          "selected_answers": ""
        },
        {
          "id": 414257,
          "date": "Fri 29 Oct 2021 13:06",
          "username": "TiredDadStelSen",
          "content": "But that seems like a reactive approach and would have atleast some impact on the performance. We want LEAST impact on the performance and hence read replicas should be the solutionIt can be proactive. We already know last 3 days are high vol. So, I will execute the lambda on 26th 11pm to change it to IOPS and run the lambda again on 1st of the month to decrease. Just bare in mind, adding read replicas comes with heavy cost and no use to do this just for 3 days high usage.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 456291,
          "date": "Sun 07 Nov 2021 00:53",
          "username": "StelSen",
          "content": "It can be proactive. We already know last 3 days are high vol. So, I will execute the lambda on 26th 11pm to change it to IOPS and run the lambda again on 1st of the month to decrease. Just bare in mind, adding read replicas comes with heavy cost and no use to do this just for 3 days high usage.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 409814,
          "date": "Fri 29 Oct 2021 06:15",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 345764,
          "date": "Wed 27 Oct 2021 18:56",
          "username": "blackgamer",
          "content": "No option is a good solution here. B is more relevant compared to others and I will go with B. ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#481",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing the storage layer for a data warehousing application. The data files are large, but they have statically placed metadata at the beginning of each file that describes the size and placement of the file's index. The data files are read in by a fleet of Amazon EC2 instances that store the index size, index location, and other category information about the data file in a database. That database is used by Amazon EMR to group files together for deeper analysis.<br>What would be the MOST cost-effective, high availability storage solution for this workflow?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#481",
          "answers": [
            {
              "choice": "<p>A. Store the data files in Amazon S3 and use Range GET for each file's metadata, then index the relevant data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the data files in Amazon EFS mounted by the EC2 fleet and EMR nodes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the data files on Amazon EBS volumes and allow the EC2 fleet and EMR to mount and unmount the volumes where they are needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the content of the data files in Amazon DynamoDB tables with the metadata, index, and data as their own keys.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12706,
          "date": "Sat 25 Sep 2021 23:20",
          "username": "donathon",
          "content": "A<br>Effectively performs a 'ranged' GET request for the part specified. Useful for downloading just a part of an object.<br>https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html<br>B: The maximum throughput you can drive for each NFS client is 250 MB/s. S3 does not have this limit.<br>C: Can only be mounted on a single instance and not scalable.<br>D: The file is too large for Dynamo DB. ",
          "upvote_count": "35",
          "selected_answers": ""
        },
        {
          "id": 11743,
          "date": "Fri 24 Sep 2021 14:40",
          "username": "dpvnme",
          "content": "A is the way",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 686246,
          "date": "Tue 04 Oct 2022 16:27",
          "username": "Ni_yot",
          "content": "I ll go for A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 514429,
          "date": "Sat 01 Jan 2022 09:23",
          "username": "cldy",
          "content": "A: S3 + EMR.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499003,
          "date": "Sat 11 Dec 2021 00:30",
          "username": "challenger1",
          "content": "My Answer: A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410198,
          "date": "Thu 28 Oct 2021 15:33",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345784,
          "date": "Wed 27 Oct 2021 22:06",
          "username": "Waiweng",
          "content": "It's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 314799,
          "date": "Tue 26 Oct 2021 18:58",
          "username": "Pupu86",
          "content": "understanding Range GET from Cloudfront to S3 perspective:<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RangeGETs.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 296374,
          "date": "Mon 25 Oct 2021 01:24",
          "username": "k",
          "content": "ans: A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290956,
          "date": "Sat 23 Oct 2021 05:35",
          "username": "Kian1",
          "content": "will go with A ofc",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 282412,
          "date": "Sat 23 Oct 2021 04:08",
          "username": "Ebi",
          "content": "I go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 268253,
          "date": "Fri 22 Oct 2021 20:22",
          "username": "sanjaym",
          "content": "A for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242822,
          "date": "Wed 20 Oct 2021 23:21",
          "username": "T14102020",
          "content": "Correct answer is A.  S3 and use Range GET",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230508,
          "date": "Mon 18 Oct 2021 19:13",
          "username": "Bulti",
          "content": "A is the answer for storage and indexing of large number of data files to be later processed by EMRFS, S3 is the best choice",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230075,
          "date": "Wed 13 Oct 2021 23:11",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 227062,
          "date": "Wed 13 Oct 2021 02:56",
          "username": "lostrilostri",
          "content": "why not B?correction, EMR does not work with EFS, has to be S3",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 227064,
          "date": "Wed 13 Oct 2021 10:48",
          "username": "lostri",
          "content": "correction, EMR does not work with EFS, has to be S3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 196437,
          "date": "Tue 12 Oct 2021 19:12",
          "username": "Paitan",
          "content": "A is the right choice.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#482",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses an Amazon EMR cluster to process data once a day. The raw data comes from Amazon S3, and the resulting processed data is also stored in<br>Amazon S3. The processing must complete within 4 hours; currently, it only takes 3 hours. However, the processing time is taking 5 to 10 minutes longer each week due to an increasing volume of raw data.<br>The team is also concerned about rising costs as the compute capacity increases. The EMR cluster is currently running on three m3.xlarge instances (one master and two core nodes).<br>Which of the following solutions will reduce costs related to the increasing compute needs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#482",
          "answers": [
            {
              "choice": "<p>A. Add additional task nodes, but have the team purchase an all-upfront convertible Reserved Instance for each additional node to offset the costs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add additional task nodes, but use instance fleets with the master node in on-Demand mode and a mix of On-Demand and Spot Instances for the core and task nodes. Purchase a scheduled Reserved Instance for the master node.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add additional task nodes, but use instance fleets with the master node in Spot mode and a mix of On-Demand and Spot Instances for the core and task nodes. Purchase enough scheduled Reserved Instances to offset the cost of running any On-Demand instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add additional task nodes, but use instance fleets with the master node in On-Demand mode and a mix of On-Demand and Spot Instances for the core and task nodes. Purchase a standard all-upfront Reserved Instance for the master node.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12208,
          "date": "Mon 20 Sep 2021 09:02",
          "username": "donathon",
          "content": "B<br>A: The cost is only reduced for additional nodes and paying for all day instead of scheduled does not make sense.<br>B: The key is Scheduled Reserved Instances. This are usually used for running batch jobs.<br>C: You cannot have spot instances for the master node since there is no guarantee that you can get that instance running.<br>D: All day RI does not make sense and will cost more.",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 345786,
          "date": "Tue 02 Nov 2021 21:58",
          "username": "WaiwengTiredDadtvs",
          "content": "It's D, schedule reserved instance no longer supported<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html\\\"You cannot purchase Scheduled Reserved Instances at this time. AWS does not have any capacity available for Scheduled Reserved Instances or any plans to make it available in the future. To reserve capacity, use On-Demand Capacity Reservations instead. For discounted rates, use Savings Plans.\\\"Agree but these exam question was created long back not sure exam systems are uptodate.",
          "upvote_count": "1322",
          "selected_answers": ""
        },
        {
          "id": 414569,
          "date": "Sat 06 Nov 2021 21:38",
          "username": "TiredDad",
          "content": "\\\"You cannot purchase Scheduled Reserved Instances at this time. AWS does not have any capacity available for Scheduled Reserved Instances or any plans to make it available in the future. To reserve capacity, use On-Demand Capacity Reservations instead. For discounted rates, use Savings Plans.\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 347646,
          "date": "Thu 04 Nov 2021 05:44",
          "username": "tvs",
          "content": "Agree but these exam question was created long back not sure exam systems are uptodate.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 502917,
          "date": "Thu 16 Dec 2021 13:34",
          "username": "Binoj_1985",
          "content": "D - Since scheduled instances are no longer supported. <br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 498309,
          "date": "Fri 10 Dec 2021 06:13",
          "username": "cldy",
          "content": "D.  Add additional task nodes, but use instance fleets with the master node in On-Demand mode and a mix of On-Demand and Spot Instances for the core and task nodes. Purchase a standard all-upfront Reserved Instance for the master node.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493973,
          "date": "Sat 04 Dec 2021 22:40",
          "username": "AzureDP900",
          "content": "B is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413786,
          "date": "Sat 06 Nov 2021 04:16",
          "username": "DerekKey",
          "content": "D correct<br>B wrong - have you noticed that answer B has \\\"on-Demand\\\" instead of \\\"On-Demand\\\"<br>As already said Scheduled Instances are not available except in one region - Ireland.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410203,
          "date": "Sat 06 Nov 2021 02:33",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 326945,
          "date": "Mon 01 Nov 2021 15:12",
          "username": "Sunflyhome",
          "content": "D. <br>1: We do not have any capacity for purchasing Scheduled Reserved Instances or any plans to make it available in the future.<br>2:When you buy Reserved Instances, the larger the upfront payment, the greater the discount. Master is always up.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 314812,
          "date": "Thu 28 Oct 2021 09:36",
          "username": "Pupu86nitinz",
          "content": "I believe this is an old question as scheduled reserve instance is no longer supported. Answer should be D.  As the workload keeps increasing week by week, it also makes sense to pay all upfront cost for reserved instance for the master node (most amount of savings $$$ for something that is guaranteed with no performance change) and leave on-demand instances for core & spot instances for tasks instead.You are right, https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 317644,
          "date": "Sun 31 Oct 2021 12:42",
          "username": "nitinz",
          "content": "You are right, https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 303120,
          "date": "Mon 25 Oct 2021 15:38",
          "username": "awsnoob",
          "content": "should be D, reserve schedule instance is no longer supported",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294931,
          "date": "Sun 24 Oct 2021 02:39",
          "username": "natpilot",
          "content": "why not D? <br>about B, view this link : https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290972,
          "date": "Sat 23 Oct 2021 00:42",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 252345,
          "date": "Fri 22 Oct 2021 21:54",
          "username": "newme",
          "content": "B is correct in this question.<br>But Scheduled Reserved Instances is no longer supported any more.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242838,
          "date": "Fri 22 Oct 2021 20:09",
          "username": "T14102020",
          "content": "B is correct answer. Scheduled reserved instance for master node",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 236887,
          "date": "Mon 18 Oct 2021 17:16",
          "username": "Kau123",
          "content": "B for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230555,
          "date": "Sun 17 Oct 2021 02:46",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230514,
          "date": "Fri 08 Oct 2021 06:41",
          "username": "Bulti",
          "content": "B is correct a scheduled reserved instance for master node will be most cost effective given the workload is handled at a specifuc time of the day.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#483",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building an AWS landing zone and has asked a Solutions Architect to design a multi-account access strategy that will allow hundreds of users to use corporate credentials to access the AWS Console. The company is running a Microsoft Active Directory, and users will use an AWS Direct Connect connection to connect to AWS. The company also wants to be able to federate to third-party services and providers, including custom applications.<br>Which solution meets the requirements by using the LEAST amount of management overhead?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#483",
          "answers": [
            {
              "choice": "<p>A. Connect the Active Directory to AWS by using single sign-on and an Active Directory Federation Services (AD FS) with SAML 2.0, and then configure the Identity Provider (IdP) system to use form-based authentication. Build the AD FS portal page with corporate branding, and integrate third-party applications that support SAML 2.0 as required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a two-way Forest trust relationship between the on-premises Active Directory and the AWS Directory Service. Set up AWS Single Sign-On with AWS Organizations. Use single sign-on integrations for connections with third-party applications.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure single sign-on by connecting the on-premises Active Directory using the AWS Directory Service AD Connector. Enable federation to the AWS services and accounts by using the IAM applications and services linking function. Leverage third-party single sign-on as needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Connect the company's Active Directory to AWS by using AD FS and SAML 2.0. Configure the AD FS claim rule to leverage Regex and a common Active Directory naming convention for the security group to allow federation of all AWS accounts. Leverage third-party single sign-on as needed, and add it to the AD FS server.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 49782,
          "date": "Fri 01 Oct 2021 14:07",
          "username": "sb333Smart",
          "content": "B is the correct answer and fully supports the requirements of the question with very little management. Managing an AD FS implementation is not a LEAST overhead solution. If you have had to support AD FS, you know this. Support for B is here: https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.htmlAgreed. I wasn't sure about two-way relationship. <br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
          "upvote_count": "195",
          "selected_answers": ""
        },
        {
          "id": 76593,
          "date": "Tue 05 Oct 2021 01:23",
          "username": "Smart",
          "content": "Agreed. I wasn't sure about two-way relationship. <br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 12211,
          "date": "Sat 25 Sep 2021 18:19",
          "username": "donathonKopa",
          "content": "D<br>A: Adds complexity to the SSO. https://aws.amazon.com/blogs/security/how-to-implement-a-general-solution-for-federated-apicli-access-using-saml-2-0/<br>B: You will need to manage AWS Directory Service which is additional overhead.<br>C: As an alternative to AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD), AD Connector is an Active Directory proxy for AWS created applications and services only. You configure the proxy to use a specified Active Directory domain. When the application must look up a user or group in Active Directory, AD Connector proxies the request to the directory. Similarly, when a user logs in to the application, AD Connector proxies the authentication request to the directory. There are no third-party applications that work with AD Connector.<br>D: https://aws.amazon.com/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/With AWS SSO, you can enable a highly available SSO service with just a few clicks. There is no additional infrastructure to deploy or AWS account to set up. AWS SSO is a highly available and a completely secure infrastructure that scales to your needs and does not require software or hardware to manage. AWS SSO records all sign-in activity in AWS CloudTrail, giving you the visibility to monitor and audit SSO activity in one place.",
          "upvote_count": "162",
          "selected_answers": ""
        },
        {
          "id": 394525,
          "date": "Sun 31 Oct 2021 16:55",
          "username": "Kopa",
          "content": "With AWS SSO, you can enable a highly available SSO service with just a few clicks. There is no additional infrastructure to deploy or AWS account to set up. AWS SSO is a highly available and a completely secure infrastructure that scales to your needs and does not require software or hardware to manage. AWS SSO records all sign-in activity in AWS CloudTrail, giving you the visibility to monitor and audit SSO activity in one place.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 686249,
          "date": "Tue 04 Oct 2022 16:41",
          "username": "Ni_yot",
          "content": "B sounds good as an option. The AWS AD workshop uses this method.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 625982,
          "date": "Sat 02 Jul 2022 08:40",
          "username": "aandc",
          "content": "B, ADFS means complexity",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 573660,
          "date": "Wed 23 Mar 2022 14:52",
          "username": "gunjan229",
          "content": "Requirement is LEAST amount of administrative overhead. Managed Services AWS Active Directory and SSO will have less Administrative overhead. So B is perfect.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 559799,
          "date": "Thu 03 Mar 2022 04:17",
          "username": "jyrajan69",
          "content": "First you must consider the requirement for a multi-account strategy, before least overhead. Answer D does not involve AWS Organizations, and it involves more complications, so based on all this the answer has to be B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 535224,
          "date": "Sat 29 Jan 2022 07:09",
          "username": "cannottellname",
          "content": "B is corret with least operational overhead. D does not even mention SSO & the article mentioned in the link just connects with 1 AWS account - not multiple.<br><br>AWS SSO is managed and easily linked with new users using Microsoft AD.  B is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 524682,
          "date": "Sun 16 Jan 2022 07:38",
          "username": "peddyua",
          "content": "B doesn't involve SAML 2.0 which is a must. So I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 509927,
          "date": "Mon 27 Dec 2021 02:00",
          "username": "vbal",
          "content": "D is perfect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493974,
          "date": "Sat 04 Dec 2021 22:43",
          "username": "AzureDP900",
          "content": "Create a two-way Forest trust relationship between the on-premises Active Directory and the AWS Directory Service.This is right so my answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 447977,
          "date": "Sat 06 Nov 2021 22:52",
          "username": "nodogoshi",
          "content": "B is minimum management overhead.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 413792,
          "date": "Sat 06 Nov 2021 00:30",
          "username": "DerekKey",
          "content": "B correct - this is a solution that I am using on a daily basis",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 410222,
          "date": "Thu 04 Nov 2021 01:23",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406854,
          "date": "Wed 03 Nov 2021 23:24",
          "username": "Akhil254",
          "content": "B Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345894,
          "date": "Sun 31 Oct 2021 02:53",
          "username": "Waiweng",
          "content": "B is the correct Answer<br>https://aws.amazon.com/solutions/implementations/aws-landing-zone/<br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html<br>https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 325745,
          "date": "Sat 30 Oct 2021 10:22",
          "username": "AJBA",
          "content": "B<br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 294918,
          "date": "Fri 29 Oct 2021 03:06",
          "username": "gpark",
          "content": "B<br>Third party application SSO intergration needs AWS SSO.<br>AWS AD any services cannot support SSO feature.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#484",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Solutions Architect is designing a network solution for a company that has applications running in a data center in Northern Virginia. The applications in the company's data center require predictable performance to applications running in a virtual private cloud (VPC) located in us-east-1, and a secondary VPC in us- west-2 within the same account. The company data center is collocated in an AWS Direct Connect facility that serves the us-east-1 region. The company has already ordered an AWS Direct Connect connection and a cross-connect has been established.<br>Which solution will meet the requirements at the LOWEST cost?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#484",
          "answers": [
            {
              "choice": "<p>A. Provision a Direct Connect gateway and attach the virtual private gateway(VGW) for the VPC in us-east-1 and the VGW for the VPC in us-west-2. Create a private VIF on the Direct Connect connection and associate it to the Direct Connect gateway.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create private VIFs on the Direct Connect connection for each of the company's VPCs in the us-east-1 and us-west-2 regions. Configure the company's data center router to connect directly with the VPCs in those regions via the private VIFs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a transit VPC solution using Amazon EC2-based router instances in the us-east-1 region. Establish IPsec VPN tunnels between the transit routers and virtual private gateways (VGWs) located in the us-east-1 and us-west-2 regions, which are attached to the company's VPCs in those regions. Create a public VIF on the Direct Connect connection and establish IPsec VPN tunnels over the public VIF between the transit routers and the company's data center router.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Order a second Direct Connect connection to a Direct Connect facility with connectivity to the us-west-2 region. Work with a partner to establish a network extension link over dark fiber from the Direct Connect facility to the company's data center. Establish private VIFs on the Direct Connect connections for each of the company's VPCs in the respective regions. Configure the company's data center router to connect directly with the VPCs in those regions via the private VIFs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12220,
          "date": "Sun 26 Sep 2021 01:54",
          "username": "donathon01037wahlbergusahfeng95",
          "content": "A<br>The main difference between A & B is that A attaches the VGW to the Direct Connect then present it via a VIP. B is the other way around which will double to data transfer cost.<br>C: There is already direct connect hence no need for a VPN.<br>D: No need for a second direct connectI understand A (using Direct Connect Gateway) is the recommended way to do the job because less of management.<br>But why does B double the cost.<br>After all the cost depends on how much data transfer between the DC and AWS.<br>A doesn't change the amount of data transfering between DC and AWS, does it?Yes, cost wise it should not be different between A and B.  Direct connect pricing is based on port hour and data transfer out (AWSto external)There could be multiple VPCs in one region. Only one VGW is required (Regional service). Do not think it has anything to do with doubling cost",
          "upvote_count": "22111",
          "selected_answers": ""
        },
        {
          "id": 252382,
          "date": "Sat 16 Oct 2021 03:25",
          "username": "01037wahlbergusahfeng95",
          "content": "I understand A (using Direct Connect Gateway) is the recommended way to do the job because less of management.<br>But why does B double the cost.<br>After all the cost depends on how much data transfer between the DC and AWS.<br>A doesn't change the amount of data transfering between DC and AWS, does it?Yes, cost wise it should not be different between A and B.  Direct connect pricing is based on port hour and data transfer out (AWSto external)There could be multiple VPCs in one region. Only one VGW is required (Regional service). Do not think it has anything to do with doubling cost",
          "upvote_count": "111",
          "selected_answers": ""
        },
        {
          "id": 535725,
          "date": "Sat 29 Jan 2022 22:26",
          "username": "wahlbergusahfeng95",
          "content": "Yes, cost wise it should not be different between A and B.  Direct connect pricing is based on port hour and data transfer out (AWSto external)There could be multiple VPCs in one region. Only one VGW is required (Regional service). Do not think it has anything to do with doubling cost",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 604344,
          "date": "Fri 20 May 2022 12:33",
          "username": "hfeng95",
          "content": "There could be multiple VPCs in one region. Only one VGW is required (Regional service). Do not think it has anything to do with doubling cost",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13804,
          "date": "Sun 26 Sep 2021 17:19",
          "username": "Moon",
          "content": "obviously A. <br>https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 447915,
          "date": "Sat 06 Nov 2021 04:50",
          "username": "moon2351",
          "content": "Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 441204,
          "date": "Tue 02 Nov 2021 15:13",
          "username": "student22Ethan169",
          "content": "A<br><br>Direct Connect Gateway --> Virtual Private Gateway --> Private Virtual Interface (VIF)should be: Customer gataway-->Direct Connect-->Private Virtual Interface (VIF)-->Direct Connect Gateway --> Virtual Private Gateway",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 738475,
          "date": "Thu 08 Dec 2022 01:00",
          "username": "Ethan169",
          "content": "should be: Customer gataway-->Direct Connect-->Private Virtual Interface (VIF)-->Direct Connect Gateway --> Virtual Private Gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437869,
          "date": "Mon 01 Nov 2021 19:02",
          "username": "tgv",
          "content": "AAA<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410227,
          "date": "Mon 01 Nov 2021 05:59",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345790,
          "date": "Sun 31 Oct 2021 01:11",
          "username": "Waiweng",
          "content": "It's A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 314855,
          "date": "Sat 30 Oct 2021 09:11",
          "username": "Pupu86ryu10_09",
          "content": "Answer is definitely A.  Let the US-WEST2 take advantage of the existing DX connection via a direct connect gateway via respective AZ's Virtual Private Gateway (VIF/s).<br><br>B makes no sense.<br><br>C doesn't answer lowest cost<br><br>D is the most expensive optionB makes sense but expensive",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 489421,
          "date": "Sun 28 Nov 2021 21:46",
          "username": "ryu10_09",
          "content": "B makes sense but expensive",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290990,
          "date": "Fri 22 Oct 2021 21:35",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 284834,
          "date": "Mon 18 Oct 2021 15:50",
          "username": "TerrenceCKiroselva",
          "content": "The Direct Connect Gateway is per-region basis. Since those two VPCs were deployed in different regions so that there are 2x Direct Connect Gateways and 2x private VIF connections are required. Therefore, the correct answer shall be B. Direct Connect Gateway is a global component .. hence A. \\\"A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions.\\\" https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html",
          "upvote_count": "221",
          "selected_answers": ""
        },
        {
          "id": 291805,
          "date": "Sat 23 Oct 2021 02:37",
          "username": "Kiro",
          "content": "Direct Connect Gateway is a global component .. hence A. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293021,
          "date": "Sun 24 Oct 2021 05:33",
          "username": "selva",
          "content": "\\\"A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions.\\\" https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281593,
          "date": "Mon 18 Oct 2021 01:49",
          "username": "Ebi",
          "content": "I go with A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 245674,
          "date": "Thu 14 Oct 2021 17:05",
          "username": "petebear55",
          "content": "Yes i'll go for C now having referred to a link further down in the conversation as i think the key here is the words LOWEST cost which normally means VPN in these type of questions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 242854,
          "date": "Wed 13 Oct 2021 23:20",
          "username": "T14102020",
          "content": "Correct answer is A.  attaches the VGW to the Direct Connect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230573,
          "date": "Wed 13 Oct 2021 04:52",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230546,
          "date": "Wed 13 Oct 2021 01:38",
          "username": "Bulti",
          "content": "A is correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230545,
          "date": "Tue 12 Oct 2021 07:37",
          "username": "Bulti",
          "content": "S is correct it is also the standard best practice to do so.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 227870,
          "date": "Sun 10 Oct 2021 11:54",
          "username": "srinivasaselva",
          "content": "B is the right answer. You need multiple VIFs one for each VPC in each region this case.Direct Connect is a regional service, while Direct Connect GW is a global service. Combining both will solve the issue here.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 293023,
          "date": "Fri 29 Oct 2021 17:20",
          "username": "selva",
          "content": "Direct Connect is a regional service, while Direct Connect GW is a global service. Combining both will solve the issue here.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#485",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a web service deployed in the following two AWS Regions: us-west-2 and us-est-1. Each AWS region runs an identical version of the web service.<br>Amazon Route 53 is used to route customers to the AWS Region that has the lowest latency.<br>The company wants to improve the availability of the web service in case an outage occurs in one of the two AWS Regions.<br>A Solutions Architect has recommended that a Route 53 health check be performed. The health check must detect a specific text on an endpoint.<br>What combination of conditions should the endpoint meet to pass the Route 53 health check? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#485",
          "answers": [
            {
              "choice": "<p>A. The endpoint must establish a TCP connection within 10 seconds.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The endpoint must return an HTTP 200 status code.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The endpoint must return an HTTP 2xx or 3xx status code.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. The specific text string must appear within the first 5,120 bytes of the response.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. The endpoint must respond to the request within the number of seconds specified when creating the health check.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12222,
          "date": "Tue 21 Sep 2021 21:21",
          "username": "donathonaksliveswithaws",
          "content": "CD<br>HTTP and HTTPS health checks – Route 53 must be able to establish a TCP connection with the endpoint within four seconds. In addition, the endpoint must respond with an HTTP status code of 2xx or 3xx within two seconds after connecting. After a Route 53 health checker receives the HTTP status code, it must receive the response body from the endpoint within the next two seconds. Route 53 searches the response body for a string that you specify. The string must appear entirely in the first 5,120 bytes of the response body or the endpoint fails the health checkhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-determining-health-of-endpoints.html",
          "upvote_count": "362",
          "selected_answers": ""
        },
        {
          "id": 443625,
          "date": "Sat 06 Nov 2021 13:05",
          "username": "aksliveswithaws",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-determining-health-of-endpoints.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 11112,
          "date": "Mon 20 Sep 2021 08:46",
          "username": "awsec2",
          "content": "cd <br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-creating-values.html#health-checks-creating-values-endpoint",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 514359,
          "date": "Sat 01 Jan 2022 05:37",
          "username": "cldy",
          "content": "C and D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493975,
          "date": "Sat 04 Dec 2021 22:52",
          "username": "AzureDP900",
          "content": "C, D make sense. There is no problem",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447918,
          "date": "Sun 07 Nov 2021 05:33",
          "username": "moon2351",
          "content": "CD is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410237,
          "date": "Wed 03 Nov 2021 21:57",
          "username": "WhyIronMan",
          "content": "I'll go with C,D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345835,
          "date": "Tue 02 Nov 2021 20:22",
          "username": "Waiweng",
          "content": "C,D correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290992,
          "date": "Tue 02 Nov 2021 11:26",
          "username": "Kian1",
          "content": "going with CD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281597,
          "date": "Thu 28 Oct 2021 02:08",
          "username": "Ebi",
          "content": "Answer is CD",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242856,
          "date": "Sun 24 Oct 2021 22:44",
          "username": "T14102020",
          "content": "Correct answer is CD.  5,120 bytes and HTTP 2xx or 3xx",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230584,
          "date": "Sat 23 Oct 2021 13:49",
          "username": "jackdryan",
          "content": "I'll go with C,D",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 230559,
          "date": "Sat 23 Oct 2021 08:29",
          "username": "Bulti",
          "content": "Answer is C and D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 150086,
          "date": "Fri 22 Oct 2021 10:00",
          "username": "fullaws",
          "content": "cd is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 134470,
          "date": "Sun 17 Oct 2021 00:43",
          "username": "NikkyDicky",
          "content": "CD make sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 110370,
          "date": "Wed 13 Oct 2021 00:33",
          "username": "mat2020",
          "content": "Ans: C & D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 105707,
          "date": "Mon 11 Oct 2021 03:02",
          "username": "meenu2225",
          "content": "CD guys",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 95185,
          "date": "Sun 10 Oct 2021 07:11",
          "username": "FreeSwan",
          "content": "C,D are correct.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#486",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company operating a website on AWS requires high levels of scalability, availability, and performance. The company is running a Ruby on Rails application on<br>Amazon EC2. It has a data tier on MySQL 5.6 on Amazon EC2 using 16 TB of Amazon EBS storage Amazon CloudFront is used to cache application content.<br>The Operations team is reporting continuous and unexpected growth of EBS volumes assigned to the MySQL database. The Solutions Architect has been asked to design a highly scalable, highly available, and high-performing solution.<br>Which solution is the MOST cost-effective at scale?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#486",
          "answers": [
            {
              "choice": "<p>A. Implement Multi-AZ and Auto Scaling for all EC2 instances in the current configuration. Ensure that all EC2 instances are purchased as reserved instances. Implement new elastic Amazon EBS volumes for the data tier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Design and implement the Docker-based containerized solution for the application using Amazon ECS. Migrate to an Amazon Aurora MySQL Multi-AZ cluster. Implement storage checks for Aurora MySQL storage utilization and an AWS Lambda function to grow the Aurora MySQL storage, as necessary. Ensure that Multi-AZ architectures are implemented.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Ensure that EC2 instances are right-sized and behind an Elastic Load Balancing load balancer. Implement Auto Scaling with EC2 instances. Ensure that the reserved instances are purchased for fixed capacity and that Auto Scaling instances run on demand. Migrate to an Amazon Aurora MySQL Multi-AZ cluster. Ensure that Multi-AZ architectures are implemented.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Ensure that EC2 instances are right-sized and behind an Elastic Load Balancer. Implement Auto Scaling with EC2 instances. Ensure that Reserved instances are purchased for fixed capacity and that Auto Scaling instances run on demand. Migrate to an Amazon Aurora MySQL Multi-AZ cluster. Implement storage checks for Aurora MySQL storage utilization and an AWS Lambda function to grow Aurora MySQL storage, as necessary. Ensure Multi-AZ architectures are implemented.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12227,
          "date": "Sat 25 Sep 2021 17:35",
          "username": "donathon",
          "content": "C<br>A: Not scalable.<br>B\\D: Aurora will auto grow and does not need Lambda.<br>The minimum storage is 10GB.  Based on your database usage, your Amazon Aurora storage will automatically grow, up to 64 TB, in 10GB increments with no impact to database performance. There is no need to provision storage in advance.",
          "upvote_count": "35",
          "selected_answers": ""
        },
        {
          "id": 46399,
          "date": "Sun 26 Sep 2021 20:10",
          "username": "arunkumarTiredDad",
          "content": "C is the correct answer. <br><br>Aurora volume will grow automatically upto 64 TB.  <br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.htmlupto 128 TB now",
          "upvote_count": "122",
          "selected_answers": ""
        },
        {
          "id": 414680,
          "date": "Thu 04 Nov 2021 02:57",
          "username": "TiredDad",
          "content": "upto 128 TB now",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 514382,
          "date": "Sat 01 Jan 2022 06:26",
          "username": "cldy",
          "content": "C: Aurora volume auto grows upto 128 TB. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 493977,
          "date": "Sat 04 Dec 2021 22:54",
          "username": "AzureDP900",
          "content": "C is perfect answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410242,
          "date": "Tue 02 Nov 2021 00:36",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345836,
          "date": "Sun 31 Oct 2021 02:04",
          "username": "Waiweng",
          "content": "correct answer is C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 290994,
          "date": "Sat 23 Oct 2021 15:58",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281603,
          "date": "Mon 18 Oct 2021 14:22",
          "username": "Ebi",
          "content": "C is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242859,
          "date": "Mon 18 Oct 2021 06:56",
          "username": "T14102020",
          "content": "Correct answer is C.  Aurora without any Lambda checking",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230669,
          "date": "Fri 15 Oct 2021 06:37",
          "username": "Bulti",
          "content": "Answeris C.  Aurora storage scales automatically and yet is cost effective because you are billed for what you use.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230599,
          "date": "Fri 15 Oct 2021 04:41",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 223528,
          "date": "Wed 13 Oct 2021 01:09",
          "username": "kj07",
          "content": "Answer: C because you don't have to manage Aurora storage. It can grow from 10GB to 64TB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 198450,
          "date": "Tue 12 Oct 2021 00:14",
          "username": "Paitan",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 150088,
          "date": "Mon 11 Oct 2021 17:09",
          "username": "fullaws",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 134476,
          "date": "Fri 08 Oct 2021 02:53",
          "username": "NikkyDicky",
          "content": "C.  tricky question on Aurora managed capacity",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 110371,
          "date": "Thu 07 Oct 2021 19:34",
          "username": "mat2020",
          "content": "ans: C is my choice",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 105710,
          "date": "Thu 07 Oct 2021 13:38",
          "username": "meenu2225",
          "content": "C is the best design option",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#487",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>The Security team needs to provide a team of interns with an AWS environment so they can build a serverless video transcoding application. The project will use<br>Amazon S3, AWS Lambda, Amazon API Gateway, Amazon Cognito, Amazon DynamoDB, and Amazon Elastic Transcoder.<br>The interns should be able to create and configure the necessary resources, but they may not have access to create or modify AWS IAM roles. The Solutions<br>Architect creates a policy and attaches it to the interns' group.<br>How should the Security team configure the environment to ensure that the interns are self-sufficient?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#487",
          "answers": [
            {
              "choice": "<p>A. Create a policy that allows creation of project-related resources only. Create roles with required service permissions, which are assumable by the services.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a policy that allows creation of all project-related resources, including roles that allow access only to specified resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create roles with the required service permissions, which are assumable by the services. Have the interns create and use a bastion host to create the project resources in the project subnet only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a policy that allows creation of project-related resources only. Require the interns to raise a request for roles to be created with the Security team. The interns will provide the requirements for the permissions to be set in the role.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 49819,
          "date": "Sat 02 Oct 2021 13:37",
          "username": "sb333",
          "content": "The answer is A.  Interns are given permissions to create project-related resources. They then just have to associate the appropriate roles (with the required service permissions, pre-created by the Security team) as they create the resources. This allows the interns to setup the environment without having to give them permissions to create or modify any roles themselves.",
          "upvote_count": "46",
          "selected_answers": ""
        },
        {
          "id": 12230,
          "date": "Mon 20 Sep 2021 17:02",
          "username": "donathonAmitv2706neshin",
          "content": "B<br>A\\C: This will allow the interns to create and modify other S3 or Lambda resources etc that does not belongs to the project.<br>B: The most restrictive policy is enforced. Assuming Permission boundary is used to restrict access to those specific project related resources.<br>When AWS evaluates the identity-based policies and permissions boundary for a user, the resulting permissions are the intersection of the two categories. That means that when you add a permissions boundary to a user with existing identity-based policies, you might reduce the actions that the user can perform. Alternatively, when you remove a permissions boundary from a user, you might increase the actions they can perform. An explicit deny in either of these policies overrides the allow.<br>D: Not self-sufficient.<br>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.htmlfor B- Do you think interns would be aware of the exact resources/names in advance ?\\\"but they may not have access to create or modify AWS IAM roles\\\"",
          "upvote_count": "1438",
          "selected_answers": ""
        },
        {
          "id": 334446,
          "date": "Thu 28 Oct 2021 08:25",
          "username": "Amitv2706",
          "content": "for B- Do you think interns would be aware of the exact resources/names in advance ?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 31189,
          "date": "Sat 02 Oct 2021 13:01",
          "username": "neshin",
          "content": "\\\"but they may not have access to create or modify AWS IAM roles\\\"",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 711343,
          "date": "Fri 04 Nov 2022 20:55",
          "username": "superuser784",
          "content": "you should always remember this from the Security Best practices in IAM, give only the \\\"least-privilege permissions\\\",so A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 577825,
          "date": "Tue 29 Mar 2022 20:28",
          "username": "jj22222",
          "content": "A.  Create a policy that allows creation of project-related resources only. Create roles with required service permissions, which are assumable by the services.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 493979,
          "date": "Sat 04 Dec 2021 22:58",
          "username": "AzureDP900",
          "content": "A is right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437871,
          "date": "Sat 06 Nov 2021 11:37",
          "username": "tgv",
          "content": "AAA<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413813,
          "date": "Sat 06 Nov 2021 05:30",
          "username": "DerekKey",
          "content": "\\\"they may not have access to create or modify AWS IAM roles\\\" + \\\"interns are self-sufficient\\\"<br>A correct<br>B wrong - \\\"allows creation\\\" & \\\"including roles\\\"<br>C wrong - \\\"bastion\\\"<br>D wrong - \\\"Require the interns to raise a request\\\"",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 410243,
          "date": "Wed 03 Nov 2021 22:28",
          "username": "WhyIronMan",
          "content": "I'll go with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 406859,
          "date": "Mon 01 Nov 2021 04:43",
          "username": "Akhil254",
          "content": "A Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345837,
          "date": "Sat 30 Oct 2021 21:51",
          "username": "Waiweng",
          "content": "It's A",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 299331,
          "date": "Wed 27 Oct 2021 08:31",
          "username": "kiev",
          "content": "ANSWER IS A.  In B, interns can create all resources and even roles which isn't right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 290997,
          "date": "Tue 26 Oct 2021 04:04",
          "username": "Kian1",
          "content": "going with A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281609,
          "date": "Thu 21 Oct 2021 11:17",
          "username": "Ebi",
          "content": "Initially I was thinking of B, but after reading all the comments I go with A too.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242862,
          "date": "Tue 19 Oct 2021 04:09",
          "username": "T14102020",
          "content": "Correct answer is A.  Create roles",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230672,
          "date": "Sat 16 Oct 2021 07:26",
          "username": "Bulti",
          "content": "Answer is A.  The best way to make the interns self-sufficient is to create a policy on the intern group that allows them to create or configure the necessary resources like API Gateway, AWS Cognito etc. for their project and assign service roles created by the security team on the services that would be assumed by these services to talk to other services.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 230613,
          "date": "Sat 16 Oct 2021 07:14",
          "username": "jackdryan",
          "content": "I'll go with A",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 161288,
          "date": "Thu 14 Oct 2021 21:26",
          "username": "shakthi000005",
          "content": "Ans is A.  Bcoz B states they will have access to role which should not be",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#488",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a commercial Apache Hadoop cluster on Amazon EC2. This cluster is being used daily to query large files on Amazon S3. The data on<br>Amazon S3 has been curated and does not require any additional transformations steps. The company is using a commercial business intelligence (BI) tool on<br>Amazon EC2 to run queries against the Hadoop cluster and visualize the data.<br>The company wants to reduce or eliminate the overhead costs associated with managing the Hadoop cluster and the BI tool. The company would like to move to a more cost-effective solution with minimal effort. The visualization is simple and requires performing some basic aggregation steps only.<br>Which option will meet the company's requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#488",
          "answers": [
            {
              "choice": "<p>A. Launch a transient Amazon EMR cluster daily and develop an Apache Hive script to analyze the files on Amazon S3. Shut down the Amazon EMR cluster when the job is complete. Then use Amazon QuickSight to connect to Amazon EMR and perform the visualization.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Develop a stored procedure invoked from a MySQL database running on Amazon EC2 to analyze the files in Amazon S3. Then use a fast in-memory BI tool running on Amazon EC2 to visualize the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Develop a script that uses Amazon Athena to query and analyze the files on Amazon S3. Then use Amazon QuickSight to connect to Athena and perform the visualization.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use a commercial extract, transform, load (ETL) tool that runs on Amazon EC2 to prepare the data for processing. Then switch to a faster and cheaper BI tool that runs on Amazon EC2 to visualize the data from Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 514785,
          "date": "Sun 02 Jan 2022 03:53",
          "username": "BuggieGeniusMikeLiubobsmith2000user0001",
          "content": "should be Awhy not C?seamless transitionA is wrong , because Amazon S3 data has been vetted and does not need any extra changes , you dont need hiveto run queries .",
          "upvote_count": "6212",
          "selected_answers": ""
        },
        {
          "id": 524807,
          "date": "Sun 16 Jan 2022 10:40",
          "username": "GeniusMikeLiubobsmith2000user0001",
          "content": "why not C?seamless transitionA is wrong , because Amazon S3 data has been vetted and does not need any extra changes , you dont need hiveto run queries .",
          "upvote_count": "212",
          "selected_answers": ""
        },
        {
          "id": 593266,
          "date": "Wed 27 Apr 2022 17:34",
          "username": "bobsmith2000user0001",
          "content": "seamless transitionA is wrong , because Amazon S3 data has been vetted and does not need any extra changes , you dont need hiveto run queries .",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 597908,
          "date": "Sat 07 May 2022 00:59",
          "username": "user0001",
          "content": "A is wrong , because Amazon S3 data has been vetted and does not need any extra changes , you dont need hiveto run queries .",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 711346,
          "date": "Fri 04 Nov 2022 21:08",
          "username": "superuser784",
          "content": "Ill go with A, questions clearly says \\\"The company wants to reduce or eliminate the overhead costs associated with managing the HADOOP cluster and the BI TOOL\\\", with option C you are just querying and visualizing the data ALREADY creaded by the hadoop cluster(which is managed be the company) , in option A you give that responsability to AWS EMR which is what we want",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 647823,
          "date": "Tue 16 Aug 2022 22:50",
          "username": "Biden",
          "content": "General Question: Are all the 800+questions still applicable for the current Professional Arch Exam ? I remember seeing some old posts that only the first 300+ are valid...appreciate any insights",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 632820,
          "date": "Mon 18 Jul 2022 06:24",
          "username": "MarkChoi",
          "content": "It's point that \\\"The visualization is straightforward and takes just a few simple aggregation processes.\\\"<br>It's simple query.<br>why don't you use Athena?<br>C is more reasonable answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 631716,
          "date": "Fri 15 Jul 2022 13:37",
          "username": "CloudHandsOn",
          "content": "First answer chosen was C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 622613,
          "date": "Sun 26 Jun 2022 16:07",
          "username": "[Removed]",
          "content": "C. <br><br>Because \\\"The organization wishes to minimize or eliminate overhead expenses associated with administering the Hadoop cluster\\\".",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 610038,
          "date": "Wed 01 Jun 2022 08:44",
          "username": "Anhdd",
          "content": "when it comes to Hadoop cluster -> think about EMR. And with EMR we using Hive to querry anh large dataset",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 607727,
          "date": "Thu 26 May 2022 18:01",
          "username": "Racinely",
          "content": "ATHENA",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 593265,
          "date": "Wed 27 Apr 2022 17:33",
          "username": "bobsmith2000",
          "content": "The question asks for seamless transition.<br>From Hadoop it's gonna be AWS EMR",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 560243,
          "date": "Thu 03 Mar 2022 18:06",
          "username": "Mechanic",
          "content": "B seems good except for using a cost-efficient tool, which may not be efficient in quality.<br>C is the answer.<br>Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds. With Athena, there’s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.<br>https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 556499,
          "date": "Sat 26 Feb 2022 04:56",
          "username": "Changwha",
          "content": "The Answer is C<br><br>The data on Amazon S3 has been curated and does not require any additional transformations steps<br><br>This means we dont need EMR and can get away with Athena",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 513889,
          "date": "Fri 31 Dec 2021 06:22",
          "username": "guruaws2021bobsmith2000",
          "content": "IS Athena good choice considering the huge volume of file?It's affective only with big files.<br>https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 609735,
          "date": "Tue 31 May 2022 15:04",
          "username": "bobsmith2000",
          "content": "It's affective only with big files.<br>https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#489",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large multinational company runs a timesheet application on AWS that is used by staff across the world. The application runs on Amazon EC2 instances in an<br>Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer, and stores data in an Amazon RDS MySQL Multi-AZ database instance.<br>The CFO is concerned about the impact on the business if the application is not available. The application must not be down for more than two hours, but he solution must be as cost-effective as possible.<br>How should the Solutions Architect meet the CFO's requirements while minimizing data loss?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#489",
          "answers": [
            {
              "choice": "<p>A. In another region, configure a read replica and create a copy of the infrastructure. When an issue occurs, promote the read replica and configure as an Amazon RDS Multi-AZ database instance. Update the DNS record to point to the other region's ELB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure a 1-day window of 60-minute snapshots of the Amazon RDS Multi-AZ database instance. Create an AWS CloudFormation template of the application infrastructure that uses the latest snapshot. When an issue occurs, use the AWS CloudFormation template to create the environment in another region. Update the DNS record to point to the other region's ELB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure a 1-day window of 60-minute snapshots of the Amazon RDS Multi-AZ database instance which is copied to another region. Create an AWS CloudFormation template of the application infrastructure that uses the latest copied snapshot. When an issue occurs, use the AWS CloudFormation template to create the environment in another region. Update the DNS record to point to the other region's ELB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a read replica in another region. Create an AWS CloudFormation template of the application infrastructure. When an issue occurs, promote the read replica and configure as an Amazon RDS Multi-AZ database instance and use the AWS CloudFormation template to create the environment in another region using the promoted Amazon RDS instance. Update the DNS record to point to the other region's ELB. <br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12244,
          "date": "Mon 20 Sep 2021 12:48",
          "username": "donathonPb55ctrueMultiAZAlwaysLearning2020",
          "content": "D<br>A: Does not address the EC2 instances in Auto Scaling group which the application runs on.<br>B\\C: Data loss of 60 minutes is too long?<br>https://aws.amazon.com/rds/details/read-replicas/The application must not be down for more than two hours, but he solution must be as cost-effective as possible.How should the Solutions Architect meet the CFOג€™s requirements while minimizing data loss? <br>Minimise data loss = D. Read replica is not good enough, it serves only in disaster scenarios. But, what happens if there is any human mistake and the change would replicate to replicas as well. So, the answer is CThere is NO requirement for RPO (data loss) being less than 1 hour, only RTO being less than 2 hours. Cost effective is major requirement so C is better.Yes. RPO is not mentioned explicitly, but \\\"A large multinational company runs a timesheet application on AWS that is used by staff across the world....\\\" So what? Once data is loss for 60 mins, the company will ask all the staffs across the world to reenter their timesheet line-by-line again ?I wonder how complex with that be...?Hmmm.... I will vote 'D'.",
          "upvote_count": "381146",
          "selected_answers": ""
        },
        {
          "id": 397594,
          "date": "Tue 02 Nov 2021 03:44",
          "username": "Pb55",
          "content": "The application must not be down for more than two hours, but he solution must be as cost-effective as possible.How should the Solutions Architect meet the CFOג€™s requirements while minimizing data loss? <br>Minimise data loss = D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 404673,
          "date": "Tue 02 Nov 2021 13:25",
          "username": "ctrue",
          "content": "Read replica is not good enough, it serves only in disaster scenarios. But, what happens if there is any human mistake and the change would replicate to replicas as well. So, the answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 143570,
          "date": "Tue 05 Oct 2021 19:55",
          "username": "MultiAZAlwaysLearning2020",
          "content": "There is NO requirement for RPO (data loss) being less than 1 hour, only RTO being less than 2 hours. Cost effective is major requirement so C is better.Yes. RPO is not mentioned explicitly, but \\\"A large multinational company runs a timesheet application on AWS that is used by staff across the world....\\\" So what? Once data is loss for 60 mins, the company will ask all the staffs across the world to reenter their timesheet line-by-line again ?I wonder how complex with that be...?Hmmm.... I will vote 'D'.",
          "upvote_count": "46",
          "selected_answers": ""
        },
        {
          "id": 194336,
          "date": "Fri 15 Oct 2021 23:17",
          "username": "AlwaysLearning2020",
          "content": "Yes. RPO is not mentioned explicitly, but \\\"A large multinational company runs a timesheet application on AWS that is used by staff across the world....\\\" So what? Once data is loss for 60 mins, the company will ask all the staffs across the world to reenter their timesheet line-by-line again ?I wonder how complex with that be...?Hmmm.... I will vote 'D'.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 13766,
          "date": "Tue 21 Sep 2021 19:33",
          "username": "MoonStec1980SamuelK",
          "content": "Prefer \\\"D\\\".<br>B/C: has a big data loss. While the question is asking for minimal data loss.Most cost efficient is the key requirement...there's no mention of zero data loss being a necessity. If cost is key then C is correct.it says that cost-effective as possible, but the main key requirement is to minimize the data loss.D is what I would go with",
          "upvote_count": "1512",
          "selected_answers": ""
        },
        {
          "id": 159847,
          "date": "Sat 09 Oct 2021 05:05",
          "username": "Stec1980SamuelK",
          "content": "Most cost efficient is the key requirement...there's no mention of zero data loss being a necessity. If cost is key then C is correct.it says that cost-effective as possible, but the main key requirement is to minimize the data loss.D is what I would go with",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 223835,
          "date": "Mon 18 Oct 2021 11:24",
          "username": "SamuelK",
          "content": "it says that cost-effective as possible, but the main key requirement is to minimize the data loss.D is what I would go with",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 643167,
          "date": "Sat 06 Aug 2022 05:07",
          "username": "naiduveerendra",
          "content": "Its DDDD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 636876,
          "date": "Mon 25 Jul 2022 19:48",
          "username": "hilft",
          "content": "D is the best option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 561805,
          "date": "Sun 06 Mar 2022 07:41",
          "username": "Alvindo",
          "content": "In my opinion, with b/c you'd have to construct the application from base with a snapshot which might take longer than the rto of 2 hours since it's a HUGE multinational company so the probability of the data being a lot is huge so...<br>and i'd go for d since since you already have a read replica, it should save more time promoting it that than just starting from a snapshot. and there's the data loss of 60 minutes with b/c",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 540876,
          "date": "Sat 05 Feb 2022 11:24",
          "username": "HellGate",
          "content": "My answer is D. <br><br>There is requirement on data of reducing data loss… which means data need to be backed up near real time.<br>a 1-day window of 60-minute snapshots have RPO of 1 hour but read replica is near zero RPO.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 526185,
          "date": "Tue 18 Jan 2022 00:38",
          "username": "weurseuk",
          "content": "D, Rto=2h but rpo: available , so read replica is the best choice",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493983,
          "date": "Sat 04 Dec 2021 23:13",
          "username": "AzureDP900",
          "content": "I will go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 468598,
          "date": "Sun 07 Nov 2021 12:25",
          "username": "Kopa",
          "content": "Im going for D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 467183,
          "date": "Sat 06 Nov 2021 23:08",
          "username": "nsei",
          "content": "I’ll go with D.  Depending on the size of the DB restoring the snapshot within 2 hours cannot be guaranteed",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 456364,
          "date": "Sat 06 Nov 2021 22:16",
          "username": "StelSen",
          "content": "Well \\\"C\\\" is most cost effective. \\\"D\\\" handles better in data loss. Let's read the requirements:<br>\\\".....solution MUST be as cost-effective as possible\\\" - The word MUST leads to Option-C<br>\\\"How SHOULD the Solutions Architect meet the CFOג€™s REQUIREMENTS \\\" - The word SHOULD and the word \\\"Requirements leads to Option-C as well<br>\\\"....while minimizing data loss\\\" - This leads to Option-D, agree. However C also minimize data loss atleast for >60 mins. <br>All these keywords leads to Option-C to me.<br>but he solution must be as cost-effective as possible.<br>while minimizing data loss",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 434889,
          "date": "Fri 05 Nov 2021 15:20",
          "username": "student22student22",
          "content": "D<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.htmlMinimize data loss + less costly",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 455710,
          "date": "Sat 06 Nov 2021 10:06",
          "username": "student22",
          "content": "Minimize data loss + less costly",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 419781,
          "date": "Wed 03 Nov 2021 22:18",
          "username": "Shran",
          "content": "D.  There is a requirement to minimize data loss. So having snapshot of 60minutes involve data loss. D with read replica looks correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413829,
          "date": "Wed 03 Nov 2021 16:23",
          "username": "DerekKey",
          "content": "The key requirement is - \\\"minimizing data loss\\\". How we can make it happen the cheapest way?<br>D correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 410253,
          "date": "Wed 03 Nov 2021 04:24",
          "username": "WhyIronMan",
          "content": "I'll go with D<br>\\\"minimizing data loss\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 370997,
          "date": "Sun 31 Oct 2021 23:03",
          "username": "zolthar_z",
          "content": "The Answer is D<br><br>\\\"With 1-day window\\\" is a little confusing... it could be 1 snapshot all nights or 1 snapshot one-time,but at the end, if you took the snapshot at 12 AM and you have a failure at 1 PM you will have a data loss of12 hours. D over C is for the cost (even the question didn't mention anything related to cost)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 356758,
          "date": "Sun 31 Oct 2021 09:08",
          "username": "digimaniac",
          "content": "BC are wrong. the snapshot is taken every hour. if there is something wrong, you have to ask international users who did their timesheet during the last hour to redo the timesheet. you will be fired and then get killed.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#490",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A Development team has created a series of AWS CloudFormation templates to help deploy services. They created a template for a network/virtual private cloud<br>(VPC) stack, a database stack, a bastion host stack, and a web application-specific stack. Each service requires the deployment of at least:<br>✑ A network/VPC stack<br>✑ A bastion host stack<br>✑ A web application stack<br>Each template has multiple input parameters that make it difficult to deploy the services individually from the AWS CloudFormation console. The input parameters from one stack are typically outputs from other stacks. For example, the VPC ID, subnet IDs, and security groups from the network stack may need to be used in the application stack or database stack.<br>Which actions will help reduce both the operational burden and the number of parameters passed into a service deployment? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#490",
          "answers": [
            {
              "choice": "<p>A. Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross-stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new stack. Call the newly created service stack from the AWS CloudFormation console to deploy the specific service with a subset of the parameters previously required.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new portfolio in AWS Service Catalog for each service. Create a product for each existing AWS CloudFormation template required to build the service. Add the products to the portfolio that represents that service in AWS Service Catalog. To deploy the service, select the specific service portfolio and launch the portfolio with the necessary parameters to deploy all templates.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up an AWS CodePipeline workflow for each service. For each existing template, choose AWS CloudFormation as a deployment action. Add the AWS CloudFormation template to the deployment action. Ensure that the deployment actions are processed to make sure that dependencies are obeyed. Use configuration files and scripts to share parameters between the stacks. To launch the service, execute the specific template by choosing the name of the service and releasing a change.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Step Functions to define a new service. Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross- stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new service template. Configure AWS Step Functions to call the service template directly. In the AWS Step Functions console, execute the step.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a new portfolio for the services in AWS Service Catalog. Create a new AWS CloudFormation template for each service. Alter the existing templates to use cross-stack references to eliminate passing many parameters to each template. Call each required stack for the application as a nested stack from the new stack. Create a product for each application. Add the service template to the product. Add each new product to the portfolio. Deploy the product from the portfolio to deploy the service with the necessary parameters only to start the deployment.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 255079,
          "date": "Sat 23 Oct 2021 21:16",
          "username": "Bulti",
          "content": "A&E.  E requires few parameters and one action to launch a portfolio to deploy all products, where one productrepresents one service. Between A ( CloudFormation console to deploy a service) vs D ( Stepfunction to deploy a service) I chose A because I didn't find Step function to be a standard practice to deploy a cloudformation stack unlike CodePipeline which is a standard way to test and deploy cloudformation stack via a codepipeline. However Option B and C fall short because they do not reduce the# of input parameters.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 215618,
          "date": "Wed 22 Sep 2021 00:52",
          "username": "NNHANkeosKelvin1477",
          "content": "A and Ethey have duplicate contentAgree, Looks like the only option to pass parameter around is still using nested cloudformation templates",
          "upvote_count": "611",
          "selected_answers": ""
        },
        {
          "id": 224698,
          "date": "Thu 23 Sep 2021 04:10",
          "username": "keos",
          "content": "they have duplicate content",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 236450,
          "date": "Fri 01 Oct 2021 07:37",
          "username": "Kelvin1477",
          "content": "Agree, Looks like the only option to pass parameter around is still using nested cloudformation templates",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 607535,
          "date": "Thu 26 May 2022 10:21",
          "username": "ASC1",
          "content": "why not B ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 529479,
          "date": "Sat 22 Jan 2022 00:43",
          "username": "tkanmani76",
          "content": "A and E - Pl refer this article - https://aws.amazon.com/blogs/mt/how-to-launch-secure-and-governed-aws-resources-with-aws-cloudformation-and-aws-service-catalog/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493988,
          "date": "Sat 04 Dec 2021 23:19",
          "username": "AzureDP900",
          "content": "AE is correct, There is no need of codepipeline.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410262,
          "date": "Sun 07 Nov 2021 15:35",
          "username": "WhyIronMan",
          "content": "I will go with A, E",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 379459,
          "date": "Sat 06 Nov 2021 02:55",
          "username": "MarcChartouny",
          "content": "I got lost since I don't have the advanced DevOps expertise, But my guts tell me to go with D & E.  Please any DevOps engineers to assist and give their feedback?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345844,
          "date": "Thu 04 Nov 2021 05:57",
          "username": "Waiweng",
          "content": "It's A , E",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 330675,
          "date": "Tue 02 Nov 2021 15:59",
          "username": "SD13",
          "content": "D & E are the correct options that eliminate param passing. All other options still require params and no easy way to implement.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293076,
          "date": "Sat 30 Oct 2021 21:41",
          "username": "Kian1",
          "content": "going with AE",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 264484,
          "date": "Sat 30 Oct 2021 20:10",
          "username": "Ebi",
          "content": "I will go with AE",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 250330,
          "date": "Wed 13 Oct 2021 00:10",
          "username": "Britts",
          "content": "why not D & E, step functions can launch a stack? i.e. one step function per service",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 248192,
          "date": "Mon 11 Oct 2021 11:07",
          "username": "arulrajjayaraj",
          "content": "A & E -A --> AWS CloudFormation console to deploy ,E -->ServiceCatalog , Cross Stacks When you need to pass export values to many stacks (VPC Id, etc…) & Nested Stacks<br>• Helpful when components must be re-used • Ex: re-use how to properly configure an<br>Application Load Balancer • The nested stack only is important to the higher level stack (it’s not shared)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 244842,
          "date": "Mon 11 Oct 2021 02:39",
          "username": "T14102020Aquavk",
          "content": "Correct AE. ServiceCatalog + without CodePipeline + without Step FunctionsWhy not D & E ?",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 253109,
          "date": "Fri 15 Oct 2021 14:01",
          "username": "Aquavk",
          "content": "Why not D & E ?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 239135,
          "date": "Sat 02 Oct 2021 17:47",
          "username": "joos",
          "content": "A and E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 233537,
          "date": "Sun 26 Sep 2021 14:26",
          "username": "alexmena1981",
          "content": "E ok, why not D?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 232552,
          "date": "Fri 24 Sep 2021 17:03",
          "username": "jackdryanarulrajjayarajdarthvoodoo",
          "content": "I'll go with C,EC - will help reduce both the operational burden.No it doesn't. You would still need to manage and maintain a script to map the parameters, which is unnecessary. The question doesn't ask which combinations address the requirements. A&E are correct answers.",
          "upvote_count": "313",
          "selected_answers": ""
        },
        {
          "id": 241140,
          "date": "Fri 08 Oct 2021 03:03",
          "username": "arulrajjayarajdarthvoodoo",
          "content": "C - will help reduce both the operational burden.No it doesn't. You would still need to manage and maintain a script to map the parameters, which is unnecessary. The question doesn't ask which combinations address the requirements. A&E are correct answers.",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 242962,
          "date": "Sat 09 Oct 2021 10:47",
          "username": "darthvoodoo",
          "content": "No it doesn't. You would still need to manage and maintain a script to map the parameters, which is unnecessary. The question doesn't ask which combinations address the requirements. A&E are correct answers.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#491",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application behind a load balancer with enough Amazon EC2 instances to satisfy peak demand. Scripts and third-party deployment solutions are used to configure EC2 instances when demand increases or an instance fails. The team must periodically evaluate the utilization of the instance types to ensure that the correct sizes are deployed.<br>How can this workload be optimized to meet these requirements?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#491",
          "answers": [
            {
              "choice": "<p>A. Use CloudFormer to create AWS CloudFormation stacks from the current resources. Deploy that stack by using AWS CloudFormation in the same region. Use Amazon CloudWatch alarms to send notifications about underutilized resources to provide cost-savings suggestions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Auto Scaling group to scale the instances, and use AWS CodeDeploy to perform the configuration. Change from a load balancer to an Application Load Balancer. Purchase a third-party product that provides suggestions for cost savings on AWS resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application by using AWS Elastic Beanstalk with default options. Register for an AWS Support Developer plan. Review the instance usage for the application by using Amazon CloudWatch, and identify less expensive instances that can handle the load. Hold monthly meetings to review new instance types and determine whether Reserved Instances should be purchased.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the application as a Docker image by using Amazon ECS. Set up Amazon EC2 Auto Scaling and Amazon ECS scaling. Register for AWS Business Support and use Trusted Advisor checks to provide suggestions on cost savings.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12256,
          "date": "Wed 22 Sep 2021 05:58",
          "username": "donathonheanysb333student22JAWS1600DirtyBerty",
          "content": "D<br>A: Cloudwatch does not provide cost-savings suggestions.<br>B: You don’t need to use 3rd party application. You can use AWS trusted adviser or cost explorer which comes free.<br>C: Reserved instances may not help if you cannot even decide which instance to use.<br>D: You do not need Business support to check have cost saving suggestions. But since this is the only open that has Trusted Advisor, I chose this.not every applications are suitable for containerization. Big monolithic applications are not suitable for containerized into ECS directly. You need to break it down first .e.g into microservices. That is typically a big projects. So you cannot assume every existing applications can be deployed into ECS.For A, Cloudwatch doesn't provide cost-savings suggestions, but the answer says it can provide notifications to help you give suggestions. So it should be AActually, you do need Business Support or Enterprise support to get the full set of Trusted Advisor checks, which include the cost optimization checks. Basic Support and Developer Support only include seven core Trust Advisor checks, which does not include costs. https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/That's right!Cost saving is not requirementAmazon CloudWatch alarms can send notifications about underutilized resources, see:<br>https://aws.amazon.com/about-aws/whats-new/2013/01/08/use-amazon-cloudwatch-to-detect-and-shut-down-unused-amazon-ec2-instances/",
          "upvote_count": "31112134",
          "selected_answers": ""
        },
        {
          "id": 695010,
          "date": "Fri 14 Oct 2022 22:52",
          "username": "heany",
          "content": "not every applications are suitable for containerization. Big monolithic applications are not suitable for containerized into ECS directly. You need to break it down first .e.g into microservices. That is typically a big projects. So you cannot assume every existing applications can be deployed into ECS.For A, Cloudwatch doesn't provide cost-savings suggestions, but the answer says it can provide notifications to help you give suggestions. So it should be A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 49843,
          "date": "Mon 27 Sep 2021 14:53",
          "username": "sb333student22",
          "content": "Actually, you do need Business Support or Enterprise support to get the full set of Trusted Advisor checks, which include the cost optimization checks. Basic Support and Developer Support only include seven core Trust Advisor checks, which does not include costs. https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/That's right!",
          "upvote_count": "121",
          "selected_answers": ""
        },
        {
          "id": 434898,
          "date": "Wed 03 Nov 2021 16:58",
          "username": "student22",
          "content": "That's right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 100458,
          "date": "Thu 07 Oct 2021 06:00",
          "username": "JAWS1600",
          "content": "Cost saving is not requirement",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 30481,
          "date": "Sun 26 Sep 2021 23:09",
          "username": "DirtyBerty",
          "content": "Amazon CloudWatch alarms can send notifications about underutilized resources, see:<br>https://aws.amazon.com/about-aws/whats-new/2013/01/08/use-amazon-cloudwatch-to-detect-and-shut-down-unused-amazon-ec2-instances/",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 282728,
          "date": "Sun 24 Oct 2021 02:47",
          "username": "Trap_D0_r",
          "content": "Question is to optimize CURRENT workload, NOT re-platform software.<br>A: Best choice without completely rebuilding environment<br>B: Issues with using CodeDeploy to configure EC2s aside, AWS will NEVER recommend you buy a 3rd party app to analyze usage--AWS has tools for this<br>C: Only a suicidal buffoon would make monthly meetings part of an architecture workflow. Absolutely not. Beanstalk is a maybe, but the end of this answer absolutely rules it out.<br>D: Again, this answer re-platforms (re-architects?) the software, it doesn't optimize the CURRENT workload, which is what the questions asked for. Also, anyone with experience using Business Support and Trusted Advisor will know that the TA utilization checks are not as good as custom cloudwatch metrics. While this answer is tempting because it gets off EC2 and uses a managed service for utilization analysis, I don't think it answers the original question and I don't think upgrading to business support is a good recommendation for a workload improvement.",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 712065,
          "date": "Sun 06 Nov 2022 00:19",
          "username": "superuser784",
          "content": "This is a really old question and the answer WAS A, but CloudFormer is already deprecated, the next answer that makes more sense is D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 689695,
          "date": "Sun 09 Oct 2022 00:04",
          "username": "wassb",
          "content": "\\\"Enough Amazon EC2 instances to satisfy peak demand\\\" > autoscaling is obvious optimization",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 559880,
          "date": "Thu 03 Mar 2022 08:05",
          "username": "Alexey79",
          "content": "Answer: D<br><br>Why NOT A:<br>In Vertical scaling, only automatic option is to shutdown EC2 instance. Manually, CloudWatch will generate a lot of Alarms almost hourly … The best solution is Horizontal scaling.<br><br>https://aws.amazon.com/about-aws/whats-new/2013/01/08/use-amazon-cloudwatch-to-detect-and-shut-down-unused-amazon-ec2-instances/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 497706,
          "date": "Thu 09 Dec 2021 13:21",
          "username": "cldy",
          "content": "D.  Deploy the application as a Docker image by using Amazon ECS. Set up Amazon EC2 Auto Scaling and Amazon ECS scaling. Register for AWS Business Support and use Trusted Advisor checks to provide suggestions on cost savings.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493990,
          "date": "Sat 04 Dec 2021 23:27",
          "username": "AzureDP900",
          "content": "D is right based onstatement\\\"The team must conduct frequent evaluations of the instance types' use to ensure that the appropriate sizes are deployed\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 459479,
          "date": "Sun 07 Nov 2021 02:17",
          "username": "wannaawstomosabc1",
          "content": "D.  Workload, IMHO, refers to the work for team periodically evaluate the utilization of the instance types. ECS minimize service impact on current running solution, and Trusted Advisor simply provide cost saving tips (with regards to right EC2 instance type)\\\"Workload refers to the work for team periodically evaluate the utilization of the instance types.\\\"<br>brilliant explanation!! I like it. Thanks.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 682319,
          "date": "Thu 29 Sep 2022 05:56",
          "username": "tomosabc1",
          "content": "\\\"Workload refers to the work for team periodically evaluate the utilization of the instance types.\\\"<br>brilliant explanation!! I like it. Thanks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 448367,
          "date": "Sat 06 Nov 2021 23:02",
          "username": "AWSum1",
          "content": "Ill choose D over A, since cloudformer is deprecated and CW doesn't do cost savings suggestions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437890,
          "date": "Thu 04 Nov 2021 23:48",
          "username": "tgvViper57",
          "content": "AAA<br>---CloudFormer is deprecated so this cannot be the answer.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 444148,
          "date": "Sat 06 Nov 2021 03:32",
          "username": "Viper57",
          "content": "CloudFormer is deprecated so this cannot be the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 430516,
          "date": "Sun 31 Oct 2021 11:16",
          "username": "Kopa",
          "content": "I think maybe D because with microservices Docker in this case the utilization is more fine grained. And also the Trust Advisor is a good tool. Also CloudFormer seems to be discontinued.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 429323,
          "date": "Sat 30 Oct 2021 03:22",
          "username": "denccc",
          "content": "I don't understand this question/the options..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 418041,
          "date": "Fri 29 Oct 2021 01:58",
          "username": "botohin687",
          "content": "A<br>https://aws.amazon.com/about-aws/whats-new/2013/01/08/use-amazon-cloudwatch-to-detect-and-shut-down-unused-amazon-ec2-instances/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410275,
          "date": "Thu 28 Oct 2021 22:59",
          "username": "WhyIronMan",
          "content": "I'll go with D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345905,
          "date": "Wed 27 Oct 2021 20:11",
          "username": "Waiweng",
          "content": "this is weird question ever encounter, by choice will be A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 316930,
          "date": "Wed 27 Oct 2021 13:48",
          "username": "Pupu86",
          "content": "Inclined to choose A since optimisation is the key but cloudformer has deprecated so going with D instead",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293971,
          "date": "Wed 27 Oct 2021 08:14",
          "username": "kiev",
          "content": "I would go for D but after reading the explanation by Trap _Do_R, it wouldn't surprise me if it is A and if I have this question in the exam, I may switch to A. ",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#492",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large global financial services company has multiple business units. The company wants to allow Developers to try new services, but there are multiple compliance requirements for different workloads. The Security team is concerned about the access strategy for on-premises and AWS implementations. They would like to enforce governance for AWS services used by business teams for regulatory workloads, including Payment Card Industry (PCI) requirements.<br>Which solution will address the Security team's concerns and allow the Developers to try new services?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#492",
          "answers": [
            {
              "choice": "<p>A. Implement a strong identity and access management model that includes users, groups, and roles in various AWS accounts. Ensure that centralized AWS CloudTrail logging is enabled to detect anomalies. Build automation with AWS Lambda to tear down unapproved AWS resources for governance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Build a multi-account strategy based on business units, environments, and specific regulatory requirements. Implement SAML-based federation across all AWS accounts with an on-premises identity store. Use AWS Organizations and build organizational units (OUs) structure based on regulations and service governance. Implement service control policies across OUs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement a multi-account strategy based on business units, environments, and specific regulatory requirements. Ensure that only PCI-compliant services are approved for use in the accounts. Build IAM policies to give access to only PCI-compliant services for governance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Build one AWS account for the company for strong security controls. Ensure that all the service limits are raised to meet company scalability requirements. Implement SAML federation with an on-premises identity store, and ensure that only approved services are used in the account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12401,
          "date": "Wed 22 Sep 2021 13:08",
          "username": "donathon",
          "content": "A: Too reactive. The users will still be able to do what they want.<br>B: Sounds feasible.<br>C: SCP should be used because this is multi-account.<br>D: Too restrictive and it does not address Developer’s needs.",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 13758,
          "date": "Thu 23 Sep 2021 20:22",
          "username": "Moon",
          "content": "My preference is \\\"B\\\".<br>A: stop developers from trying new services.<br>C: does not show the enforcement tool.<br>D: one account contradict with the requirement.",
          "upvote_count": "27",
          "selected_answers": ""
        },
        {
          "id": 625268,
          "date": "Thu 30 Jun 2022 15:04",
          "username": "aandc",
          "content": "go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499115,
          "date": "Sat 11 Dec 2021 05:01",
          "username": "cldy",
          "content": "B.  Build a multi-account strategy based on business units, environments, and specific regulatory requirements. Implement SAML-based federation across all AWS accounts with an on-premises identity store. Use AWS Organizations and build organizational units (OUs) structure based on regulations and service governance. Implement service control policies across OUs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 499113,
          "date": "Sat 11 Dec 2021 04:55",
          "username": "cldy",
          "content": "B.  Build a multi-account strategy based on business units, environments, and specific regulatory requirements. Implement SAML-based federation across all AWS accounts with an on-premises identity store. Use AWS Organizations and build organizational units (OUs) structure based on regulations and service governance. Implement service control policies across OUs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493992,
          "date": "Sat 04 Dec 2021 23:29",
          "username": "AzureDP900",
          "content": "B is suitable for the requirement.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 489740,
          "date": "Mon 29 Nov 2021 10:26",
          "username": "acloudguru",
          "content": "this is easy one ,hope i can have it in my exam. B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449510,
          "date": "Sun 07 Nov 2021 08:58",
          "username": "Kopa",
          "content": "Going for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 447942,
          "date": "Fri 05 Nov 2021 20:01",
          "username": "moon2351",
          "content": "Answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410380,
          "date": "Thu 04 Nov 2021 14:08",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 345912,
          "date": "Wed 03 Nov 2021 16:03",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 291009,
          "date": "Sun 31 Oct 2021 08:11",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281627,
          "date": "Thu 28 Oct 2021 21:49",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 252974,
          "date": "Thu 28 Oct 2021 05:59",
          "username": "01037",
          "content": "I guess B<br>Two requirements<br>1. The Security team is concerned about the access strategy for on-premises and AWS implementations.<br>→ I guess we need to use ID store on premise.<br>2. They would like to enforce governance for AWS services used by business team for regulatory workloads, including Payment Card Industry (PCI) requirements.<br>→ Organization and SCP are needed.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242932,
          "date": "Fri 22 Oct 2021 20:32",
          "username": "T14102020",
          "content": "Correct answer is B.  AWS organizations and SCP.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230698,
          "date": "Wed 20 Oct 2021 18:24",
          "username": "Bulti",
          "content": "B is a standard approach to enforcing compliance and security policies across company's business units using AWS organizations and SCP.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230641,
          "date": "Tue 19 Oct 2021 06:13",
          "username": "jackdryan",
          "content": "I'll go with B",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#493",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company had a tight deadline to migrate its on-premises environment to AWS. It moved over Microsoft SQL Servers and Microsoft Windows Servers using the virtual machine import/export service and rebuild other applications native to the cloud. The team created both Amazon EC2 databases and used Amazon RDS.<br>Each team in the company was responsible for migrating their applications, and they have created individual accounts for isolation of resources. The company did not have much time to consider costs, but now it would like suggestions on reducing its AWS spend.<br>Which steps should a Solutions Architect take to reduce costs?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#493",
          "answers": [
            {
              "choice": "<p>A. Enable AWS Business Support and review AWS Trusted Advisor's cost checks. Create Amazon EC2 Auto Scaling groups for applications that experience fluctuating demand. Save AWS Simple Monthly Calculator reports in Amazon S3 for trend analysis. Create a master account under Organizations and have teams join for consolidated billing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable Cost Explorer and AWS Business Support. Reserve Amazon EC2 and Amazon RDS DB instances. Use Amazon CloudWatch and AWS TrustedAdvisor for monitoring and to receive cost-savings suggestions. Create a master account under Organizations and have teams join for consolidated billing.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Lambda function that changes the instance size based on Amazon CloudWatch alarms. Reserve instances based on AWS Simple Monthly Calculator suggestions. Have an AWS Well-Architected framework review and apply recommendations. Create a master account under Organizations and have teams join for consolidated billing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a budget and monitor for costs exceeding the budget. Create Amazon EC2 Auto Scaling groups for applications that experience fluctuating demand. Create an AWS Lambda function that changes instance sizes based on Amazon CloudWatch alarms. Have each team upload their bill to an Amazon S3 bucket for analysis of team spending. Use Spot Instances on nightly batch processing jobs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12404,
          "date": "Thu 23 Sep 2021 07:12",
          "username": "donathonWillCloud",
          "content": "B<br>Reserved instances instantly save cost.<br>A: Can’t save Monthly Calculator reports in S3 for trend analysis??<br>C\\D: Changing instance size using Lambda is not possible?AWS Simple Monthly Calculator: It is a tool for budget estimate, not a tool for monitoring resource usage.<br><br>https://calculator.s3.amazonaws.com/index.html",
          "upvote_count": "251",
          "selected_answers": ""
        },
        {
          "id": 449710,
          "date": "Thu 04 Nov 2021 21:53",
          "username": "WillCloud",
          "content": "AWS Simple Monthly Calculator: It is a tool for budget estimate, not a tool for monitoring resource usage.<br><br>https://calculator.s3.amazonaws.com/index.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 13755,
          "date": "Sun 26 Sep 2021 15:39",
          "username": "Moon",
          "content": "My preference is \\\"B\\\".<br>Cost explorer, trusted advisor, are great tools.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 717451,
          "date": "Sun 13 Nov 2022 19:20",
          "username": "MAGICLOUD",
          "content": "B 100%",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 671188,
          "date": "Sat 17 Sep 2022 03:45",
          "username": "Dionenonly",
          "content": "B: 100%",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 575520,
          "date": "Sat 26 Mar 2022 13:03",
          "username": "Mechanic",
          "content": "The incorrect thing about A is that it doesn't reduce RDS costs.<br>Going with B. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 495111,
          "date": "Mon 06 Dec 2021 13:12",
          "username": "cldy",
          "content": "B.  Enable Cost Explorer and AWS Business Support. Reserve Amazon EC2 and Amazon RDS DB instances. Use Amazon CloudWatch and AWS Trusted Advisor for monitoring and to receive cost-savings suggestions. Create a master account under Organizations and have teams join for consolidated billing.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493993,
          "date": "Sat 04 Dec 2021 23:32",
          "username": "AzureDP900",
          "content": "I will preferer B over A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 488856,
          "date": "Sun 28 Nov 2021 06:40",
          "username": "backfringe",
          "content": "I'd go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 449031,
          "date": "Thu 04 Nov 2021 16:37",
          "username": "DonSp",
          "content": "B's Cost Explorer over A's Simple Monthly Calculator. Although it seems stupid to commit to reserving instances for 1 or 3 years without prior evaluation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 420792,
          "date": "Mon 01 Nov 2021 19:43",
          "username": "Shran",
          "content": "If database running on EC2, the EC2 cannot put in autoscaling group as database on EC2 is stateful. So right answer is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410383,
          "date": "Fri 29 Oct 2021 00:01",
          "username": "WhyIronMan",
          "content": "I'll go with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345916,
          "date": "Fri 22 Oct 2021 08:06",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291012,
          "date": "Fri 22 Oct 2021 00:52",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281071,
          "date": "Thu 21 Oct 2021 08:51",
          "username": "Ebi",
          "content": "I go with B,<br>All other answer don't make sense",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 252979,
          "date": "Wed 20 Oct 2021 00:36",
          "username": "0103701037",
          "content": "Why is Consolidated Billing needed?https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-discounts.html<br><br>Understood now",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 252981,
          "date": "Thu 21 Oct 2021 05:53",
          "username": "01037",
          "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-discounts.html<br><br>Understood now",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242937,
          "date": "Sat 16 Oct 2021 08:36",
          "username": "T14102020",
          "content": "Correct answer is B.  cost explorer, trust advisor, consolidate, reserved",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230706,
          "date": "Wed 13 Oct 2021 10:48",
          "username": "Bulti",
          "content": "Answer is B.  I prefer reserving EC2 and RDS instances upfront for the VMs that were migrated over using AWS import/export as opposed to configuring auto-scaling on EC2 instances.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#494",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p><br>A company wants to replace its call center<br>system with a solution built using AWS managed services. The company call center would like the solution to receive calls, create contact flows, and scale to handle growth projections. The call center would also like the solution to use deep learning capabilities to recognize the intent of the callers and handle basic tasks, reducing the need to speak to an agent. The solution should also be able to query business applications and provide relevant information back to callers as requested.<br>Which services should the Solutions Architect use to build this solution? (Choose three.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BDE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#494",
          "answers": [
            {
              "choice": "<p>A. Amazon Rekognition to identify who is calling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon Connect to create a cloud-based contact center.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon Alexa for Business to build conversational interfaces.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. AWS Lambda to integrate with internal systems.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Amazon Lex to recognize the intent of the caller.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Amazon SQS to add incoming callers to a queue.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12407,
          "date": "Wed 29 Sep 2021 01:32",
          "username": "donathon",
          "content": "BDE<br>A: Amazon Rekognition makes it easy to add image and video analysis to your applications. You just provide an image or video to the Rekognition API, and the service can identify the objects, people, text, scenes, and activities, as well as detect any inappropriate content.<br>C: Amazon Alexa is comparable with Google Home and is not an AWS service.<br>F: SQS add messages but not a real call.",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 13754,
          "date": "Wed 29 Sep 2021 17:14",
          "username": "Moon",
          "content": "Answer is, \\\"B, D & E\\\".<br>B: Amazon connect is contact center service from amazon.<br>D: Lambda can be used to fulfill user request from internal system.<br>E: Amazon Lex is used to build chatbots.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 671189,
          "date": "Sat 17 Sep 2022 03:47",
          "username": "Dionenonly",
          "content": "it's BDE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BDE"
        },
        {
          "id": 497711,
          "date": "Thu 09 Dec 2021 13:24",
          "username": "cldy",
          "content": "B.  Amazon Connect to create a cloud-based contact center.<br>D.  AWS Lambda to integrate with internal systems.<br>E.  Amazon Lex to recognize the intent of the caller.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493995,
          "date": "Sat 04 Dec 2021 23:35",
          "username": "AzureDP900",
          "content": "Correct answer is BDE. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410384,
          "date": "Sun 07 Nov 2021 04:48",
          "username": "WhyIronMan",
          "content": "I'll go with B,D,E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345918,
          "date": "Fri 05 Nov 2021 07:14",
          "username": "Waiweng",
          "content": "its B,D,E",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 310888,
          "date": "Wed 03 Nov 2021 08:46",
          "username": "anandbabu",
          "content": "its BCE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291015,
          "date": "Sun 31 Oct 2021 00:33",
          "username": "Kian1",
          "content": "going with BDE",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281073,
          "date": "Thu 28 Oct 2021 21:23",
          "username": "Ebi",
          "content": "BDE for sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 242942,
          "date": "Wed 27 Oct 2021 01:23",
          "username": "T14102020",
          "content": "Correct answer is BDE.  <br>Amazon Connect, Lambda, Lex",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230708,
          "date": "Sat 23 Oct 2021 16:15",
          "username": "Bulti",
          "content": "BDE is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230644,
          "date": "Tue 19 Oct 2021 00:17",
          "username": "jackdryan",
          "content": "I'll go with B,D,E",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229362,
          "date": "Mon 18 Oct 2021 17:52",
          "username": "gookseang",
          "content": "BDE for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 163622,
          "date": "Sun 17 Oct 2021 01:30",
          "username": "achambokkirrim",
          "content": "BDE<br>Amazon Connect<br>Lambda<br>Lex<br><br>https://aws.amazon.com/blogs/contact-center/automate-employee-support-lines-with-amazon-connect/The architecture diagram in this blog post is very helpful, and directly backs up your answer as part of the recommended approach, straight from AWS",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 459272,
          "date": "Sun 07 Nov 2021 08:42",
          "username": "kirrim",
          "content": "The architecture diagram in this blog post is very helpful, and directly backs up your answer as part of the recommended approach, straight from AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 150183,
          "date": "Sat 16 Oct 2021 11:47",
          "username": "fullaws",
          "content": "BDE is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 134509,
          "date": "Fri 15 Oct 2021 19:43",
          "username": "NikkyDicky",
          "content": "BDE for sure",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#495",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.<br>The Finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs.<br>The Security team requires a centralized mechanism to control IAM usage in all the company's accounts.<br>What combination of the following options meet the company's needs with the LEAST effort? (Choose two.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#495",
          "answers": [
            {
              "choice": "<p>A. Use a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Require each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Consolidate all of the company's AWS accounts into a single AWS account. Use tags for billing purposes and IAM's Access Advisor feature to enforce the least privilege model.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 12530,
          "date": "Tue 21 Sep 2021 23:54",
          "username": "donathon",
          "content": "BD<br>A: While CloudFormation is a good start, remember this does not prevent changes after the stack has been deployed.<br>B: This looks likely.<br>C: This does not allow Finance to view the bill in a centralized manner which is a requirement.<br>D: This is the best way to meet the security requirements. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.<br>E: It’s best to use different accounts for dev\\test and prod.",
          "upvote_count": "28",
          "selected_answers": ""
        },
        {
          "id": 11347,
          "date": "Sun 19 Sep 2021 20:53",
          "username": "huhupai",
          "content": "I would go for B, D. ",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 687124,
          "date": "Wed 05 Oct 2022 20:19",
          "username": "Ni_yot",
          "content": "B and Dare correct ans",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 654067,
          "date": "Tue 30 Aug 2022 15:19",
          "username": "Rocketeer",
          "content": "B,D<br>https://aws.amazon.com/organizations/faqs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 519896,
          "date": "Sun 09 Jan 2022 05:35",
          "username": "chatvinoth",
          "content": "There are two requirements here - <br>1. Finance needs [ B ] <br>2. Security Teams needs [ D ]",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496617,
          "date": "Wed 08 Dec 2021 08:45",
          "username": "cldy",
          "content": "B.  Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.<br>D.  Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493996,
          "date": "Sat 04 Dec 2021 23:40",
          "username": "AzureDP900",
          "content": "B,D is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413880,
          "date": "Sat 06 Nov 2021 12:30",
          "username": "DerekKey",
          "content": "B correct - you will see cost allocated for each connected account<br>D correct - \\\"centralized mechanism to control IAM usage in all the company's accounts\\\"",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 410389,
          "date": "Fri 05 Nov 2021 04:57",
          "username": "WhyIronMan",
          "content": "I'll go for B,D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345919,
          "date": "Thu 04 Nov 2021 16:40",
          "username": "Waiweng",
          "content": "It's B and D",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291020,
          "date": "Wed 03 Nov 2021 22:17",
          "username": "Kian1Kian1",
          "content": "Will go with C,D Tags, Cost Explorer and SCPI read thr all the comments but my second thought is also CD. .",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 294963,
          "date": "Thu 04 Nov 2021 05:34",
          "username": "Kian1",
          "content": "I read thr all the comments but my second thought is also CD. .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 288062,
          "date": "Tue 02 Nov 2021 04:47",
          "username": "tipzzztipzzz",
          "content": "AWS Organization is ok for consolidated billing(https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-procedure.html), it's at account level but finance department needmaintain visibility into each GROUP’s spending.<br>That's why we need tag to solve this, also tag is a best pratices for cost allocation (https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf#:~:text=Amazon%20Web%20Services%20allows%20customers,search%20for%2C%20and%20filter%20resources.)Answers : CD",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 288063,
          "date": "Tue 02 Nov 2021 13:49",
          "username": "tipzzz",
          "content": "Answers : CD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 281074,
          "date": "Sat 30 Oct 2021 09:20",
          "username": "Ebi",
          "content": "I go with BD",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 242945,
          "date": "Wed 27 Oct 2021 19:12",
          "username": "T14102020",
          "content": "Correct answer BD.  Organization and SCP",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 240942,
          "date": "Mon 25 Oct 2021 22:32",
          "username": "RLai",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/consolidated-linked-billing-report/<br>Show individual account usage in the organization... therefore C & D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 230711,
          "date": "Mon 25 Oct 2021 17:21",
          "username": "Bulti",
          "content": "B And D are the answers.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230660,
          "date": "Mon 25 Oct 2021 12:59",
          "username": "jackdryan",
          "content": "I'll go for B,D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#496",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company collects a steady stream of 10 million data records from 100,000 sources each day. These records are written to an Amazon RDS MySQL DB.  A query must produce the daily average of a data source over the past 30 days. There are twice as many reads as writes. Queries to the collected data are for one source<br>ID at a time.<br>How can the Solutions Architect improve the reliability and cost effectiveness of this solution?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#496",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Aurora with MySQL in a Multi-AZ mode. Use four additional read replicas.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon DynamoDB with the source ID as the partition key. Use a different table each day.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Ingest data into Amazon Kinesis using a retention period of 30 days. Use AWS Lambda to write data records to Amazon ElastiCache for read access.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13612,
          "date": "Mon 20 Sep 2021 23:10",
          "username": "MoonMoonG3Firststack",
          "content": "I would go with \\\"B\\\".<br>A: would be preferred if no replicas!, because four replicas will make it costly solution.<br>B: TTL with DynamoDB will solve the database size cost. make it cost effective.<br>C: is not good solution.<br>D: Kinesis can store date unto 7 days.Also,<br>A: does not mention any deletion of the old data! which will be much much costly!Deletion of data is not a requirement as per the question. It only says the query needs to produce results using the last 30 days.there are twice reads as much as writes. I would go for B. Amazon Kinesis Data Streams supports changes to the data record retention period of your data stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).<br>https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html",
          "upvote_count": "37421",
          "selected_answers": ""
        },
        {
          "id": 13750,
          "date": "Thu 23 Sep 2021 15:30",
          "username": "MoonG3",
          "content": "Also,<br>A: does not mention any deletion of the old data! which will be much much costly!Deletion of data is not a requirement as per the question. It only says the query needs to produce results using the last 30 days.there are twice reads as much as writes. I would go for B. ",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 19041,
          "date": "Fri 24 Sep 2021 10:13",
          "username": "G3",
          "content": "Deletion of data is not a requirement as per the question. It only says the query needs to produce results using the last 30 days.there are twice reads as much as writes. I would go for B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 269865,
          "date": "Fri 22 Oct 2021 10:12",
          "username": "Firststack",
          "content": "Amazon Kinesis Data Streams supports changes to the data record retention period of your data stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 8760 hours (365 days).<br>https://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 12535,
          "date": "Mon 20 Sep 2021 16:56",
          "username": "donathonByrneyAWSPro24qianhaopoweripindado2020tobstar86Mobidic",
          "content": "A<br>A: Although Aurora is more expensive, it does improve the reliability.<br>B\\C: DynamoDB is a NOSQL so I don’t think it’s suitable for this case.How does Aurora being more expensive \\\"improve the cost-effectiveness\\\" of the solution?For everyone supporting B, so are we just going to re-architect the database to go from SQL to NoSQL with no mention in the question of if that is acceptable.Seems like a huge inference to me.No need to re architect, the Dynamo DB can be a staging DB just supporting then queryConsidering the use case... in fact it is a simple data store... no complex queries...<br>And for sure A is very very expensive....<br>Then BNoSQL = Not Only SQL, it's fine to useThe use case is THE classic use case for an NOSQL -> DynamoDB.  I saw that before seeing the answers... You can tell that there is no 'relation' involved.",
          "upvote_count": "13251312",
          "selected_answers": ""
        },
        {
          "id": 712747,
          "date": "Mon 07 Nov 2022 02:50",
          "username": "Byrney",
          "content": "How does Aurora being more expensive \\\"improve the cost-effectiveness\\\" of the solution?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 45345,
          "date": "Mon 27 Sep 2021 11:34",
          "username": "AWSPro24qianhaopoweripindado2020tobstar86",
          "content": "For everyone supporting B, so are we just going to re-architect the database to go from SQL to NoSQL with no mention in the question of if that is acceptable.Seems like a huge inference to me.No need to re architect, the Dynamo DB can be a staging DB just supporting then queryConsidering the use case... in fact it is a simple data store... no complex queries...<br>And for sure A is very very expensive....<br>Then BNoSQL = Not Only SQL, it's fine to use",
          "upvote_count": "5131",
          "selected_answers": ""
        },
        {
          "id": 93807,
          "date": "Sat 02 Oct 2021 22:32",
          "username": "qianhaopower",
          "content": "No need to re architect, the Dynamo DB can be a staging DB just supporting then query",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 182814,
          "date": "Mon 11 Oct 2021 14:19",
          "username": "ipindado2020",
          "content": "Considering the use case... in fact it is a simple data store... no complex queries...<br>And for sure A is very very expensive....<br>Then B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 571427,
          "date": "Sun 20 Mar 2022 07:22",
          "username": "tobstar86",
          "content": "NoSQL = Not Only SQL, it's fine to use",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 55604,
          "date": "Wed 29 Sep 2021 09:24",
          "username": "Mobidic",
          "content": "The use case is THE classic use case for an NOSQL -> DynamoDB.  I saw that before seeing the answers... You can tell that there is no 'relation' involved.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 697876,
          "date": "Tue 18 Oct 2022 06:22",
          "username": "JohnPi",
          "content": "B.  Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 695236,
          "date": "Sat 15 Oct 2022 08:55",
          "username": "Dionenonly",
          "content": "A is the answer.<br>Dynamo DB is for NO SQL.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 624514,
          "date": "Wed 29 Jun 2022 10:34",
          "username": "Kinty1982",
          "content": "I would go with \\\"A\\\":<br>- Aurora supports MySQL so there is no need for architecture change<br>- MySQL have aggregation functions where DynamoDB does not (only client-side, so we have to scan few TB and fetch them - that would be a massive cost) https://stackoverflow.com/questions/26298829/does-dynamodb-support-aggregate-functions-like-avg-max-min<br>- Storing TBs of data would be cheaper in Aurora than in DynamoDB",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 569444,
          "date": "Thu 17 Mar 2022 03:55",
          "username": "jyrajan69",
          "content": "So many answers for B with no justification for switching from SQL to NO-Sql?? Its a simple problem, already running MySql, so to improve the system add Read Replicas, because more reads than writes. No mention of having to delete data as a requirement, so unless someone can tell me why, my answer will have to be A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 496577,
          "date": "Wed 08 Dec 2021 07:10",
          "username": "cldy",
          "content": "B.  Use Amazon DynamoDB with the source ID as the partition key and the timestamp as the sort key. Use a Time to Live (TTL) to delete data after 30 days.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 493997,
          "date": "Sat 04 Dec 2021 23:42",
          "username": "AzureDP900",
          "content": "I will pick B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 414799,
          "date": "Mon 01 Nov 2021 08:37",
          "username": "TiredDadAjayPrajapatiTiredDadTiredDad",
          "content": "10 million records * 100,000 sources * 100 bytes (assuming each record of 100 bytes) = (10000000*100000*100)/(1024*1024*1024*1024) = 90.9 TB of data each day! <br>Max size supported by Aurora MySQL is 128TB.  It can not take 30 days of data in one Aurora instance!I dont think it is 10 million from each source, it is 10 million from combined of all 10kk sources. I still would go with Dynamo. Aurora is expensive and it allready gives 6 replica by default so 4 read replica does not make senseSame reason can be applied for ElastiCache. Max size supported is 170.6TB, so can't hold 30 days of data.<br>https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-elasticache-for-redis-now-supports-up-to-250-nodes-per-cluster/When we query data from DynamoDB table using batchgetitem (https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html), A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. As such, when you have to write your application logic, you can query one table or 30 tables. With the 30 table approach, you can archive older tables or reduce WCU, RCU to make it cost effective.",
          "upvote_count": "4121",
          "selected_answers": ""
        },
        {
          "id": 711427,
          "date": "Sat 05 Nov 2022 00:02",
          "username": "AjayPrajapati",
          "content": "I dont think it is 10 million from each source, it is 10 million from combined of all 10kk sources. I still would go with Dynamo. Aurora is expensive and it allready gives 6 replica by default so 4 read replica does not make sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 414802,
          "date": "Thu 04 Nov 2021 18:36",
          "username": "TiredDadTiredDad",
          "content": "Same reason can be applied for ElastiCache. Max size supported is 170.6TB, so can't hold 30 days of data.<br>https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-elasticache-for-redis-now-supports-up-to-250-nodes-per-cluster/When we query data from DynamoDB table using batchgetitem (https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html), A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. As such, when you have to write your application logic, you can query one table or 30 tables. With the 30 table approach, you can archive older tables or reduce WCU, RCU to make it cost effective.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 414809,
          "date": "Fri 05 Nov 2021 10:01",
          "username": "TiredDad",
          "content": "When we query data from DynamoDB table using batchgetitem (https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html), A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. As such, when you have to write your application logic, you can query one table or 30 tables. With the 30 table approach, you can archive older tables or reduce WCU, RCU to make it cost effective.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 414798,
          "date": "Mon 01 Nov 2021 04:04",
          "username": "TiredDad",
          "content": "10 million records * 100,000 sources * 100 bytes (assuming each record of 100 bytes) = (10000000*100000*100)/(1024*1024*1024*1024) = 90.9 TB of data each day! <br>Max size supported by Aurora MySQL is 128TB.  It can not take 30 days of data in one Aurora instance!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 413893,
          "date": "Sun 31 Oct 2021 02:41",
          "username": "DerekKey",
          "content": "\\\"reliability and cost effectiveness\\\"<br>In my opinion - don't mix up cost with the daily average. Cost is related to the total cost of running such queries and the cost of supporting infrastructure.<br>B wrong - having one table to serve 300 million (and more) records will have a lot of constraints not to mention only one configuration of RCU/WCU that must support writing and reports<br>C correct - as mentioned by other people",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410395,
          "date": "Fri 29 Oct 2021 23:50",
          "username": "WhyIronMan",
          "content": "I'll go for B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345925,
          "date": "Fri 29 Oct 2021 13:27",
          "username": "Waiweng",
          "content": "it's B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 293996,
          "date": "Fri 29 Oct 2021 11:15",
          "username": "kiev",
          "content": "I am going to A.  It is almost criminal to use Dynamodb as it is not RDS. Option D only stores data by default for 7 days, so A is good for me unless the question says change of database is acceptable, I won't go near Dynamodb.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 291029,
          "date": "Fri 29 Oct 2021 11:00",
          "username": "Kian1",
          "content": "going with B",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 281078,
          "date": "Thu 28 Oct 2021 03:35",
          "username": "Ebi",
          "content": "I go with B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 279581,
          "date": "Wed 27 Oct 2021 07:37",
          "username": "certainly",
          "content": "B or C? I would go for B in for better reliability and cost effectiveness. C failed to mention deleting old data after 30 days. as the data size grow in dynamo table, it will spit into more partitions. If the RCU and WCU remain constant, they are divided equally across the partitions. When the allocation of that partition is used up, you risk throttling. AWS will permit burst RCU and WCU at times but it is not assured. To improve performance and not increase cost, you need to reduce the size of the table",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#497",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is moving a business-critical application onto AWS. It is a traditional three-tier web application using an Oracle database. Data must be encrypted in transit and at rest. The database hosts 12 TB of data. Network connectivity to the source Oracle database over the internal is allowed, and the company wants to reduce operational costs by using AWS Managed Services where possible. All resources within the web and application tiers have been migrated. The database has a few tables and a simple schema using primary keys only; however, it contains many Binary Large Object (BLOB) fields. It was not possible to use the database's native replication tools because of licensing restrictions.<br>Which database migration solution will result in the LEAST amount of impact to the application's availability?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#497",
          "answers": [
            {
              "choice": "<p>A. Provision an Amazon RDS for Oracle instance. Host the RDS database within a virtual private cloud (VPC) subnet with internet access, and set up the RDS database as an encrypted Read Replica of the source database. Use SSL to encrypt the connection between the two databases. Monitor the replication performance by watching the RDS ReplicaLag metric. During the application maintenance window, shut down the on-premises database and switch over the application connection to the RDS instance when there is no more replication lag. Promote the Read Replica into a standalone database instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Provision an Amazon EC2 instance and install the same Oracle database software. Create a backup of the source database using the supported tools. During the application maintenance window, restore the backup into the Oracle database running in the EC2 instance. Set up an Amazon RDS for Oracle instance, and create an import job between the databases hosted in AWS. Shut down the source database and switch over the database connections to the RDS instance when the job is complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS DMS to load and replicate the dataset between the on-premises Oracle database and the replication instance hosted on AWS. Provision an Amazon RDS for Oracle instance with Transparent Data Encryption (TDE) enabled and configure it as a target for the replication instance. Create a customer-managed AWS KMS master key to set it as the encryption key for the replication instance. Use AWS DMS tasks to load the data into the target RDS instance. During the application maintenance window and after the load tasks reach the ongoing replication phase, switch the database connections to the new database.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a compressed full database backup of the on-premises Oracle database during an application maintenance window. While the backup is being performed, provision a 10 Gbps AWS Direct Connect connection to increase the transfer speed of the database backup files to Amazon S3, and shorten the maintenance window period. Use SSL/TLS to copy the files over the Direct Connect connection. When the backup files are successfully copied, start the maintenance window, and rise any of the Amazon RDS supported tools to import the data into a newly provisioned Amazon RDS for Oracle instance with encryption enabled. Wait until the data is fully loaded and switch over the database connections to the new database. Delete the Direct Connect connection to cut unnecessary charges.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 13606,
          "date": "Fri 24 Sep 2021 14:45",
          "username": "Moon",
          "content": "More toward \\\"C\\\".<br>https://aws.amazon.com/blogs/apn/oracle-database-encryption-options-on-amazon-rds/<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.Options.AdvSecurity.html<br>(DMS in transit encryption)<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 12539,
          "date": "Fri 24 Sep 2021 09:23",
          "username": "donathondonathon",
          "content": "https://aws.amazon.com/blogs/database/best-practices-for-migrating-an-oracle-database-to-amazon-rds-postgresql-or-amazon-aurora-postgresql-source-database-considerations-for-the-oracle-and-aws-dms-cdc-environment/Answer is C",
          "upvote_count": "813",
          "selected_answers": ""
        },
        {
          "id": 13780,
          "date": "Sat 25 Sep 2021 19:55",
          "username": "donathon",
          "content": "Answer is C",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 496607,
          "date": "Wed 08 Dec 2021 08:25",
          "username": "cldy",
          "content": "C.  Use AWS DMS to load and replicate the dataset between the on-premises Oracle database and the replication instance hosted on AWS. Provision an Amazon RDS for Oracle instance with Transparent Data Encryption (TDE) enabled and configure it as a target for the replication instance. Create a customer-managed AWS KMS master key to set it as the encryption key for the replication instance. Use AWS DMS tasks to load the data into the target RDS instance. During the application maintenance window and after the load tasks reach the ongoing replication phase, switch the database connections to the new database.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494002,
          "date": "Sat 04 Dec 2021 23:47",
          "username": "AzureDP900",
          "content": "I will go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410399,
          "date": "Tue 02 Nov 2021 11:19",
          "username": "WhyIronMan",
          "content": "I'll go with C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345930,
          "date": "Mon 01 Nov 2021 11:28",
          "username": "Waiweng",
          "content": "it's C with DMS",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 291032,
          "date": "Fri 29 Oct 2021 21:26",
          "username": "Kian1",
          "content": "going with C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 281079,
          "date": "Wed 27 Oct 2021 06:34",
          "username": "Ebi",
          "content": "C is the answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 242985,
          "date": "Tue 26 Oct 2021 13:54",
          "username": "T14102020",
          "content": "Correct answer is C.  DMS tool",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 230725,
          "date": "Sun 24 Oct 2021 02:51",
          "username": "Bulti",
          "content": "Answer is C.  Cannot use native Oracle replication tool and Direct Connect cannot be setup during the maintenance window.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 230673,
          "date": "Sat 16 Oct 2021 00:15",
          "username": "jackdryan",
          "content": "I'll go with C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 229369,
          "date": "Fri 15 Oct 2021 11:50",
          "username": "gookseang",
          "content": "seems C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 196522,
          "date": "Mon 11 Oct 2021 07:01",
          "username": "Paitan",
          "content": "Definitely C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 150250,
          "date": "Sun 10 Oct 2021 03:44",
          "username": "fullaws",
          "content": "C is correct, D will reduce availability time and seem impossible without native replication.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 134514,
          "date": "Fri 08 Oct 2021 10:58",
          "username": "NikkyDicky",
          "content": "C clearly",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 62320,
          "date": "Sun 03 Oct 2021 02:46",
          "username": "virtualvirtual",
          "content": "I think it's C here (also Direct Connect could help). it's the only response with TDE and KMS and the question states that we must encrypt.(apologize: although Direct Connect could help ...)",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 62321,
          "date": "Sun 03 Oct 2021 04:34",
          "username": "virtual",
          "content": "(apologize: although Direct Connect could help ...)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 22215,
          "date": "Tue 28 Sep 2021 21:12",
          "username": "G3bilcat9Ow30Greg1234MobidicJoeylee",
          "content": "I think its D.  <br>https://aws.amazon.com/blogs/database/aws-dms-now-supports-r4-type-instances-and-learn-to-choose-the-right-instance-class-for-migrations-using-aws-dms/<br><br>It takes 3 days to transfer 3.5 TB, and this is 12 TB of data. The best is to take a Oracle data pump export and load it through third party udp transfer or Direct connect . The suggestion has been given by AWS in the following site. <br><br>https://d1.awsstatic.com/whitepapers/strategies-for-migrating-oracle-database-to-aws.pdf<br><br>DMS would not complete the transfer before 12 days with option C. Question states that you cannot use native replication tools of the database due to licensing restrictions. I think the only way out is using DMS.This statement makes me feel D is not very good.<br><br>While the backup is being performed, provision a 10 Gbps AWS Direct Connect connection<br><br>It is saying Direct connect can be setup immediately like VPNQuestion does not say it has to complete in \\\"X\\\" number of days. We could have it configured and running on a steady state (very low replication lag) for as long as the company can agree the cut over window.First 6 months, DMS is free, really designed to allow to \\\"take your time\\\".Direct Connect won’t be setup within a month",
          "upvote_count": "125111",
          "selected_answers": ""
        },
        {
          "id": 88037,
          "date": "Mon 04 Oct 2021 04:33",
          "username": "bilcat",
          "content": "Question states that you cannot use native replication tools of the database due to licensing restrictions. I think the only way out is using DMS.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 29393,
          "date": "Tue 28 Sep 2021 21:44",
          "username": "9Ow30",
          "content": "This statement makes me feel D is not very good.<br><br>While the backup is being performed, provision a 10 Gbps AWS Direct Connect connection<br><br>It is saying Direct connect can be setup immediately like VPN",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 44682,
          "date": "Thu 30 Sep 2021 11:47",
          "username": "Greg1234Mobidic",
          "content": "Question does not say it has to complete in \\\"X\\\" number of days. We could have it configured and running on a steady state (very low replication lag) for as long as the company can agree the cut over window.First 6 months, DMS is free, really designed to allow to \\\"take your time\\\".",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 55600,
          "date": "Sat 02 Oct 2021 21:24",
          "username": "Mobidic",
          "content": "First 6 months, DMS is free, really designed to allow to \\\"take your time\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 76544,
          "date": "Sun 03 Oct 2021 20:04",
          "username": "Joeylee",
          "content": "Direct Connect won’t be setup within a month",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#498",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has decided to move some workloads onto AWS to create a grid environment to run market analytics. The grid will consist of many similar instances, spun-up by a job-scheduling function. Each time a large analytics workload is completed, a new VPC is deployed along with job scheduler and grid nodes. Multiple grids could be running in parallel.<br>Key requirements are:<br>✑ Grid instances must communicate with Amazon S3 to retrieve data to be processed.<br>✑ Grid instances must communicate with Amazon DynamoDB to track intermediate data.<br>✑ The job scheduler needs only to communicate with the Amazon EC2 API to start new grid nodes.<br>A key requirement is that the environment has no access to the internet, either directly or via the on-premises proxy. However, the application needs to be able to seamlessly communicate to Amazon S3, Amazon DynamoDB, and Amazon EC2 API, without the need for reconfiguration for each new deployment.<br>Which of the following should the Solutions Architect do to achieve this target architecture? (Choose three.)<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AEF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#498",
          "answers": [
            {
              "choice": "<p>A. Enable VPC endpoints for Amazon S3 and DynamoDB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Disable Private DNS Name Support.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the application on the grid instances to use the private DNS name of the Amazon S3 endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Populate the on-premises DNS server with the private IP addresses of the EC2 endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Enable an interface VPC endpoint for EC2.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Configure Amazon S3 endpoint policy to permit access only from the grid nodes.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 11055,
          "date": "Wed 22 Sep 2021 09:06",
          "username": "u4x",
          "content": "AEF is correct",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 16220,
          "date": "Fri 24 Sep 2021 05:17",
          "username": "chaudhTechGuruuopspopVrushaliDbilcat",
          "content": "AEF.  <br>C is not correct. Private DNS Name for interface endpoint only while S3 is VPC endpoint.ACE : VPC endpoints can be defined under private DNS,<br>https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/aggree. Private DNS name is only available for interface endpoint.<br>S3 and Dynamodb use the \\\"gateway endpoint\\\" which require setting in the route table.agree. The link -https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html<br>talks the same.<br>Answer is AEFDNS resolutions is required for Gateway Endpoints, too. <br>https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#vpc-endpoints-limitations",
          "upvote_count": "186311",
          "selected_answers": ""
        },
        {
          "id": 17784,
          "date": "Fri 24 Sep 2021 16:00",
          "username": "TechGuru",
          "content": "ACE : VPC endpoints can be defined under private DNS,<br>https://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 21781,
          "date": "Sun 26 Sep 2021 02:04",
          "username": "uopspopVrushaliD",
          "content": "aggree. Private DNS name is only available for interface endpoint.<br>S3 and Dynamodb use the \\\"gateway endpoint\\\" which require setting in the route table.agree. The link -https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html<br>talks the same.<br>Answer is AEF",
          "upvote_count": "31",
          "selected_answers": ""
        },
        {
          "id": 107802,
          "date": "Sat 02 Oct 2021 04:15",
          "username": "VrushaliD",
          "content": "agree. The link -https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html<br>talks the same.<br>Answer is AEF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 88040,
          "date": "Fri 01 Oct 2021 10:56",
          "username": "bilcat",
          "content": "DNS resolutions is required for Gateway Endpoints, too. <br>https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#vpc-endpoints-limitations",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 671199,
          "date": "Sat 17 Sep 2022 04:13",
          "username": "Dionenonly",
          "content": "AEF is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AEF"
        },
        {
          "id": 649329,
          "date": "Sat 20 Aug 2022 10:03",
          "username": "gnic",
          "content": "AEF for sure",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AEF"
        },
        {
          "id": 626514,
          "date": "Sun 03 Jul 2022 12:36",
          "username": "KiraguJohn",
          "content": "• Use Gateway Endpoint if the AWS service is either DynamoDB or S3.<br>• Use Interface Endpoint for everything else.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 565674,
          "date": "Fri 11 Mar 2022 18:05",
          "username": "asfsdfsdf",
          "content": "I think its AEF<br>Take a look at GW Endpoint policies<br>https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html#vpc-endpoints-policies-s3<br>B - Not a valid answer will break the endpoint DNS access<br>C - you dont need to configure anything the application - once a GW endpoint is set and RTB is updated all queries to s3.xxxx.amazonaws.com will be routed privately<br>D - why do you need to integrate with on-prem DNS? the workload is in AWS<br>So AEF it is",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 498471,
          "date": "Fri 10 Dec 2021 10:17",
          "username": "cldy",
          "content": "A.  Enable VPC endpoints for Amazon S3 and DynamoDB. <br>E.  Enable an interface VPC endpoint for EC2.<br>F.  Configure Amazon S3 endpoint policy to permit access only from the grid nodes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494003,
          "date": "Sat 04 Dec 2021 23:49",
          "username": "AzureDP900",
          "content": "AEF is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 488839,
          "date": "Sun 28 Nov 2021 05:49",
          "username": "acloudguru",
          "content": "The link - https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html<br>talks the same.<br>Answer is AEF",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AEF"
        },
        {
          "id": 450952,
          "date": "Sun 07 Nov 2021 15:44",
          "username": "DonSp",
          "content": "A great question that most people got wrong. F is good in general but not a valid answer as there is no requirement. D on the other hand is required for \\\"without the need for reconfiguration for each new deployment\\\". Instead of an IP address which is different in each VPN, the scheduler needs a well-known DNS name. AE are obvious, so ADE. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 413912,
          "date": "Fri 05 Nov 2021 18:05",
          "username": "DerekKey",
          "content": "F correct - endpoint default policy allows access by any user or service within the VPC, using credentials from any AWS account, to any Amazon S3 resource; including Amazon S3 resources for an AWS account other than the account with which the VPC is associated",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 410404,
          "date": "Fri 05 Nov 2021 12:52",
          "username": "WhyIronManWhyIronMan",
          "content": "I go for A, E, F. If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.",
          "upvote_count": "21",
          "selected_answers": ""
        },
        {
          "id": 410471,
          "date": "Fri 05 Nov 2021 17:44",
          "username": "WhyIronMan",
          "content": "If you've already set up access to your Amazon S3 resources from your VPC, you can continue to use Amazon S3 DNS names to access those resources after you've set up an endpoint.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 375030,
          "date": "Fri 05 Nov 2021 04:21",
          "username": "chkmtess",
          "content": "Amazon S3 interface endpoints do not support the private DNS feature of interface endpoints<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 345933,
          "date": "Wed 03 Nov 2021 22:14",
          "username": "Waiweng",
          "content": "A,E,F is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 337970,
          "date": "Tue 02 Nov 2021 05:05",
          "username": "0103701037",
          "content": "A, E, I understand.<br>F, there is no requirement in the question, and how to implement F using Amazon S3 endpoint policy?As to C, Private DNS name is only available for interface endpoint.<br>S3 and Dynamodb use the \\\"gateway endpoint\\\" which require setting in the route table.",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 337971,
          "date": "Wed 03 Nov 2021 00:54",
          "username": "01037",
          "content": "As to C, Private DNS name is only available for interface endpoint.<br>S3 and Dynamodb use the \\\"gateway endpoint\\\" which require setting in the route table.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 326744,
          "date": "Sun 31 Oct 2021 07:41",
          "username": "chris1025",
          "content": "This question appears to be missing some info because we don't know how the on-prem communicates with AWS. Assuming there's a VPN or DX because the AWS environment isn't allowed to connect to the internet, then ADE appears correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 297193,
          "date": "Sat 30 Oct 2021 06:47",
          "username": "Chubb",
          "content": "E is not necessary. Endpoint for S3 and dynamo DB is enough",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#499",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An internal security audit of AWS resources within a company found that a number of Amazon EC2 instances running Microsoft Windows workloads were missing several important operating system-level patches. A Solutions Architect has been asked to fix existing patch deficiencies, and to develop a workflow to ensure that future patching requirements are identified and taken care of quickly. The Solutions Architect has decided to use AWS Systems Manager. It is important that EC2 instance reboots do not occur at the same time on all Windows workloads to meet organizational uptime requirements.<br>Which workflow will meet these requirements in an automated manner?<br><br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#499",
          "answers": [
            {
              "choice": "<p>A. Add a Patch Group tag with a value of Windows Servers to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-DefaultPatchBaseline to the Windows Servers patch group. Define an AWS Systems Manager maintenance window, conduct patching within it, and associate it with the Windows Servers patch group. Register instances with the maintenance window using associated subnet IDs. Assign the AWS- RunPatchBaseline document as a task within each maintenance window.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add a Patch Group tag with a value of Windows Servers to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-WindowsPatchBaseline to the Windows Servers patch group. Create an Amazon CloudWatch Events rule configured to use a cron expression to schedule the execution of patching using the AWS Systems Manager run command. Assign the AWS-RunWindowsPatchBaseline document as a task associated with the Windows Servers patch group. Create an AWS Systems Manager State Manager document to define commands to be executed during patch execution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add a Patch Group tag with a value of either Windows Servers1 or Windows Servers2 to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-DefaultPatchBaseline with both Windows Servers patch groups. Define two non-overlapping AWS Systems Manager maintenance windows, conduct patching within them, and associate each with a different patch group. Register targets with specific maintenance windows using the Patch Group tags. Assign the AWS-RunPatchBaseline document as a task within each maintenance window.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add a Patch Group tag with a value of either Windows Servers1 or Windows Server2 to all existing EC2 instances. Ensure that all Windows EC2 instances are assigned this tag. Associate the AWS-WindowsPatchBaseline with both Windows Servers patch groups. Define two non-overlapping AWS Systems Manager maintenance windows, conduct patching within them, and associate each with a different patch group. Assign the AWS-RunWindowsPatchBaseline document as a task within each maintenance window. Create an AWS Systems Manager State Manager document to define commands to be executed during patch execution.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 317702,
          "date": "Thu 23 Sep 2021 18:20",
          "username": "nitinz",
          "content": "True C is correct. Right from white page from AWS",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 310979,
          "date": "Sun 19 Sep 2021 22:16",
          "username": "anandbabu",
          "content": "correct answer is C",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 671201,
          "date": "Sat 17 Sep 2022 04:18",
          "username": "Dionenonly",
          "content": "C as per AWS whitepaper",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 669072,
          "date": "Wed 14 Sep 2022 15:53",
          "username": "dcdcdc3",
          "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-about-aws-runpatchbaseline.html<br>Will go with B as concurrency can be set on a single group",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 654093,
          "date": "Tue 30 Aug 2022 16:00",
          "username": "Rocketeer",
          "content": "I think A is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 586572,
          "date": "Sat 16 Apr 2022 04:55",
          "username": "pawanvu",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 494005,
          "date": "Sat 04 Dec 2021 23:51",
          "username": "AzureDP900",
          "content": "C is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 483747,
          "date": "Mon 22 Nov 2021 01:18",
          "username": "acloudguru",
          "content": "C Define two non-overlapping AWS Systems Manager maintenance windows + Define two non-overlapping AWS Systems Manager maintenance windows",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 450105,
          "date": "Tue 02 Nov 2021 21:34",
          "username": "Kopa",
          "content": "Going for C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 437899,
          "date": "Sun 31 Oct 2021 18:53",
          "username": "tgv",
          "content": "CCC<br>---",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 410469,
          "date": "Sun 31 Oct 2021 04:22",
          "username": "WhyIronMan",
          "content": "I'll go for C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 346111,
          "date": "Mon 25 Oct 2021 05:04",
          "username": "Waiweng",
          "content": "it's C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 335880,
          "date": "Sun 24 Oct 2021 12:02",
          "username": "ExtHoExtHo",
          "content": "Requirement:<br>It is important that EC2 instance reboots do not occur at the same time on all Windows workloads<br><br>CDefine two non-overlapping AWS Systems Manager maintenance windows + Define two non-overlapping AWS Systems Manager maintenance windows2nd part after + AWS-DefaultPatchBaseline",
          "upvote_count": "32",
          "selected_answers": ""
        },
        {
          "id": 335881,
          "date": "Sun 24 Oct 2021 17:40",
          "username": "ExtHo",
          "content": "2nd part after + AWS-DefaultPatchBaseline",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 335466,
          "date": "Sat 23 Oct 2021 09:28",
          "username": "01037",
          "content": "C for sure.<br>But if using Rate controls, I think A also works",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 328073,
          "date": "Fri 24 Sep 2021 05:54",
          "username": "SunflyhomeSD13Amitv2706",
          "content": "B is correct.Within one maintenance window, a rate control can be used to defined to run task. You don't have to split servers into 2 groups , just only, for rebooting them in different time windows.<br>https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/AWS-RunWindowsPatchBaseline does not exist.Agree. AWS-RunWindowsPatchBaseline does not exist.<br>C is correct.",
          "upvote_count": "455",
          "selected_answers": ""
        },
        {
          "id": 330710,
          "date": "Wed 06 Oct 2021 02:26",
          "username": "SD13Amitv2706",
          "content": "AWS-RunWindowsPatchBaseline does not exist.Agree. AWS-RunWindowsPatchBaseline does not exist.<br>C is correct.",
          "upvote_count": "55",
          "selected_answers": ""
        },
        {
          "id": 334557,
          "date": "Mon 18 Oct 2021 04:45",
          "username": "Amitv2706",
          "content": "Agree. AWS-RunWindowsPatchBaseline does not exist.<br>C is correct.",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    }
  ]
}
