var SAP_C02_All = 
{
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": "#1",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to architect a hybrid DNS solution. This solution will use an Amazon Route 53 private hosted zone for the domain cloud.example.com for the resources stored within VPCs.<br>The company has the following DNS resolution requirements:<br>On-premises systems should be able to resolve and connect to cloud.example.com.<br>All VPCs should be able to resolve cloud.example.com.<br>There is already an AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway.<br>Which architecture should the company use to meet these requirements with the HIGHEST performance?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#1",
          "answers": [
            {
              "choice": "<p>A. Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC.  Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Associate the private hosted zone to all the VPCs. Deploy an Amazon EC2 conditional forwarder in the shared services VPC.  Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the conditional forwarder.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Associate the private hosted zone to the shared services VPCreate a Route 53 outbound resolver in the shared services VPAttach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the outbound resolver.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Associate the private hosted zone to the shared services VPC.  Create a Route 53 inbound resolver in the shared services VPC.  Attach the shared services VPC to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741364,
          "date": "Sun 11 Dec 2022 03:06",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "A.  Correct answer. Source: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/<br><br>NOT B.  EC2 conditional forwarder will not meet Highest performance requirement.<br><br>NOT C.  Missing: Need to associate private hosted zone to all VPC. <br>\\\"All VPC's will need to associate their private hosted zones to all other VPC's if required to.\\\"<br>Source: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/<br><br>NOT D.  Missing: Need to associate private hosted zone to all VPC. <br>\\\"All VPC's will need to associate their private hosted zones to all other VPC's if required to.\\\"<br>Source: https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 852751,
          "date": "Tue 28 Mar 2023 06:11",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Associate the private hosted zone to all the VPCs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835464,
          "date": "Fri 10 Mar 2023 22:42",
          "username": "\t\t\t\tIndreshKumar\t\t\t",
          "content": "A.  Correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831995,
          "date": "Tue 07 Mar 2023 15:25",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "Correct answer is A<br><br>Why D is not correct - The transit gateway may need to forward requests to the inbound resolver in order to introduce additional latency.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831689,
          "date": "Tue 07 Mar 2023 10:08",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "ttps://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 827664,
          "date": "Fri 03 Mar 2023 05:44",
          "username": "\t\t\t\tkrushna5966\t\t\t",
          "content": "Every one has selected option A so why system is showing Option D can anyone explain",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826508,
          "date": "Thu 02 Mar 2023 06:02",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818661,
          "date": "Thu 23 Feb 2023 03:00",
          "username": "\t\t\t\tGabehcoud\t\t\t",
          "content": "can I check with those who has written the exam, <br>1. was this question even there? <br>2. Was the answer A right?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816007,
          "date": "Tue 21 Feb 2023 00:24",
          "username": "\t\t\t\tospherenet\t\t\t",
          "content": "It appears that Option A is the correct answer. The company can associate the private hosted zone to all the VPCs and create a Route 53 inbound resolver in the shared services VPC.  They can then attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver. This will allow on-premises systems to resolve and connect to cloud.example.com and all VPCs to resolve cloud.example.com with the highest performance. Option B is incorrect because an EC2 conditional forwarder will not meet the highest performance requirement. Option C and D are incorrect because they both miss the requirement of associating the private hosted zone to all the VPCs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809114,
          "date": "Wed 15 Feb 2023 05:55",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The best architecture to meet the given requirements with the HIGHEST performance would be Option A:<br>A.  Associate the private hosted zone to all the VPCs. Create a Route 53 inbound resolver in the shared services VPC.  Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.<br><br>This architecture ensures that all VPCs can resolve the cloud.example.com domain using the private hosted zone. Additionally, it creates a Route 53 inbound resolver in the shared services VPC that can handle DNS resolution requests from on-premises systems through the transit gateway. This setup allows for fast and efficient DNS resolution with minimal latency.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 804661,
          "date": "Fri 10 Feb 2023 20:10",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "A is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779764,
          "date": "Wed 18 Jan 2023 09:42",
          "username": "\t\t\t\tJacktheriser2019\t\t\t",
          "content": "A answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776540,
          "date": "Sun 15 Jan 2023 13:36",
          "username": "\t\t\t\tNicocacik\t\t\t",
          "content": "Definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774663,
          "date": "Fri 13 Jan 2023 17:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct option would be option A:<br><br>Associate the private hosted zone to all the VPCs.<br>Create a Route 53 inbound resolver in the shared services VPC. <br>Attach all VPCs to the transit gateway and create forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver.<br>This option will allow the on-premises systems to resolve and connect to cloud.example.com by forwarding the DNS queries to the inbound resolver in the shared services VPC, which will then forward the queries to the private hosted zone. All VPCs will be able to resolve cloud.example.com by resolving the queries through the private hosted zone associated to all VPCs. Additionally, this option takes advantage of the already existing AWS Direct Connect connection between the on-premises corporate network and AWS Transit Gateway, which will provide the highest performance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 771071,
          "date": "Tue 10 Jan 2023 06:14",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "A is correct answer as all VPCs need to be accessed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 752894,
          "date": "Thu 22 Dec 2022 01:16",
          "username": "\t\t\t\tWuKongCoder\t\t\t",
          "content": "A correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751834,
          "date": "Wed 21 Dec 2022 04:23",
          "username": "\t\t\t\tarron86\t\t\t",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-dns-management-of-hybrid-cloud-with-amazon-route-53-and-aws-transit-gateway/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#2",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is providing weather data over a REST-based API to several customers. The API is hosted by Amazon API Gateway and is integrated with different AWS Lambda functions for each API operation. The company uses Amazon Route 53 for DNS and has created a resource record of weather.example.com. The company stores data for the API in Amazon DynamoDB tables. The company needs a solution that will give the API the ability to fail over to a different AWS Region.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#2",
          "answers": [
            {
              "choice": "<p>A. Deploy a new set of Lambda functions in a new Region. Update the API Gateway API to use an edge-optimized API endpoint with Lambda functions from both Regions as targets. Convert the DynamoDB tables to global tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy a new API Gateway API in a new Region. Change the Lambda functions to global functions. Change the Route 53 DNS record to a multivalue answer. Add both API Gateway APIs to the answer. Enable target health monitoring. Convert the DynamoDB tables to global tables.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741377,
          "date": "Sun 11 Dec 2022 03:39",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "C. <br>https://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 852753,
          "date": "Tue 28 Mar 2023 06:12",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is good here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831693,
          "date": "Tue 07 Mar 2023 10:13",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 819832,
          "date": "Thu 23 Feb 2023 23:24",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "Easy one :)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809115,
          "date": "Wed 15 Feb 2023 05:57",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The best solution to give the API the ability to fail over to a different AWS Region would be option C:<br>C.  Deploy a new API Gateway API and Lambda functions in another Region. Change the Route 53 DNS record to a failover record. Enable target health monitoring. Convert the DynamoDB tables to global tables.<br><br>This solution involves deploying a new API Gateway API and Lambda functions in another region. The company should also convert the DynamoDB tables to global tables to enable cross-region replication of the data. Then, the company should change the Route 53 DNS record to a failover record and enable target health monitoring to automatically route traffic to the new region in the event of a failure or outage in the primary region.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 804659,
          "date": "Fri 10 Feb 2023 20:10",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "C is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774664,
          "date": "Fri 13 Jan 2023 17:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The solution that will meet these requirements is option C:<br><br>Deploy a new API Gateway API and Lambda functions in another Region.<br>Change the Route 53 DNS record to a failover record.<br>Enable target health monitoring.<br>Convert the DynamoDB tables to global tables.<br><br>This solution will allow the API to failover to a different region, by using Route 53 failover record. The failover record will direct traffic to the primary API endpoint (the one in the primary region) as long as it is healthy. If the primary endpoint becomes unavailable, traffic will be directed to the secondary endpoint (the one in the secondary region). Additionally, by converting the DynamoDB tables to global tables, the data will be available in both regions, which is required for the failover scenario. Target health monitoring can be used to monitor the health of the API Gateway, and when it is determined that the primary endpoint is unavailable, the traffic will be directed to the secondary endpoint.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 741120,
          "date": "Sat 10 Dec 2022 17:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I agree with answer C.  this is the correct use case of road 53 DNS failover record",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#3",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses AWS Organizations with a single OU named Production to manage multiple accounts. All accounts are members of the Production OU. Administrators use deny list SCPs in the root of the organization to manage access to restricted services.<br>The company recently acquired a new business unit and invited the new unit's existing AWS account to the organization. Once onboarded, the administrators of the new business unit discovered that they are not able to update existing AWS Config rules to meet the company's policies.<br>Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#3",
          "answers": [
            {
              "choice": "<p>A. Remove the organization's root SCPs that limit access to AWS Config. Create AWS Service Catalog products for the company's standard AWS Config rules and deploy them throughout the organization, including the new account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the new account to the Production OU when adjustments to AWS Config are complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Convert the organization's root SCPs from deny list SCPs to allow list SCPs to allow the required services only. Temporarily apply an SCP to the organization's root that allows AWS Config actions for principals only in the new account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization's root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 740240,
          "date": "Fri 09 Dec 2022 16:57",
          "username": "\t\t\t\tSnip\t\t\t",
          "content": "Right answer is D. <br>An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can only filter; they never add permissions.<br>SO you need to create a new OU for the new account assign an SCP, and move the root SCP to Production OU. Then move the new account to production OU when AWS config is done.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 741380,
          "date": "Sun 11 Dec 2022 03:45",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Answer: D. <br><br>Not A: too much overhead and maintenance.<br>Not B: SCP at Root will still deny Config to the temporary OU.<br>Not C: Too much overhead to create allow list.",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 852756,
          "date": "Tue 28 Mar 2023 06:16",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 852466,
          "date": "Mon 27 Mar 2023 22:42",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D is the correct answer. Explicit Deny on root can't be bypassed by just adding “allow” in the OU SCP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831697,
          "date": "Tue 07 Mar 2023 10:24",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "enforce the current policies without introducing additional long-term maintenance -> requires organisaiton SCP to move to Production OU to avoid such issues in future",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 829420,
          "date": "Sat 04 Mar 2023 22:18",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D is 100% the correct answer.<br>explicit deny in the root SCPcan't be bypassed even which explicit allow..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827090,
          "date": "Thu 02 Mar 2023 17:12",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827086,
          "date": "Thu 02 Mar 2023 17:11",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Please note Question Constraint: Which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?<br>Strategies for using SCPs<br>You can configure the service control policies (SCPs) in your organization to work as either of the following:<br>A deny list – actions are allowed by default, and you specify what services and actions are prohibited<br>An allow list – actions are prohibited by default, and you specify what services and actions are allowed.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826510,
          "date": "Thu 02 Mar 2023 06:11",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "SCP at root level is the root cause of the new not working and Answer D is right fit for it",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 811330,
          "date": "Fri 17 Feb 2023 03:52",
          "username": "\t\t\t\tpromartyr\t\t\t",
          "content": "When they say \\\"Move the organization's root SCP to the Production OU\\\" - where is it moving from? Isn't there only one OU?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>from Onboarding OU to Production OU?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823052,
          "date": "Mon 27 Feb 2023 00:38",
          "username": "\t\t\t\tkamonegi\t\t\t",
          "content": "from Onboarding OU to Production OU?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809117,
          "date": "Wed 15 Feb 2023 06:02",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The best option to allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance would be option D:<br>D.  Create a temporary OU named Onboarding for the new account. Apply an SCP to the Onboarding OU to allow AWS Config actions. Move the organization's root SCP to the Production OU. Move the new account to the Production OU when adjustments to AWS Config are complete.<br><br>This solution involves creating a temporary OU named Onboarding for the new account and applying an SCP to the Onboarding OU that allows AWS Config actions. The organization's root SCP should be moved to the Production OU, and the new account should be moved to the Production OU when the adjustments to AWS Config are complete. This approach allows the administrators of the new account to make changes to AWS Config rules while maintaining the current policies in the Production OU.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806083,
          "date": "Sun 12 Feb 2023 08:58",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "D makes sense but there is something that does not: \\\"Apply an SCP to the Onboarding OU to allow AWS Config actions.\\\" SCPs never allow. I think it mkes D incorrect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804665,
          "date": "Fri 10 Feb 2023 20:12",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "D is correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 780429,
          "date": "Wed 18 Jan 2023 22:05",
          "username": "\t\t\t\tskashanali\t\t\t",
          "content": "Right answer is D<br><br>As permission are inherited from root, they have to remove the SCP from root and apply on Production OU..<br>Also allow SCP related to AWS config for onboarding temp OU and revert the changes.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774674,
          "date": "Fri 13 Jan 2023 17:43",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Yes, in option D, the solution is to create a temporary OU named Onboarding for the new account. By creating a new OU for the new account, it allows for a new set of permissions and policies to be applied to this account, separate from the existing Production OU.<br><br>Once the new OU is created, an SCP is applied to it to allow AWS Config actions. This SCP allows the new account to make necessary adjustments to AWS Config without being blocked by the existing policies at the root level of the organization.<br><br>Then, the root SCP that is blocking these actions is moved to the Production OU, where it will continue to block these actions for all other accounts that are members of the Production OU.<br><br>Finally, once the necessary adjustments are made, the new account can be moved to the Production OU, where it will be subject to the existing policies and restrictions.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This approach is the correct solution because it allows the new account to make necessary adjustments to AWS Config while still adhering to the company's policies, and it does not introduce additional long-term maintenance. The new account will be only in the new OU temporarily, and the SCP blocking AWS Config actions will only be in the root temporarily.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774675,
          "date": "Fri 13 Jan 2023 17:43",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This approach is the correct solution because it allows the new account to make necessary adjustments to AWS Config while still adhering to the company's policies, and it does not introduce additional long-term maintenance. The new account will be only in the new OU temporarily, and the SCP blocking AWS Config actions will only be in the root temporarily.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743191,
          "date": "Mon 12 Dec 2022 19:43",
          "username": "\t\t\t\tnez15\t\t\t",
          "content": "SAP-CO1 question",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 741122,
          "date": "Sat 10 Dec 2022 17:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D for me",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#4",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a two-tier web-based application in an on-premises data center. The application layer consists of a single server running a stateful application. The application connects to a PostgreSQL database running on a separate server. The application's user base is expected to grow significantly, so the company is migrating the application and database to AWS. The solution will use Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling, and Elastic Load Balancing.<br>Which solution will provide a consistent user experience that will allow the application and database tiers to scale?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#4",
          "answers": [
            {
              "choice": "<p>A. Enable Aurora Auto Scaling for Aurora Replicas. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable Aurora Auto Scaling for Aurora writers. Use an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable Aurora Scaling for Aurora writers. Use a Network Load Balancer with the least outstanding requests routing algorithm and sticky sessions enabled.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741387,
          "date": "Sun 11 Dec 2022 04:19",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "C. <br>- Aurora writers is a distractor.<br>- Single master mode only has read replica - with Aurora replicas.<br>- Multi master mode, not in the options<br>- NLB does not support round robin and least outstanding algorithmhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 852757,
          "date": "Tue 28 Mar 2023 06:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Aurora replicas + ALB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831699,
          "date": "Tue 07 Mar 2023 10:25",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html#Aurora.Replication.Replicas",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 826511,
          "date": "Thu 02 Mar 2023 06:14",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "C.  <br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html#Aurora.Replication.Replicas",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809118,
          "date": "Wed 15 Feb 2023 06:03",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The best solution to provide a consistent user experience that will allow the application and database tiers to scale would be option C:<br>C.  Enable Aurora Auto Scaling for Aurora Replicas. Use an Application Load Balancer with the round robin routing and sticky sessions enabled.<br><br>This solution involves enabling Aurora Auto Scaling for Aurora Replicas to automatically add and remove read replicas to match the application's workload. The solution also uses an Application Load Balancer to distribute traffic to the application layer, with the round robin routing algorithm to balance the traffic evenly across multiple instances. Sticky sessions should be enabled to maintain session affinity for each user, allowing for a consistent user experience.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774678,
          "date": "Fri 13 Jan 2023 17:45",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C is correct. This solution will provide a consistent user experience by using an Application Load Balancer with the round robin routing algorithm and sticky sessions enabled. This allows the application and database tiers to scale by using Aurora Auto Scaling for Aurora Replicas. This will ensure that the application is able to handle the increased user base while maintaining a consistent user experience. The use of an Application Load Balancer also allows for better routing of traffic to the available Aurora Replicas.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 757284,
          "date": "Mon 26 Dec 2022 09:55",
          "username": "\t\t\t\tThaiNT\t\t\t",
          "content": "Using Amazon Aurora Auto Scaling with Aurora replicas<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753295,
          "date": "Thu 22 Dec 2022 13:53",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748050,
          "date": "Sat 17 Dec 2022 13:37",
          "username": "\t\t\t\tArun_Bala\t\t\t",
          "content": "Correct ans is c",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743193,
          "date": "Mon 12 Dec 2022 19:45",
          "username": "\t\t\t\tnez15\t\t\t",
          "content": "SAP-C01 Question. <br>https://www.examtopics.com/discussions/amazon/view/36075-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#5",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses a service to collect metadata from applications that the company hosts on premises. Consumer devices such as TVs and internet radios access the applications. Many older devices do not support certain HTTP headers and exhibit errors when these headers are present in responses. The company has configured an on-premises load balancer to remove the unsupported headers from responses sent to older devices, which the company identified by the User-Agent headers.<br>The company wants to migrate the service to AWS, adopt serverless technologies, and retain the ability to support the older devices. The company has already migrated the applications into a set of AWS Lambda functions.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#5",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB.  Configure the ALB to invoke the correct Lambda function for each type of request. Create a CloudFront function to remove the problematic headers based on the value of the User-Agent header.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon API Gateway REST API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Modify the default gateway responses to remove the problematic headers based on the value of the User-Agent header.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon API Gateway HTTP API for the metadata service. Configure API Gateway to invoke the correct Lambda function for each type of request. Create a response mapping template to remove the problematic headers based on the value of the User-Agent. Associate the response data mapping with the HTTP API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudFront distribution for the metadata service. Create an Application Load Balancer (ALB). Configure the CloudFront distribution to forward requests to the ALB.  Configure the ALB to invoke the correct Lambda function for each type of request. Create a Lambda@Edge function that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 746619,
          "date": "Fri 16 Dec 2022 01:13",
          "username": "\t\t\t\tEricZhang\t\t\t",
          "content": "A.  The only difference between A and D is CloudFront function vs Lambda@Edge. In this case the CloudFront function can remove the response header based on request header and much faster/light-weight.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 741134,
          "date": "Sat 10 Dec 2022 18:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I think this is answer D: Lambda@Edge can modify headers<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 852759,
          "date": "Tue 28 Mar 2023 06:20",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Cloudfront can do it.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 844556,
          "date": "Mon 20 Mar 2023 06:23",
          "username": "\t\t\t\tramyaram\t\t\t",
          "content": "CloudFront functions are very light weight and most efficient for this use case",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835540,
          "date": "Sat 11 Mar 2023 01:47",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "\\\"Header manipulation – You can insert, modify, or delete HTTP headers in the request or response.\\\" - https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835158,
          "date": "Fri 10 Mar 2023 16:30",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "B as per the question that non Http Device need response other than HTTP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831709,
          "date": "Tue 07 Mar 2023 10:40",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Confused between A and D , but will go with A in the exam based on below explainatins<br>https://medium.com/trackit/cloudfront-functions-vs-lambda-edge-which-one-should-you-choose-c88527647695<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 827615,
          "date": "Fri 03 Mar 2023 03:57",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "D is correct <br>This solution uses Amazon CloudFront with an Application Load Balancer (ALB) and AWS Lambda@Edge to remove problematic headers based on the User-Agent header. CloudFront can be used as a content delivery network (CDN) to deliver the metadata service to consumer devices while the ALB is used to invoke the correct Lambda function for each type of request. Lambda@Edge is used to modify the response headers in real-time based on the User-Agent header.<br><br>This solution addresses the requirement to support older devices that do not support certain HTTP headers by removing problematic headers based on the value of the User-Agent header. It also leverages serverless technologies such as AWS Lambda and Lambda@Edge for scalability and cost-effectiveness.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 825806,
          "date": "Wed 01 Mar 2023 13:22",
          "username": "\t\t\t\tAppon\t\t\t",
          "content": "In the question its stated that \\\"The company wants to migrate the (metadata) service to AWS...\\\"<br><br>In the answers involving CF, there is no mention of migrating metadata service...am I missing something?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820989,
          "date": "Fri 24 Feb 2023 23:04",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Per the feature comparisons between Lambda and CloudFront functions, A is the correct option as it clearly states it does header manipulation for the response headers and requests. <br><br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819889,
          "date": "Fri 24 Feb 2023 00:32",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A is the correct answer. Cloudfront functions (not @Edge) are suited for suchlight weight tasks and very important they are cheaper than Cloudfront@Edge which costs x3 the price of the Cloudfront function.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817558,
          "date": "Wed 22 Feb 2023 08:08",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "Cloudfront function is the suitable option as it is mentioned as ideal for header manipulations.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817268,
          "date": "Wed 22 Feb 2023 00:07",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Lambda@Edge can modify headers<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 816660,
          "date": "Tue 21 Feb 2023 15:22",
          "username": "\t\t\t\tPSPaul\t\t\t",
          "content": "It's should be D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816017,
          "date": "Tue 21 Feb 2023 00:33",
          "username": "\t\t\t\tospherenet\t\t\t",
          "content": "A is the correct answer.<br><br>Explanation:<br><br>CloudFront is a good option for delivering content and improving user experience with caching, reducing latency and increasing availability.<br>An Application Load Balancer (ALB) can be used with CloudFront to route requests to the correct Lambda function.<br>The CloudFront function can be used to remove the problematic headers based on the User-Agent header to support older devices.<br>Using CloudFront and Lambda functions will allow the company to adopt serverless technologies for this use case.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809121,
          "date": "Wed 15 Feb 2023 06:09",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "D: This solution involves creating an Amazon CloudFront distribution for the metadata service and configuring it to forward requests to the Application Load Balancer (ALB), which is used to invoke the correct Lambda function for each type of request. A Lambda@Edge function should be created that will remove the problematic headers in response to viewer requests based on the value of the User-Agent header. This approach allows the company to remove the problematic headers while supporting older devices and using serverless technologies.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 805833,
          "date": "Sun 12 Feb 2023 02:24",
          "username": "\t\t\t\tSubbuKhan\t\t\t",
          "content": "Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:<br><br>- After CloudFront receives a request from a viewer (viewer request)<br><br>- Before CloudFront forwards the request to the origin (origin request)<br><br>- After CloudFront receives the response from the origin (origin response)<br><br>- Before CloudFront forwards the response to the viewer (viewer response)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#6",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company needs to provide a series of data files to another company, which is its business partner. These files are saved in an Amazon S3 bucket under Account A, which belongs to the retail company. The business partner company wants one of its IAM users, User_DataProcessor, to access the files from its own AWS account (Account B).<br>Which combination of steps must the companies take so that User_DataProcessor can access the S3 bucket successfully? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C,D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#6",
          "answers": [
            {
              "choice": "<p>A. Turn on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In Account A, set the S3 bucket policy to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image1.png\"><br><br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In Account A, set the S3 bucket policy to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image2.png\"><br><br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. In Account B, set the permissions of User_DataProcessor to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image1.png\"><br><br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. In Account B, set the permissions of User_DataProcessor to the following:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image4.png\"><br><br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741606,
          "date": "Sun 11 Dec 2022 11:45",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Answer: C & D<br><br>Source: <br>https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 855471,
          "date": "Thu 30 Mar 2023 10:19",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "C and D, 100%",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 853650,
          "date": "Tue 28 Mar 2023 21:56",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C+D no doubts",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 852760,
          "date": "Tue 28 Mar 2023 06:21",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C + D are right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 835163,
          "date": "Fri 10 Mar 2023 16:32",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "I would select C as Account A need to grant access",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831715,
          "date": "Tue 07 Mar 2023 10:44",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "going with C and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 829430,
          "date": "Sat 04 Mar 2023 22:38",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C & D are the correct answers ✅",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827622,
          "date": "Fri 03 Mar 2023 04:07",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "C & D<br><br>To allow User_DataProcessor to access the S3 bucket from Account B, the following steps need to be taken:<br><br>In Account A, set the S3 bucket policy to allow access to the bucket from the IAM user in Account B.  This is done by adding a statement to the bucket policy that allows the IAM user in Account B to perform the necessary actions (GetObject and ListBucket) on the bucket and its contents.<br><br>In Account B, create an IAM policy that allows the IAM user (User_DataProcessor) to perform the necessary actions (GetObject and ListBucket) on the S3 bucket and its contents. The policy should reference the ARN of the S3 bucket and the actions that the user is allowed to perform.<br><br>Note: turning on the cross-origin resource sharing (CORS) feature for the S3 bucket in Account A is not necessary for this scenario as it is typically used for allowing web browsers to access resources from different domains.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827180,
          "date": "Thu 02 Mar 2023 18:51",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Two ways for Cross account permissions is either through bucket policies or using IAM role.<br>With Bucket Policy you need; and for this question , a user policy is required to delegate access to the user in the partner account. A bucket policy and a userpolicy. and bucket policy will include an arn<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html#access-policies-walkthrough-example4-overview<br><br>C: Bucket Policy in account A<br>D: User Policy in Account B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821217,
          "date": "Sat 25 Feb 2023 06:37",
          "username": "\t\t\t\tvandergun\t\t\t",
          "content": "c&D for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795394,
          "date": "Wed 01 Feb 2023 19:21",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "I think the answer is C & D. <br>But what's with E? You don't need the principal, but it would still work, right?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 780438,
          "date": "Wed 18 Jan 2023 22:18",
          "username": "\t\t\t\tskashanali\t\t\t",
          "content": "Allow specific user and specific actions on the mentioned S3 bucket is the right way. We always think of fine grain access.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 770967,
          "date": "Tue 10 Jan 2023 02:00",
          "username": "\t\t\t\tTeknoklutz\t\t\t",
          "content": "C and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 760996,
          "date": "Thu 29 Dec 2022 13:32",
          "username": "\t\t\t\tmmendozaf\t\t\t",
          "content": "Permissions is required to provide on the source component, at least.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 760806,
          "date": "Thu 29 Dec 2022 10:07",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "It says choose two. <br>C&A <br>C grants access and A whitelists the different domain.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 760211,
          "date": "Wed 28 Dec 2022 19:47",
          "username": "\t\t\t\tskashanali\t\t\t",
          "content": "Ans C, is for the S3 CORS bucket policy and<br>Ans D, for the User permission set to allow S3 bucket",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748053,
          "date": "Sat 17 Dec 2022 13:47",
          "username": "\t\t\t\tArun_Bala\t\t\t",
          "content": "Ans C & D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#7",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a traditional web application on Amazon EC2 instances. The company needs to refactor the application as microservices that run on containers. Separate versions of the application exist in two distinct environments: production and testing. Load for the application is variable, but the minimum load and the maximum load are known. A solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity.<br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#7",
          "answers": [
            {
              "choice": "<p>A. Upload the container images to AWS Lambda as functions. Configure a concurrency limit for the associated Lambda functions to handle the expected peak load. Configure two separate Lambda integrations within Amazon API Gateway: one for production and one for testing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Kubernetes Service (Amazon EKS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the EKS clusters.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload the container images to AWS Elastic Beanstalk. In Elastic Beanstalk, create separate environments and deployments for production and testing. Configure two separate Application Load Balancers to direct traffic to the Elastic Beanstalk deployments.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774685,
          "date": "Fri 13 Jan 2023 17:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Upload the container images to Amazon Elastic Container Registry (Amazon ECR). Configure two auto scaled Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Deploy tasks from the ECR images. Configure two separate Application Load Balancers to direct traffic to the ECS clusters.<br>This option meets the requirement of using a serverless architecture by utilizing the Fargate launch type for the ECS clusters, which allows for automatic scaling of the containers based on the expected load. It also allows for separate deployments for production and testing by configuring separate ECS clusters and Application Load Balancers for each environment. This option also minimizes operational complexity by utilizing ECS and Fargate for the container orchestration and scaling.",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 742275,
          "date": "Mon 12 Dec 2022 02:13",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "Answer is A.  ABC all works but A is most COST EFFECTIVE<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A) is not correct. AWS documentation says you can package and deploy Lambda functions AS container images. A) says Deploy Container images as lambda functions, the opposite.</li><li>Yes, would be cheap, but can't run a web app from Lambda</li><li>I do not think A is the right answer. <br>Because image must be upload to the ECR.</li><li>https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 842792,
          "date": "Sat 18 Mar 2023 14:13",
          "username": "\t\t\t\tMansaMunsa\t\t\t",
          "content": "A) is not correct. AWS documentation says you can package and deploy Lambda functions AS container images. A) says Deploy Container images as lambda functions, the opposite.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812486,
          "date": "Sat 18 Feb 2023 00:20",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "Yes, would be cheap, but can't run a web app from Lambda",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763230,
          "date": "Sun 01 Jan 2023 16:00",
          "username": "\t\t\t\tyuyuyuyuyu\t\t\t",
          "content": "I do not think A is the right answer. <br>Because image must be upload to the ECR.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742276,
          "date": "Mon 12 Dec 2022 02:15",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 855077,
          "date": "Thu 30 Mar 2023 01:34",
          "username": "\t\t\t\tcuonglc\t\t\t",
          "content": "Selected Answer: B<br>b for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 852775,
          "date": "Tue 28 Mar 2023 06:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ECS + Fargate",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835329,
          "date": "Fri 10 Mar 2023 19:58",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Confused between A and B but after a long think decided to go with A<br><br>Option A suggests uploading the container images to AWS Lambda as functions and configuring a concurrency limit to handle the expected peak load. This approach allows the company to take advantage of the benefits of serverless computing, such as auto-scaling, without having to manage any infrastructure. In addition, using Lambda integrations within Amazon API Gateway allows the company to direct traffic to the appropriate environment for testing or production.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 827940,
          "date": "Fri 03 Mar 2023 13:08",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "Option B is the most cost-effective solution that meets all the requirements.<br><br>This solution uploads the container images to Amazon Elastic Container Registry (Amazon ECR) and deploys them using Amazon Elastic Container Service (Amazon ECS) clusters with the Fargate launch type to handle the expected load. Two separate Application Load Balancers are configured to direct traffic to the ECS clusters for production and testing.<br><br>This solution is cost-effective as it leverages the benefits of serverless architecture with Fargate launch type that removes the need for server management and the cost of running idle servers. Additionally, with auto-scaling, the resources can be dynamically adjusted to handle varying traffic. Furthermore, the use of Application Load Balancers reduces operational complexity and allows for efficient traffic routing.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819275,
          "date": "Thu 23 Feb 2023 15:13",
          "username": "\t\t\t\tmacc183\t\t\t",
          "content": "I think the answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818100,
          "date": "Wed 22 Feb 2023 18:25",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/definitions.html<br><br>The question states the Solutions Architect needs to update the application with a serverless architecture.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 816194,
          "date": "Tue 21 Feb 2023 05:40",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Option B is the most cost-effective solution for the following reasons:<br><br>The use of Fargate, a serverless compute engine for containers, eliminates the need for managing and scaling the underlying infrastructure. This minimizes operational complexity and reduces costs as the resources are used only when required.<br>Auto scaling ensures that the application scales up and down based on the load, providing the required performance and availability without incurring additional costs.<br>Amazon ECS is a simpler and more cost-effective solution than Amazon EKS, which requires more management and additional resources to operate the Kubernetes control plane.<br>Using Application Load Balancers to direct traffic to the ECS clusters ensures high availability and fault tolerance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Changing to A, B is not serverless and cost-effective.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818105,
          "date": "Wed 22 Feb 2023 18:28",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Changing to A, B is not serverless and cost-effective.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807552,
          "date": "Mon 13 Feb 2023 16:49",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Although I would not use this way in production, A is the cheapest.<br>All ECS/EKS needs some LB in front, plus the hourly fee of the cluster.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806099,
          "date": "Sun 12 Feb 2023 09:20",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B is cheaper than C, otherwise both would work",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802408,
          "date": "Wed 08 Feb 2023 19:39",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "A can be cheaper but it's not performant for a web application. I assume that A does not use provisioned concurrency, so I have to deal with cold starts. If I use provisioned concurrency, I can make B cheaper.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 794492,
          "date": "Tue 31 Jan 2023 19:12",
          "username": "\t\t\t\tsergza\t\t\t",
          "content": "A is most Cost effective Does not need ALB and smallest operational overhead",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 766170,
          "date": "Thu 05 Jan 2023 01:44",
          "username": "\t\t\t\tNYB\t\t\t",
          "content": "it should be ECR + ECS + Fargate, ans: B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765305,
          "date": "Wed 04 Jan 2023 07:07",
          "username": "\t\t\t\tjeussin\t\t\t",
          "content": "EnableEKS+Fargate ??<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Currently available.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790673,
          "date": "Sat 28 Jan 2023 15:45",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Currently available.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760217,
          "date": "Wed 28 Dec 2022 19:52",
          "username": "\t\t\t\tskashanali\t\t\t",
          "content": "C & D is both valid but when it comes to cost-effective solution, I would go for ECS which does have additional cluster cost for its control plane.<br>https://www.clickittech.com/aws/amazon-ecs-vs-eks/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 758539,
          "date": "Tue 27 Dec 2022 14:21",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I Vote B. <br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-java-microservices-on-amazon-ecs-using-amazon-ecr-and-aws-fargate.html<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-java-microservices-on-amazon-ecs-using-amazon-ecr-and-load-balancing.html<br><br>Option C and D also work, but B is the most cost-effective.<br><br>Option A is wrong. It can launch only APIs and does not mention Web UI.<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-lambda-functions-with-container-images.html<br><br>Option C is wrong. Amazon EKS costs more than Amazon ECS a bit.<br>https://aws.amazon.com/ecs/pricing/<br>https://aws.amazon.com/eks/pricing/<br><br>Option D is wrong. The Docker environment of AWS Elastic Beanstalk is based on Amazon EC2. That costs more than AWS Fargate.<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's still technically possible to return html/css with AWS Lambda like what this guy did https://stackoverflow.com/a/59385039/422842</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802383,
          "date": "Wed 08 Feb 2023 19:22",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "It's still technically possible to return html/css with AWS Lambda like what this guy did https://stackoverflow.com/a/59385039/422842",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#8",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a multi-tier web application that runs on a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB and the Auto Scaling group are replicated in a backup AWS Region. The minimum value and the maximum value for the Auto Scaling group are set to zero. An Amazon RDS Multi-AZ DB instance stores the application's data. The DB instance has a read replica in the backup Region. The application presents an endpoint to end users by using an Amazon Route 53 record.<br>The company needs to reduce its RTO to less than 15 minutes by giving the application the ability to automatically fail over to the backup Region. The company does not have a large enough budget for an active-active strategy.<br>What should a solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#8",
          "answers": [
            {
              "choice": "<p>A. Reconfigure the application's Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Configure Route 53 with a health check that monitors the web application and sends an Amazon Simple Notification Service (Amazon SNS) notification to the Lambda function when the health check status is unhealthy. Update the application's Route 53 record with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the Auto Scaling group in the backup Region to have the same values as the Auto Scaling group in the primary Region. Reconfigure the application's Route 53 record with a latency-based routing policy that load balances traffic between the two ALBs. Remove the read replica. Replace the read replica with a standalone RDS DB instance. Configure Cross-Region Replication between the RDS DB instances by using snapshots and Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an endpoint in AWS Global Accelerator with the two ALBs as equal weighted targets. Create an AWS Lambda function in the backup Region to promote the read replica and modify the Auto Scaling group values. Create an Amazon CloudWatch alarm that is based on the HTTPCode_Target_5XX_Count metric for the ALB in the primary Region. Configure the CloudWatch alarm to invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741154,
          "date": "Sat 10 Dec 2022 18:52",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with B<br>https://docs.amazonaws.cn/en_us/Route53/latest/DeveloperGuide/welcome-health-checks.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is correct, because it meets the company's requirements for reducing RTO to less than 15 minutes and not having a large budget for an active-active strategy.<br><br>In this solution, the company creates an AWS Lambda function in the backup region which promotes the read replica and modifies the Auto Scaling group values. Route 53 is configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The Route 53 record is also updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This way, when the primary region goes down, the failover policy triggers and traffic is directed to the backup region, ensuring a quick recovery time.</li></ul>",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774692,
          "date": "Fri 13 Jan 2023 17:58",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is correct, because it meets the company's requirements for reducing RTO to less than 15 minutes and not having a large budget for an active-active strategy.<br><br>In this solution, the company creates an AWS Lambda function in the backup region which promotes the read replica and modifies the Auto Scaling group values. Route 53 is configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The Route 53 record is also updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This way, when the primary region goes down, the failover policy triggers and traffic is directed to the backup region, ensuring a quick recovery time.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 852778,
          "date": "Tue 28 Mar 2023 06:44",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "SNS + Health check",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831719,
          "date": "Tue 07 Mar 2023 10:50",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827941,
          "date": "Fri 03 Mar 2023 13:11",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "The best option to meet the requirements and reduce RTO to less than 15 minutes is to choose option B. <br><br>Option B involves creating an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values. Additionally, Route 53 can be configured with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. The application's Route 53 record can be updated with a failover policy that routes traffic to the ALB in the backup Region when a health check failure occurs.<br><br>This option is cost-effective as it does not require an active-active strategy, and it uses AWS services to minimize the RTO. The Lambda function can be invoked to promote the read replica in the backup region, and the Auto Scaling group values can be updated to launch EC2 instances in the backup region. Furthermore, the Route 53 health check feature can be used to monitor the web application and initiate the failover process.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807566,
          "date": "Mon 13 Feb 2023 17:08",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "It would be interesting to see if this actually works. SNS is a regional service, in the last outage of the Virginia Region, we lost SNS completely.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796179,
          "date": "Thu 02 Feb 2023 16:40",
          "username": "\t\t\t\taws0909\t\t\t",
          "content": "I will go with option B as it reduces the RTO",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 794958,
          "date": "Wed 01 Feb 2023 10:11",
          "username": "\t\t\t\tYihong\t\t\t",
          "content": "A: no health check<br>C: active active<br>D: Equal weight?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 758588,
          "date": "Tue 27 Dec 2022 14:59",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I Vote B. <br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html<br><br>Option A, C and D are wrong. The latency-based routing and endopoint weights should be used for active/active strategy.<br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html<br>https://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints-endpoint-weights.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753893,
          "date": "Fri 23 Dec 2022 06:56",
          "username": "\t\t\t\tptpho\t\t\t",
          "content": "I go with B<br>5xx is incorrectly method to cover the case of the main site completely down<br>Its not act-act loading so R53 should not load traffic between 2 ALBs",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#9",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a critical application on a single Amazon EC2 instance. The application uses an Amazon ElastiCache for Redis single-node cluster for an in-memory data store. The application uses an Amazon RDS for MariaDB DB instance for a relational database. For the application to function, each piece of the infrastructure must be healthy and must be in an active state.<br>A solutions architect needs to improve the application's architecture so that the infrastructure can automatically recover from failure with the least possible downtime.<br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ADF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#9",
          "answers": [
            {
              "choice": "<p>A. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are part of an Auto Scaling group that has a minimum capacity of two instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Elastic Load Balancer to distribute traffic across multiple EC2 instances. Ensure that the EC2 instances are configured in unlimited mode.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the DB instance to create a read replica in the same Availability Zone. Promote the read replica to be the primary DB instance in failure scenarios.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Modify the DB instance to create a Multi-AZ deployment that extends across two Availability Zones.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a replication group for the ElastiCache for Redis cluster. Configure the cluster to use an Auto Scaling group that has a minimum capacity of two instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create a replication group for the ElastiCache for Redis cluster. Enable Multi-AZ on the cluster.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 741159,
          "date": "Sat 10 Dec 2022 18:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with ADF<br>https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Using an Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances can help ensure that the application remains available in the event that one of the instances becomes unavailable. By configuring the instances as part of an Auto Scaling group with a minimum capacity of two instances, you can ensure that there is always at least one healthy instance to handle traffic.<br>D.  Modifying the DB instance to create a Multi-AZ deployment that extends across two availability zones can help ensure that the database remains available in the event of a failure. In the event of a failure, traffic will automatically be directed to the secondary availability zone, reducing the amount of downtime.<br>F.  Creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ can help ensure that the in-memory data store remains available in the event of a failure. This will allow traffic to be automatically directed to the secondary availability zone, reducing the amount of downtime.</li><li>Why C is wrong?</li><li>Other options like B.  and C.  does not meet the requirement because the instances are configured in unlimited mode, it will not be possible to ensure that there is always at least one healthy instance to handle traffic if there is a failure.</li><li>Issue with C - Read replica in the same AZ does not sound High availability</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 774694,
          "date": "Fri 13 Jan 2023 18:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Using an Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances can help ensure that the application remains available in the event that one of the instances becomes unavailable. By configuring the instances as part of an Auto Scaling group with a minimum capacity of two instances, you can ensure that there is always at least one healthy instance to handle traffic.<br>D.  Modifying the DB instance to create a Multi-AZ deployment that extends across two availability zones can help ensure that the database remains available in the event of a failure. In the event of a failure, traffic will automatically be directed to the secondary availability zone, reducing the amount of downtime.<br>F.  Creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ can help ensure that the in-memory data store remains available in the event of a failure. This will allow traffic to be automatically directed to the secondary availability zone, reducing the amount of downtime.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 752113,
          "date": "Wed 21 Dec 2022 10:58",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "Why C is wrong?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Other options like B.  and C.  does not meet the requirement because the instances are configured in unlimited mode, it will not be possible to ensure that there is always at least one healthy instance to handle traffic if there is a failure.</li><li>Issue with C - Read replica in the same AZ does not sound High availability</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774695,
          "date": "Fri 13 Jan 2023 18:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Other options like B.  and C.  does not meet the requirement because the instances are configured in unlimited mode, it will not be possible to ensure that there is always at least one healthy instance to handle traffic if there is a failure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Issue with C - Read replica in the same AZ does not sound High availability</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807107,
          "date": "Mon 13 Feb 2023 07:12",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Issue with C - Read replica in the same AZ does not sound High availability",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 853657,
          "date": "Tue 28 Mar 2023 22:08",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "ADF the correct answers ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 852780,
          "date": "Tue 28 Mar 2023 06:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ADF is the best fit.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 836974,
          "date": "Sun 12 Mar 2023 13:01",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "I believe, This is correct approach https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 833173,
          "date": "Wed 08 Mar 2023 17:46",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "adf correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 829053,
          "date": "Sat 04 Mar 2023 15:57",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Selecting E because - \\\"Multi-AZ is enabled by default on Redis (cluster mode enabled) clusters\\\" as per https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 827946,
          "date": "Fri 03 Mar 2023 13:21",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "Option B is incorrect because unlimited mode is a configuration option for an Auto Scaling group that is used to handle bursty workloads, and it does not provide any additional availability benefits.<br><br>Option C is incorrect because creating a read replica in the same Availability Zone does not provide any additional availability benefits, and it would not be able to take over in the event of a failure of the primary instance.<br><br>Option F is incorrect because Multi-AZ is not an option for ElastiCache for Redis clusters.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827943,
          "date": "Fri 03 Mar 2023 13:15",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "A, D, E are the correct options to meet the requirements.<br><br>Option A is correct because an Auto Scaling group with a minimum capacity of two instances and an Elastic Load Balancer distributing traffic across them can provide high availability and automatic recovery from failure.<br><br>Option D is correct because a Multi-AZ deployment for the RDS instance will ensure that there is a synchronized standby copy of the database in a separate Availability Zone that can be used for automatic failover.<br><br>Option E is correct because configuring an Auto Scaling group for the ElastiCache for Redis cluster will ensure that there is at least one available node at all times, and automatic recovery can be achieved by launching new instances to replace any failed nodes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827227,
          "date": "Thu 02 Mar 2023 19:29",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoScaling.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826525,
          "date": "Thu 02 Mar 2023 06:48",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 817278,
          "date": "Wed 22 Feb 2023 00:21",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Why F and Not E ? ElastiCache for Redis natively supports automatic Multi-AZ failover.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoScaling.html</li><li>This does not answer why not E</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 827230,
          "date": "Thu 02 Mar 2023 19:29",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoScaling.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This does not answer why not E</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829050,
          "date": "Sat 04 Mar 2023 15:53",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "This does not answer why not E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806111,
          "date": "Sun 12 Feb 2023 09:33",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I don't dislike C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 758600,
          "date": "Tue 27 Dec 2022 15:15",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "No doubt, ADF. <br>Option C is wrong. Creating a read replica 'in the same availability zone' makes no sense.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 757432,
          "date": "Mon 26 Dec 2022 14:05",
          "username": "\t\t\t\taimik\t\t\t",
          "content": "ADF all of them high availibility",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753899,
          "date": "Fri 23 Dec 2022 07:01",
          "username": "\t\t\t\tptpho\t\t\t",
          "content": "I go with ADF<br>Hope we have 74 questions like this =))",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748058,
          "date": "Sat 17 Dec 2022 13:59",
          "username": "\t\t\t\tArun_Bala\t\t\t",
          "content": "I go for correct answer as : ADF options",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADF"
        }
      ]
    },
    {
      "question_id": "#10",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB.  Static content is cached. Amazon Route 53 is used to host all public zones.<br>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB.  The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.<br>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.<br>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#10",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 742509,
          "date": "Mon 12 Dec 2022 08:30",
          "username": "\t\t\t\tRaj40\t\t\t",
          "content": "A & E<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedure",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 852782,
          "date": "Tue 28 Mar 2023 06:49",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AE - easy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 831723,
          "date": "Tue 07 Mar 2023 10:56",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 828287,
          "date": "Fri 03 Mar 2023 20:19",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "Explanation:<br>Option A allows the creation of a custom error page that can be hosted on an S3 bucket. Option E provides a way to configure a custom error response for CloudFront, which can point to the S3 bucket hosting the error page. This allows visitors to see a custom error page without modifying any of the application infrastructure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819931,
          "date": "Fri 24 Feb 2023 01:20",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A&E are the correct answers imo",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 813074,
          "date": "Sat 18 Feb 2023 15:16",
          "username": "\t\t\t\tPratap\t\t\t",
          "content": "A and E as per https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustomErrorResponses.html#custom-error-pages-procedure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807109,
          "date": "Mon 13 Feb 2023 07:22",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A is incorrect because, Cloud front already handles OAI and its easy to build up error page with it. DNS records apply is pretty quick, So C,E are correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788913,
          "date": "Thu 26 Jan 2023 17:25",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "A & C as S3 can be used to host the static website and Route 53 can be configured for health checks and fail-over routing.<br>Refer AWS documentation -<br>Route 53 Fail Over S3<br>(https://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/)<br><br>Option E is wrong as CloudFront would return the error response for failure and does not provide a page that Route 53 can point to.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774696,
          "date": "Fri 13 Jan 2023 18:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A: Creating an S3 bucket and uploading custom error pages to it will allow you to provide a custom error page to visitors when the ALB returns a 502 error.<br>Option E: By configuring CloudFront custom error pages, visitors will be redirected to a publicly accessible web page when a 502 error occurs.<br>DNS records can be modified to point to a publicly accessible web page, which will be displayed when the error occurs.<br><br>Option B and D are not a best practice since they would change the behavior of the load balancer and it's not the best way to display custom error pages.<br>Option C is not related to custom error pages and not the best way to handle the problem.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 761369,
          "date": "Thu 29 Dec 2022 19:37",
          "username": "\t\t\t\texcoRt\t\t\t",
          "content": "A & E - Classic Cloudfront error page mechanism",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 759334,
          "date": "Wed 28 Dec 2022 04:51",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Option A and E are the most simple way to meet the requirement.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 753911,
          "date": "Fri 23 Dec 2022 07:20",
          "username": "\t\t\t\tptpho\t\t\t",
          "content": "I go with AE<br>since R53 \\\"Evaluate Target Health\\\" works with Alias Records that support health checks, so CLDFR distribution cannot be selected",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753804,
          "date": "Fri 23 Dec 2022 03:27",
          "username": "\t\t\t\tJimmyWong0911\t\t\t",
          "content": "AE<br>SAP-C01 #831",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 752117,
          "date": "Wed 21 Dec 2022 11:03",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "AE<br>SAP-C01 #831",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 741628,
          "date": "Sun 11 Dec 2022 12:22",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Answer: A & C<br><br>C & E never state where is the publicly accessible webpage.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 741162,
          "date": "Sat 10 Dec 2022 19:07",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I want to answer AC. <br>Answer A to have a static web page.<br>The C response to have an ALB status check.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I was wrong the answer is AE<br>https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional/view/3/</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 770625,
          "date": "Mon 09 Jan 2023 17:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I was wrong the answer is AE<br>https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-professional/view/3/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#11",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has many AWS accounts and uses AWS Organizations to manage all of them. A solutions architect must implement a solution that the company can use to share a common network across multiple accounts.<br>The company's infrastructure team has a dedicated infrastructure account that has a VPC.  The infrastructure team must use this account to manage the network. Individual accounts cannot have the ability to manage their own networks. However, individual accounts must be able to create AWS resources within subnets.<br>Which combination of actions should the solutions architect perform to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#11",
          "answers": [
            {
              "choice": "<p>A. Create a transit gateway in the infrastructure account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable resource sharing from the AWS Organizations management account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create VPCs in each AWS account within the organization in AWS Organizations. Configure the VPCs to share the same CIDR range and subnets as the VPC in the infrastructure account. Peer the VPCs in each individual account with the VPC in the infrastructure account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each subnet to associate with the resource share.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a resource share in AWS Resource Access Manager in the infrastructure account. Select the specific AWS Organizations OU that will use the shared network. Select each prefix list to associate with the resource share.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 743010,
          "date": "Mon 12 Dec 2022 17:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with BD<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Step B is needed because it enables the organization to share resources across accounts.<br>Step D is needed because it allows the infrastructure account to share specific subnets with the other accounts in the organization, so that the other accounts can create resources within those subnets without having to manage their own networks.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 774700,
          "date": "Fri 13 Jan 2023 18:09",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Step B is needed because it enables the organization to share resources across accounts.<br>Step D is needed because it allows the infrastructure account to share specific subnets with the other accounts in the organization, so that the other accounts can create resources within those subnets without having to manage their own networks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752613,
          "date": "Wed 21 Dec 2022 18:40",
          "username": "\t\t\t\trazguru\t\t\t",
          "content": "A - Doesn't seem correct as the question didnt state multiple VPs, so transit gateway is not relevant. <br>I will go with B & D",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 852784,
          "date": "Tue 28 Mar 2023 06:51",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "BD is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 832492,
          "date": "Wed 08 Mar 2023 04:10",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "ANS: A & C. <br><br>Option B is not required because AWS Organizations is already being used to manage the accounts. Resource sharing needs to be enabled, but this can be done by creating a resource share.<br><br>Option D and E both involve creating a resource share in AWS Resource Access Manager (RAM), but they are not the correct solution for this scenario.Option D is specific to subnets, option E is specific to prefix lists, which are used for IP address ranges. Since VPCs are being used in this scenario, options D and E are not applicable.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831735,
          "date": "Tue 07 Mar 2023 11:08",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "wouldnt \\\"Select each prefix list to associate with the resource share.\\\" will be use to do then go with selecting each subnet",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827593,
          "date": "Fri 03 Mar 2023 02:49",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Q: A solution to share a common network across multiple accounts<br>-A because you need a way to route traffic, its either this or vpc peering(not mentioned)<br>-Dor E Because a you can use RAM to share a subnet or prefixes. I am leaning towards E bcos a prefix will be more efficient. e.g. rather than share a /24 subnet. I will share a /16 prefix.(network summarization)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798620,
          "date": "Sun 05 Feb 2023 06:04",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "Anwer is BD. <br>https://aws.amazon.com/jp/premiumsupport/knowledge-center/vpc-share-subnet-with-another-account/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/ja_jp/vpc/latest/userguide/vpc-sharing.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 798622,
          "date": "Sun 05 Feb 2023 06:05",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "https://docs.aws.amazon.com/ja_jp/vpc/latest/userguide/vpc-sharing.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792664,
          "date": "Mon 30 Jan 2023 12:05",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B & D seems to be the correct answers",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 761489,
          "date": "Thu 29 Dec 2022 22:00",
          "username": "\t\t\t\tskashanali\t\t\t",
          "content": "Ans A doesn't make sense.<br>You also need to enable sharing with AWS Organizations within Resource Access Manager service to share the subnet.<br>https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-orgs",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 759345,
          "date": "Wed 28 Dec 2022 05:12",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "AWS Resource Access Manager can share subnets with other AWS accounts.<br>https://docs.aws.amazon.com/ram/latest/userguide/shareable.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 753965,
          "date": "Fri 23 Dec 2022 08:26",
          "username": "\t\t\t\tptpho\t\t\t",
          "content": "I go with AD<br>the company can use to share a common network across multiple accounts -> TGW in infras account<br>Enable resource sharing is an optional to share all 'without having to enumerate each account'",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 741683,
          "date": "Sun 11 Dec 2022 13:33",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "B & D<br><br>https://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#12",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to use a third-party software-as-a-service (SaaS) application. The third-party SaaS application is consumed through several API calls. The third-party SaaS application also runs on AWS inside a VPC. <br>The company will consume the third-party SaaS application from inside a VPC.  The company has internal security policies that mandate the use of private connectivity that does not traverse the internet. No resources that run in the company VPC are allowed to be accessed from outside the company's VPC.  All permissions must conform to the principles of least privilege.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#12",
          "answers": [
            {
              "choice": "<p>A. Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Site-to-Site VPN connection between the third-party SaaS application and the company VPC.  Configure network ACLs to limit access across the VPN tunnels.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPC peering connection between the third-party SaaS application and the company VPUpdate route tables by adding the needed routes for the peering connection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS PrivateLink endpoint service. Ask the third-party SaaS provider to create an interface VPC endpoint for this endpoint service. Grant permissions for the endpoint service to the specific account of the third-party SaaS provider.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 742562,
          "date": "Mon 12 Dec 2022 09:49",
          "username": "\t\t\t\tRaj40\t\t\t",
          "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743013,
          "date": "Mon 12 Dec 2022 17:07",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.<br>This solution uses AWS PrivateLink, which creates a secure and private connection between the company's VPC and the third-party SaaS application VPC, without the traffic traversing the internet. The use of a security group and limiting access to the endpoint service conforms to the principle of least privilege.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774702,
          "date": "Fri 13 Jan 2023 18:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create an AWS PrivateLink interface VPC endpoint. Connect this endpoint to the endpoint service that the third-party SaaS application provides. Create a security group to limit the access to the endpoint. Associate the security group with the endpoint.<br>This solution uses AWS PrivateLink, which creates a secure and private connection between the company's VPC and the third-party SaaS application VPC, without the traffic traversing the internet. The use of a security group and limiting access to the endpoint service conforms to the principle of least privilege.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 852785,
          "date": "Tue 28 Mar 2023 06:53",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create an AWS PrivateLink interface VPC endpoint.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831742,
          "date": "Tue 07 Mar 2023 11:12",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753918,
          "date": "Fri 23 Dec 2022 07:30",
          "username": "\t\t\t\tptpho\t\t\t",
          "content": "It's A .clearly",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 753074,
          "date": "Thu 22 Dec 2022 09:08",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-access-saas.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741703,
          "date": "Sun 11 Dec 2022 13:54",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "A is correct.<br>https://docs.aws.amazon.com/vpc/latest/privatelink/create-endpoint-service.html#share-endpoint-service",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#13",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to implement a patching process for its servers. The on-premises servers and Amazon EC2 instances use a variety of tools to perform patching. Management requires a single report showing the patch status of all the servers and instances.<br>Which set of actions should a solutions architect take to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#13",
          "answers": [
            {
              "choice": "<p>A. Use AWS Systems Manager to manage patches on the on-premises servers and EC2 instances. Use Systems Manager to generate patch compliance reports.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use Amazon QuickSight integration with OpsWorks to generate patch compliance reports.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an Amazon EventBridge rule to apply patches by scheduling an AWS Systems Manager patch remediation job. Use Amazon Inspector to generate patch compliance reports.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS OpsWorks to manage patches on the on-premises servers and EC2 instances. Use AWS X-Ray to post the patch status to AWS Systems Manager OpsCenter to generate patch compliance reports.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 743019,
          "date": "Mon 12 Dec 2022 17:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A is good<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-on-premises.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A is correct. AWS Systems Manager can manage patches on both on-premises servers and EC2 instances and can generate patch compliance reports. AWS OpsWorks and Amazon Inspector are not specifically designed for patch management and therefore would not be the best choice for this use case. Using Amazon EventBridge rule and AWS X-Ray to generate patch compliance reports is not a practical solution as they are not designed for patch management reporting.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774705,
          "date": "Fri 13 Jan 2023 18:14",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A is correct. AWS Systems Manager can manage patches on both on-premises servers and EC2 instances and can generate patch compliance reports. AWS OpsWorks and Amazon Inspector are not specifically designed for patch management and therefore would not be the best choice for this use case. Using Amazon EventBridge rule and AWS X-Ray to generate patch compliance reports is not a practical solution as they are not designed for patch management reporting.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 852788,
          "date": "Tue 28 Mar 2023 06:54",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use AWS Systems Manager to manage patches",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831745,
          "date": "Tue 07 Mar 2023 11:15",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patch-management-hybrid-cloud/design-on-premises.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 826566,
          "date": "Thu 02 Mar 2023 08:06",
          "username": "\t\t\t\tgameoflove\t\t\t",
          "content": "AWS System Manager support On-premise and EC2 instance patching",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819940,
          "date": "Fri 24 Feb 2023 01:29",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A is correct ofc..easy one )",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753082,
          "date": "Thu 22 Dec 2022 09:18",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "AS THE SAME WITH SAP-C01 QUESTION 782",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 742569,
          "date": "Mon 12 Dec 2022 09:54",
          "username": "\t\t\t\tRaj40\t\t\t",
          "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 742295,
          "date": "Mon 12 Dec 2022 02:38",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#14",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application on several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer. The load on the application varies throughout the day, and EC2 instances are scaled in and out on a regular basis. Log files from the EC2 instances are copied to a central Amazon S3 bucket every 15 minutes. The security team discovers that log files are missing from some of the terminated EC2 instances.<br>Which set of actions will ensure that log files are copied to the central S3 bucket from the terminated EC2 instances?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#14",
          "answers": [
            {
              "choice": "<p>A. Create a script to copy log files to Amazon S3, and store the script in a file on the EC2 instance. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to send ABANDON to the Auto Scaling group to prevent termination, run the script to copy the log files, and terminate the instance using the AWS SDK.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Change the log delivery rate to every 5 minutes. Create a script to copy log files to Amazon S3, and add the script to EC2 instance user data. Create an Amazon EventBridge rule to detect EC2 instance termination. Invoke an AWS Lambda function from the EventBridge rule that uses the AWS CLI to run the user-data script to copy the log files and terminate the instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook that publishes a message to an Amazon Simple Notification Service (Amazon SNS) topic. From the SNS notification, call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send ABANDON to the Auto Scaling group to terminate the instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852789,
          "date": "Tue 28 Mar 2023 06:56",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Systems manager + eventbridge",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831759,
          "date": "Tue 07 Mar 2023 11:28",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774711,
          "date": "Fri 13 Jan 2023 18:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Create an AWS Systems Manager document with a script to copy log files to Amazon S3. Create an Auto Scaling lifecycle hook and an Amazon EventBridge rule to detect lifecycle events from the Auto Scaling group. Invoke an AWS Lambda function on the autoscaling:EC2_INSTANCE_TERMINATING transition to call the AWS Systems Manager API SendCommand operation to run the document to copy the log files and send CONTINUE to the Auto Scaling group to terminate the instance. This approach will use the Auto Scaling lifecycle hook to execute the script that copies log files to S3, before the instance is terminated, ensuring that all log files are copied from the terminated instances.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759354,
          "date": "Wed 28 Dec 2022 05:39",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "B<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744354,
          "date": "Tue 13 Dec 2022 19:24",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743021,
          "date": "Mon 12 Dec 2022 17:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I find answer C correct.<br>but can at the same time that an instance is terminated run a lambda function that executes the script?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I'm wrong the answer is B<br><br>https://www.examtopics.com/discussions/amazon/view/69532-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743097,
          "date": "Mon 12 Dec 2022 18:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I'm wrong the answer is B<br><br>https://www.examtopics.com/discussions/amazon/view/69532-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 742820,
          "date": "Mon 12 Dec 2022 14:19",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/tutorial-lifecycle-hook-lambda.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 742575,
          "date": "Mon 12 Dec 2022 10:00",
          "username": "\t\t\t\tRaj40\t\t\t",
          "content": "Correct answer B",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#15",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using multiple AWS accounts. The DNS records are stored in a private hosted zone for Amazon Route 53 in Account A.  The company's applications and databases are running in Account B. <br>A solutions architect will deploy a two-tier application in a new VPC.  To simplify the configuration, the db.example.com CNAME record set for the Amazon RDS endpoint was created in a private hosted zone for Amazon Route 53.<br>During deployment, the application failed to start. Troubleshooting revealed that db.example.com is not resolvable on the Amazon EC2 instance. The solutions architect confirmed that the record set was created correctly in Route 53.<br>Which combination of steps should the solutions architect take to resolve this issue? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#15",
          "answers": [
            {
              "choice": "<p>A. Deploy the database on a separate EC2 instance in the new VPC.  Create a record set for the instance's private IP in the private hosted zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use SSH to connect to the application tier EC2 instance. Add an RDS endpoint IP address to the /etc/resolv.conf file.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a private hosted zone for the example com domain in Account B.  Configure Route 53 replication between AWS accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Associate a new VPC in Account B with a hosted zone in Account A.  Delete the association authorization in Account A. <br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774714,
          "date": "Fri 13 Jan 2023 18:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C and E are correct.<br>C.  Create an authorization to associate the private hosted zone in Account A with the new VPC in Account B. <br>This step is necessary because the VPC in Account B needs to be associated with the private hosted zone in Account A to be able to resolve the DNS records.<br>E.  Associate a new VPC in Account B with a hosted zone in Account A.  Delete the association authorization in Account A. <br>This step is necessary because the association authorization needs to be removed in Account A after the association is done in Account B. ",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 852792,
          "date": "Tue 28 Mar 2023 06:59",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "CE seme like the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 832501,
          "date": "Wed 08 Mar 2023 04:44",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "ANS: A & C<br><br>B is incorrect because modifying the /etc/resolv.conf file on the EC2 instance would not resolve the issue since the issue is with the Route 53 configuration.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831763,
          "date": "Tue 07 Mar 2023 11:33",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 806146,
          "date": "Sun 12 Feb 2023 10:23",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 805463,
          "date": "Sat 11 Feb 2023 18:29",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "C and E. <br>In order to resolve the issue, the solutions architect should create an authorization to associate the private hosted zone in Account A with the new VPC in Account B (Option C). This will allow the new VPC in Account B to access the DNS records stored in the private hosted zone in Account A. <br><br>In addition, the solutions architect should associate the new VPC in Account B with the hosted zone in Account A (Option E) and delete the association authorization in Account A.  This will ensure that the new VPC in Account B is properly configured to use the private hosted zone in Account A and resolve the db.example.com CNAME record set correctly.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 752618,
          "date": "Wed 21 Dec 2022 18:50",
          "username": "\t\t\t\trazguru\t\t\t",
          "content": "C & E are correct options.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743030,
          "date": "Mon 12 Dec 2022 17:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "With comments and links the answer is C and E.  (Ty robertohyène and JosuéXu)<br><br>C = 6. Run the following command to create the association between Account A's private hosted zone and Account B's VPC.  Use the hosted zone's ID from step 3. B account.<br>E = 7.It is recommended to remove the association permission after the association is created. This will prevent you from recreating the same association later.<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/route53-private-hosted-zone/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/36113-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 743095,
          "date": "Mon 12 Dec 2022 18:26",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/36113-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742587,
          "date": "Mon 12 Dec 2022 10:11",
          "username": "\t\t\t\tRaj40\t\t\t",
          "content": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 742073,
          "date": "Sun 11 Dec 2022 21:29",
          "username": "\t\t\t\tJoshuaXu\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/route53-private-hosted-zone/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741723,
          "date": "Sun 11 Dec 2022 14:12",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Correct answers: C & E",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#16",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company used Amazon EC2 instances to deploy a web fleet to host a blog site. The EC2 instances are behind an Application Load Balancer (ALB) and are configured in an Auto Scaling group. The web application stores all blog content on an Amazon EFS volume.<br>The company recently added a feature for bloggers to add video to their posts, attracting 10 times the previous user traffic. At peak times of day, users report buffering and timeout issues while attempting to reach the site or watch videos.<br>Which is the MOST cost-efficient and scalable deployment that will resolve the issues for users?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#16",
          "answers": [
            {
              "choice": "<p>A. Reconfigure Amazon EFS to enable maximum I/O.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the blog site to use instance store volumes for storage. Copy the site contents to the volumes at launch and to Amazon S3 at shutdown.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an Amazon CloudFront distribution for all site contents, and point the distribution at the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 753100,
          "date": "Thu 22 Dec 2022 09:49",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "No brainer",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 852793,
          "date": "Tue 28 Mar 2023 07:01",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Configure an Amazon CloudFront distribution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831767,
          "date": "Tue 07 Mar 2023 11:35",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "y configuring a CloudFront distribution for the blog site and pointing it at an S3 bucket, the videos can be cached at edge locations closer to users, reducing buffering and timeout issues.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 819949,
          "date": "Fri 24 Feb 2023 01:46",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C ofc.. i hope i will get such easy question in the real exam",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792717,
          "date": "Mon 30 Jan 2023 13:08",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is the correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774726,
          "date": "Fri 13 Jan 2023 18:28",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Configure an Amazon CloudFront distribution. Point the distribution to an S3 bucket, and migrate the videos from EFS to Amazon S3.<br><br>Amazon CloudFront is a content delivery network (CDN) that can be used to deliver content to users with low latency and high data transfer speeds. By configuring a CloudFront distribution for the blog site and pointing it at an S3 bucket, the videos can be cached at edge locations closer to users, reducing buffering and timeout issues. Additionally, S3 is designed for scalable storage and can handle high levels of user traffic. Migrating the videos from EFS to S3, would also improve the performance and scalability of the website.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743442,
          "date": "Tue 13 Dec 2022 01:23",
          "username": "\t\t\t\tkomorebi\t\t\t",
          "content": "CCCCCCCCCCCCCCCC",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 743378,
          "date": "Mon 12 Dec 2022 23:25",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct. Do works but not as cheaper as C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Agree that C is correct, why do you think D is not cheaper ?</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 817448,
          "date": "Wed 22 Feb 2023 05:02",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Agree that C is correct, why do you think D is not cheaper ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743032,
          "date": "Mon 12 Dec 2022 17:34",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "answer C makes sense<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/6008-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743093,
          "date": "Mon 12 Dec 2022 18:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/6008-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#17",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company with global offices has a single 1&nbsp;Gbps AWS Direct Connect connection to a single AWS Region. The company's on-premises network uses the connection to communicate with the company's resources in the AWS Cloud. The connection has a single private virtual interface that connects to a single VPC. <br>A solutions architect must implement a solution that adds a redundant Direct Connect connection in the same Region. The solution also must provide connectivity to other Regions through the same pair of Direct Connect connections as the company expands into other Regions.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#17",
          "answers": [
            {
              "choice": "<p>A. Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new private virtual interface on the new connection, and connect the new private virtual interface to the single VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Keep the existing private virtual interface. Create the second Direct Connect connection. Create a new public virtual interface on the new connection, and connect the new public virtual interface to the single VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision a transit gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. Associate the transit gateway with the single VPC. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852794,
          "date": "Tue 28 Mar 2023 07:03",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Provision a Direct Connect gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817479,
          "date": "Wed 22 Feb 2023 06:08",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : B and C are good for existing architecture in question. But with redundant DX connection requirement, only solution is Gateway. that resolves to A(Direct connect gateway) or D(Transit gateway), but D as transit gateway is wrong because it mentions private interfaces connecting with transit gateway which is weird [usually VPC attachments are made connecting transit gateway]. So answer is A - Direct Connect Gateway. (Infact, this is future proof when we want different VPCs in different regions later with this architecture)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792725,
          "date": "Mon 30 Jan 2023 13:18",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A is the correct solution and the best",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774727,
          "date": "Fri 13 Jan 2023 18:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Provision a Direct Connect gateway. Delete the existing private virtual interface from the existing connection. Create the second Direct Connect connection. Create a new private virtual interface on each connection, and connect both private virtual interfaces to the Direct Connect gateway. Connect the Direct Connect gateway to the single VPC. <br><br>This solution provides a redundant Direct Connect connection in the same Region by creating a new private virtual interface on each connection, and connecting both private virtual interfaces to a Direct Connect gateway. The Direct Connect gateway is then connected to the single VPC.  This solution also allows the company to expand into other Regions while providing connectivity through the same pair of Direct Connect connections.<br>The Direct Connect Gateway allows you to connect multiple VPCs and on-premises networks in different accounts and different regions to a single Direct Connect connection.<br>It also provides automatic failover and routing capabilities.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D is not possible at all. You connect to TGW using transit VIF, not private VIF</li><li>Option D is not the best solution because it uses a Transit Gateway, which is used to connect multiple VPCs and on-premises networks in different accounts and different regions, but it is not necessary in this scenario. The company only wants to add a redundant Direct Connect connection in the same Region and connect it to the same VPC.  Additionally, using a Transit Gateway in this scenario would add more complexity and might not be necessary.<br>Also, Transit Gateway does not provide automatic failover and routing capabilities, which is required in this scenario.<br>The Direct Connect Gateway is a better choice in this scenario as it provides the necessary functionality of automatic failover and routing capabilities, and it is more suitable for connecting multiple Direct Connect connections to a single VPC. </li><li>All options here are problematic. The DX-GW is a control plane-only device; in other words, no actual traffic goes over it; it is just a Route-Reflector it only carries the routing table. TGW is not a region construct, so by itself, it cannot provide regional redundancy. In any case, all things considered, maybe A is the closest but it should mention VGW.</li><li>I meant to say, \\\"TGW is a region construct\\\".</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 813872,
          "date": "Sun 19 Feb 2023 09:51",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "Option D is not possible at all. You connect to TGW using transit VIF, not private VIF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774728,
          "date": "Fri 13 Jan 2023 18:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D is not the best solution because it uses a Transit Gateway, which is used to connect multiple VPCs and on-premises networks in different accounts and different regions, but it is not necessary in this scenario. The company only wants to add a redundant Direct Connect connection in the same Region and connect it to the same VPC.  Additionally, using a Transit Gateway in this scenario would add more complexity and might not be necessary.<br>Also, Transit Gateway does not provide automatic failover and routing capabilities, which is required in this scenario.<br>The Direct Connect Gateway is a better choice in this scenario as it provides the necessary functionality of automatic failover and routing capabilities, and it is more suitable for connecting multiple Direct Connect connections to a single VPC. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>All options here are problematic. The DX-GW is a control plane-only device; in other words, no actual traffic goes over it; it is just a Route-Reflector it only carries the routing table. TGW is not a region construct, so by itself, it cannot provide regional redundancy. In any case, all things considered, maybe A is the closest but it should mention VGW.</li><li>I meant to say, \\\"TGW is a region construct\\\".</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 816812,
          "date": "Tue 21 Feb 2023 17:13",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "All options here are problematic. The DX-GW is a control plane-only device; in other words, no actual traffic goes over it; it is just a Route-Reflector it only carries the routing table. TGW is not a region construct, so by itself, it cannot provide regional redundancy. In any case, all things considered, maybe A is the closest but it should mention VGW.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I meant to say, \\\"TGW is a region construct\\\".</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816815,
          "date": "Tue 21 Feb 2023 17:16",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I meant to say, \\\"TGW is a region construct\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759382,
          "date": "Wed 28 Dec 2022 06:20",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A<br>https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-vgw-multi-regions-and-aws-public-peering.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753399,
          "date": "Thu 22 Dec 2022 15:52",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "transit gateway does not support cross-region<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/about-aws/whats-new/2019/12/aws-transit-gateway-supports-inter-region-peering/<br>But Still answer is A</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 815401,
          "date": "Mon 20 Feb 2023 15:54",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2019/12/aws-transit-gateway-supports-inter-region-peering/<br>But Still answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743377,
          "date": "Mon 12 Dec 2022 23:24",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct because direct connect gateway support multi region",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 743039,
          "date": "Mon 12 Dec 2022 17:39",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with A<br>https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html<br>https://jayendrapatil.com/aws-direct-connect-gateway/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/69343-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743092,
          "date": "Mon 12 Dec 2022 18:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/69343-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#18",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a web application that allows users to upload short videos. The videos are stored on Amazon EBS volumes and analyzed by custom recognition software for categorization.<br>The website contains static content that has variable traffic with peaks in certain months. The architecture consists of Amazon EC2 instances running in an Auto Scaling group for the web application and EC2 instances running in an Auto Scaling group to process an Amazon SQS queue. The company wants to re-architect the application to reduce operational overhead using AWS managed services where possible and remove dependencies on third-party software.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#18",
          "answers": [
            {
              "choice": "<p>A. Use Amazon ECS containers for the web application and Spot instances for the Auto Scaling group that processes the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the uploaded videos in Amazon EFS and mount the file system to the EC2 instances for the web application. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Host the web application in Amazon S3. Store the uploaded videos in Amazon S3. Use S3 event notification to publish events to the SQS queue. Process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Elastic Beanstalk to launch EC2 instances in an Auto Scaling group for the web application and launch a worker environment to process the SQS queue. Replace the custom software with Amazon Rekognition to categorize the videos.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852795,
          "date": "Tue 28 Mar 2023 07:05",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Host the web application in Amazon S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832518,
          "date": "Wed 08 Mar 2023 05:10",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "ANS is D<br><br>Point to consider, \\\"reduce operational overhead using AWS managed services\\\" and not to redesign. Therefore, EC2 will be replaced with ElasticBeans",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831772,
          "date": "Tue 07 Mar 2023 11:42",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "This solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 821734,
          "date": "Sat 25 Feb 2023 19:30",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "The answer is C. <br><br>This solution eliminates the need for managing and scaling EC2 instances for the web application and the worker environment for processing the SQS queue. Instead, Amazon S3 can host the web application, and store the uploaded videos, which can trigger S3 event notifications to send messages to the SQS queue. Then, an AWS Lambda function can process the messages in the SQS queue and use Amazon Rekognition API to categorize the videos. This approach also takes advantage of AWS-managed services, such as S3, SQS, and Lambda, which reduces operational overhead and dependency on third-party software.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 817943,
          "date": "Wed 22 Feb 2023 15:48",
          "username": "\t\t\t\tPSPaul\t\t\t",
          "content": "Vote C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 817487,
          "date": "Wed 22 Feb 2023 06:23",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : Key here is reduced operational head and use aws managed services which takes to serverless solutions. which is Lambda and Rekognition (aws managed). Mounting to EFS is overhead and moreover is good for file system, in future can pose problem scaling it with large video content in future. S3 is good for static videos storage obviously, So C is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790976,
          "date": "Sat 28 Jan 2023 19:59",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I don't like C.  It says that it CONTAINS static content, but it does not say ONLY static content. The S3 would not be suitable.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The most appropriate solution would be to use Amazon S3 for storing the uploaded videos, and hosting the web application. This approach reduces operational overhead, and removes dependencies on third-party software. S3 event notifications can be used to publish events to an SQS queue, which can then be processed using AWS Lambda functions that call the Amazon Rekognition API to categorize the videos.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810173,
          "date": "Thu 16 Feb 2023 02:47",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The most appropriate solution would be to use Amazon S3 for storing the uploaded videos, and hosting the web application. This approach reduces operational overhead, and removes dependencies on third-party software. S3 event notifications can be used to publish events to an SQS queue, which can then be processed using AWS Lambda functions that call the Amazon Rekognition API to categorize the videos.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774730,
          "date": "Fri 13 Jan 2023 18:35",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This solution meets the requirements by using multiple managed services offered by AWS which can reduce the operational overhead. Hosting the web application in Amazon S3 would make it highly available, scalable and can handle variable traffic. The uploaded videos can be stored in S3 and processed using S3 event notifications that trigger a Lambda function, which calls the Amazon Rekognition API to categorize the videos. SQS can be used to process the event notifications and also it is a managed service.<br>This solution eliminates the need to manage EC2 instances, EBS volumes and the custom software. Additionally, using Lambda function in this case, eliminates the need for managing additional servers to process the SQS queue which will reduce operational overhead.<br><br>By using this solution, the company can benefit from the scalability, reliability, and cost-effectiveness that these services offer, which can help to reduce operational overhead and improve the overall performance and security of the application.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759396,
          "date": "Wed 28 Dec 2022 06:40",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I vote C. <br>The serverless architecture reduces operational overhead the most for the requirement.<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-a-react-based-single-page-application-to-amazon-s3-and-cloudfront.html<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html<br>https://docs.aws.amazon.com/rekognition/latest/dg/video-analyzing-with-sqs.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753403,
          "date": "Thu 22 Dec 2022 15:56",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "no brainer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743055,
          "date": "Mon 12 Dec 2022 17:51",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "website contains static content = S3<br>I go with C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/35889-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743091,
          "date": "Mon 12 Dec 2022 18:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/35889-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742849,
          "date": "Mon 12 Dec 2022 14:37",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "Correct answer is C",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#19",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a serverless application comprised of Amazon CloudFront, Amazon API Gateway, and AWS Lambda functions. The current deployment process of the application code is to create a new version number of the Lambda function and run an AWS CLI script to update. If the new function version has errors, another CLI script reverts by deploying the previous working version of the function. The company would like to decrease the time to deploy new versions of the application logic provided by the Lambda functions, and also reduce the time to detect and revert when errors are identified.<br>How can this be accomplished?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#19",
          "answers": [
            {
              "choice": "<p>A. Create and deploy nested AWS CloudFormation stacks with the parent stack consisting of the AWS CloudFront distribution and API Gateway, and the child stack containing the Lambda function. For changes to Lambda, create an AWS CloudFormation change set and deploy; if errors are triggered, revert the AWS CloudFormation change set to the previous version.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS SAM and built-in AWS CodeDeploy to deploy the new Lambda version, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code. Rollback if Amazon CloudWatch alarms are triggered.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Refactor the AWS CLI scripts into a single script that deploys the new Lambda version. When deployment is completed, the script tests execute. If errors are detected, revert to the previous Lambda version.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create and deploy an AWS CloudFormation stack that consists of a new API Gateway endpoint that references the new Lambda version. Change the CloudFront origin to the new API Gateway endpoint, monitor errors and if detected, change the AWS CloudFront origin to the previous API Gateway endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774735,
          "date": "Fri 13 Jan 2023 18:39",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "AWS Serverless Application Model (SAM) is a framework that helps you build, test and deploy your serverless applications. It uses CloudFormation under the hood, so it is a way to simplify the process of creating, updating, and deploying CloudFormation templates. CodeDeploy is a service that automates code deployments to any instance, including on-premises instances and Lambda functions.<br>With AWS SAM you can use the built-in CodeDeploy to deploy new versions of the Lambda function, gradually shift traffic to the new version, and use pre-traffic and post-traffic test functions to verify code.<br>You can also define CloudWatch Alarms to trigger a rollback in case of any issues.<br>This allows for a faster and more efficient deployment process, as well as a more reliable rollback process when errors are identified. This way you can increase the speed of deployment and reduce the time to detect and revert when errors are identified.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 852800,
          "date": "Tue 28 Mar 2023 07:06",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use AWS SAM and built-in AWS CodeDeploy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 844771,
          "date": "Mon 20 Mar 2023 11:44",
          "username": "\t\t\t\t5up3rm4n\t\t\t",
          "content": "https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html<br><br>AWS Serverless Application Model (AWS SAM) comes built-in with CodeDeploy to provide gradual AWS Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you:<br><br>Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version.<br><br>Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected. If an update doesn't work correctly, you can roll back the changes.<br><br>Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and that your application operates as expected.<br><br>Automatically rolls back the deployment if CloudWatch alarms are triggered.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831775,
          "date": "Tue 07 Mar 2023 11:46",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "AWS Serverless Application Model (SAM)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753409,
          "date": "Thu 22 Dec 2022 16:03",
          "username": "\t\t\t\tspencer_sharp\t\t\t",
          "content": "sam typical use case",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743060,
          "date": "Mon 12 Dec 2022 17:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "AWS CodeDeploy is intended for this kind of use<br>https://aws.amazon.com/fr/codedeploy/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/5158-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743090,
          "date": "Mon 12 Dec 2022 18:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/5158-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#20",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet. Employees will access the system by connecting through a client VPN service that is attached to a VPC.  The data must not be accessible to the public.<br>The documents that the company is storing are copies of data that is held on physical media elsewhere. The number of requests will be low. Availability and speed of retrieval are not concerns of the company.<br>Which solution will meet these requirements at the LOWEST cost?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#20",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Launch an Amazon EC2 instance that runs a web server. Attach an Amazon Elastic File System (Amazon EFS) file system to store the archived data in the EFS One Zone-Infrequent Access (EFS One Zone-IA) storage class Configure the instance security groups to allow access only from private networks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch an Amazon EC2 instance that runs a web server Attach an Amazon Elastic Block Store (Amazon EBS) volume to store the archived data. Use the Cold HDD (sc1) volume type. Configure the instance security groups to allow access only from private networks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon S3 bucket. Configure the S3 bucket to use the S3 Glacier Deep Archive storage class as default. Configure the S3 bucket for website hosting. Create an S3 interface endpoint. Configure the S3 bucket to allow access only through that endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 751295,
          "date": "Tue 20 Dec 2022 18:46",
          "username": "\t\t\t\ttman22\t\t\t",
          "content": "A - Glacier Deep Archive can't be used for web hosting, regardless if the company says retrieval time is no concern.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Nevermind, I go for D. <br>It should be technically possible - and mostly dependent on the intranet web application logic - It could present users with the ability to start file retrieval, for then to later access the data.</li></ul>",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 751300,
          "date": "Tue 20 Dec 2022 18:52",
          "username": "\t\t\t\ttman22\t\t\t",
          "content": "Nevermind, I go for D. <br>It should be technically possible - and mostly dependent on the intranet web application logic - It could present users with the ability to start file retrieval, for then to later access the data.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 742868,
          "date": "Mon 12 Dec 2022 14:48",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct. HA is not required here. <br>D use Glacier deep archive that need hours to access that will cause time out for web",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 853723,
          "date": "Tue 28 Mar 2023 23:31",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A makes more sense than D. . Deep Archive retrieval time is 12 hours and I'm not sure it's possible to host static website in such long retrieval time!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 842076,
          "date": "Fri 17 Mar 2023 15:41",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is the only correct. I looked up the AWS docs...<br>S3 Glacier Deep Archive is a completely separate service that does not support web hosting.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 841361,
          "date": "Thu 16 Mar 2023 22:11",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "A , i created bucket with web hosting and put some html pages in glacier deep archive and had403 error, operation invalid for object storage class",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 839324,
          "date": "Tue 14 Mar 2023 22:42",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "D - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Question says availability and speed of retrieval are not concerns of the company.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836995,
          "date": "Sun 12 Mar 2023 13:23",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "Availability and speed of retrieval are not concerns of the company.<br>but they did not mention high durability which is not provided by OneZone-IA<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A is the only correct. I looked up the AWS docs... <br>S3 Glacier Deep Archive is a completely separate service that does not support web hosting.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842075,
          "date": "Fri 17 Mar 2023 15:41",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is the only correct. I looked up the AWS docs... <br>S3 Glacier Deep Archive is a completely separate service that does not support web hosting.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836782,
          "date": "Sun 12 Mar 2023 08:40",
          "username": "\t\t\t\tlimjieson\t\t\t",
          "content": "D is c orrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 835602,
          "date": "Sat 11 Mar 2023 04:15",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html<br>Store large number of archived docs, and available through corp intranet. <br>Copies of data held on physical media elsewhere (could be re-created). <br>Requests low (but it doesn't say RARE so think monthly/quarterly).<br>\\\"AVAILABILITY\\\" and speed of retrieval are not concerns.<br><br>It is A, yes Glacier is \\\"cheaper\\\", but I have to leave the archives for at least 180 days, would be available on corp intranet and it is more cost-effective if I want to migrate the data to Glacier if I monitor use and see it is \\\"rarely\\\" touched and know I have to hold it due to regulatory for at minimal 180 days.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831781,
          "date": "Tue 07 Mar 2023 11:56",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "will go with A considering following hints<br>1) data is copy of somethign stored else where (hints to One zone)<br>2) traffic is low(but it still exist)<br>3) minimum storage duration <br><br>D might alos be correct but i will select A in exam",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821756,
          "date": "Sat 25 Feb 2023 20:02",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "This solution provides cost-effective storage for the archived documents using the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class, which is the lowest cost storage option for infrequently accessed data in a single availability zone. Hosting the S3 bucket as a website enables easy access to the documents through the intranet, and creating an S3 interface endpoint ensures that access is only possible through the VPN attached to the VPC.  Additionally, S3 provides built-in security features, such as bucket policies and access control lists (ACLs), to control access to the data.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 819340,
          "date": "Thu 23 Feb 2023 16:23",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I will use A, but the question does not specify how often the files are retrieved. If they are retrieved frequently A for sure if they aren't then D. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.linkedin.com/pulse/s3-standard-more-cost-effective-than-glacier-jon-bonso;<br>Definitely A:Glacier has the highest minimum storage duration, which is 180 days, it becomes cost prohibitive if you factor in retrieval costs</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 828292,
          "date": "Fri 03 Mar 2023 20:22",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://www.linkedin.com/pulse/s3-standard-more-cost-effective-than-glacier-jon-bonso;<br>Definitely A:Glacier has the highest minimum storage duration, which is 180 days, it becomes cost prohibitive if you factor in retrieval costs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818102,
          "date": "Wed 22 Feb 2023 18:25",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tricky one - Glacier storage class has different levels which can fetch documents quickly with instant retrieval too. so many people go for A but answer is D to save more!. - https://aws.amazon.com/s3/storage-classes/glacier/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I'm on the fence on this question, Option A is offering a Single AZ S3 bucket with infrequent access that has the feature to enable web hosting. I can't find a web hosting feature with any of the archive classes unless the archive is restored and transitioned back to the standard class.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821007,
          "date": "Fri 24 Feb 2023 23:55",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "I'm on the fence on this question, Option A is offering a Single AZ S3 bucket with infrequent access that has the feature to enable web hosting. I can't find a web hosting feature with any of the archive classes unless the archive is restored and transitioned back to the standard class.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 817967,
          "date": "Wed 22 Feb 2023 16:13",
          "username": "\t\t\t\tPSPaul\t\t\t",
          "content": "D is good!<br>Keyword is \\\"speed of retrieval are not concerns\\\"<br>So, Glacier Deep Archive is the choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816632,
          "date": "Tue 21 Feb 2023 14:47",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "Lowest cost gives the hint. it should be option D. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 816630,
          "date": "Tue 21 Feb 2023 14:46",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "Lowest cost gives the hint. it should be option D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816315,
          "date": "Tue 21 Feb 2023 08:23",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "The employees can connect via intranet point to note is it's not via web application, so ppl can wait 12hours to get the documents for the lowest storage cost",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#21",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using an on-premises Active Directory service for user authentication. The company wants to use the same authentication service to sign in to the company's AWS accounts, which are using AWS Organizations. AWS Site-to-Site VPN connectivity already exists between the on-premises environment and all the company's AWS accounts.<br>The company's security policy requires conditional access to the accounts based on user groups and roles. User identities must be managed in a single location.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#21",
          "answers": [
            {
              "choice": "<p>A. Configure AWS IAM Identity Center (AWS Single Sign-On) to connect to Active Directory by using SAML 2.0. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using attribute-based access controls (ABACs).<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure AWS IAM Identity Center (AWS Single Sign-On) by using IAM Identity Center as an identity source. Enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. Grant access to the AWS accounts by using IAM Identity Center permission sets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use a SAML 2.0 identity provider. Provision IAM users that are mapped to the federated users. Grant access that corresponds to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM users.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. In one of the company's AWS accounts, configure AWS Identity and Access Management (IAM) to use an OpenID Connect (OIDC) identity provider. Provision IAM roles that grant access to the AWS account for the federated users that correspond to appropriate groups in Active Directory. Grant access to the required AWS accounts by using cross-account IAM roles.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852801,
          "date": "Tue 28 Mar 2023 07:12",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A is the best choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 846436,
          "date": "Wed 22 Mar 2023 00:15",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "A and B are wrong. https://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Changed to D.  https://aws.amazon.com/ru/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 846443,
          "date": "Wed 22 Mar 2023 00:21",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "Changed to D.  https://aws.amazon.com/ru/blogs/security/aws-federated-authentication-with-active-directory-federation-services-ad-fs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832528,
          "date": "Wed 08 Mar 2023 05:34",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "ANS is B<br><br>Option A is incorrect because it suggests using SAML 2.0 for authentication but does not address the requirements for managing user identities in a single location or providing conditional access based on user groups and roles.<br><br>Option C is incorrect because it suggests creating cross-account IAM users, which would require duplicating user identities across AWS accounts, defeating the purpose of using a single location for managing user identities.<br><br>Option D is incorrect because it suggests using an OpenID Connect (OIDC) identity provider, which does not integrate with Active Directory.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821759,
          "date": "Sat 25 Feb 2023 20:05",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "AWS Single Sign-On (SSO) is designed to manage access to multiple AWS accounts and business applications, and it allows users to sign in once using their existing credentials, including those from Active Directory. By configuring AWS SSO to connect to Active Directory by using SAML 2.0, the user identities can be managed in a single location. Additionally, automatic provisioning can be enabled using the System for Cross-domain Identity Management (SCIM) v2.0 protocol. This will ensure that users are created and updated in AWS SSO based on changes in Active Directory.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 820210,
          "date": "Fri 24 Feb 2023 08:34",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "A: imo not possible with on premises AD (SCIM not supported)<br>B: imo not possible with on premises AD (SCIM not supported)<br>C: \\\"Provision users in IAM\\\" violates the requirement of central user Management.<br>D: OIDC may be an ugly pig but works. Usage of roles removes the necessity of maintaining users in AWS.<br>(admitted: A would be much nicer if it was possible. )",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 818405,
          "date": "Wed 22 Feb 2023 22:01",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : SAML, Existing Active Directory services authentication mechanism and ABAC are key terms for the requirement. A fits well. D is wrong because, OIDC does not need to be implemented as the auth mechanism is already in place with AD.  OIDC does not jell with Active Directory. AD and SAML is a workable solution though.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780191,
          "date": "Wed 18 Jan 2023 17:12",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "A is has options for SAML and SCIM configuration with AD<br>C is all about users and no roles are mentioned. AD User attributes cannot be mapped to IAM users direct<br>D is openID based, MS AD would not support this<br><br>so I go with A",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774750,
          "date": "Fri 13 Jan 2023 18:52",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/74174-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>Both option C and option A are valid solutions that meet the requirements for the scenario.<br><br>ABAC, or attribute-based access control, is a method of granting access to resources based on the attributes of the user, the resource, and the action. This allows for fine-grained access control, which can be useful for implementing a security policy that requires conditional access to the accounts based on user groups and roles.<br><br>AWS IAM Identity Center (AWS SSO) allows you to connect to your on-premises Active Directory service using SAML 2.0. With this, you can enable automatic provisioning by using the System for Cross-domain Identity Management (SCIM) v2.0 protocol, which allows for the management of user identities in a single location.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>In option C, the company will use IAM to use a SAML 2.0 identity provider, and it will use the appropriate groups in Active Directory to grant access to the required AWS accounts by using cross-account IAM users. In this way, it can implement its security policy of conditional access to the accounts based on user groups and roles.<br><br>In summary, both option A and C are valid solutions, both of them allow you to use your on-premises Active Directory service for user authentication, and both of them allow you to manage user identities in a single location and grant access to the AWS accounts based on user groups and roles.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774751,
          "date": "Fri 13 Jan 2023 18:52",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "In option C, the company will use IAM to use a SAML 2.0 identity provider, and it will use the appropriate groups in Active Directory to grant access to the required AWS accounts by using cross-account IAM users. In this way, it can implement its security policy of conditional access to the accounts based on user groups and roles.<br><br>In summary, both option A and C are valid solutions, both of them allow you to use your on-premises Active Directory service for user authentication, and both of them allow you to manage user identities in a single location and grant access to the AWS accounts based on user groups and roles.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751311,
          "date": "Tue 20 Dec 2022 19:02",
          "username": "\t\t\t\ttman22\t\t\t",
          "content": "C. <br>On-premises Active Directory does not support SCIM or OIDC.  Azure AD is not mentioned.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750626,
          "date": "Tue 20 Dec 2022 08:55",
          "username": "\t\t\t\taragon_saa\t\t\t",
          "content": "I choose A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 743079,
          "date": "Mon 12 Dec 2022 18:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I prefer to go to answer A for ABAC<br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html<br>https://docs.aws.amazon.com/singlesignon/latest/userguide/abac.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/74174-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743088,
          "date": "Mon 12 Dec 2022 18:22",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/74174-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#22",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A software company has deployed an application that consumes a REST API by using Amazon API Gateway, AWS Lambda functions, and an Amazon DynamoDB table. The application is showing an increase in the number of errors during PUT requests. Most of the PUT calls come from a small number of clients that are authenticated with specific API keys.<br>A solutions architect has identified that a large number of the PUT requests originate from one client. The API is noncritical, and clients can tolerate retries of unsuccessful calls. However, the errors are displayed to customers and are causing damage to the API's reputation.<br>What should the solutions architect recommend to improve the customer experience?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#22",
          "answers": [
            {
              "choice": "<p>A. Implement retry logic with exponential backoff and irregular variation in the client application. Ensure that the errors are caught and handled with descriptive error messages.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Implement API throttling through a usage plan at the API Gateway level. Ensure that the client application handles code 429 replies without error.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Turn on API caching to enhance responsiveness for the production stage. Run 10-minute load tests. Verify that the cache capacity is appropriate for the workload.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement reserved concurrency at the Lambda function level to provide the resources that are needed during sudden increases in traffic.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 760167,
          "date": "Wed 28 Dec 2022 18:41",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct. API gateway throttling is applied to single account - https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html. Retry will make it even worse.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 774790,
          "date": "Fri 13 Jan 2023 20:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "API throttling is a technique that can be used to control the rate of requests to an API. This can be useful in situations where a small number of clients are making a large number of requests, which is causing errors. By implementing API throttling through a usage plan at the API Gateway level, the solutions architect can limit the number of requests that a client can make, which will help to reduce the number of errors.<br><br>It's important that the client application handles the code 429 replies without error, this will help to improve the customer experience by reducing the number of errors that are displayed to customers. Additionally, it will prevent the API's reputation from being damaged by the errors.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It is important to note that other solutions such as retry logic with exponential backoff and irregular variation in the client application or turn on API caching to enhance responsiveness for the production stage may help to improve the customer experience and reduce errors, but they do not address the root cause of the problem which is a large number of requests coming from a small number of clients.<br><br>Implementing reserved concurrency at the Lambda function level can provide resources that are needed during sudden increases in traffic, but it does not address the issue of a client making a large number of requests and causing errors.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774791,
          "date": "Fri 13 Jan 2023 20:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "It is important to note that other solutions such as retry logic with exponential backoff and irregular variation in the client application or turn on API caching to enhance responsiveness for the production stage may help to improve the customer experience and reduce errors, but they do not address the root cause of the problem which is a large number of requests coming from a small number of clients.<br><br>Implementing reserved concurrency at the Lambda function level can provide resources that are needed during sudden increases in traffic, but it does not address the issue of a client making a large number of requests and causing errors.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 852804,
          "date": "Tue 28 Mar 2023 07:14",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818485,
          "date": "Wed 22 Feb 2023 22:55",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : While catching errors and showing nice error message is good for customers,<br>it still does damage to API as clients think API is not working/responding well.<br>Retry and showing nice error will still invoke frustration to clients and damage to API :-)<br>As the api is being bombarded with small number of clients (note they are successfully <br>authenticated already trying to update resources with PUT) so assuming they are just bombarding with 429 too many requests.<br>So API throttling helps. Caching may give stale data (C is not apt here) Reserved concurrency<br>when lambda is overloaded (D is not a fit either). B should be correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816314,
          "date": "Tue 21 Feb 2023 08:22",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "API throttling helps",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 816257,
          "date": "Tue 21 Feb 2023 06:48",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Exponential backoff is a boto3 client retry logic that will impact all clients. The question is stating it's one client causing the issue, so A is not the correct choice. <br>B as API Gateway can throttle the requests and handle the error pages correctly.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 815509,
          "date": "Mon 20 Feb 2023 17:14",
          "username": "\t\t\t\tjaysparky\t\t\t",
          "content": "It is B.  Don't think PUT Method should be cached.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792755,
          "date": "Mon 30 Jan 2023 14:06",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The problem is that an error is displayed==>solution API throttling",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 789070,
          "date": "Thu 26 Jan 2023 20:53",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "Option B is wrong as API throttling would be applied to all the customers.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It can be applied to requests with specific API key.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819361,
          "date": "Thu 23 Feb 2023 16:46",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "It can be applied to requests with specific API key.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743086,
          "date": "Mon 12 Dec 2022 18:22",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I go with A:<br>https://www.examtopics.com/discussions/amazon/view/69110-exam-aws-certified-solutions-architect-professional-topic-1/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Implementing retry logic with exponential backoff and irregular variation in the client application can be a good way to improve the reliability of the application and reduce errors, but it does not address the root cause of the problem, which is a large number of requests coming from a small number of clients.<br><br>Retry logic with exponential backoff works by increasing the time between retries by a certain factor (e.g. doubling it) after each failed attempt. This can help to reduce the number of errors by giving the API time to recover from a high load. However, it does not address the issue of the high load itself. If the number of requests that a client is making is causing errors, retry logic alone may not be sufficient to resolve the issue.</li><li>Handling errors with descriptive error messages can improve the customer experience, but it does not address the underlying problem of high number of requests from a small number of clients that causes errors.<br><br>Throttling is a way to control the rate of requests to an API, which can help to reduce the number of errors. By limiting the number of requests that a client can make, throttling can help to reduce the high number of requests that is causing errors, and it addresses the root cause of the problem.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774792,
          "date": "Fri 13 Jan 2023 20:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Implementing retry logic with exponential backoff and irregular variation in the client application can be a good way to improve the reliability of the application and reduce errors, but it does not address the root cause of the problem, which is a large number of requests coming from a small number of clients.<br><br>Retry logic with exponential backoff works by increasing the time between retries by a certain factor (e.g. doubling it) after each failed attempt. This can help to reduce the number of errors by giving the API time to recover from a high load. However, it does not address the issue of the high load itself. If the number of requests that a client is making is causing errors, retry logic alone may not be sufficient to resolve the issue.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Handling errors with descriptive error messages can improve the customer experience, but it does not address the underlying problem of high number of requests from a small number of clients that causes errors.<br><br>Throttling is a way to control the rate of requests to an API, which can help to reduce the number of errors. By limiting the number of requests that a client can make, throttling can help to reduce the high number of requests that is causing errors, and it addresses the root cause of the problem.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774793,
          "date": "Fri 13 Jan 2023 20:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Handling errors with descriptive error messages can improve the customer experience, but it does not address the underlying problem of high number of requests from a small number of clients that causes errors.<br><br>Throttling is a way to control the rate of requests to an API, which can help to reduce the number of errors. By limiting the number of requests that a client can make, throttling can help to reduce the high number of requests that is causing errors, and it addresses the root cause of the problem.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#23",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a data-intensive application on AWS. The application runs on a cluster of hundreds of Amazon EC2 instances. A shared file system also runs on several EC2 instances that store 200&nbsp;TB of data. The application reads and modifies the data on the shared file system and generates a report. The job runs once monthly, reads a subset of the files from the shared file system, and takes about 72&nbsp;hours to complete. The compute instances scale in an Auto Scaling group, but the instances that host the shared file system run continuously. The compute and storage instances are all in the same AWS Region.<br>A solutions architect needs to reduce costs by replacing the shared file system instances. The file system must provide high performance access to the needed data for the duration of the 72-hour run.<br>Which solution will provide the LARGEST overall cost reduction while meeting these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#23",
          "answers": [
            {
              "choice": "<p>A. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Intelligent-Tiering storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using lazy loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the data from the existing shared file system to a large Amazon Elastic Block Store (Amazon EBS) volume with Multi-Attach enabled. Attach the EBS volume to each of the instances by using a user data script in the Auto Scaling group launch template. Use the EBS volume as the shared storage for the duration of the job. Detach the EBS volume when the job is complete<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the data from the existing shared file system to an Amazon S3 bucket that uses the S3 Standard storage class. Before the job runs each month, use Amazon FSx for Lustre to create a new file system with the data from Amazon S3 by using batch loading. Use the new file system as the shared storage for the duration of the job. Delete the file system when the job is complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the data from the existing shared file system to an Amazon S3 bucket. Before the job runs each month, use AWS Storage Gateway to create a file gateway with the data from Amazon S3. Use the file gateway as the shared storage for the job. Delete the file gateway when the job is complete.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852805,
          "date": "Tue 28 Mar 2023 07:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 843056,
          "date": "Sat 18 Mar 2023 19:27",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "definitely A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831797,
          "date": "Tue 07 Mar 2023 12:14",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Lazy loading is cost-effective because only a subset of data is used at every job",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823462,
          "date": "Mon 27 Feb 2023 10:17",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "A: Lazy loading is cost-effective because only a subset of data is used at every job<br>B: There are hundreds of EC2 instances using the volume which is not possible (one EBS volume is limited to 16 nitro instances attached)<br>C: Batching would load too much data<br>D: storage gateway is used for on premises data access, I don't know is you can install a gateway in AWS, but Amazon would never advise this",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 820277,
          "date": "Fri 24 Feb 2023 10:05",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "A: provides High performance Access<br>B: EBS is by far more expensive than s3.<br>C: Lustre with Lazy Loading(A) is Cheaper than Batch loading<br>D: might be cheaper than A but does not provide High performance Access.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818544,
          "date": "Thu 23 Feb 2023 00:31",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "My Logical answer : D has blunder in it. Storage gateway is not a storage solution. its just a gateway for large data transfers usual usecase of on premises. Block storage is not fit as data is being modified here in this<br>usecase. So B is wrong. am guessing data analytics usecase here for high performant compute which Lustre provides.that leaves A or C.  The tricky word here is Overall cost storage saving comparing S3 Intelligent tier vs Standard tier. Intelligent tier can recognise that data is not being touched<br>for a month and it will use its intelligence to move into other cheaper storage class and gets to work whenever that high performance job needs to be started. So A fits well.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think you are wrong about the S3 file gateway:<br>https://docs.aws.amazon.com/filegateway/latest/files3/create-gateway-file.html#connect-to-amazon-s3-file</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820251,
          "date": "Fri 24 Feb 2023 09:18",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "I think you are wrong about the S3 file gateway:<br>https://docs.aws.amazon.com/filegateway/latest/files3/create-gateway-file.html#connect-to-amazon-s3-file",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815190,
          "date": "Mon 20 Feb 2023 13:01",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "FSx for Lustre with lazy loading should work",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 813907,
          "date": "Sun 19 Feb 2023 10:30",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "Can someone please explain why the correct answer is A, and not C?<br>A will actually come out more expensive, as (because you load the file each month) the files will never transition out from S3 standard, and you also pay for the inteligent-tiering.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Hey Anita - Intelligent tier will observe that data is not touched for a month right, so data can actually be moved into other cheaper storage based on usage patterns. it is cheaper compared to standard S3. AWS says Since the launch of S3 Intelligent-Tiering in 2018, customers have saved $750 million from adopting S3 Intelligent-Tiering when compared to S3 Standard. Read thru this as this article says S3 intelligent tiering provides automatic save storage costs- https://aws.amazon.com/s3/storage-classes/intelligent-tiering/</li><li>OK, just noticed only a subset of the files are read during the monthly job, so in that case A is correct.</li><li>This job runs 1x every month, so the data has no opportunity to transition into cheaper storage tier.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html<br>\\\" For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they have not been accessed for 30 consecutive days.\\\"<br>So basically all you achieve with using intelligent tiering is keeping the data in standard storage for 30 days, moving the data to infrequent access and moving it back to standard the same day or the next. Seems pretty pointless to me</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818548,
          "date": "Thu 23 Feb 2023 00:38",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Hey Anita - Intelligent tier will observe that data is not touched for a month right, so data can actually be moved into other cheaper storage based on usage patterns. it is cheaper compared to standard S3. AWS says Since the launch of S3 Intelligent-Tiering in 2018, customers have saved $750 million from adopting S3 Intelligent-Tiering when compared to S3 Standard. Read thru this as this article says S3 intelligent tiering provides automatic save storage costs- https://aws.amazon.com/s3/storage-classes/intelligent-tiering/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>OK, just noticed only a subset of the files are read during the monthly job, so in that case A is correct.</li><li>This job runs 1x every month, so the data has no opportunity to transition into cheaper storage tier.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html<br>\\\" For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they have not been accessed for 30 consecutive days.\\\"<br>So basically all you achieve with using intelligent tiering is keeping the data in standard storage for 30 days, moving the data to infrequent access and moving it back to standard the same day or the next. Seems pretty pointless to me</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822954,
          "date": "Sun 26 Feb 2023 21:53",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "OK, just noticed only a subset of the files are read during the monthly job, so in that case A is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822937,
          "date": "Sun 26 Feb 2023 21:34",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "This job runs 1x every month, so the data has no opportunity to transition into cheaper storage tier.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html<br>\\\" For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access patterns and automatically moves objects to the Infrequent Access tier when they have not been accessed for 30 consecutive days.\\\"<br>So basically all you achieve with using intelligent tiering is keeping the data in standard storage for 30 days, moving the data to infrequent access and moving it back to standard the same day or the next. Seems pretty pointless to me",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791518,
          "date": "Sun 29 Jan 2023 10:38",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I go for D.  In A, we are missing a critical step, exporting the resutls back from FSX to S3. Instead, we are deleting the FSX file system, which means that results and modifications are lost. Please check https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-between-amazon-fsx-for-lustre-and-amazon-s3/ which explains that.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think we could assume it because it deliberately uses FSx for Lustre linked to S3.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 803427,
          "date": "Thu 09 Feb 2023 16:49",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I think we could assume it because it deliberately uses FSx for Lustre linked to S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 786710,
          "date": "Tue 24 Jan 2023 17:55",
          "username": "\t\t\t\tMasterP007\t\t\t",
          "content": "Option A is Correct. <br>As for Option B - is there such a thing as EBS Multi-attach!<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>yeah there is but its not feasible here as there is hundreds of instance.<br><br>Multi-Attach enabled volumes can be attached to up to 16 Linux instances built on the Nitro System that are in the same Availability Zone. You can attach a volume that is Multi-Attach enabled to Windows instances, but the operating system does not recognize the data on the volume that is shared between the instances, which can result in data inconsistency.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817178,
          "date": "Tue 21 Feb 2023 22:15",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "yeah there is but its not feasible here as there is hundreds of instance.<br><br>Multi-Attach enabled volumes can be attached to up to 16 Linux instances built on the Nitro System that are in the same Availability Zone. You can attach a volume that is Multi-Attach enabled to Windows instances, but the operating system does not recognize the data on the volume that is shared between the instances, which can result in data inconsistency.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774796,
          "date": "Fri 13 Jan 2023 20:22",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This solution would provide the largest overall cost reduction while meeting the requirements. By migrating the data to an S3 bucket using the S3 Intelligent-Tiering storage class, the company can take advantage of the automatic cost optimization provided by the storage class, which can result in significant cost savings. Additionally, by using Amazon FSx for Lustre to create a new file system with the data from Amazon S3, the company can take advantage of the high-performance access to the needed data for the duration of the 72-hour run. When the job is complete, the company can delete the file system, further reducing costs.<br><br>Option B, C and D may provide some cost savings over the current solution, but the savings would be less significant than the option A, because of the cost of the storage, the cost of the data transfer, and the cost of the storage gateway, the solution using the S3 and FSx for Lustre is the most cost-effective while meeting the requirements.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 755230,
          "date": "Sat 24 Dec 2022 23:49",
          "username": "\t\t\t\tcloudfever\t\t\t",
          "content": "using FSx for Lustre with lazy loading allows you to only pay for the data that is accessed during the job, rather than paying for the entire file system upfront.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 744182,
          "date": "Tue 13 Dec 2022 16:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/80991-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#24",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing a new service that will be accessed using TCP on a static port. A solutions architect must ensure that the service is highly available, has redundancy across Availability Zones, and is accessible using the DNS name my.service.com, which is publicly accessible. The service must use fixed address assignments so other companies can add the addresses to their allow lists.<br>Assuming that resources are deployed in multiple Availability Zones in a single Region, which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#24",
          "answers": [
            {
              "choice": "<p>A. Create Amazon EC2 instances with an Elastic IP address for each instance. Create a Network Load Balancer (NLB) and expose the static TCP port. Register EC2 instances with the NLB.  Create a new name server record set named my.service.com, and assign the Elastic IP addresses of the EC2 instances to the record set. Provide the Elastic IP addresses of the EC2 instances to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP addresses for the ECS cluster. Create a Network Load Balancer (NLB) and expose the TCP port. Create a target group and assign the ECS cluster name to the NLCreate a new A record set named my.service.com, and assign the public IP addresses of the ECS cluster to the record set. Provide the public IP addresses of the ECS cluster to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB.  Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon ECS cluster and a service definition for the application. Create and assign public IP address for each host in the cluster. Create an Application Load Balancer (ALB) and expose the static TCP port. Create a target group and assign the ECS service definition name to the ALB.  Create a new CNAME record set and associate the public IP addresses to the record set. Provide the Elastic IP addresses of the Amazon EC2 instances to the other companies to add to their allow lists.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852809,
          "date": "Tue 28 Mar 2023 07:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831802,
          "date": "Tue 07 Mar 2023 12:17",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "IP address using NLB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 818607,
          "date": "Thu 23 Feb 2023 01:56",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : Non http port like TCP should hint to NLB immediately.(ALB does not fit here) Sharing IP address of EC2 is not apt<br>whether it is from individual EC2 instances or those from ECS cluster.this eliminates A,B. D, infact the NLB's address which stays in front of / associates to ec2 instances need to be shared. So, only solution is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817197,
          "date": "Tue 21 Feb 2023 22:36",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "C looks correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792845,
          "date": "Mon 30 Jan 2023 14:27",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C.  NLB with one Elastic IP per AZ to handle TCP traffic. Alias record set named my.service.com.<br>https://www.examtopics.com/discussions/amazon/view/28045-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 791541,
          "date": "Sun 29 Jan 2023 11:11",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "C looks correct. I did not read the rest.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774801,
          "date": "Fri 13 Jan 2023 20:29",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A more appropriate solution would be option C.  Create an Amazon EC2 instances for the service. Create one Elastic IP address for each Availability Zone. Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB.  Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set. As it uses the NLB as the resource in the A-record, traffic will be routed through the NLB, and it will automatically route the traffic to the healthy instances based on the health checks and also it provides the fixed address assignments as the other companies can add the NLB's Elastic IP addresses to their allow lists.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744188,
          "date": "Tue 13 Dec 2022 16:15",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/28045-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#25",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses an on-premises data analytics platform. The system is highly available in a fully redundant configuration across 12 servers in the company's data center.<br>The system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. Scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight SLAs. The scheduled jobs account for 65% of the system usage. User jobs typically finish running in less than 5 minutes and have no SLA.  The user jobs account for 35% of system usage. During system failures, scheduled jobs must continue to meet SLAs. However, user jobs can be delayed.<br>A solutions architect needs to move the system to Amazon EC2 instances and adopt a consumption-based model to reduce costs with no long-term commitments. The solution must maintain high availability and must not affect the SLAs.<br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#25",
          "answers": [
            {
              "choice": "<p>A. Split the 12 instances across two Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run four instances in each Availability Zone as Spot Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Split the 12 instances across three Availability Zones in the chosen AWS Region. In one of the Availability Zones, run all four instances as On-Demand Instances with Capacity Reservations. Run the remaining instances as Spot Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Split the 12 instances across three Availability Zones in the chosen AWS Region. Run three instances in each Availability Zone as On-Demand Instances with Capacity Reservations. Run one instance in each Availability Zone as a Spot Instance.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 854528,
          "date": "Wed 29 Mar 2023 16:41",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "12 nodes in redundant configuration ..Means 6 nodes can handle load at any given time. <br>Out of 6 nodes, 65 % is SLA driven (~4nodes) and 35% load can be paused. <br>This lead to 4 nodes with single point of failure. D- If one -az down you still have 4 nodes available.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 852813,
          "date": "Tue 28 Mar 2023 07:22",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "...Run one instance in each Availability Zone as a Spot Instance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 852642,
          "date": "Tue 28 Mar 2023 03:28",
          "username": "\t\t\t\thigashikumi\t\t\t",
          "content": "The solution that meets the requirements most cost-effectively is Split the 12 instances across three Availability Zones in the chosen AWS Region. Run two instances in each Availability Zone as On-Demand Instances with a Savings Plan. Run two instances in each Availability Zone as Spot Instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832250,
          "date": "Tue 07 Mar 2023 21:01",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "D -> No long term commitment. Please hourly jobs require 65% capacity",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 830288,
          "date": "Sun 05 Mar 2023 21:22",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "I can't understand people who voted D. . Capacity Reserved instances are very expensive and have the same price of on-demand so it's not a “cost-effectively“ solution .<br>C is the most cost effectively solution that also makes sense.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option-C uses savings plan and needs commitment; The question says no long-term commitment; Hence option-D is the best.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832536,
          "date": "Wed 08 Mar 2023 05:52",
          "username": "\t\t\t\tNPN\t\t\t",
          "content": "Option-C uses savings plan and needs commitment; The question says no long-term commitment; Hence option-D is the best.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823791,
          "date": "Mon 27 Feb 2023 15:09",
          "username": "\t\t\t\t_lasco_\t\t\t",
          "content": "Voted D because of the 65% / 35% proportion. C seems to be good but with only 50% instances available we break the SLA",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 823489,
          "date": "Mon 27 Feb 2023 10:37",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "D has no long term commitment (e.g. saving plans) and has 75% on demand instances / 25% spot instances which is near the requirements",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 821781,
          "date": "Sat 25 Feb 2023 20:49",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "Option D is also a good solution because it splits the 12 instances across three Availability Zones and uses a mix of On-Demand Instances with Capacity Reservations and Spot Instances. However, it allocates fewer On-Demand Instances than Option C, which could result in lower availability.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821780,
          "date": "Sat 25 Feb 2023 20:48",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "C is a good solution because it splits the 12 instances across three Availability Zones and uses a mix of On-Demand Instances with a Savings Plan and Spot Instances.<br><br>On-Demand instances provide a consumption-based model with no long-term commitments, which is one of the requirements mentioned in the scenario. Although other purchasing options such as Reserved Instances or Savings Plans could offer significant discounts over On-Demand pricing, they require longer commitments and upfront payments, which may not align with the requirement of a consumption-based model with no long-term commitments. Additionally, using On-Demand instances can help to maintain high availability and meet the tight SLAs required for the scheduled jobs, as they provide the fastest and most reliable way to provision EC2 instances.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 820376,
          "date": "Fri 24 Feb 2023 11:53",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "We have an SLA to meet, that cannot be guaranteed with spot instances. We need to ensure that 65% of capacity is always available.<br>The only option that has at least 65% capacity always available is D. <br>Other options may be cheaper but do not provide the required Service Level.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 819224,
          "date": "Thu 23 Feb 2023 14:09",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "\\\" with no long-term commitments.\\\" -> option c require atleast 1=3 years of commitments, so we can ignore it. So D is the best option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 818788,
          "date": "Thu 23 Feb 2023 05:26",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : A and B gets eliminated because one says two AZs and other is wierd proportion of 4 OnDemand, rest Spot instances.<br>that leaves C and D.  Most might go for D thinking 65-35 proportion but question asks for MOST cost effective which is option with Savings plans and its just 1 year commitment [its not really long term] (https://aws.amazon.com/savingsplans/compute-pricing/)<br>In fact one standing out in this aspect is only C.  Two OnDemand with savings plan saves and Two Spot instances save costs too. Win win situation<br>and we have this same proportion in other two AZs as well, good for High Availability. So, I choose C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>without knowing what the company considers \\\"long-term\\\" we cannot make that assumption. Yes, I leaned to it at first but reviewing the statement \\\"which solution will meet these requirements most cost-effectively?\\\" they don't want a commitment at all.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 836606,
          "date": "Sun 12 Mar 2023 00:44",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "without knowing what the company considers \\\"long-term\\\" we cannot make that assumption. Yes, I leaned to it at first but reviewing the statement \\\"which solution will meet these requirements most cost-effectively?\\\" they don't want a commitment at all.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818731,
          "date": "Thu 23 Feb 2023 04:18",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : A and B gets eliminated because one says two AZs and other is wierd proportion of 4 OnDemand, rest Spot instances.<br>that leaves C and D.  Most might go for D thinking 65-35 proportion but question asks for MOST cost effective which is option with Savings plans and its just 1 year commitment [its not really long term] (https://aws.amazon.com/savingsplans/compute-pricing/)<br>In fact one standing out in this aspect is only C.  Two OnDemand with savings plan saves and Two Spot instances save costs too. Win win situation<br>and we have this same proportion in other two AZs as well, good for High Availability. So, I choose C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796612,
          "date": "Fri 03 Feb 2023 02:46",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "C<br>Savings plans are 60-75% savings, capacity reservations guarantee the capacity (no savings).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792858,
          "date": "Mon 30 Jan 2023 14:32",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "SLA looks like 65%",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 783299,
          "date": "Sat 21 Jan 2023 14:04",
          "username": "\t\t\t\tPugsley\t\t\t",
          "content": "The math is more logical for D - look at the 65% vs 35%.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 774804,
          "date": "Fri 13 Jan 2023 20:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D is correct because it meets the requirements of maintaining high availability, meeting SLAs for scheduled jobs, and reducing costs with a consumption-based model. By splitting the 12 instances across three Availability Zones, the system can maintain high availability and availability of resources in case of a failure. Option D also uses a combination of On-Demand Instances with Capacity Reservations and Spot Instances, which allows for scheduled jobs to be run on the On-Demand instances with guaranteed capacity, while also taking advantage of the cost savings from Spot Instances for the user jobs which have lower SLA requirements.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#26",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A security engineer determined that an existing application retrieves credentials to an Amazon RDS for MySQL database from an encrypted file in Amazon S3. For the next version of the application, the security engineer wants to implement the following application design changes to improve security:<br>The database must use strong, randomly generated passwords stored in a secure AWS managed service.<br>The application resources must be deployed through AWS CloudFormation.<br>The application must rotate credentials for the database every 90&nbsp;days.<br>A solutions architect will generate a CloudFormation template to deploy the application.<br>Which resources specified in the CloudFormation template will meet the security engineer's requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#26",
          "answers": [
            {
              "choice": "<p>A. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Specify a Secrets Manager RotationSchedule resource to rotate the database password every 90 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Create an AWS Lambda function resource to rotate the database password. Specify a Parameter Store RotationSchedule resource to rotate the database password every 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Generate the database password as a secret resource using AWS Secrets Manager. Create an AWS Lambda function resource to rotate the database password. Create an Amazon EventBridge scheduled rule resource to trigger the Lambda function password rotation every 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Generate the database password as a SecureString parameter type using AWS Systems Manager Parameter Store. Specify an AWS AppSync DataSource resource to automatically rotate the database password every 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 759544,
          "date": "Wed 28 Dec 2022 09:03",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A<br>https://docs.aws.amazon.com/secretsmanager/latest/userguide/cloudformation.html<br>Option B is wrong. The ParameterStore::RotationSchedule resource does not exist in CloudFormation.<br>Option C is wrong. It does not meet the requirement because it does not use CloudFormation.<br>Option D is wrong. The AWS::AppSync::DataSource resource is what to create data sources for resolvers in AWS AppSync to connect to.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 852815,
          "date": "Tue 28 Mar 2023 07:23",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Secrets Manager RotationSchedule resource",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832252,
          "date": "Tue 07 Mar 2023 21:04",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823796,
          "date": "Mon 27 Feb 2023 15:13",
          "username": "\t\t\t\t_lasco_\t\t\t",
          "content": "voted A, rotation with secrets manager:<br>https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821785,
          "date": "Sat 25 Feb 2023 20:54",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "The best solution is either A or C, but A may be the LEAST amount of operational overhead since it uses AWS Secrets Manager's built-in rotation functionality.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818766,
          "date": "Thu 23 Feb 2023 04:51",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : Secrets manager only can support password rotation, not parameter store.<br>Parameter store is just a location as its name suggest to refer to or <br>be referred from elsewhere. B,D are eliminated.C is wrong<br>because, there is no necessity for event bridge rule to capture known 90 days trigger.<br>Rotation schedule is already available when you configure a secret in Secrets manager.<br>That leaves option A as correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 792862,
          "date": "Mon 30 Jan 2023 14:34",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Secrets Manager support RotationSchedule.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791552,
          "date": "Sun 29 Jan 2023 11:32",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Option B is not wrong, but it has more operational overhead compared to option A.  Option B uses AWS Systems Manager Parameter Store, which is less automated and requires manual intervention to perform password rotation. Option A uses AWS Secrets Manager, which provides a built-in mechanism to rotate secrets, reducing operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774807,
          "date": "Fri 13 Jan 2023 20:45",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is the correct answer because it meets the security engineer's requirements with the least amount of operational overhead. This option uses AWS Secrets Manager to generate the database password as a secret resource, which is a secure and managed service for storing and rotating secrets such as database credentials. The CloudFormation template also includes a Lambda function resource to rotate the password, and a Secrets Manager RotationSchedule resource to schedule the password rotation every 90 days.<br>This option is the correct answer because it is the best way to manage the password rotation, Secrets Manager is a fully managed service that encrypts and stores the credentials and rotates the credentials automatically, and CloudFormation is used to automate the deployment of the resources.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 744928,
          "date": "Wed 14 Dec 2022 10:24",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Secrets Manager support RotationSchedule.<br>Not ParameterStore.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 744197,
          "date": "Tue 13 Dec 2022 16:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/47127-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743627,
          "date": "Tue 13 Dec 2022 06:42",
          "username": "\t\t\t\tnyunyu\t\t\t",
          "content": "A <br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationschedule.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 742894,
          "date": "Mon 12 Dec 2022 15:22",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct - https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Appreciate your participation in the discussions. However, I suggest do a proper research before voicing out your opinion.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808995,
          "date": "Wed 15 Feb 2023 02:38",
          "username": "\t\t\t\tCloud_noob\t\t\t",
          "content": "Appreciate your participation in the discussions. However, I suggest do a proper research before voicing out your opinion.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#27",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is storing data in several Amazon DynamoDB tables. A solutions architect must use a serverless architecture to make the data accessible publicly through a simple API over HTTPS. The solution must scale automatically in response to demand.<br>Which solutions meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#27",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon API Gateway HTTP API. Configure this API with direct integrations to Dynamo DB by using API Gateway's AWS integration type.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an accelerator in AWS Global Accelerator. Configure this accelerator with AWS Lambda@Edge function integrations that return data from the DynamoDB tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a Network Load Balancer. Configure listener rules to forward requests to the appropriate AWS Lambda functions.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 759799,
          "date": "Wed 28 Dec 2022 13:23",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A and C. <br>API Gateway REST API can invoke DynamoDB directly.<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 852817,
          "date": "Tue 28 Mar 2023 07:24",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AC is a good fit",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 832553,
          "date": "Wed 08 Mar 2023 06:17",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "Ans is A & C<br><br>Option B: HTTP APIs do not currently support integrations with DynamoDB, and therefore this solution would not work.<br>Option D: AWS Global Accelerator and AWS Lambda@Edge, which both involve infrastructure management.<br>Option E: NLB does not meet the requirement of being serverless.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832257,
          "date": "Tue 07 Mar 2023 21:08",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "going with A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 823809,
          "date": "Mon 27 Feb 2023 15:22",
          "username": "\t\t\t\t_lasco_\t\t\t",
          "content": "I voted A and C<br>Api gateway REST APis support direct integration with DynamoDb<br>The same can be achieved with HTTP APIs using a lambda between the two",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 818891,
          "date": "Thu 23 Feb 2023 07:59",
          "username": "\t\t\t\tGabehcoud\t\t\t",
          "content": "Think it should CD.  snippet from the link https://aws.amazon.com/api-gateway/faqs/ below<br><br>HTTP APIs are ideal for:<br>Building proxy APIs for AWS Lambda or any HTTP endpoint<br>Building modern APIs that are equipped with OIDC and OAuth 2 authorization <br>Workloads that are likely to grow very large<br>APIs for latency sensitive workloadsREST APIs are ideal for:<br>Customers looking to pay a single price point for an all-inclusive set of features needed to build, manage, and publish their APIs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818797,
          "date": "Thu 23 Feb 2023 05:38",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "API Gateway is the solution for simple API. D is Cloudfront/Lambda@edge solution for faster<br>response. Rcoequirement says API. So D gets eliminated. E is irrelevant<br>of course. B is wrong because DynamoDB vs Dynamo DB. (no brainer) That leaves A and C<br>as correct answers. (If question asks for more secure not exposing DynamoDB<br>directly, I'd go for C)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816296,
          "date": "Tue 21 Feb 2023 07:38",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "To make the data accessible publicly through a simple API over HTTPS while using a serverless architecture, the recommended solutions are to use Amazon API Gateway with direct integrations to DynamoDB or with integrations to AWS Lambda functions.<br><br>Option A is a valid solution. With a REST API, API Gateway can be configured with direct integrations to DynamoDB, which eliminates the need for a Lambda function.<br><br>Option C is also a valid solution. With an HTTP API, API Gateway can be configured with integrations to AWS Lambda functions that return data from the DynamoDB tables. This solution provides more flexibility since Lambda can be used to customize the data returned from the DynamoDB tables before it is sent back to the client.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 792892,
          "date": "Mon 30 Jan 2023 14:56",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and C are the correct answers.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 774809,
          "date": "Fri 13 Jan 2023 20:47",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A and C are the correct answers.A.  Create an Amazon API Gateway REST API. Configure this API with direct integrations to DynamoDB by using API Gateway's AWS integration type.C.  Create an Amazon API Gateway HTTP API. Configure this API with integrations to AWS Lambda functions that return data from the DynamoDB tables.<br>By Using Amazon API Gateway, the solution will automatically scale in response to demand, and it will also provide a simple API over HTTPS. While using the Lambda function the data can be accessed from the DynamoDB tables.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For A, this one to be specific https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 804426,
          "date": "Fri 10 Feb 2023 15:32",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "For A, this one to be specific https://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 762206,
          "date": "Fri 30 Dec 2022 18:52",
          "username": "\t\t\t\teraser2021999\t\t\t",
          "content": "Lambda@Edge is available for CloudFront and not for Global Accelerator.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 744203,
          "date": "Tue 13 Dec 2022 16:28",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "OK with CD<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-dynamo-db.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CD"
        }
      ]
    },
    {
      "question_id": "#28",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has registered 10 new domain names. The company uses the domains for online marketing. The company needs a solution that will redirect online visitors to a specific URL for each domain. All domains and target URLs are defined in a JSON document. All DNS records are managed by Amazon Route 53.<br>A solutions architect must implement a redirect service that accepts HTTP and HTTPS requests.<br>Which combination of steps should the solutions architect take to meet these requirements with the LEAST amount of operational effort? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CEF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#28",
          "answers": [
            {
              "choice": "<p>A. Create a dynamic webpage that runs on an Amazon EC2 instance. Configure the webpage to use the JSON document in combination with the event message to look up and respond with a redirect URL.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Application Load Balancer that includes HTTP and HTTPS listeners.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon API Gateway API with a custom domain to publish an AWS Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an Amazon CloudFront distribution. Deploy a Lambda@Edge function.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create an SSL certificate by using AWS Certificate Manager (ACM). Include the domains as Subject Alternative Names.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774812,
          "date": "Fri 13 Jan 2023 20:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C: By creating an AWS Lambda function, the solution architect can use the JSON document to look up the target URLs for each domain and respond with the appropriate redirect URL. This way, the solution does not need to rely on a web server to handle the redirects, which reduces operational effort.<br><br>E: By creating an Amazon CloudFront distribution, the solution architect can deploy a Lambda@Edge function that can look up the target URLs for each domain and respond with the appropriate redirect URL. This way, CloudFront can handle the redirection, which reduces operational effort.<br><br>F: By creating an SSL certificate with ACM and including the domains as Subject Alternative Names, the solution architect can ensure that the redirect service can handle both HTTP and HTTPS requests, which is required by the company.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>SAN cannot handle redirects.We need to do http - https</li><li>A and B are not the right answer because they would require configuring and maintaining a web server to handle the redirects, which would increase operational effort.<br>D is not the right answer because it would require creating an API Gateway API, which increases operational effort.</li><li>Wrong for B, Lambda can be an ALB target, you do not need web server</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: CEF"
        },
        {
          "id": 796339,
          "date": "Thu 02 Feb 2023 19:47",
          "username": "\t\t\t\tShahul75\t\t\t",
          "content": "SAN cannot handle redirects.We need to do http - https",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774813,
          "date": "Fri 13 Jan 2023 20:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A and B are not the right answer because they would require configuring and maintaining a web server to handle the redirects, which would increase operational effort.<br>D is not the right answer because it would require creating an API Gateway API, which increases operational effort.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Wrong for B, Lambda can be an ALB target, you do not need web server</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 847428,
          "date": "Wed 22 Mar 2023 20:04",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "Wrong for B, Lambda can be an ALB target, you do not need web server",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832557,
          "date": "Wed 08 Mar 2023 06:24",
          "username": "\t\t\t\tmKrishna\t\t\t",
          "content": "Key point \\\"LEAST amount of operational effort\\\"<br><br>ANS: B, C, D<br><br>Option A is not a serverless solution and would require more operational effort to manage an EC2 instance. <br>Option E is also a valid solution, but deploying a CloudFront distribution would introduce additional complexity and operational overhead. <br>Option F is not necessary for this solution since the redirection is based on domain name and not SSL certificates.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821016,
          "date": "Sat 25 Feb 2023 00:18",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Options A, D, and E are not necessary for meeting the requirements and would add additional complexity and operational effort. Option A suggests creating a dynamic webpage that runs on an EC2 instance, which is unnecessary as the redirect can be handled by the ALB and Lambda function. Option D suggests using an Amazon API Gateway API with a custom domain to publish an AWS Lambda function, which adds additional complexity and operational effort. Option E suggests creating a CloudFront distribution and deploying a Lambda@Edge function, which is more complex than the solution described above and is not necessary for the given requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818841,
          "date": "Thu 23 Feb 2023 07:05",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "My Logical answer : CloudFront /edge services does not fit here on the requirement.E is not apt. Its for online marketing and all domains users need to be<br>redirected. Redirect service steps are all asked for. Needs a Load balancer as front controller which accepts requests from all domains and SSL certificate is certainly needed..A is irrelevant as creating a single web page does not help in redirection. I go with BCF as correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 818840,
          "date": "Thu 23 Feb 2023 07:04",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "My Logical answer : CloudFront /edge services does not fit here on the requirement.E is not apt. Its for online marketing and all domains users need to be<br>redirected. Redirect service steps are all asked for. Needs a Load balancer as front controller which accepts requests from all domains and SSL certificate is certainly needed..A is irrelevant as creating a single web page does not help in redirection. I go with BCF as correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810214,
          "date": "Thu 16 Feb 2023 04:09",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "I choose B,C,E because the question is focused on implementing a redirect service. F will not work as it's for creating an SSL certificate, not creating the redirect service.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Step 1: Create an Application Load Balancer (ALB) that includes HTTP and HTTPS listeners. The ALB can be used to route incoming requests to the appropriate backend service, in this case, the AWS Lambda function.<br><br>Step 2: Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL. We can use the ALB as a trigger for the Lambda function to process the incoming requests and return the appropriate redirect response.<br><br>Step 3: Create an Amazon CloudFront distribution. We can use the ALB as the origin for the CloudFront distribution. This allows us to use the global edge network of CloudFront for faster and more reliable content delivery. We can also deploy a Lambda@Edge function to modify the response headers and redirect the incoming requests to the appropriate target URL.</li><li>Switching to F: <br>Option F is also a valid solution to create an SSL certificate using AWS Certificate Manager (ACM) that includes the domains as Subject Alternative Names, allowing secure HTTPS connections.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BCE"
        },
        {
          "id": 810217,
          "date": "Thu 16 Feb 2023 04:12",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Step 1: Create an Application Load Balancer (ALB) that includes HTTP and HTTPS listeners. The ALB can be used to route incoming requests to the appropriate backend service, in this case, the AWS Lambda function.<br><br>Step 2: Create an AWS Lambda function that uses the JSON document in combination with the event message to look up and respond with a redirect URL. We can use the ALB as a trigger for the Lambda function to process the incoming requests and return the appropriate redirect response.<br><br>Step 3: Create an Amazon CloudFront distribution. We can use the ALB as the origin for the CloudFront distribution. This allows us to use the global edge network of CloudFront for faster and more reliable content delivery. We can also deploy a Lambda@Edge function to modify the response headers and redirect the incoming requests to the appropriate target URL.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Switching to F: <br>Option F is also a valid solution to create an SSL certificate using AWS Certificate Manager (ACM) that includes the domains as Subject Alternative Names, allowing secure HTTPS connections.</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 816297,
          "date": "Tue 21 Feb 2023 07:44",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Switching to F: <br>Option F is also a valid solution to create an SSL certificate using AWS Certificate Manager (ACM) that includes the domains as Subject Alternative Names, allowing secure HTTPS connections.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792938,
          "date": "Mon 30 Jan 2023 15:47",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "CEF are the answers",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CEF"
        },
        {
          "id": 759826,
          "date": "Wed 28 Dec 2022 13:47",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "CEF<br>The serverless architecture reduces operational overhead the most.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-generating-http-responses-in-requests.html<br>https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CEF"
        },
        {
          "id": 744206,
          "date": "Tue 13 Dec 2022 16:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/69017-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CEF"
        }
      ]
    },
    {
      "question_id": "#29",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company that has multiple AWS accounts is using AWS Organizations. The company's AWS accounts host VPCs, Amazon EC2 instances, and containers.<br>The company's compliance team has deployed a security tool in each VPC where the company has deployments. The security tools run on EC2 instances and send information to the AWS account that is dedicated for the compliance team. The company has tagged all the compliance-related resources with a key of “costCenter” and a value or “compliance”.<br>The company wants to identify the cost of the security tools that are running on the EC2 instances so that the company can charge the compliance team's AWS account. The cost calculation must be as accurate as possible.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#29",
          "answers": [
            {
              "choice": "<p>A. In the management account of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the member accounts of the organization, activate the costCenter user-defined tag. Configure monthly AWS Cost and Usage Reports to save to an Amazon S3 bucket in the management account. Schedule a monthly AWS Lambda function to retrieve the reports and calculate the total cost for the costCenter tagged resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the member accounts of the organization activate the costCenter user-defined tag. From the management account, schedule a monthly AWS Cost and Usage Report. Use the tag breakdown in the report to calculate the total cost for the costCenter tagged resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a custom report in the organization view in AWS Trusted Advisor. Configure the report to generate a monthly billing summary for the costCenter tagged resources in the compliance team's AWS account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 759868,
          "date": "Wed 28 Dec 2022 14:13",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I vote A. <br>https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html<br>https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/configurecostallocreport.html",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 852820,
          "date": "Tue 28 Mar 2023 07:29",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Cost center tag int he management account.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832277,
          "date": "Tue 07 Mar 2023 21:32",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Management account for reports",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 792940,
          "date": "Mon 30 Jan 2023 15:53",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Answer A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774816,
          "date": "Fri 13 Jan 2023 21:09",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Answer A : because we do not depend on the users, I prefer management account<br><br>Option C or A would be the correct answer. In option C, the solution architect would activate the costCenter user-defined tag in the member accounts of the organization, and then schedule a monthly AWS Cost and Usage Report from the management account to retrieve the reports and calculate the total cost for the costCenter tagged resources. In option A, the management account of the organization would activate the costCenter user-defined tag and configure monthly AWS Cost and Usage Reports to be saved to an Amazon S3 bucket in the management account. Then, use the tag breakdown in the report to obtain the total cost for the costCenter tagged resources. Both options would allow the company to accurately identify the cost of the security tools running on the EC2 instances and charge the compliance team's AWS account.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759369,
          "date": "Wed 28 Dec 2022 06:04",
          "username": "\t\t\t\tyimicc\t\t\t",
          "content": "Should be a C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Change to A, the activation of user tag for billing can only be done by management account</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759376,
          "date": "Wed 28 Dec 2022 06:07",
          "username": "\t\t\t\tyimicc\t\t\t",
          "content": "Change to A, the activation of user tag for billing can only be done by management account",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 751340,
          "date": "Tue 20 Dec 2022 19:34",
          "username": "\t\t\t\ttman22\t\t\t",
          "content": "A.  You want the cost information across all accounts - So you use the management account.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 744217,
          "date": "Tue 13 Dec 2022 16:40",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I want to answer C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#30",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has 50 AWS accounts that are members of an organization in AWS Organizations. Each account contains multiple VPCs. The company wants to use AWS Transit Gateway to establish connectivity between the VPCs in each member account. Each time a new member account is created, the company wants to automate the process of creating a new VPC and a transit gateway attachment.<br>Which combination of steps will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#30",
          "answers": [
            {
              "choice": "<p>A. From the management account, share the transit gateway with member accounts by using AWS Resource Access Manager.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. From the management account, share the transit gateway with member accounts by using an AWS Organizations SCP.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account. Associate the attachment with the transit gateway in the management account by using the transit gateway ID. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Launch an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a peering transit gateway attachment in a member account. Share the attachment with the transit gateway in the management account by using a transit gateway service-linked role.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. From the management account, share the transit gateway with member accounts by using AWS Service Catalog.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852822,
          "date": "Tue 28 Mar 2023 07:32",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AC are my choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 792941,
          "date": "Mon 30 Jan 2023 15:54",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and C are the answer for me",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 774821,
          "date": "Fri 13 Jan 2023 21:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is sharing the transit gateway with member accounts by using AWS Resource Access Manager, which allows the management account to share resources with member accounts. Option C is launching an AWS CloudFormation stack set from the management account that automatically creates a new VPC and a VPC transit gateway attachment in a member account, and associates the attachment with the transit gateway in the management account by using the transit gateway ID.  This automation of creating a new VPC and transit gateway attachment in new member accounts can help to streamline the process and reduce operational effort.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 760904,
          "date": "Thu 29 Dec 2022 11:51",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A & C<br>https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-transitgatewayattachment.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 744223,
          "date": "Tue 13 Dec 2022 16:43",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/60090-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        }
      ]
    },
    {
      "question_id": "#31",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An enterprise company wants to allow its developers to purchase third-party software through AWS Marketplace. The company uses an AWS Organizations account structure with full features enabled, and has a shared services account in each organizational unit (OU) that will be used by procurement managers. The procurement team's policy indicates that developers should be able to obtain third-party software from an approved list only and use Private Marketplace in AWS Marketplace to achieve this requirement. The procurement team wants administration of Private Marketplace to be restricted to a role named procurement-manager-role, which could be assumed by procurement managers. Other IAM users, groups, roles, and account administrators in the company should be denied Private Marketplace administrative access.<br>What is the MOST efficient way to design an architecture to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#31",
          "answers": [
            {
              "choice": "<p>A. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the PowerUserAccess managed policy to the role. Apply an inline policy to all IAM users and roles in every AWS account to deny permissions on the AWSPrivateMarketplaceAdminFullAccess managed policy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM role named procurement-manager-role in all AWS accounts in the organization. Add the AdministratorAccess managed policy to the role. Define a permissions boundary with the AWSPrivateMarketplaceAdminFullAccess managed policy and attach it to all the developer roles.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM role named procurement-manager-role in all the shared services accounts in the organization. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Create another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM role named procurement-manager-role in all AWS accounts that will be used by developers. Add the AWSPrivateMarketplaceAdminFullAccess managed policy to the role. Create an SCP in Organizations to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. Apply the SCP to all the shared services accounts in the organization.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852916,
          "date": "Tue 28 Mar 2023 09:10",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create an IAM role named procurement-manager-role in all the shared services accounts in the organization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 821883,
          "date": "Sat 25 Feb 2023 23:52",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "Confirmed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792947,
          "date": "Mon 30 Jan 2023 16:00",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "should be C i guess",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 782128,
          "date": "Fri 20 Jan 2023 11:46",
          "username": "\t\t\t\task4cloud\t\t\t",
          "content": "This approach allows the procurement managers to assume the procurement-manager-role in shared services accounts, which have the AWSPrivateMarketplaceAdminFullAccess managed policy attached to it and can then manage the Private Marketplace. The organization root-level SCP denies the permission to administer Private Marketplace to everyone except the role named procurement-manager-role and another SCP denies the permission to create an IAM role named procurement-manager-role to everyone in the organization, ensuring that only the procurement team can assume the role and manage the Private Marketplace. This approach provides a centralized way to manage and restrict access to Private Marketplace while maintaining a high level of security.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774824,
          "date": "Fri 13 Jan 2023 21:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The most efficient way to design an architecture to meet these requirements is option C.  By creating an IAM role named procurement-manager-role in all the shared services accounts in the organization and adding the AWSPrivateMarketplaceAdminFullAccess managed policy to the role, the procurement managers will have the necessary permissions to administer Private Marketplace. Then, by creating an organization root-level SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role and another organization root-level SCP to deny permissions to create an IAM role named procurement-manager-role to everyone in the organization, the company can restrict access to Private Marketplace administrative access to only the procurement managers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744242,
          "date": "Tue 13 Dec 2022 16:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/28410-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#32",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is in the process of implementing AWS Organizations to constrain its developers to use only Amazon EC2, Amazon S3, and Amazon DynamoDB.  The developers account resides in a dedicated organizational unit (OU). The solutions architect has implemented the following SCP on the developers account:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image5.png\"><br>When this policy is deployed, IAM users in the developers account are still able to use AWS services that are not listed in the policy.<br>What should the solutions architect do to eliminate the developers' ability to use services outside the scope of this policy?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#32",
          "answers": [
            {
              "choice": "<p>A. Create an explicit deny statement for each AWS service that should be constrained.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the FullAWSAccess SCP from the developers account's OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the FullAWSAccess SCP to explicitly deny all services.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add an explicit deny statement using a wildcard to the end of the SCP.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852929,
          "date": "Tue 28 Mar 2023 09:13",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Remove the FullAWSAccess SCP from the developers account's OU",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828663,
          "date": "Sat 04 Mar 2023 05:12",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "An allow list strategy has you remove the FullAWSAccess SCP that is attached by default to every OU and account. This means that no APIs are permitted anywhere unless you explicitly allow them. To allow a service API to operate in an AWS account, you must create your own SCPs and attach them to the account and every OU above it, up to and including the root. Every SCP in the hierarchy, starting at the root, must explicitly allow the APIs that you want to be usable in the OUs and accounts below it<br><br>A deny list strategy makes use of the FullAWSAccess SCP that is attached by default to every OU and account. This SCP overrides the default implicit deny, and explicitly allows all permissions to flow down from the root to every account, unless you explicitly deny a permission with an additional SCP that you create and attach to the appropriate OU or account<br>If the developers can access other services it implies the \\\"Deny List Strategy\\\" henceFullAWSAccess is in place and should be removed",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820230,
          "date": "Fri 24 Feb 2023 08:55",
          "username": "\t\t\t\tGabehcoud\t\t\t",
          "content": "the question doesn't state that there is another SCP applied to developers account. By choosing B, are we just assuming ? Why can't it be D?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 804546,
          "date": "Fri 10 Feb 2023 17:09",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I was confused at first but the intersection of sets here allowed me to understand the flow of SCPs from root to child OUs https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_auth.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792917,
          "date": "Mon 30 Jan 2023 15:24",
          "username": "\t\t\t\tjooncco\t\t\t",
          "content": "B is correct.<br>By removing FullAWSAccess SCP, default deny will be applied.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 780727,
          "date": "Thu 19 Jan 2023 05:35",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "B is correct<br>https://docs.aws.amazon.com/organizations/latest/APIReference/API_DetachPolicy.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744248,
          "date": "Tue 13 Dec 2022 16:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/46899-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 742312,
          "date": "Mon 12 Dec 2022 02:51",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct because default FullAWSAccess SCP is applied",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#33",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a monolithic REST-based API for a mobile app on five Amazon EC2 instances in public subnets of a VPC.  Mobile clients connect to the API by using a domain name that is hosted on Amazon Route 53. The company has created a Route 53 multivalue answer routing policy with the IP addresses of all the EC2 instances. Recently, the app has been overwhelmed by large and sudden increases to traffic. The app has not been able to keep up with the traffic.<br>A solutions architect needs to implement a solution so that the app can handle the new and varying load.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#33",
          "answers": [
            {
              "choice": "<p>A. Separate the API into individual AWS Lambda functions. Configure an Amazon API Gateway REST API with Lambda integration for the backend. Update the Route 53 record to point to the API Gateway API.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Containerize the API logic. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Run the containers in the cluster by using Amazon EC2. Create a Kubernetes ingress. Update the Route 53 record to point to the Kubernetes ingress.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Auto Scaling group. Place all the EC2 instances in the Auto Scaling group. Configure the Auto Scaling group to perform scaling actions that are based on CPU utilization. Create an AWS Lambda function that reacts to Auto Scaling group changes and updates the Route 53 record.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Application Load Balancer (ALB) in front of the API. Move the EC2 instances to private subnets in the VPC.  Add the EC2 instances as targets for the ALB.  Update the Route 53 record to point to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 747081,
          "date": "Fri 16 Dec 2022 12:02",
          "username": "\t\t\t\tEricZhang\t\t\t",
          "content": "Serverless requires least operational effort.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How can this be the answer ?? It says: Separate the API into individual AWS Lambda functions. Can you calculate the operational overhead to do that?</li></ul>",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 822775,
          "date": "Sun 26 Feb 2023 19:00",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "How can this be the answer ?? It says: Separate the API into individual AWS Lambda functions. Can you calculate the operational overhead to do that?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 793547,
          "date": "Tue 31 Jan 2023 03:10",
          "username": "\t\t\t\tjooncco\t\t\t",
          "content": "Suppose there are a 100 REST APIs (Since this application is monolithic, it's quite common).<br>Are you still going to copy and paste all those API codes into lambda?<br>What if business logic changes?<br>This is not MINIMAL. I would go with C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It says \\\"a monolithic REST-based API \\\" - hence only 1 API. Initially I thought C, but I'll go with A as it says least operation overhead (not least implementation effort). Lambda has virtually no operation overhead compared to EC2.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 819817,
          "date": "Thu 23 Feb 2023 23:14",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "It says \\\"a monolithic REST-based API \\\" - hence only 1 API. Initially I thought C, but I'll go with A as it says least operation overhead (not least implementation effort). Lambda has virtually no operation overhead compared to EC2.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 852937,
          "date": "Tue 28 Mar 2023 09:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "I vote A - sep lambda functions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 837581,
          "date": "Mon 13 Mar 2023 04:10",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://aws.amazon.com/getting-started/hands-on/break-monolith-app-microservices-ecs-docker-ec2/module-one/ and https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/serverless-microservices.html<br><br>Just saying, moving it to a microservice architecture not only makes sense but will remove a lot of operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 830343,
          "date": "Sun 05 Mar 2023 22:29",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "This question is the mother of all tricky questions lol<br>The main issue of the current design is that R53 is used to distribute the load to the app. Which is a bad practice. This why i think ALB is the best solution here. Answer A is incorrect because a big refactor and this is the last think you want to think about !<br>Answer D solve only the Autoscaling, but miss the ALB and still use the R53 as a load balancer!<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Agree, with u<br>Makes no sense refactor the APP not knowing details ( A &amp; B)<br>I dont see why to create a lambda to add and remove records to route 53 that could be cached as long as the duration of the TTL.( C )</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831999,
          "date": "Tue 07 Mar 2023 15:28",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "Agree, with u<br>Makes no sense refactor the APP not knowing details ( A & B)<br>I dont see why to create a lambda to add and remove records to route 53 that could be cached as long as the duration of the TTL.( C )",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826432,
          "date": "Thu 02 Mar 2023 03:52",
          "username": "\t\t\t\tdoto\t\t\t",
          "content": "ccccccccccccc",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 823847,
          "date": "Mon 27 Feb 2023 15:59",
          "username": "\t\t\t\t_lasco_\t\t\t",
          "content": "C is correct<br><br>A: may require a lot of effort in refactoring to lambda and different architecture<br>B: may require a lot of effort in refactoring to containers/kubernetes and different architecture <br>C: correct<br>D: would be great to have a load balancer but the solution does not involve autoscaling so by itself does not satisfy the increase in demand. Also moving instances to private subnet may be not viable, depending on the app behaviour.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 821887,
          "date": "Sun 26 Feb 2023 00:00",
          "username": "\t\t\t\tcudbyanc\t\t\t",
          "content": "Option A and B suggest re-architecting the application, which may require significant development work and operational overhead. Option C adds complexity by requiring an additional Lambda function to update the Route 53 record.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's correct, but unfortunately D is not scaleable as it's missing the ASG</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 828455,
          "date": "Fri 03 Mar 2023 23:42",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "That's correct, but unfortunately D is not scaleable as it's missing the ASG",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821486,
          "date": "Sat 25 Feb 2023 13:58",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Why do they give a question with a set of answers that are all bad given the scenario. <br>D: misses Autoscaler. It just does not do what the architect was asked to find a solution for.<br>C: it simply does not work: changing DNS need to take TTL into account... <br>B: adds overhead for Kubernetes. <br>A: works but is ridiculous expensive and comes with operational effort to maintain lambda.<br><br>So I guess the only possible option is A.  <br><br>Disclaimer: No one reasonable would use Lambda if it comes to high load. If the load justifies an EC2let alone 5 EC instances. EC2 is way to go. Autoscaler, Loadbalancer. This is simple and simple means less operational overhead while complexity means operational overhead: Lambda adds complexity and is expensive when it comes to load (one invocation: cheap but not massive number of invocations).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 820059,
          "date": "Fri 24 Feb 2023 04:00",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "My Logical answer : After reading some discussion comments, my take - Least operation effort does not mean quick fix, its least work to maintain it. C is wrong , it seemed good reading first part but at the end it mentions wierd statement \\\"lambda updating Route53 all the time when it reacts ? why updating DNS service every time? \\\" D is not apt because, why would we put internet facing EC2 instances in private subnet? that adds additional overhead of maintaining NAT gateways /route tables etc. So serverless solution for least operational effort leaves A or B.  I feel B is over provisioning with ECS/EKS clustering because it looks like a low/medium scale app with just 5 ec2 instances. I'd go with A as best answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 820056,
          "date": "Fri 24 Feb 2023 03:55",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "My Logical answer : After reading some discussion comments, my take - Least operation effort does not mean quick fix, its least work to maintain it. C is wrong , it seemed good reading first part but at the end it mentions wierd statement \\\"lambda updating Route53 all the time when it reacts ? why updating DNS service every time? \\\" D is not apt because, why would we put internet facing EC2 instances in private subnet? that adds additional overhead of maintaining NAT gateways /route tables etc. So serverless solution for least operational effort leaves A or B.  I feel B is over provisioning with ECS/EKS clustering because it looks like a low/medium scale app with just 5 ec2 instances. I'd go with A as best answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819391,
          "date": "Thu 23 Feb 2023 17:22",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "C based on minimal operational overhead<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Decided to update my answer to D</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 835351,
          "date": "Fri 10 Mar 2023 20:34",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Decided to update my answer to D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812355,
          "date": "Fri 17 Feb 2023 21:47",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "API Gateway is the option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 801587,
          "date": "Wed 08 Feb 2023 02:51",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "No C<br>Because max 8 EC2s on Route53 multivalue answers",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 799872,
          "date": "Mon 06 Feb 2023 15:59",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "I know this question is gonna be a controversial one. The real issue is what the mean of LEAST OPERATIONAL OVERHEAD means. It could mean the least amount of work to set up initially, in which case the answer is definitely C.  Converting a monolithic application to lambda is not a simple task.<br>But in operation overhead means how much work it would take to maintain, the answer is definitely A because serverless has a lot less effort once its operational. <br>Personally, I would go with A on this question. I've been taking these cert exams for a while now and I get a sense that AWS wants you to use serverless.<br>Additionally, not quite sure what it means in C to have the lambda update Route 53...",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 798657,
          "date": "Sun 05 Feb 2023 07:57",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "The answer is C, no idea, why ppl are voting for A.  C requires the minimum amount of effort.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>operational overhead means less effort in the long run, so i would change my answer to A. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 806905,
          "date": "Mon 13 Feb 2023 00:31",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "operational overhead means less effort in the long run, so i would change my answer to A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792961,
          "date": "Mon 30 Jan 2023 16:11",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "why not C?",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#34",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has created an OU in AWS Organizations for each of its engineering teams. Each OU owns multiple AWS accounts. The organization has hundreds of AWS accounts.<br>A solutions architect must design a solution so that each OU can view a breakdown of usage costs across its AWS accounts.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#34",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Cost and Usage Report (CUR) for each OU by using AWS Resource Access Manager. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Cost and Usage Report (CUR) in each AWS Organizations member account. Allow each team to visualize the CUR through an Amazon QuickSight dashboard.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Cost and Usage Report (CUR) by using AWS Systems Manager. Allow each team to visualize the CUR through Systems Manager OpsCenter dashboards.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852948,
          "date": "Tue 28 Mar 2023 09:26",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774832,
          "date": "Fri 13 Jan 2023 21:33",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is the correct answer. The solution would be to create an AWS Cost and Usage Report (CUR) from the AWS Organizations management account. This would allow the management account to view the usage costs across all the member accounts, and the teams can visualize the CUR through an Amazon QuickSight dashboard. This allows the organization to have a centralized place to view the cost breakdown and the teams to access the cost breakdown in an easy way.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744252,
          "date": "Tue 13 Dec 2022 17:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/71951-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#35",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is storing data on premises on a Windows file server. The company produces 5&nbsp;GB of new data daily.<br>The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. The company already has established an AWS Direct Connect connection between the on-premises network and AWS.<br>Which data migration strategy should the company use?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#35",
          "answers": [
            {
              "choice": "<p>A. Use the file gateway option in AWS Storage Gateway to replace the existing Windows file server, and point the existing file share to the new file gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Data Pipeline to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852959,
          "date": "Tue 28 Mar 2023 09:31",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the right answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 840108,
          "date": "Wed 15 Mar 2023 18:27",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "The company migrated part of its Windows-based workload to AWS and needs the data to be available on a file system in the cloud. <br>Here It is open to discussion. Do they want to migrate the entire data to the cloud or do they just want data to be available in the cloud.<br>It sound like data will sync to the cloud and remain active on prem. Which leads to option A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 837587,
          "date": "Mon 13 Mar 2023 04:19",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/efs/latest/ug/trnsfr-data-using-datasync.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 823860,
          "date": "Mon 27 Feb 2023 16:08",
          "username": "\t\t\t\t_lasco_\t\t\t",
          "content": "B<br>I was in doubt between B and D, but EFS does not support windows for mounting:<br>https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 804602,
          "date": "Fri 10 Feb 2023 18:23",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I am curious if Amazon FSx File Gateway from Azure Storage Gateway (https://aws.amazon.com/storagegateway/file/) can address this.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 798665,
          "date": "Sun 05 Feb 2023 08:20",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "My initial thought was A, but the solution requires data to be available in the cloud, not to replace a Windows File server with a Cloud-based sol'n-like storage gateway. So B is correct.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Correct, it says \\\"The company migrated part of its Windows-based workload to AWS\\\" so there is still some workload onpremise, this is not about data also workloads, so A is incorrect as smiply replacing the existing windows file server is not an option. Also DataSync work with Direct Connect which the company already uses further giving a hint to B</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 847153,
          "date": "Wed 22 Mar 2023 15:31",
          "username": "\t\t\t\tvvahe\t\t\t",
          "content": "Correct, it says \\\"The company migrated part of its Windows-based workload to AWS\\\" so there is still some workload onpremise, this is not about data also workloads, so A is incorrect as smiply replacing the existing windows file server is not an option. Also DataSync work with Direct Connect which the company already uses further giving a hint to B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774833,
          "date": "Fri 13 Jan 2023 21:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon FSx.D.  Use AWS DataSync to schedule a daily task to replicate data between the on-premises Windows file server and Amazon Elastic File System (Amazon EFS) are also valid options. They both use DataSync to schedule a daily task to replicate the data between on-premises and cloud, the main difference is the type of file system in the cloud, Amazon FSx or Amazon Elastic File System (Amazon EFS).",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744255,
          "date": "Tue 13 Dec 2022 17:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/47620-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#36",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's solutions architect is reviewing a web application that runs on AWS. The application references static assets in an Amazon S3 bucket in the us-east-1 Region. The company needs resiliency across multiple AWS Regions. The company already has created an S3 bucket in a second Region.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#36",
          "answers": [
            {
              "choice": "<p>A. Configure the application to write each object to both S3 buckets. Set up an Amazon Route 53 public hosted zone with a record set by using a weighted routing policy for each S3 bucket. Configure the application to reference the objects by using the Route 53 DNS name.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function to copy objects from the S3 bucket in us-east-1 to the S3 bucket in the second Region. Invoke the Lambda function each time an object is written to the S3 bucket in us-east-1. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. If failover is required, update the application code to load S3 objects from the S3 bucket in the second Region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852968,
          "date": "Tue 28 Mar 2023 09:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "S3 + Cloudfront",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851750,
          "date": "Mon 27 Mar 2023 07:31",
          "username": "\t\t\t\tCloud_noob\t\t\t",
          "content": "you can configure Amazon CloudFront to use two different Amazon S3 buckets from different regions as the origin for your content.<br><br>To do this, you would need to create two separate Amazon S3 bucket origins in your CloudFront distribution settings, each one pointing to a different S3 bucket in a different region.<br><br>When creating the CloudFront distribution, you can add multiple origins to the distribution configuration. You can specify the origin domain name for each origin, which will correspond to the domain name of the S3 bucket you want to use as the origin.<br><br>You can also specify the origin protocol policy, which determines whether CloudFront uses HTTP or HTTPS to communicate with the origin.<br><br>Keep in mind that you will need to configure cross-region replication between the two S3 buckets in order to keep the content in both buckets synchronized. Additionally, you will need to make sure that both S3 buckets are publicly accessible or that CloudFront has the appropriate permissions to access the buckets.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793573,
          "date": "Tue 31 Jan 2023 03:55",
          "username": "\t\t\t\tjooncco\t\t\t",
          "content": "Modifying any existing application code IS a operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 782160,
          "date": "Fri 20 Jan 2023 12:17",
          "username": "\t\t\t\task4cloud\t\t\t",
          "content": "This solution will meet the requirements with the least operational overhead as it allows the company to use Amazon CloudFront to automatically distribute the static assets of the web application across multiple regions, and if the primary S3 bucket in us-east-1 becomes unavailable, CloudFront will automatically route the traffic to the secondary S3 bucket in the second region. This solution eliminates the need for additional Lambda function or updating the application code for failover.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774835,
          "date": "Fri 13 Jan 2023 21:37",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Configure replication on the S3 bucket in us-east-1 to replicate objects to the S3 bucket in the second Region. Set up an Amazon CloudFront distribution with an origin group that contains the two S3 buckets as origins. This option provides automatic replication of objects across the two S3 buckets, and CloudFront automatically routes requests to the nearest origin, providing low latency and high availability for the application. This solution requires minimal operational overhead to maintain as the replication and failover is handled automatically by S3 and CloudFront.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 766972,
          "date": "Thu 05 Jan 2023 20:09",
          "username": "\t\t\t\tVVish\t\t\t",
          "content": "C - LEAST operational overhead",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759778,
          "date": "Wed 28 Dec 2022 13:00",
          "username": "\t\t\t\taimik\t\t\t",
          "content": "involves updating the application code to load S3 objects from the second region in case of a failover, which is not necessary if you are using CloudFront with an origin group as in option C. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744348,
          "date": "Tue 13 Dec 2022 19:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Answer C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 742310,
          "date": "Mon 12 Dec 2022 02:45",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct.<br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#37",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a three-tier web application in an on-premises environment. Due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to AWS. The application is written in .NET and has a dependency on a MySQL database. A solutions architect must design a scalable and highly available solution to meet the demand of 200,000 daily users.<br>Which steps should the solutions architect take to design an appropriate solution?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#37",
          "answers": [
            {
              "choice": "<p>A. Use AWS Elastic Beanstalk to create a new application with a web server environment and an Amazon RDS MySQL Multi-AZ DB instance. The environment should launch a Network Load Balancer (NLB) in front of an Amazon EC2 Auto Scaling group in multiple Availability Zones. Use an Amazon Route 53 alias record to route traffic from the company's domain to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Elastic Beanstalk to create an automatically scaling web server environment that spans two separate Regions with an Application Load Balancer (ALB) in each Region. Create a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a cross-Region read replica. Use Amazon Route 53 with a geoproximity routing policy to route traffic between the two Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon ECS cluster of Spot instances spanning three Availability Zones. The stack should launch an Amazon RDS MySQL DB instance with a Snapshot deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852975,
          "date": "Tue 28 Mar 2023 09:43",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 840361,
          "date": "Wed 15 Mar 2023 23:16",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B makes sense to me ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837601,
          "date": "Mon 13 Mar 2023 04:39",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.concepts.design.html<br><br>AWS EB does support .NET and MySQL; the difference now is that it is not supported separate regions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 812369,
          "date": "Fri 17 Feb 2023 21:56",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "ALB and Rou53 Alias",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792995,
          "date": "Mon 30 Jan 2023 16:44",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Answer is B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 786938,
          "date": "Tue 24 Jan 2023 21:35",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "Answer is A. B.  R53 alias record?C.  No requirement for multi-region. Just HA. D.  Spot instance not HA. A.  Yes. NLB fine, EC2 ASG fine, R53 alias to NLB EIP fine.<br>Question does not mention regions, NLB can work with websites - yes ALB is the better option but NLB works perfectly fine for HTTP/HTTPS traffic.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774839,
          "date": "Fri 13 Jan 2023 21:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is correct. The solution architect should use AWS CloudFormation to launch a stack containing an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones. The stack should launch a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy. Use an Amazon Route 53 alias record to route traffic from the company's domain to the ALB. <br><br>This solution provides scalability and high availability for the web application by using an Application Load Balancer and an Auto Scaling group in multiple availability zones, which can automatically scale in and out based on traffic demand. The use of a Multi-AZ Amazon Aurora MySQL DB cluster provides high availability for the database layer and the Retain deletion policy ensures the data is retained even if the DB instance is deleted. Additionally, the use of Route 53 with an alias record ensures traffic is routed to the correct location.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744835,
          "date": "Wed 14 Dec 2022 08:44",
          "username": "\t\t\t\trobertohyena\t\t\t",
          "content": "Agree with B. <br>Not A: we will not use NLB for web app<br>Not C: Beanstalk is region service. It CANNOT \\\"automatically scaling web server environment that spans two separate Regions\\\"<br>Not D:spot instances cant meet 'highly available'<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's correct, option C is not a valid solution because AWS Elastic Beanstalk is a region-specific service, it cannot span multiple regions. Option B is a valid solution that uses CloudFormation to launch a stack with an Application Load Balancer in front of an Auto Scaling group, a Multi-AZ Aurora MySQL cluster and Route 53 to route traffic to the load balancer, it meets the requirements of scalability and high availability with a good performance and with less operational overhead.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774840,
          "date": "Fri 13 Jan 2023 21:44",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "That's correct, option C is not a valid solution because AWS Elastic Beanstalk is a region-specific service, it cannot span multiple regions. Option B is a valid solution that uses CloudFormation to launch a stack with an Application Load Balancer in front of an Auto Scaling group, a Multi-AZ Aurora MySQL cluster and Route 53 to route traffic to the load balancer, it meets the requirements of scalability and high availability with a good performance and with less operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744352,
          "date": "Tue 13 Dec 2022 19:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "for me the answer is B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/28502-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744353,
          "date": "Tue 13 Dec 2022 19:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/28502-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742305,
          "date": "Mon 12 Dec 2022 02:42",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "Answer is B.  <br>AC not correct because Beanstalk does not support .NET<br>D user spot instance that is not reliable<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Beanstalk does support .NET https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.container.console.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747094,
          "date": "Fri 16 Dec 2022 12:17",
          "username": "\t\t\t\tEricZhang\t\t\t",
          "content": "Beanstalk does support .NET https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_NET.container.console.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#38",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS Organizations to manage multiple AWS accounts. For security purposes, the company requires the creation of an Amazon Simple Notification Service (Amazon SNS) topic that enables integration with a third-party alerting system in all the Organizations member accounts.<br>A solutions architect used an AWS CloudFormation template to create the SNS topic and stack sets to automate the deployment of CloudFormation stacks. Trusted access has been enabled in Organizations.<br>What should the solutions architect do to deploy the CloudFormation StackSets in all AWS accounts?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#38",
          "answers": [
            {
              "choice": "<p>A. Create a stack set in the Organizations member accounts. Use service-managed permissions. Set deployment options to deploy to an organization. Use CloudFormation StackSets drift detection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create stacks in the Organizations member accounts. Use self-service permissions. Set deployment options to deploy to an organization. Enable the CloudFormation StackSets automatic deployment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a stack set in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets automatic deployment.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create stacks in the Organizations management account. Use service-managed permissions. Set deployment options to deploy to the organization. Enable CloudFormation StackSets drift detection.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852978,
          "date": "Tue 28 Mar 2023 09:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create a stack set in the Organizations management account.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 812373,
          "date": "Fri 17 Feb 2023 22:02",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Stack Set in Mgmt account",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774843,
          "date": "Fri 13 Jan 2023 21:47",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The best solution is C, because it involves creating the stack set in the management account of the organization, which is the central point of control for all the member accounts. This allows the solutions architect to manage the deployment of the stack set across all member accounts from a single location. Service-managed permissions are used, which allows the CloudFormation service to deploy the stack set to all member accounts. The deployment options are set to deploy to the organization and automatic deployment is enabled, which ensures that the stack set is automatically deployed to all member accounts as soon as it is created in the management account.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744459,
          "date": "Tue 13 Dec 2022 21:56",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "I THINK I SHOULD BE A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744357,
          "date": "Tue 13 Dec 2022 19:26",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/47723-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#39",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its workloads from on premises to AWS. The workloads run on Linux and Windows. The company has a large on-premises infrastructure that consists of physical machines and VMs that host numerous applications.<br><br>The company must capture details about the system configuration, system performance, running processes, and network connections of its on-premises workloads. The company also must divide the on-premises applications into groups for AWS migrations. The company needs recommendations for Amazon EC2 instance types so that the company can run its workloads on AWS in the most cost-effective manner.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ADE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#39",
          "answers": [
            {
              "choice": "<p>A. Assess the existing applications by installing AWS Application Discovery Agent on the physical machines and VMs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Assess the existing applications by installing AWS Systems Manager Agent on the physical machines and VMs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Group servers into applications for migration by using AWS Systems Manager Application Manager.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Group servers into applications for migration by using AWS Migration Hub.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Generate recommended instance types and associated costs by using AWS Migration Hub.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Import data about server sizes into AWS Trusted Advisor. Follow the recommendations for cost optimization.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 780551,
          "date": "Thu 19 Jan 2023 00:55",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "trusted advisor doesn't have option to upload data, so option F is irrelavent",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 855227,
          "date": "Thu 30 Mar 2023 04:56",
          "username": "\t\t\t\thgc2023\t\t\t",
          "content": "B is incorrect because the servers are on prem.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 840371,
          "date": "Wed 15 Mar 2023 23:33",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "ADE no doubts ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 821014,
          "date": "Sat 25 Feb 2023 00:13",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : Falls under the domain \\\"Accelerate Workload Migration and Modernization\\\"<br>promoting MigrationHub<br>Step 1 - Identify the apps<br>Step 2 - Group them<br>Step 3 - Before hand, find out what instance types would need to be in when <br>actual migration happens<br>https://d1.awsstatic.com/Product-Page-Diagram_AWS-Migration-Hub-Orchestrator%402x.0c34c9483d13ebd26cf9072193384a58531624f3.png<br>For OnPremises migrations, first phase is Discovery which can be done with<br>Discovery agent , A<br>https://d1.awsstatic.com/products/application-discovery-service/Product-Page-Diagram_AWS-Application-Discovery-Service%201.9d81c27f3de50349a9406b8def61b8eb914e2930.png<br><br>I wont go with Trusted Advisor although it advises how cost can be advised because-<br>This applies for already aws available environment. Here, about to get migrated into<br>AWS and Architects need to discover lot of info before hand to plan alot. So I choose E between E and F.  My answer - A,D,E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808615,
          "date": "Tue 14 Feb 2023 18:10",
          "username": "\t\t\t\taws0909\t\t\t",
          "content": "WhyOption C Group servers into applications for migration by using AWS Systems Manager Application Manager is incorrect?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS SSM Application Manager is used for existing resources deployed to AWS</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824773,
          "date": "Tue 28 Feb 2023 13:23",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "AWS SSM Application Manager is used for existing resources deployed to AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804736,
          "date": "Fri 10 Feb 2023 21:22",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "A is better than B. <br><br>> Agent-based discovery can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers. The agent installer is available for Windows and Linux operating systems. It collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running.<br><br>https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 782366,
          "date": "Fri 20 Jan 2023 15:46",
          "username": "\t\t\t\tboomx\t\t\t",
          "content": "BDE.  Trusted Advisor is not for onprem assessments. Migration hub does EC2 ones",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778167,
          "date": "Mon 16 Jan 2023 20:39",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ADE is my answer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 774847,
          "date": "Fri 13 Jan 2023 21:58",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "in order to meet the requirements of capturing details about the system configuration, system performance, running processes, and network connections of on-premises workloads, the company should install the AWS Application Discovery Agent on the physical machines and VMs. This will allow the company to assess the existing applications and gather information about their system configurations, performance, and network connections.<br><br>To group servers into applications for migration, the company should use the AWS Migration Hub. This will allow the company to organize their servers and applications in a way that makes migration to AWS more manageable and efficient.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Hey Maestro, appreciate your responses man..but you are wrong in this question. E is correct because this is for on premises requirement. F is correct in aws environment. ADE should be correct. I gave detailed logical answer as well if you are interested in other comments area</li><li>In order to generate recommended instance types and associated costs, the company should use AWS Trusted Advisor. Trusted Advisor can analyze the data collected by the Application Discovery Agent and provide recommendations for cost-optimized EC2 instances that will be suitable for the company's workloads. This will allow the company to run their workloads on AWS in the most cost-effective manner.<br><br>Option E, which involves generating recommended instance types and associated costs using AWS Migration Hub, is not the best choice for cost optimization, Trusted Advisor is a service that analyzes the resources in your AWS environment and provides recommendations to help you save money, improve system performance, or close security gaps.</li><li>I think option E is correct. Considering the fact Trusted Advisor provides suggestion based on utilization of resources which is already deployed in AWS. Whereas Migration Hub can suggest recommended EC2 instances.<br>https://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 821019,
          "date": "Sat 25 Feb 2023 00:21",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Hey Maestro, appreciate your responses man..but you are wrong in this question. E is correct because this is for on premises requirement. F is correct in aws environment. ADE should be correct. I gave detailed logical answer as well if you are interested in other comments area",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774848,
          "date": "Fri 13 Jan 2023 21:59",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "In order to generate recommended instance types and associated costs, the company should use AWS Trusted Advisor. Trusted Advisor can analyze the data collected by the Application Discovery Agent and provide recommendations for cost-optimized EC2 instances that will be suitable for the company's workloads. This will allow the company to run their workloads on AWS in the most cost-effective manner.<br><br>Option E, which involves generating recommended instance types and associated costs using AWS Migration Hub, is not the best choice for cost optimization, Trusted Advisor is a service that analyzes the resources in your AWS environment and provides recommendations to help you save money, improve system performance, or close security gaps.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think option E is correct. Considering the fact Trusted Advisor provides suggestion based on utilization of resources which is already deployed in AWS. Whereas Migration Hub can suggest recommended EC2 instances.<br>https://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 783136,
          "date": "Sat 21 Jan 2023 10:17",
          "username": "\t\t\t\tshputhan\t\t\t",
          "content": "I think option E is correct. Considering the fact Trusted Advisor provides suggestion based on utilization of resources which is already deployed in AWS. Whereas Migration Hub can suggest recommended EC2 instances.<br>https://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#40",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting an image-processing service on AWS in a VPC.  The VPC extends across two Availability Zones. Each Availability Zone contains one public subnet and one private subnet.<br><br>The service runs on Amazon EC2 instances in the private subnets. An Application Load Balancer in the public subnets is in front of the service. The service needs to communicate with the internet and does so through two NAT gateways. The service uses Amazon S3 for image storage. The EC2 instances retrieve approximately 1 ТВ of data from an S3 bucket each day.<br><br>The company has promoted the service as highly secure. A solutions architect must reduce cloud expenditures as much as possible without compromising the service's security posture or increasing the time spent on ongoing operations.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#40",
          "answers": [
            {
              "choice": "<p>A. Replace the NAT gateways with NAT instances. In the VPC route table, create a route from the private subnets to the NAT instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Move the EC2 instances to the public subnets. Remove the NAT gateways.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up an S3 gateway VPC endpoint in the VPAttach an endpoint policy to the endpoint to allow the required actions on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Attach an Amazon Elastic File System (Amazon EFS) volume to the EC2 instances. Host the images on the EFS volume.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852993,
          "date": "Tue 28 Mar 2023 10:06",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Set up an S3 gateway VPC endpoint",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 840379,
          "date": "Wed 15 Mar 2023 23:36",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C - easy one ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 821033,
          "date": "Sat 25 Feb 2023 00:54",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "The only reason for C is - Gateway endpoints are not Billed and so cost effective (https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html#types-of-vpc-endpoints-for-s3) If the question changes from single region to across region, the answer would be B (overhead of NAT gateways and traversing TBs of data across NAT is expensive) because gateway endpoints are region specific<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B wouldn't be highly secure and data transfer would also be slower</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 824194,
          "date": "Mon 27 Feb 2023 23:03",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "B wouldn't be highly secure and data transfer would also be slower",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793012,
          "date": "Mon 30 Jan 2023 17:02",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C for sure",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774854,
          "date": "Fri 13 Jan 2023 22:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Setting up an S3 gateway VPC endpoint in the VPC and attaching an endpoint policy to the endpoint will allow the EC2 instances to securely access the S3 bucket for image storage without the need for NAT gateways, reducing costs without compromising security or increasing ongoing operations. This option reduces the costs associated with the NAT gateways and allows for faster data retrieval from the S3 bucket as traffic does not have to go through the internet gateway.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#41",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently deployed an application on AWS. The application uses Amazon DynamoDB.  The company measured the application load and configured the RCUs and WCUs on the DynamoDB table to match the expected peak load. The peak load occurs once a week for a 4-hour period and is double the average load. The application load is close to the average load for the rest of the week. The access pattern includes many more writes to the table than reads of the table.<br><br>A solutions architect needs to implement a solution to minimize the cost of the table.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#41",
          "answers": [
            {
              "choice": "<p>A. Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure on-demand capacity mode for the table.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure DynamoDB Accelerator (DAX) in front of the table. Reduce the provisioned read capacity to match the new peak load on the table.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure DynamoDB Accelerator (DAX) in front of the table. Configure on-demand capacity mode for the table.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776630,
          "date": "Sun 15 Jan 2023 14:51",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct. On demand mode is for unknown load pattern, auto scaling is for know burst pattern",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 855230,
          "date": "Thu 30 Mar 2023 05:01",
          "username": "\t\t\t\thgc2023\t\t\t",
          "content": "read and write units are more expensive in on demand mode so I don't think D is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 852997,
          "date": "Tue 28 Mar 2023 10:10",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use AWS Application Auto Scaling makes the most sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 844830,
          "date": "Mon 20 Mar 2023 12:48",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "A for me, not B.  On-demand is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when underprovisioned capacity would impact the user experience.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 840387,
          "date": "Wed 15 Mar 2023 23:45",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D - no doubts..<br>In addition to on-demand, DAX can reduce the Dynamodb cost up to 60%✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 835355,
          "date": "Fri 10 Mar 2023 20:43",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Will go with A in exam as peak load is known",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832805,
          "date": "Wed 08 Mar 2023 11:13",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "tuning dynamo db is not sufficient, you also need to scale the applicaiton to meet peak loads",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 830372,
          "date": "Sun 05 Mar 2023 23:13",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "Answer D makes sense. On-demand is the good option for infrequent access to dynamoDB. <br>A option requires a code refactoring",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 823827,
          "date": "Mon 27 Feb 2023 15:47",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "In this link https://aws.amazon.com/blogs/aws/amazon-dynamodb-on-demand-no-capacity-planning-and-pay-per-request-pricing/ I found this: \\\"DynamoDB on-demand is useful if your application traffic is difficult to predict and control, your workload has large spikes of short duration, or if your average table utilization is well below the peak.\\\" I think this is very close to what we are looking for so maybe B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Here, the traffic is predicted \\\"The peak load occurs once a week for a 4-hour period and is double the average load\\\". Hence, with AWS Autoscaling we can schedule the WCU scaling, which would be way cheaper than on-demand capacity.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 824798,
          "date": "Tue 28 Feb 2023 13:46",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "Here, the traffic is predicted \\\"The peak load occurs once a week for a 4-hour period and is double the average load\\\". Hence, with AWS Autoscaling we can schedule the WCU scaling, which would be way cheaper than on-demand capacity.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821062,
          "date": "Sat 25 Feb 2023 01:44",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "OnDemand when needed is good, but here, we know that only 4 hours is peak.<br>So better purchase the reserved RCUs/WCUs and on top of it enable auto scaling which<br>meets 4 hour high demand. DAX is a extreme performant cache cluster. <br>DAX is not ideal for write intensitive, that does not mean use DAX for reads. Look at where DAX does not fit -https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html<br>Here for reducing costs,A is correct. See here how provisoned reduce costs-<br>https://aws.amazon.com/dynamodb/pricing/?refid=ce6876ca-ceb9-46a2-adaa-d36fce8fbba7",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810227,
          "date": "Thu 16 Feb 2023 04:43",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "A.  Use AWS Application Auto Scaling to increase capacity during the peak period. Purchase reserved RCUs and WCUs to match the average load.<br><br>Since the peak period is only 4 hours a week and the application load is close to the average load for the rest of the week, it is not cost-effective to configure on-demand capacity mode for the table. Instead, AWS Application Auto Scaling can be used to increase the RCUs and WCUs during the peak period to meet the increased demand, and then decrease them to match the average load for the rest of the week. Additionally, reserved capacity can be purchased to match the average load, further reducing costs. Using DynamoDB Accelerator (DAX) in front of the table does not directly address the issue of cost optimization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793015,
          "date": "Mon 30 Jan 2023 17:05",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "has nothing with DAX here.<br>between A and B==> A is the answer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>DAX is useful for read-intensive loads.</li><li>This, DAX is not an option, on demand isn't either, leaves A</li><li>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 804774,
          "date": "Fri 10 Feb 2023 21:35",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "DAX is useful for read-intensive loads.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This, DAX is not an option, on demand isn't either, leaves A</li><li>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847166,
          "date": "Wed 22 Mar 2023 15:54",
          "username": "\t\t\t\tvvahe\t\t\t",
          "content": "This, DAX is not an option, on demand isn't either, leaves A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804775,
          "date": "Fri 10 Feb 2023 21:36",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791391,
          "date": "Sun 29 Jan 2023 05:42",
          "username": "\t\t\t\tpravi1\t\t\t",
          "content": "A makes sense here. On-demand more costly compared to reserved ones.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 786028,
          "date": "Tue 24 Jan 2023 02:19",
          "username": "\t\t\t\tDDONG\t\t\t",
          "content": "A SAPC01 #1005",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785162,
          "date": "Mon 23 Jan 2023 10:11",
          "username": "\t\t\t\tccort\t\t\t",
          "content": "A<br>on-demand prices can be 7 times higher, given the options it is better to have reserved WCU and RCU and auto scale in the given schedule",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774855,
          "date": "Fri 13 Jan 2023 22:09",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Configure on-demand capacity mode for the table. This solution will allow the table to automatically scale its capacity based on the actual usage, and will minimize the cost of the table as it will only pay for the capacity used during the peak load period, and not the entire week. Additionally, since the access pattern includes more writes than reads, on-demand capacity mode is a good fit as it is more cost-effective for write-heavy workloads.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D is a possible solution that could meet the requirements, as it leverages DynamoDB Accelerator (DAX) to improve the performance of read operations on the table and also configures on-demand capacity mode for the table which will minimize the cost as it only charges for the requests made to the table.<br><br>However, it's important to consider that DAX will add some costs to the solution, and it's not guaranteed that the on-demand capacity mode will be enough to handle the peak load, so it's important to monitor the table and make sure that the performance is meeting the expectations.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774856,
          "date": "Fri 13 Jan 2023 22:09",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D is a possible solution that could meet the requirements, as it leverages DynamoDB Accelerator (DAX) to improve the performance of read operations on the table and also configures on-demand capacity mode for the table which will minimize the cost as it only charges for the requests made to the table.<br><br>However, it's important to consider that DAX will add some costs to the solution, and it's not guaranteed that the on-demand capacity mode will be enough to handle the peak load, so it's important to monitor the table and make sure that the performance is meeting the expectations.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#42",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to advise a company on how to migrate its on-premises data processing application to the AWS Cloud. Currently, users upload input files through a web portal. The web server then stores the uploaded files on NAS and messages the processing server over a message queue. Each media file can take up to 1 hour to process. The company has determined that the number of media files awaiting processing is significantly higher during business hours, with the number of files rapidly declining after business hours.<br><br>What is the MOST cost-effective migration recommendation?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#42",
          "answers": [
            {
              "choice": "<p>A. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, create a new Amazon EC2 instance to pull requests from the queue and process the files. Store the processed files in Amazon EFS. Shut down the EC2 instance after the task is complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a queue using Amazon MQ. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in Amazon EFS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. Use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. Scale the EC2 instances based on the SQS queue length. Store the processed files in an Amazon S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852999,
          "date": "Tue 28 Mar 2023 10:12",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "SQS and Auto Scaling",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840395,
          "date": "Wed 15 Mar 2023 23:57",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D - makes sense.. Lambda can't run more than 15m.<br>And Amazon MQ is only recommended when migrating existing message brokers that rely on compatibility with APIs such as JMS or protocols such as AMQP, MQTT, OpenWire, and STOMP.. in the question there is no mention for these services ..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 821178,
          "date": "Sat 25 Feb 2023 05:11",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A and C are out because lambda does not support more than 15 min. B says, to create an EC2 for each new message which is certainly not cost effective and bad design as well. So answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810230,
          "date": "Thu 16 Feb 2023 04:52",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The most cost-effective migration recommendation to handle peak loads during business hours is to use Amazon SQS to create a queue, configure the existing web server to publish to the new queue, and use Amazon EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. The EC2 instances should be scaled based on the SQS queue length. Storing the processed files in an Amazon S3 bucket will help in reducing the storage cost. This approach is scalable and can handle peak loads during business hours, while still being cost-effective during non-business hours. Option A is also a possible solution, but using EC2 instances in an EC2 Auto Scaling group is a more scalable and cost-effective solution. Options B and C involve using Amazon EFS, which can be more expensive than Amazon S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 793019,
          "date": "Mon 30 Jan 2023 17:12",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "D is the right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 791753,
          "date": "Sun 29 Jan 2023 16:05",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Because A is not valid due to time",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 791394,
          "date": "Sun 29 Jan 2023 05:43",
          "username": "\t\t\t\tpravi1\t\t\t",
          "content": "D will be correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776912,
          "date": "Sun 15 Jan 2023 19:14",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer would be option D. <br><br>This option suggests creating a queue using Amazon SQS, configuring the existing web server to publish to the new queue, and using EC2 instances in an EC2 Auto Scaling group to pull requests from the queue and process the files. The EC2 instances can be scaled based on the SQS queue length, which ensures that the resources are available during peak usage times and reduces costs during non-peak times.<br><br>Option A is not correct because it suggests using AWS Lambda which has a maximum execution time of 15 minutes.<br>Option B is not correct because it suggests creating a new EC2 instance for each message in the queue, which is not cost-effective.<br>Option C is not correct because it suggests using Amazon EFS, which is not a suitable option for long-term storage of large files.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776636,
          "date": "Sun 15 Jan 2023 14:53",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct because it took 1 hour to process the file. Lambda only run 15 minutes",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774859,
          "date": "Fri 13 Jan 2023 22:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create a queue using Amazon SQS. Configure the existing web server to publish to the new queue. When there are messages in the queue, invoke an AWS Lambda function to pull requests from the queue and process the files. Store the processed files in an Amazon S3 bucket.<br>This approach will be the most cost-effective as it uses serverless AWS Lambda to process the files, which only incurs charges while the function is running, and is therefore well suited for workloads with variable and unpredictable usage patterns. Additionally, using Amazon S3 for storage is a cost-effective option as it allows for the storage of large amounts of data at a low cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Although this answer is the most cost-effective, AWS Lambda only allows functions to run up to 15 minutes.</li><li>correct ans is D</li><li>You cannot use Lambda function since the question mentioned \\\"process time take up to 1 hour for processing\\\" Aws Lambda functions can run only 15 minutes per function.</li><li>https://www.examtopics.com/discussions/amazon/view/36333-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>you are right, I was wrong despite the fact that I already knew this question. sorry</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776918,
          "date": "Sun 15 Jan 2023 19:23",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "Although this answer is the most cost-effective, AWS Lambda only allows functions to run up to 15 minutes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>correct ans is D</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776921,
          "date": "Sun 15 Jan 2023 19:27",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "correct ans is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776207,
          "date": "Sun 15 Jan 2023 07:37",
          "username": "\t\t\t\tandctygr\t\t\t",
          "content": "You cannot use Lambda function since the question mentioned \\\"process time take up to 1 hour for processing\\\" Aws Lambda functions can run only 15 minutes per function.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/36333-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>you are right, I was wrong despite the fact that I already knew this question. sorry</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776879,
          "date": "Sun 15 Jan 2023 18:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/36333-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>you are right, I was wrong despite the fact that I already knew this question. sorry",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#43",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using Amazon OpenSearch Service to analyze data. The company loads data into an OpenSearch Service cluster with 10 data nodes from an Amazon S3 bucket that uses S3 Standard storage. The data resides in the cluster for 1 month for read-only analysis. After 1 month, the company deletes the index that contains the data from the cluster. For compliance purposes, the company must retain a copy of all input data.<br><br>The company is concerned about ongoing costs and asks a solutions architect to recommend a new solution.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#43",
          "answers": [
            {
              "choice": "<p>A. Replace all the data nodes with UltraWarm nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Reduce the number of data nodes in the cluster to 2 Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Transition the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Reduce the number of data nodes in the cluster to 2. Add UltraWarm nodes to handle the expected capacity. Configure the indexes to transition to UltraWarm when OpenSearch Service ingests the data. Add cold storage nodes to the cluster Transition the indexes from UltraWarm to cold storage. Delete the input data from the S3 bucket after 1 month by using an S3 Lifecycle policy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Reduce the number of data nodes in the cluster to 2. Add instance-backed data nodes to handle the expected capacity. Transition the input data from S3 Standard to S3 Glacier Deep Archive when the company loads the data into the cluster.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 845307,
          "date": "Mon 20 Mar 2023 23:38",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "If you look at the IAM documentation here, you can see that the ec2:AuthorizeSecurityGroupIngress action doesn't have any conditions that would allow you to specify the ip addresses in the inbound/outbound rules.https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 840402,
          "date": "Thu 16 Mar 2023 00:14",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B - makes more sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 828687,
          "date": "Sat 04 Mar 2023 06:21",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "UltraWarm provides a cost-effective way to store large amounts of read-only data on Amazon OpenSearch Service. Standard data nodes use \\\"hot\\\" storage, which takes the form of instance stores or Amazon EBS volumes attached to each node. Hot storage provides the fastest possible performance for indexing and searching new data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804820,
          "date": "Fri 10 Feb 2023 23:09",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I asked ChatGPT. Can I use all UltraWarm nodes in AWS OpenSearch instead of data nodes? :)<br><br>No, UltraWarm nodes in AWS OpenSearch are designed for storage and retrieval of infrequently accessed data, while data nodes are optimized for faster indexing and searching of data. While UltraWarm nodes can be used as a complement to data nodes, they are not a replacement for them.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This eliminates option A</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821954,
          "date": "Sun 26 Feb 2023 01:40",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "This eliminates option A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791760,
          "date": "Sun 29 Jan 2023 16:12",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Option B is the most cost-effective solution that meets the requirements. Reducing the number of data nodes in the cluster and adding UltraWarm nodes will help to reduce the ongoing costs of running the OpenSearch Service cluster. Configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data will further reduce costs. Additionally, transitioning the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy will lower the storage costs of retaining the input data for compliance purposes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774862,
          "date": "Fri 13 Jan 2023 22:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is the most cost-effective solution as it reduces the number of data nodes in the cluster to 2 and adds UltraWarm nodes to handle the expected capacity. By configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data, the company can take advantage of the lower storage costs of UltraWarm. Additionally, by transitioning the input data to S3 Glacier Deep Archive after 1 month using an S3 Lifecycle policy, the company can further reduce costs by using the lower storage costs of S3 Glacier Deep Archive for long-term data retention.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C can meet the requirements of reducing the number of data nodes in the cluster and using UltraWarm and cold storage nodes to handle the expected capacity and moving the data to lower cost storage after 1 month. However, it may not be the most cost-effective solution as it involves additional complexity in configuring the indexes to transition between different storage tiers, and may also require additional management and maintenance of the cold storage nodes. Option B, where the data is transitioned from S3 Standard to S3 Glacier Deep Archive using an S3 Lifecycle policy is simpler and more cost-effective as it eliminates the need for additional storage tiers and management.</li><li>B says to delete but question asks for saving on compliance purposes.</li><li>* I meant C says..</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774863,
          "date": "Fri 13 Jan 2023 22:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C can meet the requirements of reducing the number of data nodes in the cluster and using UltraWarm and cold storage nodes to handle the expected capacity and moving the data to lower cost storage after 1 month. However, it may not be the most cost-effective solution as it involves additional complexity in configuring the indexes to transition between different storage tiers, and may also require additional management and maintenance of the cold storage nodes. Option B, where the data is transitioned from S3 Standard to S3 Glacier Deep Archive using an S3 Lifecycle policy is simpler and more cost-effective as it eliminates the need for additional storage tiers and management.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B says to delete but question asks for saving on compliance purposes.</li><li>* I meant C says..</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 821204,
          "date": "Sat 25 Feb 2023 06:10",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "B says to delete but question asks for saving on compliance purposes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>* I meant C says..</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821205,
          "date": "Sat 25 Feb 2023 06:10",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "* I meant C says..",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#44",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has 10 accounts that are part of an organization in AWS Organizations. AWS Config is configured in each account. All accounts belong to either the Prod OU or the NonProd OU.<br><br>The company has set up an Amazon EventBridge rule in each AWS account to notify an Amazon Simple Notification Service (Amazon SNS) topic when an Amazon EC2 security group inbound rule is created with 0.0.0.0/0 as the source. The company's security team is subscribed to the SNS topic.<br><br>For all accounts in the NonProd OU, the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source.<br><br>Which solution will meet this requirement with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#44",
          "answers": [
            {
              "choice": "<p>A. Modify the EventBridge rule to invoke an AWS Lambda function to remove the security group inbound rule and to publish to the SNS topic. Deploy the updated rule to the NonProd OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure an SCP to allow the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. Apply the SCP to the NonProd OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774865,
          "date": "Fri 13 Jan 2023 22:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The solution that meets this requirement with the LEAST operational overhead is D.  Configuring an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0, and applying the SCP to the NonProd OU. This solution would prevent the security group inbound rule from being created in the first place and will not require any additional steps or actions to be taken in order to remove the rule. This is less operationally intensive than modifying the EventBridge rule to invoke an AWS Lambda function, adding a Config rule or allowing the ec2:AuthorizeSecurityGroupIngress action with a specific IP.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C does not meet the requirement that the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source. It only allows the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. It does not prevent the creation of a security group inbound rule that includes 0.0.0.0/0 as the source, it only allows for the ingress action on non-0.0.0.0/0 IPs.<br>Option D is the best solution as it denies the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. This will prevent the creation of any security group inbound rule that includes 0.0.0.0/0 as the source.</li></ul>",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 774866,
          "date": "Fri 13 Jan 2023 22:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C does not meet the requirement that the security team needs to remove the ability to create a security group inbound rule that includes 0.0.0.0/0 as the source. It only allows the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is not 0.0.0.0/0. It does not prevent the creation of a security group inbound rule that includes 0.0.0.0/0 as the source, it only allows for the ingress action on non-0.0.0.0/0 IPs.<br>Option D is the best solution as it denies the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. This will prevent the creation of any security group inbound rule that includes 0.0.0.0/0 as the source.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 849321,
          "date": "Fri 24 Mar 2023 14:36",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "As Damijo said from the docs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 847484,
          "date": "Wed 22 Mar 2023 21:20",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "The \\\"aws:SourceIp\\\" is used for restrict access to AWS from user which have specific IP specified in aws:SourceIp.<br>This is not a condition for source ip in a SG<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>So it cannot be D for sure</li><li>C is not true for the same (and say allow...)<br>B is not true because it's partial, the rule will be flag as NON COMPLIANT but will not be delete without using System Manager automation document<br>A is true and does not add a lot of operational overhead because there is already an eventbridge rule for that</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847487,
          "date": "Wed 22 Mar 2023 21:22",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "So it cannot be D for sure<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C is not true for the same (and say allow...)<br>B is not true because it's partial, the rule will be flag as NON COMPLIANT but will not be delete without using System Manager automation document<br>A is true and does not add a lot of operational overhead because there is already an eventbridge rule for that</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847498,
          "date": "Wed 22 Mar 2023 21:31",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "C is not true for the same (and say allow...)<br>B is not true because it's partial, the rule will be flag as NON COMPLIANT but will not be delete without using System Manager automation document<br>A is true and does not add a lot of operational overhead because there is already an eventbridge rule for that",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 845310,
          "date": "Mon 20 Mar 2023 23:39",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "If you look at the IAM documentation here, you can see that the ec2:AuthorizeSecurityGroupIngress action doesn't have any conditions that would allow you to specify the ip addresses in the inbound/outbound rules.https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonec2.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 845103,
          "date": "Mon 20 Mar 2023 18:21",
          "username": "\t\t\t\tramyaram\t\t\t",
          "content": "D would be the best option to meet operational overhead requirement",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840638,
          "date": "Thu 16 Mar 2023 08:33",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840408,
          "date": "Thu 16 Mar 2023 00:28",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D is the LEAST operational overhead solution<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Changing my answer to A<br>Well.. after investigating I found out that it's not possible to prevent security changes with SCP</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840417,
          "date": "Thu 16 Mar 2023 00:51",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "Changing my answer to A<br>Well.. after investigating I found out that it's not possible to prevent security changes with SCP",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 838514,
          "date": "Tue 14 Mar 2023 03:52",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "C and D are out; for security groups you cannot do a deny, only allow so D is out and C is out because you cant do a \\\"is not\\\" since that is still a deny - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html<br>B.  out because that AWS Config managed rule is detective only, not proactive, go ahead and review the list for different evaluation modes: https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-evaluation-mode.html<br><br>This is a legit \\\"trick\\\" question, you have to modify the rule to invoke an AWS Lambda to always remove it. All the other stuff in this statement is to through you off - you must use EventBridge to create a rule.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833291,
          "date": "Wed 08 Mar 2023 20:02",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "D meets the requirements<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>later I found that SourceIP is the IP address of a requester. So D isn't correct !!!</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 837090,
          "date": "Sun 12 Mar 2023 15:05",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "later I found that SourceIP is the IP address of a requester. So D isn't correct !!!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 832813,
          "date": "Wed 08 Mar 2023 11:23",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "creating a lambda and removing it seems weird and definatly lot of operation overhead. will go with D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 825215,
          "date": "Tue 28 Feb 2023 20:42",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/how-to-automatically-revert-and-receive-notifications-about-changes-to-your-amazon-vpc-security-groups/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825208,
          "date": "Tue 28 Feb 2023 20:34",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "I don't understand people that says D, can you tell me what is the differences between C and D?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822938,
          "date": "Sun 26 Feb 2023 21:35",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "Answer should be B;<br>The solution that will meet the requirement with the LEAST operational overhead is option B: Add the vpc-sg-open-only-to-authorized-ports AWS Config managed rule to the NonProd OU.<br><br>This option is the least operational overhead because it utilizes an existing AWS Config managed rule, which means that there is no need to create or deploy any new resources or code. The vpc-sg-open-only-to-authorized-ports rule will automatically evaluate all security groups in the NonProd OU and report any that allow inbound traffic from 0.0.0.0/0. This rule will also allow security groups to be created or updated with any other source IP address.<br><br>Option A requires the creation and deployment of a Lambda function, which will require additional operational overhead. Option C requires the configuration of an SCP, which can be complex and may cause unintended consequences if not configured properly. Option D is similar to Option C but uses a deny policy instead of an allow policy, which can be more difficult to manage and troubleshoot.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Here is the link:<br>https://docs.aws.amazon.com/config/latest/developerguide/vpc-sg-open-only-to-authorized-ports.html</li><li>But does it act upon it or just marked as non-compliance?</li><li>Even if it acts upon it and deletes the rule it didn't stop developers to create the rule in the first place, hence doesn't meet the criteria</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822943,
          "date": "Sun 26 Feb 2023 21:39",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "Here is the link:<br>https://docs.aws.amazon.com/config/latest/developerguide/vpc-sg-open-only-to-authorized-ports.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But does it act upon it or just marked as non-compliance?</li><li>Even if it acts upon it and deletes the rule it didn't stop developers to create the rule in the first place, hence doesn't meet the criteria</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823843,
          "date": "Mon 27 Feb 2023 15:58",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "But does it act upon it or just marked as non-compliance?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Even if it acts upon it and deletes the rule it didn't stop developers to create the rule in the first place, hence doesn't meet the criteria</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 824209,
          "date": "Mon 27 Feb 2023 23:41",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "Even if it acts upon it and deletes the rule it didn't stop developers to create the rule in the first place, hence doesn't meet the criteria",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 821220,
          "date": "Sat 25 Feb 2023 06:47",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "D is correct. Refer SCP usage strategies- https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html<br>In AWS Organizations, FullAWSAccess SCP is by default added and applied to all OUs/member accounts.So, an allow is already there, so we just need to add a deny and apply to NonProd OU<br>For C to be answer, we need to do additional step of adding deny rule for all OUs and member accounts which is tedious and against least operational overhead. that is the whole reason FullAWSAccess isadded by default on AWS Organizations.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820535,
          "date": "Fri 24 Feb 2023 14:54",
          "username": "\t\t\t\tNidjo\t\t\t",
          "content": "Answer is A, the conditions aws:SourceIP don't exist for this API call.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820319,
          "date": "Fri 24 Feb 2023 11:00",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "option D as it has least operational overhead",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 816972,
          "date": "Tue 21 Feb 2023 19:11",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "D.  Configure an SCP to deny the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. Apply the SCP to the NonProd OU.<br><br>This solution leverages AWS Organizations' Service Control Policies (SCPs) to deny the ec2:AuthorizeSecurityGroupIngress action when the source IP is 0.0.0.0/0. This means that any attempt to create a security group inbound rule with that source IP will be blocked at the organizational level, without the need for any additional resources or configurations in individual accounts. This approach has the least operational overhead as it requires only the configuration of an SCP in the NonProd OU, which can be easily managed and updated.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#45",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts a Git repository in an on-premises data center. The company uses webhooks to invoke functionality that runs in the AWS Cloud. The company hosts the webhook logic on a set of Amazon EC2 instances in an Auto Scaling group that the company set as a target for an Application Load Balancer (ALB). The Git server calls the ALB for the configured webhooks. The company wants to move the solution to a serverless architecture.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#45",
          "answers": [
            {
              "choice": "<p>A. For each webhook, create and configure an AWS Lambda function URL. Update the Git servers to call the individual Lambda function URLs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the webhook logic to AWS App Runner. Create an ALB, and set App Runner as the target. Update the Git servers to call the ALB endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Containerize the webhook logic. Create an Amazon Elastic Container Service (Amazon ECS) cluster, and run the webhook logic in AWS Fargate. Create an Amazon API Gateway REST API, and set Fargate as the target. Update the Git servers to call the API Gateway endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774868,
          "date": "Fri 13 Jan 2023 22:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Create an Amazon API Gateway HTTP API. Implement each webhook logic in a separate AWS Lambda function. Update the Git servers to call the API Gateway endpoint. This solution will provide low operational overhead as it utilizes the serverless capabilities of AWS Lambda and API Gateway, which automatically scales and manages the underlying infrastructure and resources. It also allows for the webhook logic to be easily managed and updated through the API Gateway interface.<br><br>The answer should be B because it is the best solution in terms of operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A would require updating the Git servers to call individual Lambda function URLs for each webhook, which would be more complex and time-consuming than calling a single API Gateway endpoint. <br><br>Option C would require deploying the webhook logic to AWS App Runner, which would also be more complex and time-consuming than using an API Gateway. <br><br>Option D would also require containerizing the webhook logic and creating an ECS cluster and Fargate, which would also add complexity and operational overhead compared to using an API Gateway.</li><li>I do agree with B.  <br><br>However on Git server side it does make no difference if one calls aws or do a rest call via gateway. <br>Eg. if you use Python it makes no difference if you use boto(call Lambda) or request(rest api) module.<br>If one implemets via shell it makes no difference if one uses aws-cli(invoke Lambda directly) or curl(do a rest call).<br>Similar for other implementations.</li><li>As addition why B is still better: it hides the implementation details and decouples by introducing a interface.<br>With that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774869,
          "date": "Fri 13 Jan 2023 22:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A would require updating the Git servers to call individual Lambda function URLs for each webhook, which would be more complex and time-consuming than calling a single API Gateway endpoint. <br><br>Option C would require deploying the webhook logic to AWS App Runner, which would also be more complex and time-consuming than using an API Gateway. <br><br>Option D would also require containerizing the webhook logic and creating an ECS cluster and Fargate, which would also add complexity and operational overhead compared to using an API Gateway.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I do agree with B.  <br><br>However on Git server side it does make no difference if one calls aws or do a rest call via gateway. <br>Eg. if you use Python it makes no difference if you use boto(call Lambda) or request(rest api) module.<br>If one implemets via shell it makes no difference if one uses aws-cli(invoke Lambda directly) or curl(do a rest call).<br>Similar for other implementations.</li><li>As addition why B is still better: it hides the implementation details and decouples by introducing a interface.<br>With that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 821979,
          "date": "Sun 26 Feb 2023 02:43",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "I do agree with B.  <br><br>However on Git server side it does make no difference if one calls aws or do a rest call via gateway. <br>Eg. if you use Python it makes no difference if you use boto(call Lambda) or request(rest api) module.<br>If one implemets via shell it makes no difference if one uses aws-cli(invoke Lambda directly) or curl(do a rest call).<br>Similar for other implementations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As addition why B is still better: it hides the implementation details and decouples by introducing a interface.<br>With that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821991,
          "date": "Sun 26 Feb 2023 02:50",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "As addition why B is still better: it hides the implementation details and decouples by introducing a interface.<br>With that a team for Aws may change what ever it needs to change to implement the interface. On the other hand on git side can use whatever deems necessary without caring about implementation details.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 840428,
          "date": "Thu 16 Mar 2023 01:05",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B makes sense ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 804868,
          "date": "Sat 11 Feb 2023 00:43",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Here's what ChatGPT has to say.<br>In general, if you're looking for the option with the least operational overhead and you're comfortable with a fully managed, serverless environment, then AWS Lambda with API Gateway may be the better choice. However, if you require more control over your environment or need to use containers, then AWS App Runner with ALB may be the better option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 790702,
          "date": "Sat 28 Jan 2023 16:13",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://aws.amazon.com/solutions/implementations/git-to-s3-using-webhooks/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 780947,
          "date": "Thu 19 Jan 2023 10:15",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "Api Gateway with Lambda<br>https://medium.com/mindorks/building-webhook-is-easy-using-aws-lambda-and-api-gateway-56f5e5c3a596",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#46",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to migrate 1,000 on-premises servers to AWS. The servers run on several VMware clusters in the company's data center. As part of the migration plan, the company wants to gather server metrics such as CPU details, RAM usage, operating system information, and running processes. The company then wants to query and analyze the data.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#46",
          "answers": [
            {
              "choice": "<p>A. Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 779246,
          "date": "Tue 17 Jan 2023 20:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D: Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3.<br><br>Here is why the other choices are not correct:<br>A.  Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select. - AWS Agentless Discovery Connector will help in discovering and inventory servers but it does not provide the same level of detailed metrics as the AWS Application Discovery Agent, it also does not cover process information.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B.  Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight. - It does not cover process information and it's not the best way to collect the required data, it's not efficient and it might miss some important information.<br>C.  Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console. - this solution might not be very reliable and it does not cover process information, also it does not provide a way to query and analyze the data.</li><li>D.  Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3. - This is the correct answer as it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena.</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779247,
          "date": "Tue 17 Jan 2023 20:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Export only the VM performance information from the on-premises hosts. Directly import the required data into AWS Migration Hub. Update any missing information in Migration Hub. Query the data by using Amazon QuickSight. - It does not cover process information and it's not the best way to collect the required data, it's not efficient and it might miss some important information.<br>C.  Create a script to automatically gather the server information from the on-premises hosts. Use the AWS CLI to run the put-resource-attributes command to store the detailed server data in AWS Migration Hub. Query the data directly in the Migration Hub console. - this solution might not be very reliable and it does not cover process information, also it does not provide a way to query and analyze the data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D.  Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3. - This is the correct answer as it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779248,
          "date": "Tue 17 Jan 2023 20:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D.  Deploy the AWS Application Discovery Agent to each on-premises server. Configure Data Exploration in AWS Migration Hub. Use Amazon Athena to run predefined queries against the data in Amazon S3. - This is the correct answer as it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 853131,
          "date": "Tue 28 Mar 2023 12:58",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D is the answer because agentless cant grab everything",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840437,
          "date": "Thu 16 Mar 2023 01:26",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A is wrong.. because Agentless can't collect processes .. only CPU/RAM and disk IO",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 828871,
          "date": "Sat 04 Mar 2023 12:51",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "If you have virtual machines (VMs) that are running in the VMware vCenter environment, you can use the Agentless Collector to collect system information without having to install an agent on each VM. Instead, you load this on-premises appliance into vCenter and allow it to discover all of its hosts and VMs.<br><br>Agentless Collector captures system performance information and resource utilization for each VM running in the vCenter, regardless of what operating system is in use. However, it cannot “look inside” each of the VMs, and as such, cannot figure out what processes are running on each VM nor what network connections exist.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Going with D; Agentless discovery Connector does not gather process information; \\\"THE\\\" on premises HOSTs(physical servers?) will be running on esxi server.<br>You can deploy Discovery agent on Server(VM) . I might be overthinking it.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828891,
          "date": "Sat 04 Mar 2023 13:04",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Going with D; Agentless discovery Connector does not gather process information; \\\"THE\\\" on premises HOSTs(physical servers?) will be running on esxi server.<br>You can deploy Discovery agent on Server(VM) . I might be overthinking it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824823,
          "date": "Tue 28 Feb 2023 14:13",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "With the agentless collector you cannot get running processes on the VMs, and you cannot export the data to CSV or to Athena for further querying",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 821596,
          "date": "Sat 25 Feb 2023 16:36",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Even though question does not ask for least operational effort, performance, HA etc, the solution needs to be thinking those in mind. deploying on each server is not practically good solution. So D cannot be answer. Instead, an appliance which does this discovery job is good which is right there in A.  Moreover A is exclusively for VMWare use case. I choose A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819077,
          "date": "Thu 23 Feb 2023 11:40",
          "username": "\t\t\t\tmonkeyfish\t\t\t",
          "content": "Answer is A. <br>The AWS Agentless Discovery Connector is used when performing migration of servers in vmware clusters. S3 Select can be used to query.<br>AWS SA's would only recommend installing the agent on each on-prem server for physical hosts, not vmware server.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>S3 Select supports querying one file at a time. With Amazon Athena, you can perform SQL against any number of objects, or even entire bucket paths.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819902,
          "date": "Fri 24 Feb 2023 00:45",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "S3 Select supports querying one file at a time. With Amazon Athena, you can perform SQL against any number of objects, or even entire bucket paths.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791396,
          "date": "Sun 29 Jan 2023 05:46",
          "username": "\t\t\t\tpravi1\t\t\t",
          "content": "D will be correct in my opinion.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 779995,
          "date": "Wed 18 Jan 2023 13:44",
          "username": "\t\t\t\ticassp\t\t\t",
          "content": "Choosing between A and D.  For A, how can S3 select query?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think A is a better solution because the Agentless discovery connector is custom-made for the VMware environment. It will save us time and collect all the necessary data we need. Installing a Discovery agent in every server would be very time-consuming. S3 select allows simple select operations against your raw data. I don't think we need athena for</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 798538,
          "date": "Sun 05 Feb 2023 02:24",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "I think A is a better solution because the Agentless discovery connector is custom-made for the VMware environment. It will save us time and collect all the necessary data we need. Installing a Discovery agent in every server would be very time-consuming. S3 select allows simple select operations against your raw data. I don't think we need athena for",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778768,
          "date": "Tue 17 Jan 2023 11:00",
          "username": "\t\t\t\tsilkroad78\t\t\t",
          "content": "D Since Agentless Collector can't collect process https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are correct, AWS Agentless Discovery does not collect information about processes running on the servers. It primarily focuses on gathering information about the server's hardware, operating system, and network configuration. It is mainly used to discover and inventory servers, but it doesn't provide the same level of detailed metrics as the AWS Application Discovery Agent. The AWS Application Discovery Agent is the best option if the company wants to gather information about running processes on the servers, as it can provide more detailed metrics than Agentless Discovery.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 779244,
          "date": "Tue 17 Jan 2023 20:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "You are correct, AWS Agentless Discovery does not collect information about processes running on the servers. It primarily focuses on gathering information about the server's hardware, operating system, and network configuration. It is mainly used to discover and inventory servers, but it doesn't provide the same level of detailed metrics as the AWS Application Discovery Agent. The AWS Application Discovery Agent is the best option if the company wants to gather information about running processes on the servers, as it can provide more detailed metrics than Agentless Discovery.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774871,
          "date": "Fri 13 Jan 2023 22:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is A.  Deploy and configure the AWS Agentless Discovery Connector virtual appliance on the on-premises hosts. Configure Data Exploration in AWS Migration Hub. Use AWS Glue to perform an ETL job against the data. Query the data by using Amazon S3 Select.<br><br>This solution allows the company to gather detailed server metrics from the on-premises hosts by deploying the Agentless Discovery Connector virtual appliance. The data can then be imported into AWS Migration Hub for further analysis. The company can then use AWS Glue to perform an ETL job on the data and query it using Amazon S3 Select for further analysis.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#47",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a serverless application that runs on an AWS Lambda function that is attached to a VPC.  The company needs to integrate the application with a new service from an external provider. The external provider supports only requests that come from public IPv4 addresses that are in an allow list.<br><br>The company must provide a single public IP address to the external provider before the application can start using the new service.<br><br>Which solution will give the application the ability to access the new service?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#47",
          "answers": [
            {
              "choice": "<p>A. Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an egress-only internet gateway. Associate an Elastic IP address with the egress-only internet gateway. Configure the elastic network interface on the Lambda function to use the egress-only internet gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the Lambda function to use the internet gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an internet gateway. Associate an Elastic IP address with the internet gateway. Configure the default route in the public VPC route table to use the internet gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774872,
          "date": "Fri 13 Jan 2023 22:26",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway. Configure the VPC to use the NAT gateway.<br><br>This solution will give the Lambda function access to the internet by routing its outbound traffic through the NAT gateway, which has a public Elastic IP address. This will allow the external provider to whitelist the single public IP address associated with the NAT gateway, and enable the application to access the new service.",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 853133,
          "date": "Tue 28 Mar 2023 13:00",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Deploy a NAT gateway. Associate an Elastic IP address with the NAT gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 847201,
          "date": "Wed 22 Mar 2023 16:25",
          "username": "\t\t\t\tvvahe\t\t\t",
          "content": "A<br><br>https://docs.aws.amazon.com/lambda/latest/operatorguide/networking-vpc.html<br><br>\\\"By default, Lambda functions have access to the public internet. This is not the case after they have been configured with access to one of your VPCs. If you continue to need access to resources on the internet, set up a NAT instance or Amazon NAT Gateway. Alternatively, you can also use VPC endpoints to enable private communications between your VPC and supported AWS services.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 840441,
          "date": "Thu 16 Mar 2023 01:34",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "NAT gateway is needed✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833642,
          "date": "Thu 09 Mar 2023 07:22",
          "username": "\t\t\t\tmacc183\t\t\t",
          "content": "why D is incorrect? I guess IGW also has public IP address?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>IGW cannot have an EIP</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834670,
          "date": "Fri 10 Mar 2023 06:56",
          "username": "\t\t\t\tdoto\t\t\t",
          "content": "IGW cannot have an EIP",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828896,
          "date": "Sat 04 Mar 2023 13:08",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Easy \\\"A\\\".<br>B is wrong; Egress is a VPC component that allows outbound communication over IPv6 . C and D are wrong",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810242,
          "date": "Thu 16 Feb 2023 05:13",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "A.  Deploying a NAT gateway is the best solution for this scenario. Since the external provider supports only public IPv4 addresses, the Lambda function can be configured with a private IP address in the VPC.  A NAT gateway is used to provide a public IP address to the Lambda function when it accesses the external provider's service. This allows the Lambda function to access the new service while also securing it within the VPC",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793162,
          "date": "Mon 30 Jan 2023 19:18",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "option A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 788998,
          "date": "Thu 26 Jan 2023 19:32",
          "username": "\t\t\t\tMasterP007\t\t\t",
          "content": "Option -B is incorrect, cause that's more for IPv6 use-case.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#48",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect has developed a web application that uses an Amazon API Gateway Regional endpoint and an AWS Lambda function. The consumers of the web application are all close to the AWS Region where the application will be deployed. The Lambda function only queries an Amazon Aurora MySQL database. The solutions architect has configured the database to have three read replicas.<br><br>During testing, the application does not meet performance requirements. Under high load, the application opens a large number of database connections. The solutions architect must improve the application's performance.<br><br>Which actions should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#48",
          "answers": [
            {
              "choice": "<p>A. Use the cluster endpoint of the Aurora database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the Lambda Provisioned Concurrency feature.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Move the code for opening the database connection in the Lambda function outside of the event handler.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Change the API Gateway endpoint to an edge-optimized endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 774876,
          "date": "Fri 13 Jan 2023 22:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B and D. <br>B.  Using RDS Proxy to set up a connection pool to the reader endpoint of the Aurora database can help improve the performance of the application by reducing the number of connections opened to the database. RDS Proxy manages the connection pool and routes incoming connections to the available read replicas, which can help with connection management and reduce the number of connections that need to be opened and closed.<br>D.  Moving the code for opening the database connection in the Lambda function outside of the event handler can help to improve the performance of the application by allowing the database connection to be reused across multiple requests. This avoids the need to open and close a new connection for each request, which can be time-consuming and resource-intensive.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Using the cluster endpoint of the Aurora database instead of the reader endpoint would not help improve performance in this case, because the solution architect is already using read replicas to offload read traffic from the primary instance.<br>C.  Using the Lambda Provisioned Concurrency feature would not help improve performance in this case, as the problem is related to the number of connections to the database, not the number of instances running the Lambda function.<br>E.  Changing the API Gateway endpoint to an edge-optimized endpoint would not help improve performance in this case, as the problem is related to the number of connections to the database, not the location of the API Gateway endpoint.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 774877,
          "date": "Fri 13 Jan 2023 22:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Using the cluster endpoint of the Aurora database instead of the reader endpoint would not help improve performance in this case, because the solution architect is already using read replicas to offload read traffic from the primary instance.<br>C.  Using the Lambda Provisioned Concurrency feature would not help improve performance in this case, as the problem is related to the number of connections to the database, not the number of instances running the Lambda function.<br>E.  Changing the API Gateway endpoint to an edge-optimized endpoint would not help improve performance in this case, as the problem is related to the number of connections to the database, not the location of the API Gateway endpoint.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 853138,
          "date": "Tue 28 Mar 2023 13:02",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "RDS proxy + Lambda function",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 840446,
          "date": "Thu 16 Mar 2023 01:51",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "RDX proxy & connecting outside the handler method is up to 5 times faster than connecting inside.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 832831,
          "date": "Wed 08 Mar 2023 11:43",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "he Lambda function only queries an Amazon Aurora MySQL database- so i would reject option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 821649,
          "date": "Sat 25 Feb 2023 17:35",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "This may be too logical answer :-) - Setting up RDS proxy will help connection pooling, So B is one answer. Now C vs D<br>This question focuses on serverless solutions and best practices of lambda. and question hints that lambda only contains simple code.so lambda concurrency improvements may not be be the cause for performance issues detected while testing, and guess what - app is still in testing phase. so code might have a flaw can be reviewed and changed as per lambda best practices - https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html. I choose B and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804873,
          "date": "Sat 11 Feb 2023 01:10",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT,<br>By reusing the same database connection across multiple invocations of the function, you can reduce the number of database connections that are opened and closed, which can help conserve resources and reduce the risk of running into database connection limits.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 798563,
          "date": "Sun 05 Feb 2023 03:29",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "BD<br>https://awstut.com/en/2022/04/30/connect-to-rds-outside-of-lambda-handler-method-to-improve-performance-en/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782532,
          "date": "Fri 20 Jan 2023 17:57",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "B/C<br>lambda provisioned concurrency and RDS proxy are mentioned in same page.<br>https://quintagroup.com/blog/aws-lambda-provisioned-concurrency",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781929,
          "date": "Fri 20 Jan 2023 07:34",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html<br>https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 779429,
          "date": "Wed 18 Jan 2023 01:39",
          "username": "\t\t\t\tjhonivy\t\t\t",
          "content": "B/C<br>Provisioned Concurrency needed: https://www.reddit.com/r/aws/comments/gcwtqt/lambda_provisioned_concurrency_with_aurora/<br>With connection Pool, no to worry D",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#49",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to host a web application on AWS and wants to load balance the traffic across a group of Amazon EC2 instances. One of the security requirements is to enable end-to-end encryption in transit between the client and the web server.<br><br>Which solution will meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#49",
          "answers": [
            {
              "choice": "<p>A. Place the EC2 instances behind an Application Load Balancer (ALB). Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB.  Export the SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Associate the EC2 instances with a target group. Provision an SSL certificate using AWS Certificate Manager (ACM). Create an Amazon CloudFront distribution and configure it to use the SSL certificate. Set CloudFront to use the target group as the origin server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Place the EC2 instances behind an Application Load Balancer (ALB) Provision an SSL certificate using AWS Certificate Manager (ACM), and associate the SSL certificate with the ALB.  Provision a third-party SSL certificate and install it on each EC2 instance. Configure the ALB to listen on port 443 and to forward traffic to port 443 on the instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Place the EC2 instances behind a Network Load Balancer (NLB). Provision a third-party SSL certificate and install it on the NLB and on each EC2 instance. Configure the NLB to listen on port 443 and to forward traffic to port 443 on the instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 781927,
          "date": "Fri 20 Jan 2023 07:32",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Vote D. <br>If you need to pass encrypted traffic to targets without the load balancer decrypting it, you can create a Network Load Balancer or Classic Load Balancer with a TCP listener on port 443.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can use NLB with ACM cert on it. NLB can do TLS termination (https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/) and re-encrypt to target</li><li>how can this be true? Option D says to install on NLB. <br>You say bypass the NLB.  If you bypass the NLB why are you installing the cert?</li><li>coorect. but they want to upload the the certificate to the NLB for unknown reasons.</li></ul>",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 847832,
          "date": "Thu 23 Mar 2023 06:32",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "You can use NLB with ACM cert on it. NLB can do TLS termination (https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/) and re-encrypt to target",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823521,
          "date": "Mon 27 Feb 2023 11:20",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "how can this be true? Option D says to install on NLB. <br>You say bypass the NLB.  If you bypass the NLB why are you installing the cert?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 822037,
          "date": "Sun 26 Feb 2023 04:07",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "coorect. but they want to upload the the certificate to the NLB for unknown reasons.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785469,
          "date": "Mon 23 Jan 2023 15:43",
          "username": "\t\t\t\tpitakk\t\t\t",
          "content": "Amazon-issued public certificates can't be installed on an EC2 instance. To enable end-to-end encryption, you must use a third-party SSL certificate. https://aws.amazon.com/premiumsupport/knowledge-center/acm-ssl-certificate-ec2-elb/ so it's C or D.  I choose C as it's ALB<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>correct, but then you would use that ordered certificate for the alb as well. The other reason to order certificates is because some clients cannot verify ACM certificates which is not acceptable for a productive public service.<br><br>Between ALB and EC2 a self signed certificate is sufficient as alb does no verification of the EC2's certificate at all.</li></ul>",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822036,
          "date": "Sun 26 Feb 2023 04:05",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "correct, but then you would use that ordered certificate for the alb as well. The other reason to order certificates is because some clients cannot verify ACM certificates which is not acceptable for a productive public service.<br><br>Between ALB and EC2 a self signed certificate is sufficient as alb does no verification of the EC2's certificate at all.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 854901,
          "date": "Wed 29 Mar 2023 22:05",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "https://repost.aws/knowledge-center/acm-ssl-certificate-ec2-elb",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 853183,
          "date": "Tue 28 Mar 2023 13:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is my vote",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 841336,
          "date": "Thu 16 Mar 2023 21:31",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C is the best solution and it does actually work (you can google it)<br>Answer D is wrong .. why you would importthe certificate on the NLB stage if it's end to end? The host (ec2) should handle the certificate..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839670,
          "date": "Wed 15 Mar 2023 09:23",
          "username": "\t\t\t\taqiao\t\t\t",
          "content": "Amazon-issued public certificates can't be installed on an EC2 instance. To enable end-to-end encryption, you must use a third-party SSL certificate<br>https://aws.amazon.com/premiumsupport/knowledge-center/acm-ssl-certificate-ec2-elb/?nc1=h_ls",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 838529,
          "date": "Tue 14 Mar 2023 04:40",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html<br>Says to configure NLB to 'listen' and forward for end to end<br><br>under ALB it points you to NLB \\\" If you must ensure that the targets decrypt HTTPS traffic instead of the load balancer, you can create a Network Load Balancer with a TCP listener on port 443.\\\" https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 835775,
          "date": "Sat 11 Mar 2023 10:07",
          "username": "\t\t\t\tcherep87\t\t\t",
          "content": "Vote for D<br>option C will unencrypt the traffic on ALB, and goes against end-to-end encryption from server to client",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 833659,
          "date": "Thu 09 Mar 2023 07:43",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "C is correct.<br>AppMesh can be used here?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832835,
          "date": "Wed 08 Mar 2023 11:47",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "AWS Certificate Manager (ACM) SSL certificates cannot be directly applied to EC2 instances . - I will go with C on this one",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832165,
          "date": "Tue 07 Mar 2023 18:37",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "Not idea why AWS says that these solutions guarantee \\\"complete end-to-end encryption in transit\\\"https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/<br><br>\\\"After choosing the certificate and the policy, I click Next:Configure Routing. I can choose the communication protocol (TCP or TLS) that will be used between my NLB and my targets. If I choose TLS, communication is encrypted; this allows you to make use of complete end-to-end encryption in transit:\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 828915,
          "date": "Sat 04 Mar 2023 13:32",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/use_cases/nlb_tls_termination/#:~:text=AWS%20introduced%20TLS%20termination%20for,access%20to%20the%20private%20key.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"After choosing the certificate and the policy, I click Next:Configure Routing. I can choose the communication protocol (TCP or TLS) that will be used between my NLB and my targets. If I choose TLS, communication is encrypted; this allows you to make use of complete end-to-end encryption in transit:\\\"<br><br>I did not know TLS termination was possible with NLB's <br>https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 828921,
          "date": "Sat 04 Mar 2023 13:38",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "\\\"After choosing the certificate and the policy, I click Next:Configure Routing. I can choose the communication protocol (TCP or TLS) that will be used between my NLB and my targets. If I choose TLS, communication is encrypted; this allows you to make use of complete end-to-end encryption in transit:\\\"<br><br>I did not know TLS termination was possible with NLB's <br>https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822028,
          "date": "Sun 26 Feb 2023 03:56",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Retry neclecting end to end:<br>It is imo not possible to export ACM keys. Therefore I think one cannot install an ACM certificate on EC2.<br>This excludes A. <br>Now we have three technically possible solutions.<br>B) has no encryption at all between cloudfront and ec2.<br>C) ordera certificate from a third party to not deliver it to the client.<br>D) NLB certificate support is limited, can't do strong encryption.<br><br>From that it D and C are slightly better than B as they provide encryption between server and ALB/NLB - even though its not end to end.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You cannot export ACM Keys? Who says that? You can if you are in the same account and region.<br>\\\"You can't export an ACM certificate from one AWS Region to another or from one AWS account to another. This is because the default AWS Key Management Service (AWS KMS) key used to encrypt the private key of the certificate is unique for each AWS Region and AWS account.\\\"</li><li>\\\"You cannot use ACM to install a public certificate directly on your AWS based website or application. You must use one of the services integrated with ACM\\\"<br>In our case, we want to install the certificate on the EC2, which is not possible when it is stored in ACM.<br>https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-install.html<br>It is only possible in ACM PCA. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823524,
          "date": "Mon 27 Feb 2023 11:24",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "You cannot export ACM Keys? Who says that? You can if you are in the same account and region.<br>\\\"You can't export an ACM certificate from one AWS Region to another or from one AWS account to another. This is because the default AWS Key Management Service (AWS KMS) key used to encrypt the private key of the certificate is unique for each AWS Region and AWS account.\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"You cannot use ACM to install a public certificate directly on your AWS based website or application. You must use one of the services integrated with ACM\\\"<br>In our case, we want to install the certificate on the EC2, which is not possible when it is stored in ACM.<br>https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-install.html<br>It is only possible in ACM PCA. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824842,
          "date": "Tue 28 Feb 2023 14:28",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "\\\"You cannot use ACM to install a public certificate directly on your AWS based website or application. You must use one of the services integrated with ACM\\\"<br>In our case, we want to install the certificate on the EC2, which is not possible when it is stored in ACM.<br>https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-install.html<br>It is only possible in ACM PCA. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822006,
          "date": "Sun 26 Feb 2023 03:23",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "The key in this question is *end to end* encryption between client and server. That means we are not allowed to offload encryption to a load balancer but instead need the load balancer to pass the encrypted traffic as is to the server. As we may not interfere in the encrypted traffic all benefits of an application load balancer are void. An NLB is the best choice.<br><br>As a side note: C is ridiculous: order a certificate to not deliver it to clients? If one offloads one would use the same certificate for server and alb or use the ordered certificate on the alb and create a cheaper one for internal encryption between server and alb. You want the ordered certificate delivered to the clients.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Actually, I was not able to read correctly.D also violates end to end encryption. (C is still ridiculous.)</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 822016,
          "date": "Sun 26 Feb 2023 03:37",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Actually, I was not able to read correctly.D also violates end to end encryption. (C is still ridiculous.)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815563,
          "date": "Mon 20 Feb 2023 17:56",
          "username": "\t\t\t\tjaysparky\t\t\t",
          "content": "Both C and D are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812520,
          "date": "Sat 18 Feb 2023 01:48",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C 100% - ACM for ALb and Third Party SSL certificate for EC2",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 812161,
          "date": "Fri 17 Feb 2023 18:22",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "the prob that i can see with C.  is that [ do not guarantee ] end-to-end. you need to off load ssl in the ALB and then re_encrypt again, meaning that at some point (inside the ALB) you have the data in plain text.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#50",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its data analytics environment from on premises to AWS. The environment consists of two simple Node.js applications. One of the applications collects sensor data and loads it into a MySQL database. The other application aggregates the data into reports. When the aggregation jobs run, some of the load jobs fail to run correctly.<br><br>The company must resolve the data loading issue. The company also needs the migration to occur without interruptions or changes for the company's customers.<br><br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#50",
          "answers": [
            {
              "choice": "<p>A. Set up an Amazon Aurora MySQL database as a replication target for the on-premises database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind a Network Load Balancer (NLB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Move the aggregation jobs to run against the Aurora MySQL database. Set up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group. When the databases are synced, point the collector DNS record to the ALDisable the AWS DMS sync task after the cutover from on premises to AWS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB.  Disable the AWS DMS sync task after the cutover from on premises to AWS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an Amazon Aurora MySQL database. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as an Amazon Kinesis data stream. Use Amazon Kinesis Data Firehose to replicate the data to the Aurora MySQL database. When the databases are synced, disable the replication job and restart the Aurora Replica as the primary instance. Point the collector DNS record to the Kinesis data stream.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 853152,
          "date": "Tue 28 Mar 2023 13:12",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ill go with C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 841345,
          "date": "Thu 16 Mar 2023 21:47",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C. . even though question didn't mention the total time of each job. If the job takes more than 15m then Lambda can't be used. Probably the solution with ASG and EC2 is better .. not sure!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 838541,
          "date": "Tue 14 Mar 2023 05:02",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "ALB because you are pointing to to Lambda function, not a network address<br><br>Look at AWS DMS feature https://aws.amazon.com/dms/features/ <br><br>Main requirement - needs the migration to occur w/out interruptions or changes to the company's customers.<br><br>C keeps it stupid simple w/ no service interruption",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 833666,
          "date": "Thu 09 Mar 2023 07:57",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "Could anybody explain why ALB? I'd go with API Gateway<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Application - you are using Lambda functions that will be sending api commands, you would use network when it is just about routing</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 838542,
          "date": "Tue 14 Mar 2023 05:04",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Application - you are using Lambda functions that will be sending api commands, you would use network when it is just about routing",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824162,
          "date": "Mon 27 Feb 2023 22:11",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I would say C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822413,
          "date": "Sun 26 Feb 2023 14:30",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "I have a feeling that none of the approaches will work.<br>a) We have two sources that change the database: migration and new data coming in. In a relational database this results in inconsistent data. Constraints will not be fulfilled. <br>b) until the database is fully synced the second database has inconsistent data. Some parts of relations and parts of entities are still missing. Constraints will not be fulfilled. <br>None if the approaches addresses that aggregation tasks fail because of inconsistency of the data base.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ACID principle:atomicity, consistency, isolation and durability.All solutions violate this basic principle of relational databases.<br>https://en.wikipedia.org/wiki/ACID</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822475,
          "date": "Sun 26 Feb 2023 14:50",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "ACID principle:atomicity, consistency, isolation and durability.All solutions violate this basic principle of relational databases.<br>https://en.wikipedia.org/wiki/ACID",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822103,
          "date": "Sun 26 Feb 2023 07:24",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Issue could be because of same db used for writing and reading heavily. solution to separate this into<br>read replica only for reading. DMS for data migration to aws from onpremises.Writing app to DB and Reading app from DB for reports. Writing app needs RDSProxy and saves data.Reading app reads from replica.<br>B is wrong because, Reading job (aggregation) needs to use replicawhich is mentioned in C.  C is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812586,
          "date": "Sat 18 Feb 2023 04:00",
          "username": "\t\t\t\tFatoch\t\t\t",
          "content": "is it C or B?<br>Same person answers two times two different answers",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793190,
          "date": "Mon 30 Jan 2023 19:41",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is corect",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779255,
          "date": "Tue 17 Jan 2023 20:22",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  <br>This option would meet the requirements of resolving the data loading issue and migrating without interruption or changes for the company's customers. By using AWS DMS for continuous data replication, the company can ensure that the data being migrated is up to date. By setting up an Aurora Replica and moving the aggregation jobs to run against it, the company can offload some of the read workload from the primary database and reduce the risk of issues with the load jobs. By using AWS Lambda functions behind an ALB and Amazon RDS Proxy to write to the Aurora MySQL database, the company can add an extra layer of security and scalability to the data collection process. Finally, by pointing the collector DNS record to the ALB after the databases are synced and disabling the AWS DMS sync task, the company can ensure a smooth cutover to the new environment.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  <br>This option would not work as it would require to change the primary database and also it may cause interruption for the company's customers during the cutover process.<br>B. <br>This option would not work as it would not include Aurora Replica to offload the read workload, this would result in aggregation jobs running on the primary database which can cause the load jobs to fail during heavy loads.<br>D. <br>This option would not work as it would require to use kinesis data stream which may cause performance issues and also it may not be the best fit for this use case. Additionally, using Kinesis Data Firehose would add complexity to the data replication process, and may result in increased latency or data loss.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779256,
          "date": "Tue 17 Jan 2023 20:22",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  <br>This option would not work as it would require to change the primary database and also it may cause interruption for the company's customers during the cutover process.<br>B. <br>This option would not work as it would not include Aurora Replica to offload the read workload, this would result in aggregation jobs running on the primary database which can cause the load jobs to fail during heavy loads.<br>D. <br>This option would not work as it would require to use kinesis data stream which may cause performance issues and also it may not be the best fit for this use case. Additionally, using Kinesis Data Firehose would add complexity to the data replication process, and may result in increased latency or data loss.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776683,
          "date": "Sun 15 Jan 2023 15:48",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct. need more read replica for aggregation jobs to read data",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775238,
          "date": "Sat 14 Jan 2023 11:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Setting up an Amazon Aurora MySQL database and using AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora will ensure that data is continuously replicated to the new environment with minimal interruption. Moving the aggregation jobs to run against the Aurora MySQL database will ensure that the data is being read from the same database that is being loaded, which will resolve the data loading issue. Setting up collection endpoints behind an Application Load Balancer (ALB) as Amazon EC2 instances in an Auto Scaling group, and disabling the AWS DMS sync task after the cutover from on-premises to AWS, will ensure that the migration occurs without interruptions or changes for the company's customers.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer A is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario, and disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.<br><br>Answer C is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario.<br><br>Answer D is incorrect because it's not necessary to use Amazon Kinesis data stream and Firehose to replicate the data when AWS DMS can be used to perform continuous data replication. Also, disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.</li><li>Dude can u pls stop copy-pasting from chatgpt I am so sick of it. It is not a reliable source. Just stop it for the god sake.</li><li>hhhhhhhhhh.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775239,
          "date": "Sat 14 Jan 2023 11:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Answer A is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario, and disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.<br><br>Answer C is incorrect because it's not necessary to set up an Aurora Replica for the Aurora MySQL database, doing this will introduce additional complexity and cost. Using Amazon RDS Proxy is not necessary for this scenario.<br><br>Answer D is incorrect because it's not necessary to use Amazon Kinesis data stream and Firehose to replicate the data when AWS DMS can be used to perform continuous data replication. Also, disabling the replication job and restarting the Aurora Replica as the primary instance will cause an interruption to the service.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Dude can u pls stop copy-pasting from chatgpt I am so sick of it. It is not a reliable source. Just stop it for the god sake.</li><li>hhhhhhhhhh.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788137,
          "date": "Wed 25 Jan 2023 22:11",
          "username": "\t\t\t\tandctygr\t\t\t",
          "content": "Dude can u pls stop copy-pasting from chatgpt I am so sick of it. It is not a reliable source. Just stop it for the god sake.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>hhhhhhhhhh.</li></ul>",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 794060,
          "date": "Tue 31 Jan 2023 12:19",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "hhhhhhhhhh.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#51",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A health insurance company stores personally identifiable information (PII) in an Amazon S3 bucket. The company uses server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the objects. According to a new requirement, all current and future objects in the S3 bucket must be encrypted by keys that the company's security team manages. The S3 bucket does not have versioning enabled.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#51",
          "answers": [
            {
              "choice": "<p>A. In the S3 bucket properties, change the default encryption to SSE-S3 with a customer managed key. Use the AWS CLI to re-upload all objects in the S3 bucket. Set an S3 bucket policy to deny unencrypted PutObject requests.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to automatically encrypt objects on GetObject and PutObject requests.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. Attach a policy to deny unencrypted PutObject requests to any entities that access the S3 bucket. Use the AWS CLI to re-upload all objects in the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775242,
          "date": "Sat 14 Jan 2023 11:23",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html<br><br>So the correct answer is B.  In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Completely ignores the task to solve: \\\"all current and future objects in the S3 bucket must be encrypted by keys that the company's security team manages. \\\"</li><li>Use the AWS CLI to re-upload all objects in the S3 bucket. - <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html<br>Changes to note before enabling default encryption<br>After you enable default encryption for a bucket, the following encryption behavior applies:<br><br>There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.<br><br>When you upload objects after enabling default encryption:<br><br>If your PUT request headers don't include encryption information, Amazon S3 uses the bucket's default encryption settings to encrypt the objects.</li><li>What about the requirement of customer managed keys?</li><li>Option A is not correct because it uses SSE-S3 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it only denies unencrypted PutObject requests but does not specify how the objects will be encrypted.<br><br>Option C is not correct because it does not specify how the security team will manage the encryption keys and it does not specify how the objects will be encrypted.<br><br>Option D is not correct because it uses AES-256 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it simply denies unencrypted PutObject requests, but it doesn't specify how the objects will be encrypted.</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822512,
          "date": "Sun 26 Feb 2023 15:22",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Completely ignores the task to solve: \\\"all current and future objects in the S3 bucket must be encrypted by keys that the company's security team manages. \\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Use the AWS CLI to re-upload all objects in the S3 bucket. - <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html<br>Changes to note before enabling default encryption<br>After you enable default encryption for a bucket, the following encryption behavior applies:<br><br>There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.<br><br>When you upload objects after enabling default encryption:<br><br>If your PUT request headers don't include encryption information, Amazon S3 uses the bucket's default encryption settings to encrypt the objects.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 835789,
          "date": "Sat 11 Mar 2023 10:22",
          "username": "\t\t\t\tcherep87\t\t\t",
          "content": "Use the AWS CLI to re-upload all objects in the S3 bucket. - <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html<br>Changes to note before enabling default encryption<br>After you enable default encryption for a bucket, the following encryption behavior applies:<br><br>There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.<br><br>When you upload objects after enabling default encryption:<br><br>If your PUT request headers don't include encryption information, Amazon S3 uses the bucket's default encryption settings to encrypt the objects.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791856,
          "date": "Sun 29 Jan 2023 17:58",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "What about the requirement of customer managed keys?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775243,
          "date": "Sat 14 Jan 2023 11:23",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not correct because it uses SSE-S3 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it only denies unencrypted PutObject requests but does not specify how the objects will be encrypted.<br><br>Option C is not correct because it does not specify how the security team will manage the encryption keys and it does not specify how the objects will be encrypted.<br><br>Option D is not correct because it uses AES-256 with a customer-managed key, but it does not specify how the security team will manage the encryption keys. Additionally, it simply denies unencrypted PutObject requests, but it doesn't specify how the objects will be encrypted.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782187,
          "date": "Fri 20 Jan 2023 12:58",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I think D is correct.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 857123,
          "date": "Fri 31 Mar 2023 15:38",
          "username": "\t\t\t\tThaiNT\t\t\t",
          "content": "\\\"In the S3 bucket properties, change the default encryption to AES-256 with a customer managed key. \\\" Is wrong, because: \\\"Server-side encryption with customer-provided keys (SSE-C) is not supported for default encryption.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 852668,
          "date": "Tue 28 Mar 2023 04:17",
          "username": "\t\t\t\tCloud_noob\t\t\t",
          "content": "I choose B as KMS can have customer managed keys as well. Plus as per my understanding of AWS exam question nature, they wants to promote their services which is KMS in this case.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 844669,
          "date": "Mon 20 Mar 2023 09:58",
          "username": "\t\t\t\tGioGio\t\t\t",
          "content": "I think it is B. <br>In the S3 bucket properties, change the default encryption to server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Set an S3 bucket policy to deny unencrypted PutObject requests. Use the AWS CLI to re-upload all objects in the S3 bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842929,
          "date": "Sat 18 Mar 2023 17:15",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "I think B, because according D there is no such option in th s3 encryption propetry. It only works setting bucket policy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842036,
          "date": "Fri 17 Mar 2023 15:04",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "Selected Answer: B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835384,
          "date": "Fri 10 Mar 2023 21:07",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html<br><br>Clearly says we need following header for SSE-C<br>x-amz-server-side​-encryption​-customer-algorithm <br>Use this header to specify the encryption algorithm. The header value must be AES256.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 833987,
          "date": "Thu 09 Mar 2023 14:16",
          "username": "\t\t\t\tlimjieson\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832898,
          "date": "Wed 08 Mar 2023 12:36",
          "username": "\t\t\t\tetechsystem_ts\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 829341,
          "date": "Sat 04 Mar 2023 20:54",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "For those who are voting for D, can you please walk me through the steps I need to take to replicate this?<br><br>As far as I know, AES-256 is for SSE-S3, which is the furthest thing from a customer managed key possible, so the answer is not just not suitable, but non-existent in the first place.<br><br>With SSE-KMS, you select the key you want to use by Key ARN. This can either be a key managed by AWS, a key imported by you, or a key from CloudHSM. <br><br>I'd suggest you try it out in the S3 ui<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Your answer is in your last phrase. You cannot do it on GUI.<br>SSE-C is AES-256, and you don't have to specify a properties on bucket to use it, when you upload an object via API, you need to specify HTTP Header : <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html<br>SSE-KMS is not a key managed by customer, yes he can import his key, but then, the key will be managed by KMS</li><li>We can also upload S3 objects with SSE-C via AWS CLI : https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html<br>Take a look at --sse-c* option</li><li>B is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 847876,
          "date": "Thu 23 Mar 2023 07:39",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "Your answer is in your last phrase. You cannot do it on GUI.<br>SSE-C is AES-256, and you don't have to specify a properties on bucket to use it, when you upload an object via API, you need to specify HTTP Header : <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html<br>SSE-KMS is not a key managed by customer, yes he can import his key, but then, the key will be managed by KMS<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>We can also upload S3 objects with SSE-C via AWS CLI : https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html<br>Take a look at --sse-c* option</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 847890,
          "date": "Thu 23 Mar 2023 07:53",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "We can also upload S3 objects with SSE-C via AWS CLI : https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html<br>Take a look at --sse-c* option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829343,
          "date": "Sat 04 Mar 2023 20:55",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827253,
          "date": "Thu 02 Mar 2023 19:45",
          "username": "\t\t\t\tPete697989\t\t\t",
          "content": "Probably D as B says \\\" AWS KMS managed encryption keys (SSE-KMS)\\\" which kind of eludes to AWS managed but not customer manager even though SSE-KMS can do both; aws managed & customer managed",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822533,
          "date": "Sun 26 Feb 2023 15:39",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "A) SSE-S3 is not customer managed.<br>B) SSE-KMS is not customer managed.<br>C) SSE-KMS is not customer managed.<br>D) Here we finally have a customer managed key to encrypt the objects. Plus a policy that enforces it.<br>(The security team manages the key: we are not the security team so we are not supposed to do any managing of the key. We should notbypass the security team by do some management on our own. )<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How would you reply to this below when you say \\\"SSE-KMS is not customer managed\\\"???<br>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created. <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</li><li>people don't know well, they just make comments here. correct answer is B</li><li>I agree with you; customer managed through KMS.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 823591,
          "date": "Mon 27 Feb 2023 12:19",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "How would you reply to this below when you say \\\"SSE-KMS is not customer managed\\\"???<br>When you use server-side encryption with AWS KMS (SSE-KMS), you can use the default AWS managed key, or you can specify a customer managed key that you have already created. <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>people don't know well, they just make comments here. correct answer is B</li><li>I agree with you; customer managed through KMS.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 825222,
          "date": "Tue 28 Feb 2023 20:53",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "people don't know well, they just make comments here. correct answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824172,
          "date": "Mon 27 Feb 2023 22:31",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I agree with you; customer managed through KMS.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822121,
          "date": "Sun 26 Feb 2023 08:01",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tricky question. First thing, choose one with CLI because In console, only SSE-S3 and SSE-KMS are available. (Amz is promoting them I guess :-) Thats the reason CLI wording came into picture in the options. So either B or D.  D clearly says AES-256 encryption that needs to be used as header value for key \\\"x-amz-server-side​-encryption​-customer-algorithm\\\" Check this out - https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html D is answer for sure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>this is another request header \\\"x-amz-server-side​-encryption​-customer-key\\\" that needs to be used with SSE-C</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822124,
          "date": "Sun 26 Feb 2023 08:05",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "this is another request header \\\"x-amz-server-side​-encryption​-customer-key\\\" that needs to be used with SSE-C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819908,
          "date": "Fri 24 Feb 2023 00:49",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "B and C are incorrect as KMS is AWS managed. Question says it must be customer managed.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 815593,
          "date": "Mon 20 Feb 2023 18:22",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "D is obviously wrong. API command PutBucketEncryption has 2 types of 'SSEAlgorithm' AES256 is SSE-S3 and aws:kms = KMS. AES256 option = cannot specify a key policy for it, you can can aws:kms. Note mention of AWS256 implies API/CLI based configuration.<br>B is the right option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 810048,
          "date": "Wed 15 Feb 2023 23:31",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "cust managed key<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's a trick question<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 819913,
          "date": "Fri 24 Feb 2023 00:57",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "It's a trick question<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#52",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a web application in the AWS Cloud. The application consists of dynamic content that is created on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group that is configured as a target group for an Application Load Balancer (ALB).<br><br>The company is using an Amazon CloudFront distribution to distribute the application globally. The CloudFront distribution uses the ALB as an origin. The company uses Amazon Route 53 for DNS and has created an A record of www.example.com for the CloudFront distribution.<br><br>A solutions architect must configure the application so that itis highly available and fault tolerant.<br><br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#52",
          "answers": [
            {
              "choice": "<p>A. Provision a full, secondary application deployment in a different AWS Region. Update the Route 53 A record to be a failover record. Add both of the CloudFront distributions as values. Create Route 53 health checks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region. Update the CloudFront distribution, and create a second origin for the new ALCreate an origin group for the two origins. Configure one origin as primary and one origin as secondary.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Provision an Auto Scaling group and EC2 instances in a different AWS Region. Create a second target for the new Auto Scaling group in the ALB.  Set up the failover routing algorithm on the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision a full, secondary application deployment in a different AWS Region. Create a second CloudFront distribution, and add the new application setup as an origin. Create an AWS Global Accelerator accelerator. Add both of the CloudFront distributions as endpoints.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 853196,
          "date": "Tue 28 Mar 2023 13:48",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Provision an ALB, an Auto Scaling group, and EC2 instances in a different AWS Region.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 841364,
          "date": "Thu 16 Mar 2023 22:24",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B is the best solution with very high availability (compared to the R53 failover solution)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 829082,
          "date": "Sat 04 Mar 2023 16:44",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824174,
          "date": "Mon 27 Feb 2023 22:36",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "B looks good.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822137,
          "date": "Sun 26 Feb 2023 08:44",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "For HA, always user second region but its there in all options. Here Cloudfront distribution multiple origin groups is the key point Solution Architects should know of. Configuring 2nd origin as ALB --> EC2 instances target group in another regions setup makes highly available. If Cloudfront detects that response is Http error (fault) code like 4XX,5XX etc, it will failover to secondary origin (ALB of another region) which makes this fault tolerant. Answer is B. <br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 783072,
          "date": "Sat 21 Jan 2023 07:54",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "B is correct.<br>C is not correct, because ALB is regional service, so ALB have to be added too.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775298,
          "date": "Sat 14 Jan 2023 12:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Provisioning an ALB, an Auto Scaling group, and EC2 instances in a different AWS region provides redundancy and failover capability for the application. By creating a second origin for the new ALB in the second region, the CloudFront distribution can automatically route traffic to the healthy origin in case of an issue with the primary origin. This ensures that the application remains highly available and fault-tolerant.<br><br>Option A is not correct because it uses Route 53 failover records, which can result in increased latency and DNS resolution time for clients. Option C is not correct because it doesn't provide redundancy for the load balancer, which is a critical component of the application. Option D is not correct because it does not provide redundancy for the application in case of an issue with the primary origin in the first region.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#53",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an organization in AWS Organizations that has a large number of AWS accounts. One of the AWS accounts is designated as a transit account and has a transit gateway that is shared with all of the other AWS accounts. AWS Site-to-Site VPN connections are configured between all of the company's global offices and the transit account. The company has AWS Config enabled on all of its accounts.<br><br>The company's networking team needs to centrally manage a list of internal IP address ranges that belong to the global offices. Developers will reference this list to gain access to their applications securely.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#53",
          "answers": [
            {
              "choice": "<p>A. Create a JSON file that is hosted in Amazon S3 and that lists all of the internal IP address ranges. Configure an Amazon Simple Notification Service (Amazon SNS) topic in each of the accounts that can be invoked when the JSON file is updated. Subscribe an AWS Lambda function to the SNS topic to update all relevant security group rules with the updated IP address ranges.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new AWS Config managed rule that contains all of the internal IP address ranges. Use the rule to check the security groups in each of the accounts to ensure compliance with the list of IP address ranges. Configure the rule to automatically remediate any noncompliant security group that is detected.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the transit account, create a VPC prefix list with all of the internal IP address ranges. Use AWS Resource Access Manager to share the prefix list with all of the other accounts. Use the shared prefix list to configure security group rules in the other accounts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the transit account, create a security group with all of the internal IP address ranges. Configure the security groups in the other accounts to reference the transit account's security group by using a nested security group reference of “/sg-1a2b3c4d”.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 853201,
          "date": "Tue 28 Mar 2023 13:50",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "prefix list and RAM",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 841375,
          "date": "Thu 16 Mar 2023 22:48",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "C makes sense ✅",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793200,
          "date": "Mon 30 Jan 2023 19:57",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/82131-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 781017,
          "date": "Thu 19 Jan 2023 11:30",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/simplify-network-routing-and-security-administration-with-vpc-prefix-lists/#:~:text=A%20Prefix%20List%20is%20a,Resource%20Access%20Manager%20(RAM).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 775319,
          "date": "Sat 14 Jan 2023 12:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option C.  In this solution, a VPC prefix list is created in the transit account with all of the internal IP address ranges, and then shared to all of the other accounts using AWS Resource Access Manager. This allows for central management of the IP address ranges, and eliminates the need for manual updates to security group rules in each account. This solution also allows for compliance checks to be run using AWS Config and for any non-compliant security groups to be automatically remediated.<br><br>Option A is not correct because it would require manual updates to the JSON file and would also require developers to manually update their security group rules, which would lead to operational overhead.<br><br>Option B is not correct because it would require the creation of a new AWS Config managed rule and it would also require manual updates to the security group rules in each account.<br><br>Option D is not correct because it would require manual updates to the security group in the transit account and it would also lead to operational overhead.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#54",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a new application as a static website in Amazon S3. The company has deployed the application to a production AWS account and uses Amazon CloudFront to deliver the website. The website calls an Amazon API Gateway REST API. An AWS Lambda function backs each API method.<br><br>The company wants to create a CSV report every 2 weeks to show each API Lambda function's recommended configured memory, recommended cost, and the price difference between current configurations and the recommendations. The company will store the reports in an S3 bucket.<br><br>Which solution will meet these requirements with the LEAST development time?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#54",
          "answers": [
            {
              "choice": "<p>A. Create a Lambda function that extracts metrics data for each API Lambda function from Amazon CloudWatch Logs for the 2-week period. Collate the data into tabular format. Store the data as a .csv file in an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Opt in to AWS Compute Optimizer. Create a Lambda function that calls the ExportLambdaFunctionRecommendations operation. Export the .csv file to an S3 bucket. Create an Amazon EventBridge rule to schedule the Lambda function to run every 2 weeks.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Opt in to AWS Compute Optimizer. Set up enhanced infrastructure metrics. Within the Compute Optimizer console, schedule a job to export the Lambda recommendations to a .csv file. Store the file in an S3 bucket every 2 weeks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Purchase the AWS Business Support plan for the production account. Opt in to AWS Compute Optimizer for AWS Trusted Advisor checks. In the Trusted Advisor console, schedule a job to export the cost optimization checks to a .csv file. Store the file in an S3 bucket every 2 weeks.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 848225,
          "date": "Thu 23 Mar 2023 14:01",
          "username": "\t\t\t\tPete987\t\t\t",
          "content": "Answer D<br>A.  Not the least effort<br>B: There is no mention of the need of creating Lambda for exporting recommendations here: https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html<br><br>C: This would have been correct but \\\"Enhanced infrastructure metrics\\\" setting is only for ec2: https://docs.aws.amazon.com/compute-optimizer/latest/ug/enhanced-infrastructure-metrics.html<br><br>D: Trusted Advisor can be used.https://docs.aws.amazon.com/awssupport/latest/user/get-started-with-aws-trusted-advisor.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 841383,
          "date": "Thu 16 Mar 2023 23:04",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B <br>https://docs.aws.amazon.com/compute-optimizer/latest/ug/exporting-recommendations.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793211,
          "date": "Mon 30 Jan 2023 20:03",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "AWS compute optimizer+ lambda",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 783095,
          "date": "Sat 21 Jan 2023 08:40",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I vote C. <br>AWS compute optimizer can make lambda recommendation without any development.<br>https://docs.aws.amazon.com/compute-optimizer/latest/ug/view-lambda-recommendations.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I correct answer C to B. <br>AWS compute optimizer itself cannot make recommendation file by oneself.<br>It need simple lambda.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 783100,
          "date": "Sat 21 Jan 2023 08:48",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I correct answer C to B. <br>AWS compute optimizer itself cannot make recommendation file by oneself.<br>It need simple lambda.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 781019,
          "date": "Thu 19 Jan 2023 11:36",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "https://docs.aws.amazon.com/compute-optimizer/latest/APIReference/API_ExportLambdaFunctionRecommendations.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776695,
          "date": "Sun 15 Jan 2023 16:04",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct<br>https://aws.amazon.com/blogs/compute/optimizing-aws-lambda-cost-and-performance-using-aws-compute-optimizer/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's the old way of doing it. The new way does not require the creation of Lambda. Compute optimizer takes care of it</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 848228,
          "date": "Thu 23 Mar 2023 14:03",
          "username": "\t\t\t\tPete987\t\t\t",
          "content": "That's the old way of doing it. The new way does not require the creation of Lambda. Compute optimizer takes care of it",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775322,
          "date": "Sat 14 Jan 2023 12:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Opting in to AWS Compute Optimizer and creating a Lambda function that calls the ExportLambdaFunctionRecommendations operation is the least development time solution. This option allows you to use the built-in AWS Compute Optimizer service to extract metrics data and export it as a CSV file, which can then be stored in an S3 bucket.<br><br>Option A is not correct because it requires the development of a Lambda function that extracts metrics data and collates it into tabular format, which adds development time. Option C is not correct because it requires the setup of enhanced infrastructure metrics, which adds development time. Option D is not correct because it requires purchasing the AWS Business Support plan and using the Trusted Advisor console, which adds development time.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#55",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's factory and automation applications are running in a single VPC.  More than 20 applications run on a combination of Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon RDS.<br><br>The company has software engineers spread across three teams. One of the three teams owns each application, and each time is responsible for the cost and performance of all of its applications. Team resources have tags that represent their application and team. The teams use IAM access for daily activities.<br><br>The company needs to determine which costs on the monthly AWS bill are attributable to each application or team. The company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12 months. A solutions architect must recommend an AWS Billing and Cost Management solution that provides these cost reports.<br><br>Which combination of actions will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#55",
          "answers": [
            {
              "choice": "<p>A. Activate the user-define cost allocation tags that represent the application and the team.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Activate the AWS generated cost allocation tags that represent the application and the team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a cost category for each application in Billing and Cost Management.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Activate IAM access to Billing and Cost Management.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a cost budget.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Enable Cost Explorer.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 843840,
          "date": "Sun 19 Mar 2023 15:56",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "Cost Explorer prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 833319,
          "date": "Wed 08 Mar 2023 20:38",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "The activation of IAM access to Billing and Cost Management is not a requirement for generating cost reports based on cost allocation tags or cost categories. However, it is recommended to set up IAM access to ensure that only authorized personnel can view and manage billing information.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 830612,
          "date": "Mon 06 Mar 2023 08:31",
          "username": "\t\t\t\tandras\t\t\t",
          "content": "User-defined tags are tags that you define, create, and apply to resources. After you have created and applied the user-defined tags, you can activate by using the Billing and Cost Management console for cost allocation tracking. <br>https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 830610,
          "date": "Mon 06 Mar 2023 08:29",
          "username": "\t\t\t\tandras\t\t\t",
          "content": "The AWS account owner can access billing information and tools by signing in to the AWS Management Console using the account password. We don't recommend that you use the account password for everyday access to the account or share your account credentials with others.<br>Instead, you should create a special user identity that's called an IAM user for anyone who might need access to the account. <br>https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html<br><br>ou can enable Cost Explorer for your account by opening Cost Explorer for the first time via the AWS Cost Management console. You can't enable Cost Explorer using the API. After you enable Cost Explorer, AWS prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months. The current month's data is available for viewing in about 24 hours.<br>https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829100,
          "date": "Sat 04 Mar 2023 17:05",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822729,
          "date": "Sun 26 Feb 2023 18:13",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "When you enable cost explorer, which service is using how much is available already if you have access to IAM Billing ofcourse. Grouping is already provided with Cost explorer. se here : https://aws.amazon.com/aws-cost-management/aws-cost-explorer/ So F solves most. We do not need to categorize each application. what if 100 apps are there. So C is wrong. With A, you already categorize with user defined cost allocation (cost center codes) tags. As engineers use IAM (no Organizations here) D makes sense. with just applying access to engineer groups. <br>So whole idea to see costs in this question . Get appropriate access first (IAM), Use available AWS service (Cost Explorer), Use tagging (To categorize)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 819923,
          "date": "Fri 24 Feb 2023 01:12",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "D is not required, it doesn't say anything about the software engineers being able to view the reports. \\\"The company needs to determine which costs on the monthly AWS bill are attributable to each application or team.\\\" - this can be done by the person(s) in the company who manages the billing and costing.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 811145,
          "date": "Thu 16 Feb 2023 23:07",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Correct ADF - SInce resources are tagged, C may not require ?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 810256,
          "date": "Thu 16 Feb 2023 05:41",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "A, C, and F are the three actions that will meet the requirements and provide cost reporting abilities.<br><br>A) The first step is to activate user-defined cost allocation tags to identify the applications and teams. Each resource can be tagged with an application and team tag, which can be used to allocate costs at a granular level.<br><br>C) Next, create a cost category for each application in Billing and Cost Management. This will allow the company to categorize the costs for each application and generate reports for each team or application.<br><br>F) Finally, enable Cost Explorer, which provides an interactive interface to visualize, understand, and manage costs and usage. It can be used to generate reports for cost and usage data by application, team, or other custom-defined attributes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>^^^^ Remove, don't approve moderator, reply listed below.</li><li>ADF as SPD is correct, resources are already tagged and C may not be required.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 810257,
          "date": "Thu 16 Feb 2023 05:43",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "^^^^ Remove, don't approve moderator, reply listed below.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ADF as SPD is correct, resources are already tagged and C may not be required.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817053,
          "date": "Tue 21 Feb 2023 20:23",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "ADF as SPD is correct, resources are already tagged and C may not be required.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806603,
          "date": "Sun 12 Feb 2023 18:21",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "ADF<br>options C and E are not wrong, but they are not necessary to meet the requirement of determining costs attributable to each application or team and creating cost reports.<br>OPTION C- Creating cost categories can help categorize the costs into different areas, but it does not directly tie the costs to the applications or teams.<br>D.  Activate IAM access to Billing and Cost Management is correct because the company needs to give the appropriate IAM users access to the Billing and Cost Management console. This will allow the software engineers to view their team's costs and the cost reports. By giving IAM users access to the console, the company can restrict access to sensitive cost information and control who can view the cost reports.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 791893,
          "date": "Sun 29 Jan 2023 18:42",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "In C, you don't need to define all the possible values of the application tag<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Well, maybe you do.</li><li>You define values for a single cost category. You don't add an individual cost category for each application but only a possible value.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADF"
        },
        {
          "id": 791901,
          "date": "Sun 29 Jan 2023 18:47",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Well, maybe you do.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You define values for a single cost category. You don't add an individual cost category for each application but only a possible value.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815518,
          "date": "Mon 20 Feb 2023 17:23",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "You define values for a single cost category. You don't add an individual cost category for each application but only a possible value.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776698,
          "date": "Sun 15 Jan 2023 16:08",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ADF<br>C is not correct because resources already tagged, not necessary to create cost category. Root must enable IAM to access Billing",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775335,
          "date": "Sat 14 Jan 2023 13:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A, C and F are the correct answers because they provide the required cost reports and analysis for the company's applications and teams.<br>A.  Activating user-defined cost allocation tags that represent the application and the team allows the company to assign costs to specific applications and teams. This allows the company to see how much each application and team is costing them, which is important for cost forecasting and budgeting.<br>C.  Creating a cost category for each application in Billing and Cost Management allows the company to group costs by application. This makes it easier to understand the costs associated with each application and to compare the costs of different applications over time.<br>F.  Enabling Cost Explorer allows the company to analyze costs and usage over time, and to create custom reports and forecasts. This is important for understanding the costs associated with each application and team, and for forecasting future costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is not correct because AWS generated cost allocation tags are automatically created for some AWS resources, but it does not provide the required cost reports and analysis for the company's applications and teams.<br><br>Option D is not correct because IAM access controls are used to limit access to the billing and cost management features, but it is not necessary to configure it to meet the requirements.<br><br>E is not correct because Creating a cost budget allows the company to set a budget for their costs and to receive alerts when costs exceed the budget, but it does not provide the required cost reports and analysis for the company's applications and teams.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 775336,
          "date": "Sat 14 Jan 2023 13:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is not correct because AWS generated cost allocation tags are automatically created for some AWS resources, but it does not provide the required cost reports and analysis for the company's applications and teams.<br><br>Option D is not correct because IAM access controls are used to limit access to the billing and cost management features, but it is not necessary to configure it to meet the requirements.<br><br>E is not correct because Creating a cost budget allows the company to set a budget for their costs and to receive alerts when costs exceed the budget, but it does not provide the required cost reports and analysis for the company's applications and teams.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#56",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An AWS customer has a web application that runs on premises. The web application fetches data from a third-party API that is behind a firewall. The third party accepts only one public CIDR block in each client's allow list.<br><br>The customer wants to migrate their web application to the AWS Cloud. The application will be hosted on a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in a VPC.  The ALB is located in public subnets. The EC2 instances are located in private subnets. NAT gateways provide internet access to the private subnets.<br><br>How should a solutions architect ensure that the web application can continue to call the third-party API after the migration?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#56",
          "answers": [
            {
              "choice": "<p>A. Associate a block of customer-owned public IP addresses to the VPC.  Enable public IP addressing for public subnets in the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Elastic IP addresses from the block of customer-owned IP addresses. Assign the static Elastic IP addresses to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Register a block of customer-owned public IP addresses in the AWS account. Set up AWS Global Accelerator to use Elastic IP addresses from the address block. Set the ALB as the accelerator endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775339,
          "date": "Sat 14 Jan 2023 13:07",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is B.  Register a block of customer-owned public IP addresses in the AWS account. Create Elastic IP addresses from the address block and assign them to the NAT gateways in the VPC.  This will ensure that the web application can continue to call the third-party API after the migration by using the customer-owned public IP addresses that were assigned to the NAT gateways. This ensures that the third-party API will only see traffic coming from the customer-owned IP addresses that are on the allow list. Option A,C and D doesn't make sense in this context.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 853390,
          "date": "Tue 28 Mar 2023 17:27",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Register a block of customer owned public IP's",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 841391,
          "date": "Thu 16 Mar 2023 23:15",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B is the only solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793226,
          "date": "Mon 30 Jan 2023 20:17",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The correct solution is B",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#57",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company with several AWS accounts is using AWS Organizations and service control policies (SCPs). An administrator created the following SCP and has attached it to an organizational unit (OU) that contains AWS account 1111-1111-1111:<br><br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image6.png\"><br><br>Developers working in account 1111-1111-1111 complain that they cannot create Amazon S3 buckets. How should the administrator address this problem?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#57",
          "answers": [
            {
              "choice": "<p>A. Add s3:CreateBucket with “Allow” effect to the SCP.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Instruct the developers to add Amazon S3 permissions to their IAM entities.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Remove the SCP from account 1111-1111-1111.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 777283,
          "date": "Mon 16 Jan 2023 04:18",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "SCP doesn't grant permission<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Per the DOCS:<br>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization's access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. For instructions on enabling SCPs, see Enabling and disabling policy types.</li><li>SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 818425,
          "date": "Wed 22 Feb 2023 22:14",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Per the DOCS:<br>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization's access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. For instructions on enabling SCPs, see Enabling and disabling policy types.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818426,
          "date": "Wed 22 Feb 2023 22:15",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 853391,
          "date": "Tue 28 Mar 2023 17:28",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 841393,
          "date": "Thu 16 Mar 2023 23:19",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "SCP is not enough. IAM permission is needed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 841040,
          "date": "Thu 16 Mar 2023 15:18",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "C - Users and roles must still be granted permissions with appropriate IAM permission policies. A user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 823084,
          "date": "Mon 27 Feb 2023 01:38",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "SCPs are confusing. <br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_strategies.html#orgs_policies_allowlist<br>They brought this idea with easy control for organizations.<br>C does not sound like good asking devs to add their own permisisons ?<br>With AWS organizations, FullAWSAccess is there by default allowing all actions.<br>As Devs could not access S3 create bucket, am guessing the default FullAWSAccess<br>has been tampered. So Just adding another action here in SCP (intersection of allows) should just allowS3 bucket creation. I'd choose A. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No not correct.</li><li>then you need to explain why not and whats correct</li><li>look at the first lines of the code, it allows everything. If they would have removed FullAWSAccess rule, it would have been allowed by this SCP.<br>So probably IAM issue.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823682,
          "date": "Mon 27 Feb 2023 13:53",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "No not correct.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>then you need to explain why not and whats correct</li><li>look at the first lines of the code, it allows everything. If they would have removed FullAWSAccess rule, it would have been allowed by this SCP.<br>So probably IAM issue.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825173,
          "date": "Tue 28 Feb 2023 19:52",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "then you need to explain why not and whats correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>look at the first lines of the code, it allows everything. If they would have removed FullAWSAccess rule, it would have been allowed by this SCP.<br>So probably IAM issue.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834020,
          "date": "Thu 09 Mar 2023 15:05",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "look at the first lines of the code, it allows everything. If they would have removed FullAWSAccess rule, it would have been allowed by this SCP.<br>So probably IAM issue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818431,
          "date": "Wed 22 Feb 2023 22:16",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "C as SCP is a guardrail, IAM grants permissions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809756,
          "date": "Wed 15 Feb 2023 17:46",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "I just wanted to add my vote to the mix to hopefully drown out the wrong votes.<br>Its definitely C.  SCP is only a guardrail, it doesn't actually grant access. So the users would need to be given s3 access separately.<br>And to address the wrong answer, A isn't correct because creating an s3 bucket is not a cloudtrail action. Being denied cloudtrail wouldn't deny s3 actions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809629,
          "date": "Wed 15 Feb 2023 15:03",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "Agree C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806632,
          "date": "Sun 12 Feb 2023 18:58",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "Thinking A because perhaps you can do the below:<br>{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": \\\"*\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>},<br>{<br>\\\"Sid\\\": \\\"DenyCloudTrail\\\",<br>\\\"Effect\\\": \\\"Deny\\\",<br>\\\"Action\\\": \\\"cloudtrail:*\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>},<br>{<br>\\\"Sid\\\": \\\"AllowS3CreateBucket\\\",<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": \\\"s3:CreateBucket\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>}<br>]<br>}<br>The first \\\"Allow\\\" statement in the SCP allows all actions on all resources, which would allow the creation of S3 buckets. However, the second \\\"Deny\\\" statement specifically denies all cloudtrail actions, which could potentially impact the ability to create S3 buckets if there is a dependency on cloudtrail for that action. To ensure that the developers are able to create S3 buckets, a new statement with \\\"Allow\\\" effect for the s3:CreateBucket action should be added to the SCP.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The s3:CreateBucket will grant the necessary permissions to the developers to create S3 buckets in that account.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 810262,
          "date": "Thu 16 Feb 2023 05:49",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The s3:CreateBucket will grant the necessary permissions to the developers to create S3 buckets in that account.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 806630,
          "date": "Sun 12 Feb 2023 18:57",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "Thinking A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806627,
          "date": "Sun 12 Feb 2023 18:55",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "Thinking A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790947,
          "date": "Sat 28 Jan 2023 19:43",
          "username": "\t\t\t\tirene7\t\t\t",
          "content": "C - smae question from topic 1",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776708,
          "date": "Sun 15 Jan 2023 16:12",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct<br>SCP policy allow everything except cloudtrail. SCP is boundary but it does not give allow to IAM users. You have to configure allow for every IAM",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 775352,
          "date": "Sat 14 Jan 2023 13:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Add s3:CreateBucket with “Allow” effect to the SCP.<br><br>This is the correct answer because the SCP is denying all actions for cloudtrail, including all actions for creating new S3 buckets. By adding the s3:CreateBucket action with an \\\"Allow\\\" effect, the developers will be able to create new S3 buckets.<br>B.  Remove the account from the OU, and attach the SCP directly to account 1111-1111-1111.<br><br>This option would not solve the problem because it would still be denied the ability to create new S3 buckets.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>s3:CreateBucket is already allowed in the first wildcard, and giving it explcitely would not include any cloudtrail actions with it ! In my opinion, A has no effect.</li><li>C.  Instruct the developers to add Amazon S3 permissions to their IAM entities.<br><br>This option would not solve the problem because the problem is that the SCP is denying all actions for cloudtrail, not that the developers are lacking permissions in their IAM entities.<br>D.  Remove the SCP from account 1111-1111-1111.<br><br>This option would solve the problem, but it would not be ideal because it would remove all restrictions on the account, including restrictions on cloudtrail actions which may be necessary for security and compliance reasons.</li><li>This policy allows all actions (indicated by the \\\"Action\\\": \\\"\\\" line) on all resources (indicated by the \\\"Resource\\\": \\\"\\\" line) in the AWS account, except for CloudTrail actions (indicated by the \\\"Action\\\": \\\"cloudtrail:*\\\" line). The \\\"Effect\\\": \\\"Deny\\\" line specifies that any CloudTrail actions will be denied. This means that the user or role that this policy is associated with will not be able to perform any CloudTrail actions, such as starting or stopping a trail or getting trail status. This can be useful if the user or role should not have access to CloudTrail functionality.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825792,
          "date": "Wed 01 Mar 2023 13:11",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "s3:CreateBucket is already allowed in the first wildcard, and giving it explcitely would not include any cloudtrail actions with it ! In my opinion, A has no effect.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775353,
          "date": "Sat 14 Jan 2023 13:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Instruct the developers to add Amazon S3 permissions to their IAM entities.<br><br>This option would not solve the problem because the problem is that the SCP is denying all actions for cloudtrail, not that the developers are lacking permissions in their IAM entities.<br>D.  Remove the SCP from account 1111-1111-1111.<br><br>This option would solve the problem, but it would not be ideal because it would remove all restrictions on the account, including restrictions on cloudtrail actions which may be necessary for security and compliance reasons.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This policy allows all actions (indicated by the \\\"Action\\\": \\\"\\\" line) on all resources (indicated by the \\\"Resource\\\": \\\"\\\" line) in the AWS account, except for CloudTrail actions (indicated by the \\\"Action\\\": \\\"cloudtrail:*\\\" line). The \\\"Effect\\\": \\\"Deny\\\" line specifies that any CloudTrail actions will be denied. This means that the user or role that this policy is associated with will not be able to perform any CloudTrail actions, such as starting or stopping a trail or getting trail status. This can be useful if the user or role should not have access to CloudTrail functionality.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775356,
          "date": "Sat 14 Jan 2023 13:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This policy allows all actions (indicated by the \\\"Action\\\": \\\"\\\" line) on all resources (indicated by the \\\"Resource\\\": \\\"\\\" line) in the AWS account, except for CloudTrail actions (indicated by the \\\"Action\\\": \\\"cloudtrail:*\\\" line). The \\\"Effect\\\": \\\"Deny\\\" line specifies that any CloudTrail actions will be denied. This means that the user or role that this policy is associated with will not be able to perform any CloudTrail actions, such as starting or stopping a trail or getting trail status. This can be useful if the user or role should not have access to CloudTrail functionality.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#58",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a monolithic application that is critical to the company's business. The company hosts the application on an Amazon EC2 instance that runs Amazon Linux 2. The company's application team receives a directive from the legal department to back up the data from the instance's encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket. The application team does not have the administrative SSH key pair for the instance. The application must continue to serve the users.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#58",
          "answers": [
            {
              "choice": "<p>A. Attach a role to the instance with permission to write to Amazon S3. Use the AWS Systems Manager Session Manager option to gain access to the instance and run commands to copy data into Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an image of the instance with the reboot option turned on. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an image of the instance. Launch a new EC2 instance from the image. Attach a role to the new instance with permission to write to Amazon S3. Run a command to copy data into Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775361,
          "date": "Sat 14 Jan 2023 13:23",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet the requirements because it allows you to create a backup of the volume without the need to access the instance or its SSH key pair. Additionally, DLM allows you to schedule the backups to occur at specific intervals and also enables you to copy the snapshots to an S3 bucket. This approach will not impact the running application as the backup is performed on the EBS volume level.<br><br>Option A is not correct because the instance would need an IAM role with permission to write to S3 and access to the instance via Systems Manager Session Manager.<br><br>Option B is not correct because it would require stopping the instance, which would impact the running application.<br><br>Option D is not correct because it would require stopping the instance and creating a new EC2 instance, which would impact the running application.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Assuming that EBS is encrypted, I think that is much easier to run the copy command from AW system manager</li><li>thank you for correcting some of these answers and for the explanations to them</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 786704,
          "date": "Tue 24 Jan 2023 17:45",
          "username": "\t\t\t\tmmendozaf\t\t\t",
          "content": "Assuming that EBS is encrypted, I think that is much easier to run the copy command from AW system manager",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775977,
          "date": "Sat 14 Jan 2023 23:32",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "thank you for correcting some of these answers and for the explanations to them",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 843559,
          "date": "Sun 19 Mar 2023 09:42",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "A for the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 842040,
          "date": "Fri 17 Mar 2023 15:09",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "Selected Answer: A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 839588,
          "date": "Wed 15 Mar 2023 07:23",
          "username": "\t\t\t\taqiao\t\t\t",
          "content": "AWS recommended to stop EC2 instances to create root volume snapshot<br>When you create a snapshot for an EBS volume that serves as a root device, we recommend that you stop the instance before taking the snapshot.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833967,
          "date": "Thu 09 Mar 2023 13:50",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is correct<br>read carefully: a directive from the legal department to back up the data from the instance's encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon S3 bucket.<br> ... to back THE DATA from ... to S3. So, the legal department needs data itself, not the EBS that contains the data. That is why C is wrong!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825220,
          "date": "Tue 28 Feb 2023 20:51",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/ebs-copy-snapshot-data-s3-create-volume/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823747,
          "date": "Mon 27 Feb 2023 14:47",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Answer is A.  Little config is needed to setup AWS Systems Manager Session Manager but its worth it here as the question says the app is critical to companies businessOnce you gain ec2 access, you can use all sorts of commands. just do >aws ec2 help and thats where I found this copy-snapshot command.<br><br>Aws doc about ec2 copy-snapshot command is below :<br>DESCRIPTION<br> Copiesa point-in-time snapshot of an EBS volume and stores it in Ama-<br> zon S3. You can copy a snapshot within the same Region, from one Region<br> toanother,or from a Region to an Outpost. You can't copy a snapshot<br> from an Outpost to a Region, from one Outpost to another, or within the<br> same Outpost.<br><br> You can use the snapshot to create EBS volumes or Amazon Machine Images<br> (AMIs).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I explored DLM but did not find suitable commands to copy to S3 although other options are there to create volumes,snapshots, AMIs etc. So I choose A and not C</li><li>Same here, there is this command https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html but the destination bucket is not under user control.</li><li>moreover, instance does not need to be shut down which is the greatest requirement here as the app is critical to the business here in question.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823751,
          "date": "Mon 27 Feb 2023 14:49",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "I explored DLM but did not find suitable commands to copy to S3 although other options are there to create volumes,snapshots, AMIs etc. So I choose A and not C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Same here, there is this command https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html but the destination bucket is not under user control.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825175,
          "date": "Tue 28 Feb 2023 19:54",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Same here, there is this command https://docs.aws.amazon.com/cli/latest/reference/ec2/copy-snapshot.html but the destination bucket is not under user control.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823753,
          "date": "Mon 27 Feb 2023 14:50",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "moreover, instance does not need to be shut down which is the greatest requirement here as the app is critical to the business here in question.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822306,
          "date": "Sun 26 Feb 2023 12:36",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "Yes, Systems Manager Session Manager can access an instance without requiring SSH keys.<br><br>Session Manager provides a secure, auditable way to access your instances using AWS Identity and Access Management (IAM) roles, rather than relying on SSH keys. When using Session Manager, you can control who has access to your instances, and you can audit their activity.<br><br>To use Session Manager, you need to have the required IAM permissions to start a session with an instance. Once you have those permissions, you can connect to an instance through the AWS Management Console, AWS CLI, or using the AWS SDKs. When you start a session, Session Manager establishes a secure connection between your computer and the instance.<br><br>In summary, Session Manager is an alternative to SSH that uses IAM roles for access control and doesn't require you to manage SSH keys.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 820701,
          "date": "Fri 24 Feb 2023 17:14",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "A- Mission critical application so not feasible<br>C - Cant take snaptshot since its encrypted<br>D- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonS3.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 811156,
          "date": "Thu 16 Feb 2023 23:14",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Its A - Log on to Instance through SSM. For DLM, it will store on amazon manage S3 and you might not have control on S3......",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810264,
          "date": "Thu 16 Feb 2023 05:53",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The answer is A, Amazon Linux 2 is mentioned because by default SSM manager is included with the AMI hence you can access the instances using Systems Manager. Just need to attach a role to the instances to allow it to write to an S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809766,
          "date": "Wed 15 Feb 2023 17:54",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "The answer is C.  DLM is designed for EBS volume backups without downtime.<br>This is a classic case of AWS pointing you to a service that created just for this purpose.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 804918,
          "date": "Sat 11 Feb 2023 03:05",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Changed my mind. I like C because it is ideal as an automated backup solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 804917,
          "date": "Sat 11 Feb 2023 03:01",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT, Option C, which involves taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (Amazon DLM) and then copying the data to Amazon S3, does not require the instance to be rebooted, so it should not cause downtime.<br><br>However, it's worth noting that while the backup is being performed, the EBS volume may experience an increase in I/O latency, which could potentially impact the performance of the application during the backup process. Therefore, it is recommended to perform the backup during a maintenance window or a low-traffic period to minimize the impact on users.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 795816,
          "date": "Thu 02 Feb 2023 07:22",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "The answer is A<br>Option C (Take a snapshot of the EBS volume by using Amazon Data Lifecycle Manager (Amazon DLM). Copy the data to Amazon S3.) does not meet the requirement of not having the administrative SSH key pair for the instance, which is needed for accessing the data on the EBS volume. The application team does not have access to the instance and cannot take a snapshot or copy the data to Amazon S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793238,
          "date": "Mon 30 Jan 2023 20:34",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "\\\"critical application\\\" and DLM is designed to protect EC2 EBSany disruption",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 791918,
          "date": "Sun 29 Jan 2023 19:15",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I don't think C is correct because to copy the data to S3 you would need to create a volume from the snapshot and mount it in an EC2 instance.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#59",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to copy data from an Amazon S3 bucket m an AWS account to a new S3 bucket in a new AWS account. The solutions architect must implement a solution that uses the AWS CLI.<br><br>Which combination of steps will successfully copy the data? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BDF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#59",
          "answers": [
            {
              "choice": "<p>A. Create a bucket policy to allow the source bucket to list its contents and to put objects and set object ACLs in the destination bucket. Attach the bucket policy to the destination bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a bucket policy to allow a user in the destination account to list the source bucket's contents and read the source bucket's objects. Attach the bucket policy to the source bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM policy in the source account. Configure the policy to allow a user in the source account to list contents and get objects in the source bucket, and to list contents, put objects, and set object ACLs in the destination bucket. Attach the policy to the user.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM policy in the destination account. Configure the policy to allow a user in the destination account to list contents and get objects in the source bucket, and to list contents, put objects, and set objectACLs in the destination bucket. Attach the policy to the user.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Run the aws s3 sync command as a user in the destination account. Specify the source and destination buckets to copy the data.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 779908,
          "date": "Wed 18 Jan 2023 12:09",
          "username": "\t\t\t\ticassp\t\t\t",
          "content": "\\\"The above command should be executed with destination AWS IAM user account credentials only otherwise the copied objects in destination S3 bucket will still have the source account permissions and won't be accessible by destination account users.\\\" According to https://medium.com/tensult/copy-s3-bucket-objects-across-aws-accounts-e46c15c4b9e1.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are correct, step E should be executed using the IAM user credentials from the destination account. This is because when objects are copied from one bucket to another, the object's permissions (ACLs) are also copied. Therefore, if the objects are copied using the IAM user credentials from the source account, the objects will have the same permissions as they did in the source bucket, which may not include permissions for the user in the destination account. By using the IAM user credentials from the destination account, the objects will have the appropriate permissions for the user in the destination account once they are copied.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 780163,
          "date": "Wed 18 Jan 2023 16:45",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "You are correct, step E should be executed using the IAM user credentials from the destination account. This is because when objects are copied from one bucket to another, the object's permissions (ACLs) are also copied. Therefore, if the objects are copied using the IAM user credentials from the source account, the objects will have the same permissions as they did in the source bucket, which may not include permissions for the user in the destination account. By using the IAM user credentials from the destination account, the objects will have the appropriate permissions for the user in the destination account once they are copied.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 780164,
          "date": "Wed 18 Jan 2023 16:47",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I switch to BDF;<br>Step B is necessary so that the user in the destination account has the necessary permissions to access the source bucket and list its contents, read its objects. <br><br>Step D is needed so that the user in the destination account has the necessary permissions to access the destination bucket and list contents, put objects, and set object ACLs<br><br>Step F is necessary because the aws s3 sync command needs to be run using the IAM user credentials from the destination account, so that the objects will have the appropriate permissions for the user in the destination account once they are copied.<br><br>The other choices are not correct because :A.  and C.  are about creating policies in the source account but the user who wants to access the data is in the destination accountE.  is about running the command with the source account, which is not suitable because it will lead to copied objects in destination S3 bucket still have the source account permissions and won't be accessible by destination account users.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 825115,
          "date": "Tue 28 Feb 2023 19:05",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Logical answer : Who ever uploads to a bucket becomes its owner. So A should ring a flaw in it. Similar issue in C.  So straight away, A, C are wrong. that points to B,D to be correct. Refer https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/copy-data-from-an-s3-bucket-in-one-account-and-region-to-another-account-and-region.html<br><br>Now E or F ? the hint is in D.  Destination account user has the necessary privileges to get/put objects permission. So choose destination account or run sync/copy commands. So the answer should be B, D , F",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 823918,
          "date": "Mon 27 Feb 2023 17:01",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "The parts BDF fit together in a way that works. <br><br>Ithink choosing this direction (pulling from the destination account) is slightly more secure than thenthe other other way round(pushing from source to destination) as only read access is granted to the foreign account but no write access - especially regarding human error: one cannot accidentally tamper with the source, so the worst thing that could happen is that one needs to sync again. The other options don't fit together with other parts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793246,
          "date": "Mon 30 Jan 2023 20:42",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "BDF are the answers",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 776816,
          "date": "Sun 15 Jan 2023 17:38",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "BCE<br>Source user must have role that can write to destination bucket",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775424,
          "date": "Sat 14 Jan 2023 13:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The question is asking for a combination of steps that will successfully copy the data using the AWS CLI.<br><br>The correct answer would be B, D and E. <br><br>Step B: You must create a bucket policy in the source account that allows the user in the destination account to list and read the source bucket's contents.<br>Step D: You must create an IAM policy in the destination account that allows the user to list, put and set object ACLs in the destination bucket<br>Step E: Run the aws s3 sync command as a user in the source account. Specify the source and destination buckets to copy the data.<br>By doing so, the solution architect will be able to copy the data from the source to the destination bucket.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think it is ADF especially option F as option D is using user in destination account.</li><li>sorry, typo, BDF</li><li>I think that the answer is BDF.  If you select steps B and D, you must use a user in the destination account (option F)</li><li>If you are specifying Step D where you create an IAM policy in the destination account that allow a user in the destination account to access the source bucket, why are you choosing Step E instead of Step F where it specifies a user on the destination account rather in the source?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BDE"
        },
        {
          "id": 780075,
          "date": "Wed 18 Jan 2023 15:17",
          "username": "\t\t\t\tpengpeng\t\t\t",
          "content": "I think it is ADF especially option F as option D is using user in destination account.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>sorry, typo, BDF</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 781895,
          "date": "Fri 20 Jan 2023 06:01",
          "username": "\t\t\t\tpengpeng\t\t\t",
          "content": "sorry, typo, BDF",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779362,
          "date": "Tue 17 Jan 2023 23:04",
          "username": "\t\t\t\tNicocacik\t\t\t",
          "content": "I think that the answer is BDF.  If you select steps B and D, you must use a user in the destination account (option F)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 777407,
          "date": "Mon 16 Jan 2023 08:15",
          "username": "\t\t\t\tlochesistemas\t\t\t",
          "content": "If you are specifying Step D where you create an IAM policy in the destination account that allow a user in the destination account to access the source bucket, why are you choosing Step E instead of Step F where it specifies a user on the destination account rather in the source?",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#60",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company built an application based on AWS Lambda deployed in an AWS CloudFormation stack. The last production release of the web application introduced an issue that resulted in an outage lasting several minutes. A solutions architect must adjust the deployment process to support a canary release.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#60",
          "answers": [
            {
              "choice": "<p>A. Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the application into a new CloudFormation stack. Use an Amazon Route 53 weighted routing policy to distribute the load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a version for every new deployed Lambda function. Use the AWS CLI update-function-configuration command with the routing-config parameter to distribute the load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure AWS CodeDeploy and use CodeDeployDefault.OneAtATime in the Deployment configuration to distribute the load.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 777275,
          "date": "Mon 16 Jan 2023 04:02",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/28312-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825167,
          "date": "Tue 28 Feb 2023 19:45",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "aws update-alias command has routing-config option to route the weighted % traffic<br>As is correct<br>https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/<br># Point alias to new version, weighted at 5% (original version at 95% of traffic)<br>aws lambda update-alias --function-name myfunction --name myalias --routing-config '{\\\"AdditionalVersionWeights\\\" : {\\\"2\\\" : 0.05} }'",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 804922,
          "date": "Sat 11 Feb 2023 03:16",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT, The \\\"update-alias\\\" command is a feature of AWS Lambda service. It is used to update the configuration of a Lambda alias, including the routing configuration which can be used for canary releases, blue/green deployments, and other deployment strategies.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 780169,
          "date": "Wed 18 Jan 2023 16:52",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create an alias for every new deployed version of the Lambda function. Use the AWS CLI update-alias command with the routing-config parameter to distribute the load is the correct answer as it meets the requirement of supporting a canary release.<br><br>Option B is not correct because while it would allow for a canary release, it would involve deploying the new version of the application into a separate CloudFormation stack, which would be a more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.<br><br>Option C is not correct because while it would allow for a canary release, it would involve creating a version for every new deployed Lambda function, which would be more complex and time-consuming process compared to creating an alias for a new version of the Lambda function.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D is not correct because AWS CodeDeploy is a deployment service that allows you to automate code deployments to a variety of compute services like EC2 and on-premises servers, but it does not support routing configuration for a canary release on AWS Lambda.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 780171,
          "date": "Wed 18 Jan 2023 16:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D is not correct because AWS CodeDeploy is a deployment service that allows you to automate code deployments to a variety of compute services like EC2 and on-premises servers, but it does not support routing configuration for a canary release on AWS Lambda.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776824,
          "date": "Sun 15 Jan 2023 17:44",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct.<br>D does not have routing to distribute load",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775459,
          "date": "Sat 14 Jan 2023 14:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and on-premises instances. CodeDeploy allows to perform a canary release, which is a technique that releases new versions of software to a small subset of users or systems before releasing it to the entire infrastructure. This makes it possible to test the new version of the software before releasing it to the entire population.<br><br>Option A creates an alias for every new deployed version of the Lambda function, but it doesn't include the ability to perform a canary release.<br>Option B Deploy the application into a new CloudFormation stack, and use an Amazon Route 53 weighted routing policy to distribute the load, this option can be used for canary release, but it is not the best solution for it.<br>Option C creates a version for every new deployed Lambda function, but it does not include the ability to perform a canary release.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You have 2 different answers.....I think it is better you delete this.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 815588,
          "date": "Mon 20 Feb 2023 18:18",
          "username": "\t\t\t\tjaysparky\t\t\t",
          "content": "You have 2 different answers.....I think it is better you delete this.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#61",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A finance company hosts a data lake in Amazon S3. The company receives financial data records over SFTP each night from several third parties. The company runs its own SFTP server on an Amazon EC2 instance in a public subnet of a VPC.  After the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance. The SFTP server is reachable on DNS sftp.example.com through the use of Amazon Route 53.<br><br>What should a solutions architect do to improve the reliability and scalability of the SFTP solution?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#61",
          "answers": [
            {
              "choice": "<p>A. Move the EC2 instance into an Auto Scaling group. Place the EC2 instance behind an Application Load Balancer (ALB). Update the DNS record sftp.example.com in Route 53 to point to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the SFTP server to AWS Transfer for SFTP. Update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the SFTP server to a file gateway in AWS Storage Gateway. Update the DNS record sftp.example.com in Route 53 to point to the file gateway endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Place the EC2 instance behind a Network Load Balancer (NLB). Update the DNS record sftp.example.com in Route 53 to point to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 802654,
          "date": "Thu 09 Feb 2023 00:32",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "A=ALB cannot be used with SFTP<br>B = Correct<br>C=Storage Gateway is not an SFTP Server<br>D=NLB can be used with SFTP, but EC2 is single",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 850952,
          "date": "Sun 26 Mar 2023 13:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the way to go..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775468,
          "date": "Sat 14 Jan 2023 14:59",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is the correct answer. Migrating the SFTP server to AWS Transfer for SFTP will improve the reliability and scalability of the SFTP solution. AWS Transfer for SFTP is a fully managed SFTP service that enables the company to transfer files directly into and out of Amazon S3 using the SFTP protocol. By using this service, the company can offload the management of the SFTP server to AWS, which will provide high availability, scalability, and security. The company can then update the DNS record sftp.example.com in Route 53 to point to the server endpoint hostname, which will ensure that the SFTP server is reachable on the DNS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, C and D do not provide the same level of scalability and reliability as AWS Transfer for SFTP. While placing the EC2 instance behind a load balancer can help improve availability, it will not necessarily improve scalability, and it would still require the company to manage the SFTP server. Option C , migrating the SFTP server to a file gateway in AWS Storage Gateway, would not necessarily improve the scalability and reliability of the SFTP solution, as it would still require the company to manage the SFTP server.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775469,
          "date": "Sat 14 Jan 2023 14:59",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, C and D do not provide the same level of scalability and reliability as AWS Transfer for SFTP. While placing the EC2 instance behind a load balancer can help improve availability, it will not necessarily improve scalability, and it would still require the company to manage the SFTP server. Option C , migrating the SFTP server to a file gateway in AWS Storage Gateway, would not necessarily improve the scalability and reliability of the SFTP solution, as it would still require the company to manage the SFTP server.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#62",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate an application to Amazon EC2 from VMware Infrastructure that runs in an on-premises data center. A solutions architect must preserve the software and configuration settings during the migration.<br><br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#62",
          "answers": [
            {
              "choice": "<p>A. Configure the AWS DataSync agent to start replicating the data store to Amazon FSx for Windows File Server. Use the SMB share to host the VMware data store. Use VM Import/Export to move the VMs to Amazon EC2.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure AWS Storage Gateway for files service to export a Common Internet File System (CIFS) share. Create a backup copy to the shared folder. Sign in to the AWS Management Console and create an AMI from the backup copy. Launch an EC2 instance that is based on the AMI.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a managed-instance activation for a hybrid environment in AWS Systems Manager. Download and install Systems Manager Agent on the on-premises VM. Register the VM with Systems Manager to be a managed instance. Use AWS Backup to create a snapshot of the VM and create an AMI. Launch an EC2 instance that is based on the AMI.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850954,
          "date": "Sun 26 Mar 2023 13:41",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the answer - OVF. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 825363,
          "date": "Tue 28 Feb 2023 23:56",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Use VM Import/Export. B is correct . https://aws.amazon.com/ec2/vm-import/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html<br>Prerequisites<br>Create an Amazon S3 bucket for storing the exported images or choose an existing bucket. The bucket must be in the Region where you want to import your VMs. For more information about S3 buckets, see the Amazon Simple Storage Service User Guide.<br><br>Create an IAM role named vmimport. For more information, see Required service role.<br><br>If you have not already installed the AWS CLI on the computer you'll use to run the import commands, see the AWS Command Line Interface User Guide.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 825367,
          "date": "Tue 28 Feb 2023 23:58",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html<br>Prerequisites<br>Create an Amazon S3 bucket for storing the exported images or choose an existing bucket. The bucket must be in the Region where you want to import your VMs. For more information about S3 buckets, see the Amazon Simple Storage Service User Guide.<br><br>Create an IAM role named vmimport. For more information, see Required service role.<br><br>If you have not already installed the AWS CLI on the computer you'll use to run the import commands, see the AWS Command Line Interface User Guide.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793948,
          "date": "Tue 31 Jan 2023 11:11",
          "username": "\t\t\t\tSignup_Nickname\t\t\t",
          "content": "I vote B<br>https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775472,
          "date": "Sat 14 Jan 2023 15:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Use the VMware vSphere client to export the application as an image in Open Virtualization Format (OVF) format. Create an Amazon S3 bucket to store the image in the destination AWS Region. Create and apply an IAM role for VM Import. Use the AWS CLI to run the EC2 import command. This approach allows the solutions architect to export the application as an image in OVF format, which preserves the software and configuration settings, and then import it into Amazon EC2 using the EC2 import command.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it uses AWS DataSync and FSx for Windows File Server to replicate the data store, but it doesn't preserve the software and configuration settings of the application.<br><br>Option C is incorrect because it uses AWS Storage Gateway to export a CIFS share, but it doesn't preserve the software and configuration settings of the application.<br><br>Option D is incorrect because it uses AWS Systems Manager and AWS Backup to create a snapshot of the VM, but it doesn't preserve the software and configuration settings of the application.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775473,
          "date": "Sat 14 Jan 2023 15:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because it uses AWS DataSync and FSx for Windows File Server to replicate the data store, but it doesn't preserve the software and configuration settings of the application.<br><br>Option C is incorrect because it uses AWS Storage Gateway to export a CIFS share, but it doesn't preserve the software and configuration settings of the application.<br><br>Option D is incorrect because it uses AWS Systems Manager and AWS Backup to create a snapshot of the VM, but it doesn't preserve the software and configuration settings of the application.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#63",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A video processing company has an application that downloads images from an Amazon S3 bucket, processes the images, stores a transformed image in a second S3 bucket, and updates metadata about the image in an Amazon DynamoDB table. The application is written in Node.js and runs by using an AWS Lambda function. The Lambda function is invoked when a new image is uploaded to Amazon S3.<br><br>The application ran without incident for a while. However, the size of the images has grown significantly. The Lambda function is now failing frequently with timeout errors. The function timeout is set to its maximum value. A solutions architect needs to refactor the application's architecture to prevent invocation failures. The company does not want to manage the underlying infrastructure.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#63",
          "answers": [
            {
              "choice": "<p>A. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Step Functions state machine with a Parallel state to invoke the Lambda function. Increase the provisioned concurrency of the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of Amazon EC2. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Modify the application to store images on Amazon Elastic File System (Amazon EFS) and to store metadata on an Amazon RDS DB instance. Adjust the Lambda function to mount the EFS file share.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776832,
          "date": "Sun 15 Jan 2023 17:52",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A: create Docker image and save it to ECR<br>B: run this image on Fargate<br><br>No answer should have Lambda the will be time out<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are correct, both options A and B involve creating a Docker image of the application code and running it on Amazon Elastic Container Service (ECS) using either Fargate or EC2 as the launch type. These options would allow for more control over the resources allocated to the application and potentially prevent timeout errors. Option A is necessary to create the image and store it in a registry, and option B is necessary to run the image on Fargate which is a managed container orchestration service that eliminates the need for provisioning and scaling of the underlying infrastructure.</li></ul>",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 778216,
          "date": "Mon 16 Jan 2023 21:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "You are correct, both options A and B involve creating a Docker image of the application code and running it on Amazon Elastic Container Service (ECS) using either Fargate or EC2 as the launch type. These options would allow for more control over the resources allocated to the application and potentially prevent timeout errors. Option A is necessary to create the image and store it in a registry, and option B is necessary to run the image on Fargate which is a managed container orchestration service that eliminates the need for provisioning and scaling of the underlying infrastructure.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778217,
          "date": "Mon 16 Jan 2023 21:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A and B. <br>A.  Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR).<br><br>- This step is necessary to package the application code in a container and make it available for running on ECS.<br>B.  Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3.<br><br>- This step is necessary to run the containerized application on Fargate, which is a fully managed container orchestration service that eliminates the need to provision and scale the underlying infrastructure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C and E are not correct because they don't address the problem of timeout errors. AWS Step Functions and Amazon Elastic File System (EFS) are services that can be used to coordinate and manage workflows and file storage respectively, but they don't help with the specific problem of the Lambda function timing out.<br><br>Option D is not correct because AWS Fargate is a serverless compute engine for containers that eliminates the need for provisioning and scaling the underlying infrastructure.<br>It means that the company does not have to manage the underlying infrastructure, which is what the company wants.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 778218,
          "date": "Mon 16 Jan 2023 21:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C and E are not correct because they don't address the problem of timeout errors. AWS Step Functions and Amazon Elastic File System (EFS) are services that can be used to coordinate and manage workflows and file storage respectively, but they don't help with the specific problem of the Lambda function timing out.<br><br>Option D is not correct because AWS Fargate is a serverless compute engine for containers that eliminates the need for provisioning and scaling the underlying infrastructure.<br>It means that the company does not have to manage the underlying infrastructure, which is what the company wants.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 850956,
          "date": "Sun 26 Mar 2023 13:43",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A + B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842363,
          "date": "Sat 18 Mar 2023 00:31",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A+B makes sense to me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 825481,
          "date": "Wed 01 Mar 2023 03:30",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Based on Serverless solutions used, need to go with Fargate in combination with either ECS/EC2.As company does not want to manage infra, we go for because Fargate-ECS combo as Fargate-EC2 needs more maintenance .That means D is out. E is obviously out EFS does not contribute to lambda invocation timeouts.<br>C is wrong because, increased concurrency (more lambda versions) won't solve timeouts.<br>That leaves A and B as right answers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 809640,
          "date": "Wed 15 Feb 2023 15:26",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "C is not right, question clearly said no involve infrastructure, EC2 is a infrastructure, Lamda time out 15 mins.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 793269,
          "date": "Mon 30 Jan 2023 20:59",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "lamda will time out<br>A: create Docker image and save it to ECR<br>B: run this image on Fargate",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 792049,
          "date": "Sun 29 Jan 2023 21:42",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "AB makes most sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 775484,
          "date": "Sat 14 Jan 2023 15:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B and C are correct choices for this question.<br><br>B: Creating a new Amazon Elastic Container Service (ECS) task definition with a compatibility type of AWS Fargate and adjusting the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3 can help to prevent invocation failures by breaking up the image processing work into smaller tasks that can be processed concurrently.<br><br>C: Creating an AWS Step Functions state machine with a Parallel state to invoke the Lambda function and increasing the provisioned concurrency of the Lambda function can also help to prevent invocation failures by allowing the Lambda function to handle more requests in parallel.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not a correct answer because it does not address the issue of the Lambda function timing out.<br><br>Option D is not a correct answer because it is similar to option B, but it uses Amazon EC2 instead of AWS Fargate which is a more modern and serverless way to run containerized applications.<br><br>Option E is not a correct answer because it does not address the issue of the Lambda function timing out.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 775485,
          "date": "Sat 14 Jan 2023 15:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not a correct answer because it does not address the issue of the Lambda function timing out.<br><br>Option D is not a correct answer because it is similar to option B, but it uses Amazon EC2 instead of AWS Fargate which is a more modern and serverless way to run containerized applications.<br><br>Option E is not a correct answer because it does not address the issue of the Lambda function timing out.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#64",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an organization in AWS Organizations. The company is using AWS Control Tower to deploy a landing zone for the organization. The company wants to implement governance and policy enforcement. The company must implement a policy that will detect Amazon RDS DB instances that are not encrypted at rest in the company's production OU.<br><br>Which solution will meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#64",
          "answers": [
            {
              "choice": "<p>A. Turn on mandatory guardrails in AWS Control Tower. Apply the mandatory guardrails to the production OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable the appropriate guardrail from the list of strongly recommended guardrails in AWS Control Tower. Apply the guardrail to the production OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Config to create a new mandatory guardrail. Apply the rule to all accounts in the production OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a custom SCP in AWS Control Tower. Apply the SCP to the production OU.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850957,
          "date": "Sun 26 Mar 2023 13:45",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Enable the appropriate guardrail",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 829490,
          "date": "Sun 05 Mar 2023 00:35",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Mandatory controls are owned by AWS Control Tower, and they apply to every OU on your landing zone. These controls are applied by default when you set up your landing zone, and they can't be deactivated.<br>The solution requirement falls under a proactive(Recommended Control). <br>https://docs.aws.amazon.com/controltower/latest/userguide/rds-rules.html#ct-rds-pr-16-description<br>Optional controls are OU specific.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 825505,
          "date": "Wed 01 Mar 2023 04:46",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tip - As this detective guardrail is available, answer is B.  But if the guardrail is not available in that predefined list, the answer would be --C https://aws.amazon.com/blogs/mt/aws-control-tower-detective-guardrails-as-an-aws-config-conformance-pack/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809646,
          "date": "Wed 15 Feb 2023 15:29",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "question is asking for detection, not mandate",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 787745,
          "date": "Wed 25 Jan 2023 15:38",
          "username": "\t\t\t\tpitakk\t\t\t",
          "content": "https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The only thing is that this option talks about guardrails, while the article talks about controls, not mandatory.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792053,
          "date": "Sun 29 Jan 2023 21:51",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "The only thing is that this option talks about guardrails, while the article talks about controls, not mandatory.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775495,
          "date": "Sat 14 Jan 2023 15:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  AWS Control Tower provides a set of \\\"strongly recommended guardrails\\\" that can be enabled to implement governance and policy enforcement. One of these guardrails is \\\"Encrypt Amazon RDS instances\\\" which will detect RDS DB instances that are not encrypted at rest. By enabling this guardrail and applying it to the production OU, the company will be able to enforce encryption for RDS instances in the production environment.<br><br>Option A is incorrect because mandatory guardrails are pre-defined by AWS and cannot be customized.<br>Option C is incorrect because AWS Config does not provide mandatory guardrails for RDS instances.<br>Option D is incorrect because AWS Control Tower does not provide a feature called custom SCP (Service Control Policy), it uses guardrails instead.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#65",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A startup company hosts a fleet of Amazon EC2 instances in private subnets using the latest Amazon Linux 2 AMI. The company's engineers rely heavily on SSH access to the instances for troubleshooting.<br><br>The company's existing architecture includes the following:<br><br>• A VPC with private and public subnets, and a NAT gateway.<br>• Site-to-Site VPN for connectivity with the on-premises environment.<br>• EC2 security groups with direct SSH access from the on-premises environment.<br><br>The company needs to increase security controls around SSH access and provide auditing of commands run by the engineers.<br><br>Which strategy should a solutions architect use?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#65",
          "answers": [
            {
              "choice": "<p>A. Install and configure EC2 Instance Connect on the fleet of EC2 instances. Remove all security group rules attached to EC2 instances that allow inbound TCP on port 22. Advise the engineers to remotely access the instances by using the EC2 Instance Connect CLI.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Install the Amazon CloudWatch agent on all EC2 instances and send operating system audit logs to CloudWatch Logs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the EC2 security groups to only allow inbound TCP on port 22 to the IP addresses of the engineer's devices. Enable AWS Config for EC2 security group resource changes. Enable AWS Firewall Manager and apply a security group policy that automatically remediates changes to rules.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. Attach the IAM role to all the EC2 instances. Remove all security group rules attached to the EC2 instances that allow inbound TCP on port 22. Have the engineers install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850959,
          "date": "Sun 26 Mar 2023 13:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D for sure.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 829492,
          "date": "Sun 05 Mar 2023 00:43",
          "username": "\t\t\t\tAjani\t\t\t",
          "content": "Why its NOT A<br>To connect using the Amazon EC2 console, the instance must have a public IPv4 address.<br><br>If the instance does not have a public IP address, you can connect to the instance over a private network using an SSH client or the EC2 Instance Connect CLI. For example, you can connect from within the same VPC or through a VPN connection, transit gateway, or AWS Direct Connect.<br><br>EC2 Instance Connect does not support connecting using an IPv6 address.<br>going with D:",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827297,
          "date": "Thu 02 Mar 2023 20:10",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A is wrong because Instance connect does not provided auditing<br>B is wrong because it mentions OS audit logs. we need to audit SSH trafic<br>C is wrong because we want to audit not remediate as asked in question. config service is to record <br>using predefined rules and remediate as well<br><br>D is correct because,<br>By attaching the AmazonSSMManagedInstanceCore policy to an IAM role, EC2 instances can be controlled and monitored through the Systems Manager service, enabling capabilities such as remote instance management, patching, and compliance reporting. (ChatGPT response its answers are brief and helpful sometimes)",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 814694,
          "date": "Mon 20 Feb 2023 01:36",
          "username": "\t\t\t\tlygf\t\t\t",
          "content": "Need to be able to audit the commands ran on the machine.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 810003,
          "date": "Wed 15 Feb 2023 22:34",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "I don't understand why it can't be A for this one. Why is AWS Systems Manager Session better than EC2 Instance Connect? They both require installing something on the instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For EC2 instance connect there are a few requirements:<br>- instance has public IP (the instances in question are private)<br>- you have port 22 open (A says remove port 22 inbound)</li><li>Could option A audit the commands ran on the server, as required by the question? I knew D certainly can.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829378,
          "date": "Sat 04 Mar 2023 21:23",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "For EC2 instance connect there are a few requirements:<br>- instance has public IP (the instances in question are private)<br>- you have port 22 open (A says remove port 22 inbound)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814693,
          "date": "Mon 20 Feb 2023 01:35",
          "username": "\t\t\t\tlygf\t\t\t",
          "content": "Could option A audit the commands ran on the server, as required by the question? I knew D certainly can.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804943,
          "date": "Sat 11 Feb 2023 04:09",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT,<br><br>Yes, AWS Systems Manager Session Manager can track the commands that are executed during a session. The session is recorded in the form of a log, which can be accessed and reviewed later. The log contains information such as the start time, end time, and the user who initiated the session, as well as a record of all the commands executed during the session, including their output and exit codes. This information can be useful for auditing purposes, troubleshooting, and compliance reporting.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 802678,
          "date": "Thu 09 Feb 2023 00:52",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "provide auditing of commands run by the engineers= B Only",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775506,
          "date": "Sat 14 Jan 2023 15:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  This strategy uses IAM roles and AWS Systems Manager to provide secure and auditable SSH access to the instances. The IAM role is attached to all the EC2 instances and has the AmazonSSMManagedInstanceCore managed policy attached, which allows the instances to be managed by Systems Manager. The engineers then install the AWS Systems Manager Session Manager plugin for their devices and remotely access the instances by using the start-session API call from Systems Manager. This approach provides secure and auditable access to the instances without the need for IP-based security group rules or additional infrastructure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A uses EC2 Instance Connect to provide secure and auditable SSH access to the instances, but it requires additional infrastructure and configuration. <br><br>Option B provides auditing of commands run by the engineers, but it relies on IP-based security group rules, which can be difficult to manage and may not be as secure as using IAM roles. <br><br>Option C uses AWS Config and Firewall Manager to automatically remediate changes to security group rules, but it still relies on IP-based security group rules and does not provide an auditable method of access to the instances.</li><li>For option A to work, the following additional infrastructure and configuration would be required:<br><br>The EC2 Instance Connect service needs to be enabled in the AWS account and the appropriate IAM permissions would need to be granted to the engineers.<br><br>The EC2 instances would need to have the EC2 Instance Connect agent installed and configured.<br><br>The engineers would need to install the EC2 Instance Connect CLI on their devices and have the necessary credentials to authenticate with AWS.<br><br>In addition, the company would need to update their processes and procedures to ensure that engineers are only using EC2 Instance Connect to access the instances and that all access is being logged and audited.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 775507,
          "date": "Sat 14 Jan 2023 15:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A uses EC2 Instance Connect to provide secure and auditable SSH access to the instances, but it requires additional infrastructure and configuration. <br><br>Option B provides auditing of commands run by the engineers, but it relies on IP-based security group rules, which can be difficult to manage and may not be as secure as using IAM roles. <br><br>Option C uses AWS Config and Firewall Manager to automatically remediate changes to security group rules, but it still relies on IP-based security group rules and does not provide an auditable method of access to the instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For option A to work, the following additional infrastructure and configuration would be required:<br><br>The EC2 Instance Connect service needs to be enabled in the AWS account and the appropriate IAM permissions would need to be granted to the engineers.<br><br>The EC2 instances would need to have the EC2 Instance Connect agent installed and configured.<br><br>The engineers would need to install the EC2 Instance Connect CLI on their devices and have the necessary credentials to authenticate with AWS.<br><br>In addition, the company would need to update their processes and procedures to ensure that engineers are only using EC2 Instance Connect to access the instances and that all access is being logged and audited.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775508,
          "date": "Sat 14 Jan 2023 15:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "For option A to work, the following additional infrastructure and configuration would be required:<br><br>The EC2 Instance Connect service needs to be enabled in the AWS account and the appropriate IAM permissions would need to be granted to the engineers.<br><br>The EC2 instances would need to have the EC2 Instance Connect agent installed and configured.<br><br>The engineers would need to install the EC2 Instance Connect CLI on their devices and have the necessary credentials to authenticate with AWS.<br><br>In addition, the company would need to update their processes and procedures to ensure that engineers are only using EC2 Instance Connect to access the instances and that all access is being logged and audited.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#66",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company that uses AWS Organizations allows developers to experiment on AWS. As part of the landing zone that the company has deployed, developers use their company email address to request an account. The company wants to ensure that developers are not launching costly services or running services unnecessarily. The company must give developers a fixed monthly budget to limit their AWS costs.<br><br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BCF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#66",
          "answers": [
            {
              "choice": "<p>A. Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an SCP to deny access to costly services and components. Apply the SCP to the developer accounts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM policy to deny access to costly services and components. Apply the IAM policy to the developer accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an AWS Budgets alert action to terminate services when the budgeted amount is reached. Configure the action to terminate all services.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850961,
          "date": "Sun 26 Mar 2023 13:53",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "BCF -SCP is more efficient at restrictions than using IAM across accounts.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 841507,
          "date": "Fri 17 Mar 2023 03:00",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "First sentence \\\"A company that uses AWS Organizations...\\\" -<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html<br><br>It is BCF - when they are granted access to the AWS Organization, you will want to set the SCP for the: OrganizationAccountAccessRole. Yes, you \\\"could create\\\" a new IAM role specific to developers, but you can create a SCP for only what is necessary for the developers to do their job.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 834006,
          "date": "Thu 09 Mar 2023 14:41",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "Invoke an AWS Lambda function to terminate all services<br>Is there a Lambda to terminate all services?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823841,
          "date": "Mon 27 Feb 2023 15:57",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "I ignore everyone here answer includes C and D. <br>\\\"deny access to costly services and components.\\\" What does that mean? WHO is going to decide which services are costly one by one? Come on guys.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer should be A-B-FA.  Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.B.  Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.F.  Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823852,
          "date": "Mon 27 Feb 2023 16:01",
          "username": "\t\t\t\tlkyixoayffasdrlaqd\t\t\t",
          "content": "Answer should be A-B-FA.  Create an SCP to set a fixed monthly account usage limit. Apply the SCP to the developer accounts.B.  Use AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process.F.  Create an AWS Budgets alert action to send an Amazon Simple Notification Service (Amazon SNS) notification when the budgeted amount is reached. Invoke an AWS Lambda function to terminate all services.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 820771,
          "date": "Fri 24 Feb 2023 18:24",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "I prefer D over C as IAM cant be applied to Account",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 819955,
          "date": "Fri 24 Feb 2023 01:55",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "Not D as you can't apply IAM policy to an AWS Account.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 812390,
          "date": "Fri 17 Feb 2023 22:18",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "BCF<br>I dont think C is a valid answer [ strictly speaking ], <br>In a Landing Zone you cant apply SCPs directly to accounts, <br>You apply guardrails or controls,,,, that can have scp inside as artifacts. <br><br>Either way an account is not a principal you can apply either an IAM policy to an account.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 811171,
          "date": "Thu 16 Feb 2023 23:46",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Clear - BCF - SCP is preferable over IAM",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 810272,
          "date": "Thu 16 Feb 2023 06:16",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The questions is asking which steps will ensure that developers are not launching costly services or running services unnecessarily that will meet the requirements.Both E and F mention \\\"Terminate All Services\\\", which implies it will terminate all of the services once the budget is exceeded.B and C are correct and so is D by process of elimination.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The moderator remove the above comment and forgot to see the word running services so BCF seems logical in this case.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810277,
          "date": "Thu 16 Feb 2023 06:21",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The moderator remove the above comment and forgot to see the word running services so BCF seems logical in this case.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810009,
          "date": "Wed 15 Feb 2023 22:37",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "BDF<br>I see some votes for C over D, but that can't be because you can't apply a policy to an account. Additionally, an SCP would make more sense for a situation where you don't want anyone in the account to use the services",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 809653,
          "date": "Wed 15 Feb 2023 15:37",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "IAM should apply to user/groups, not accounts",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 806628,
          "date": "Sun 12 Feb 2023 18:56",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "Thinking A because perhaps you can do the below:<br>{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": \\\"*\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>},<br>{<br>\\\"Sid\\\": \\\"DenyCloudTrail\\\",<br>\\\"Effect\\\": \\\"Deny\\\",<br>\\\"Action\\\": \\\"cloudtrail:*\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>},<br>{<br>\\\"Sid\\\": \\\"AllowS3CreateBucket\\\",<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": \\\"s3:CreateBucket\\\",<br>\\\"Resource\\\": \\\"*\\\"<br>}<br>]<br>}<br>The first \\\"Allow\\\" statement in the SCP allows all actions on all resources, which would allow the creation of S3 buckets. However, the second \\\"Deny\\\" statement specifically denies all cloudtrail actions, which could potentially impact the ability to create S3 buckets if there is a dependency on cloudtrail for that action. To ensure that the developers are able to create S3 buckets, a new statement with \\\"Allow\\\" effect for the s3:CreateBucket action should be added to the SCP.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804960,
          "date": "Sat 11 Feb 2023 04:34",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I mean BCF",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 804957,
          "date": "Sat 11 Feb 2023 04:33",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I go for C vs D because accounts usually mean AWS account in the context of AWS Organization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 795498,
          "date": "Wed 01 Feb 2023 21:24",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "BCF is the correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 793275,
          "date": "Mon 30 Jan 2023 21:06",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "BDF seems ok",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 787088,
          "date": "Wed 25 Jan 2023 00:42",
          "username": "\t\t\t\tharleydog\t\t\t",
          "content": "You don't attach a policy to an account, you attach a policy to a user, group, or role.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#67",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has applications in an AWS account that is named Source. The account is in an organization in AWS Organizations. One of the applications uses AWS Lambda functions and stores inventory data in an Amazon Aurora database. The application deploys the Lambda functions by using a deployment package. The company has configured automated backups for Aurora.<br><br>The company wants to migrate the Lambda functions and the Aurora database to a new AWS account that is named Target. The application processes critical data, so the company must minimize downtime.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#67",
          "answers": [
            {
              "choice": "<p>A. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the automated Aurora DB cluster snapshot with the Target account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Download the Lambda function deployment package from the Source account. Use the deployment package and create new Lambda functions in the Target account. Share the Aurora DB cluster with the Target account by using AWS Resource Access Manager {AWS RAM). Grant the Target account permission to clone the Aurora DB cluster.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions and the Aurora DB cluster with the Target account. Grant the Target account permission to clone the Aurora DB cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Resource Access Manager (AWS RAM) to share the Lambda functions with the Target account. Share the automated Aurora DB cluster snapshot with the Target account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850962,
          "date": "Sun 26 Mar 2023 13:55",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the way forward",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827395,
          "date": "Thu 02 Mar 2023 21:20",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "AWS RAM can share ec2 instances,lambdas, DB clusters, RDS, event Redshift clusters.<br>Refer AWS SA video here - https://www.youtube.com/watch?v=KL9SICG52zY<br>If company would not have had critical data, answer C is good. as existing app should not be down, we have to download lambda and then share. so answer is B.  other wise you can stop app and share with RAM (Resource shares)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793281,
          "date": "Mon 30 Jan 2023 21:12",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B is correct. Move Lambda and Aurora both to target account",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792074,
          "date": "Sun 29 Jan 2023 22:19",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B can be done with this: https://aws.amazon.com/about-aws/whats-new/2019/07/amazon_aurora_supportscloningacrossawsaccounts-/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 782866,
          "date": "Sat 21 Jan 2023 01:02",
          "username": "\t\t\t\tSK_Cert_master\t\t\t",
          "content": "B. <br>It seems that Lambda cannot be shared via RAM<br>https://docs.aws.amazon.com/ram/latest/userguide/shareable.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As per the above link, Lambda can be shared. Please see the \\\"Subnets\\\" section .</li><li>You cannot share lambda, but creating a Lambda in a shared subnet is allowed.</li><li>scratch that</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824843,
          "date": "Tue 28 Feb 2023 14:29",
          "username": "\t\t\t\tSatya80\t\t\t",
          "content": "As per the above link, Lambda can be shared. Please see the \\\"Subnets\\\" section .<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You cannot share lambda, but creating a Lambda in a shared subnet is allowed.</li><li>scratch that</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825359,
          "date": "Tue 28 Feb 2023 23:46",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "You cannot share lambda, but creating a Lambda in a shared subnet is allowed.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824917,
          "date": "Tue 28 Feb 2023 15:45",
          "username": "\t\t\t\tSatya80\t\t\t",
          "content": "scratch that",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776955,
          "date": "Sun 15 Jan 2023 19:59",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct. Move Lambda and Aurora both to target account<br>A: not move Aurora <br>C: Lambda not move<br>d: Lambda and Aurora both not moved",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775641,
          "date": "Sat 14 Jan 2023 16:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option B.  This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime.<br><br>In this solution, the Lambda function deployment package is downloaded from the Source account and used to create new Lambda functions in the Target account. The Aurora DB cluster is shared with the Target account using AWS RAM and the Target account is granted permission to clone the Aurora DB cluster, allowing for a new copy of the Aurora database to be created in the Target account. This approach allows for the data to be migrated to the Target account while minimizing downtime, as the Target account can use the cloned Aurora database while the original Aurora database continues to be used in the Source account.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not the best solution because it doesn't share the Aurora DB cluster with the Target account and this would cause data inconsistencies as the Source and Target accounts would not share the same data.<br><br>Option C is not the best solution because, it does not specify how the data will be migrated and it would cause downtime as the Source and Target accounts are not sharing the same data.<br><br>Option D is not the best solution because it does not specify how the Lambda function will be migrated and it would cause data inconsistencies as the Source and Target accounts are not sharing the same data.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775643,
          "date": "Sat 14 Jan 2023 16:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not the best solution because it doesn't share the Aurora DB cluster with the Target account and this would cause data inconsistencies as the Source and Target accounts would not share the same data.<br><br>Option C is not the best solution because, it does not specify how the data will be migrated and it would cause downtime as the Source and Target accounts are not sharing the same data.<br><br>Option D is not the best solution because it does not specify how the Lambda function will be migrated and it would cause data inconsistencies as the Source and Target accounts are not sharing the same data.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#68",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a Python script on an Amazon EC2 instance to process data. The script runs every 10 minutes. The script ingests files from an Amazon S3 bucket and processes the files. On average, the script takes approximately 5 minutes to process each file The script will not reprocess a file that the script has already processed.<br><br>The company reviewed Amazon CloudWatch metrics and noticed that the EC2 instance is idle for approximately 40% of the time because of the file processing speed. The company wants to make the workload highly available and scalable. The company also wants to reduce long-term management overhead.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#68",
          "answers": [
            {
              "choice": "<p>A. Migrate the data processing script to an AWS Lambda function. Use an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure Amazon S3 to send event notifications to the SQS queue. Create an EC2 Auto Scaling group with a minimum size of one instance. Update the data processing script to poll the SQS queue. Process the S3 objects that the SQS message identifies.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the data processing script to a container image. Run the data processing container on an EC2 instance. Configure the container to poll the S3 bucket for new objects and to process the resulting objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the data processing script to a container image that runs on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Create an AWS Lambda function that calls the Fargate RunTaskAPI operation when the container processes the file. Use an S3 event notification to invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775652,
          "date": "Sat 14 Jan 2023 16:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A, migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects. This solution meets the company's requirements of high availability and scalability, as well as reducing long-term management overhead, and is likely to be the most cost-effective option.<br><br>Option B involves creating an SQS queue and configuring S3 to send event notifications to it. The data processing script would then poll the SQS queue and process the S3 objects that the SQS message identifies. While this option also provides high availability and scalability, it is less cost-effective than using Lambda, as it requires additional resources such as an SQS queue and an EC2 Auto Scaling group.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, migrating the data processing script to a container image and running it on an EC2 instance, would still require the company to manage the underlying EC2 instances and may not be as cost-effective as using Lambda.<br><br>Option D, migrating the data processing script to a container image that runs on Amazon ECS on AWS Fargate, would still require the company to manage the underlying infrastructure and may not be as cost-effective as using Lambda. Additionally, it introduces additional complexity by adding a Lambda function that calls the Fargate RunTask API operation.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 775653,
          "date": "Sat 14 Jan 2023 16:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C, migrating the data processing script to a container image and running it on an EC2 instance, would still require the company to manage the underlying EC2 instances and may not be as cost-effective as using Lambda.<br><br>Option D, migrating the data processing script to a container image that runs on Amazon ECS on AWS Fargate, would still require the company to manage the underlying infrastructure and may not be as cost-effective as using Lambda. Additionally, it introduces additional complexity by adding a Lambda function that calls the Fargate RunTask API operation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776962,
          "date": "Sun 15 Jan 2023 20:06",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct, it provide HA, scale, less management. Task only need 5 minutes<br>B: enen more complex<br>C: container still run on one EC2, not scale<br>d: need container, Farget and Lambda. Complex than A",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 850965,
          "date": "Sun 26 Mar 2023 13:58",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Migrate the data processing script to an AWS Lambda function.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 849977,
          "date": "Sat 25 Mar 2023 10:50",
          "username": "\t\t\t\tAsagumo\t\t\t",
          "content": "There are two points of concern when choosing Lambda in the following two ways<br>The fact that the original EC2 specs are so fast that it may take only 5 minutes to complete.<br>The fact that the average time is only 5 minutes, so there may be cases where the time exceeds 15 minutes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842398,
          "date": "Sat 18 Mar 2023 02:11",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A best practice to handle files in S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 830409,
          "date": "Mon 06 Mar 2023 00:03",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "It asks for the most cost effective solution.<br>While Lambda may be simple and cheap for if you have only a few invocations and low memory requirements. <br>As processing is called every 10 minutes. The EC2 is indeed idle for 40% of the time, 60% of the time its under load.But we are asked to look at how it scales - in regards to cost.<br>We have a 60% used EC2. Lambda costs explode when it scales.<br>Lambda is the by far most expensive solution.<br>B) is more cost effective.<br>(Who votes for Lambda when it comes to cost for processing big load, never had to pay the AWS bill for it.)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Also \\\"long-term management overhead\\\" should be reduced. Ec2 the long term management overhead is way lower than maintaining Lambda.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 830413,
          "date": "Mon 06 Mar 2023 00:12",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Also \\\"long-term management overhead\\\" should be reduced. Ec2 the long term management overhead is way lower than maintaining Lambda.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827456,
          "date": "Thu 02 Mar 2023 22:28",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A and D are good but A is most cost effective as asked in question. B has only one instance that means not highly available. C has container/ec2 combo with more work on ec2 which is cost ineffective and more operating effort.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D is not cost effective and not good.. (meant C in above comment)</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 827458,
          "date": "Thu 02 Mar 2023 22:29",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "D is not cost effective and not good.. (meant C in above comment)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793296,
          "date": "Mon 30 Jan 2023 21:29",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "the script takes approximately 5 minutes==>Lamda is the simpliest soltion (compared to D)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#69",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A financial services company in North America plans to release a new online web application to its customers on AWS. The company will launch the application in the us-east-1 Region on Amazon EC2 instances. The application must be highly available and must dynamically scale to meet user traffic. The company also wants to implement a disaster recovery environment for the application in the us-west-1 Region by using active-passive failover.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#69",
          "answers": [
            {
              "choice": "<p>A. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPC.  Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPC.  Place the Auto Scaling group behind the ALSet up the same configuration in the us-west-1 VPC.  Create an Amazon Route 53 hosted zone. Create separate records for each ALEnable health checks to ensure high availability between Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPC in us-east-1 and a VPC in us-west-1. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in that VPCreate an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in the us-east-1 VPPlace the Auto Scaling group behind the ALB.  Set up the same configuration in the us-west-1 VPCreate an Amazon Route 53 hosted zone. Create separate records for each ALB.  Enable health checks and configure a failover routing policy for each record.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a VPC in us-east-1 and a VPC in us-west-1. Configure VPC peering. In the us-east-1 VPC, create an Application Load Balancer (ALB) that extends across multiple Availability Zones in both VPCs. Create an Auto Scaling group that deploys the EC2 instances across the multiple Availability Zones in both VPCs. Place the Auto Scaling group behind the ALB.  Create an Amazon Route 53 hosted zone. Create a record for the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775667,
          "date": "Sat 14 Jan 2023 17:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Choice C meets the requirements for the application to be highly available and to dynamically scale to meet user traffic, as well as implementing a disaster recovery environment in the us-west-1 Region through active-passive failover.<br><br>In choice C, the company creates a VPC in us-east-1 and a VPC in us-west-1, and sets up an Application Load Balancer (ALB) and Auto Scaling group in both VPCs. The ALB extends across multiple Availability Zones in each VPC, and the Auto Scaling group deploys the EC2 instances across these Availability Zones. The Auto Scaling group is placed behind the ALB, which allows for automatic scaling of the instances to meet user traffic.<br><br>An Amazon Route 53 hosted zone is also created, with separate records for each ALB.  Health checks are enabled for each record, and a failover routing policy is configured. This allows for active-passive failover between the two regions, ensuring high availability for the application.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Choice A, B, and D do not fully meet the requirements of the disaster recovery environment in the us-west-1 Region and the failover routing policy because they do not include the necessary configurations for active-passive failover.<br><br>In choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br><br>Choice B, the VPCs in us-east-1 and us-west-1 are separate, and the configuration is replicated in both regions but there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.</li><li>Choice D is similar to choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br><br>Choice C is the correct answer as it includes all the necessary components for a disaster recovery environment in the us-west-1 region. It creates separate VPCs, Application Load Balancer, and Auto Scaling Group in both regions, and it enables health checks and configure a failover routing policy for each record. This ensures that in the event of an outage, the application can automatically failover to the us-west-1 region with minimal downtime.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 775668,
          "date": "Sat 14 Jan 2023 17:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Choice A, B, and D do not fully meet the requirements of the disaster recovery environment in the us-west-1 Region and the failover routing policy because they do not include the necessary configurations for active-passive failover.<br><br>In choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br><br>Choice B, the VPCs in us-east-1 and us-west-1 are separate, and the configuration is replicated in both regions but there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Choice D is similar to choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br><br>Choice C is the correct answer as it includes all the necessary components for a disaster recovery environment in the us-west-1 region. It creates separate VPCs, Application Load Balancer, and Auto Scaling Group in both regions, and it enables health checks and configure a failover routing policy for each record. This ensures that in the event of an outage, the application can automatically failover to the us-west-1 region with minimal downtime.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775671,
          "date": "Sat 14 Jan 2023 17:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Choice D is similar to choice A, the VPCs in us-east-1 and us-west-1 are peered and the Auto Scaling group and Application Load Balancer (ALB) are extended across multiple availability zones in both regions. However, there is no explicit failover routing policy configured, so it is not clear how the application would failover to the us-west-1 region in the event of an outage.<br><br>Choice C is the correct answer as it includes all the necessary components for a disaster recovery environment in the us-west-1 region. It creates separate VPCs, Application Load Balancer, and Auto Scaling Group in both regions, and it enables health checks and configure a failover routing policy for each record. This ensures that in the event of an outage, the application can automatically failover to the us-west-1 region with minimal downtime.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 850967,
          "date": "Sun 26 Mar 2023 14:00",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C for DR",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827524,
          "date": "Thu 02 Mar 2023 23:59",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Active-Passive failover with primary and secondary records in Route53<br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html<br>https://d1tcczg8b21j1t.cloudfront.net/strapi-assets/32_Route_53_health_checks_4_64165fc533.png<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>VPC Peering is good for fully accessing all resources in a shared env but thats not asked here, so A and D gets eliminated. B does not mention the weighted routing config enable ment although setup is good. So answer is C</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827525,
          "date": "Fri 03 Mar 2023 00:03",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "VPC Peering is good for fully accessing all resources in a shared env but thats not asked here, so A and D gets eliminated. B does not mention the weighted routing config enable ment although setup is good. So answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793300,
          "date": "Mon 30 Jan 2023 21:33",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "active-passive failover==>a failover routing policy within route 53",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776964,
          "date": "Sun 15 Jan 2023 20:07",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#70",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an environment that has a single AWS account. A solutions architect is reviewing the environment to recommend what the company could improve specifically in terms of access to the AWS Management Console. The company's IT support workers currently access the console for administrative tasks, authenticating with named IAM users that have been mapped to their job role.<br><br>The IT support workers no longer want to maintain both their Active Directory and IAM user accounts. They want to be able to access the console by using their existing Active Directory credentials. The solutions architect is using AWS IAM Identity Center (AWS Single Sign-On) to implement this functionality.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#70",
          "answers": [
            {
              "choice": "<p>A. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory. Configure IAM Identity Center and set the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an organization in AWS Organizations. Turn on the IAM Identity Center feature in Organizations. Create and configure an AD Connector to connect to the company's on-premises Active Directory. Configure IAM Identity Center and select the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company's Active Directory.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory. Configure IAM Identity Center and select the AWS Managed Microsoft AD directory as the identity source. Create permission sets and map them to the existing groups within the AWS Managed Microsoft AD directory.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an organization in AWS Organizations. Turn on all features for the organization. Create and configure an AD Connector to connect to the company's on-premises Active Directory. Configure IAM Identity Center and set the AD Connector as the identity source. Create permission sets and map them to the existing groups within the company's Active Directory.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 778224,
          "date": "Mon 16 Jan 2023 21:35",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/69172-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>You are correct, I apologize for the oversight. To meet the requirements of the IT support workers, option D would be the correct solution:<br><br>This option will first enable all features in AWS Organizations, then create and configure an AD Connector to connect to the company's on-premises Active Directory. Then, it will configure IAM Identity Center (AWS SSO) and set the AD Connector as the identity source, allowing the IT support workers to access the console using their existing Active Directory credentials. Finally, it will create permission sets and map them to the existing groups within the company's Active Directory. This solution will also be cost-effective as it does not involve creating a new directory in AWS Directory Service.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 853716,
          "date": "Tue 28 Mar 2023 23:19",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "D as Vherman said below",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 850969,
          "date": "Sun 26 Mar 2023 14:06",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Option D is the most cost-effective",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 843198,
          "date": "Sat 18 Mar 2023 23:05",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "D is the correct answer.. B is wrong answer <br><br>From aws documentation:<br>Q: Which AWS accounts can I connect to IAM Identity Center?<br><br>You can add any AWS account managed using AWS Organizations to IAM Identity Center. You need to enable all features in your organizations to manage your accounts single sign-on.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 837878,
          "date": "Mon 13 Mar 2023 12:12",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "D is correct<br>There is no IAM Identity Center feature in Organizations. hence, B is out<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes there is and it should be all you need to enable, therefore B is correct.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 846152,
          "date": "Tue 21 Mar 2023 17:17",
          "username": "\t\t\t\tsenhorjorge\t\t\t",
          "content": "Yes there is and it should be all you need to enable, therefore B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829394,
          "date": "Sat 04 Mar 2023 21:39",
          "username": "\t\t\t\tanita_student\t\t\t",
          "content": "See pre-requisites for AWS SSO: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827608,
          "date": "Fri 03 Mar 2023 03:40",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Question : Does aws AD connector configuration needs aws organization features turned on ?<br><br>ChatGPT : Answer No, AWS Microsoft AD Connector does not require AWS Organization features to be turned on.<br><br>The AWS Microsoft AD Connector is a standalone service that enables you to connect your AWS resources to an existing Microsoft Active Directory (AD) domain or forest. It does not depend on or require any specific AWS organization features or settings.<br><br>However, if you are using AWS Directory Service to create a new AD directory in AWS, you can choose to enable AWS Organizations integration to simplify the management of multiple AWS accounts. This integration allows you to manage AWS Directory Service directories across multiple AWS accounts and regions from a single master account.<br><br>But again, this is an optional feature and does not affect the functionality of the AWS Microsoft AD Connector itself. You can use the AWS Microsoft AD Connector without enabling AWS Organizations integration if you prefer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This link someone posted https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html<br>is in favor of D.  but I think, here in question , its a single account only. I am resistant to choose D, I could be wrong though.</li><li>https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html<br><br>I assume single account is Management account and it does not need org features enabled.See above link and follow all steps listed there. no necessity to enable organizations features. I stick with B only.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827612,
          "date": "Fri 03 Mar 2023 03:45",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "This link someone posted https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html<br>is in favor of D.  but I think, here in question , its a single account only. I am resistant to choose D, I could be wrong though.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827630,
          "date": "Fri 03 Mar 2023 04:17",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html<br><br>I assume single account is Management account and it does not need org features enabled.See above link and follow all steps listed there. no necessity to enable organizations features. I stick with B only.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814656,
          "date": "Mon 20 Feb 2023 00:20",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Correcting the Answer - Its D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 811198,
          "date": "Fri 17 Feb 2023 00:30",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "B - Why need all feature<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's D.  You need it s explained in https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html</li><li>Thanks, so its not B. </li><li>Exactly, it's not B.  Additionally, B refers to enabling IAM Identity Center in Organizations, and you would enable that in the IAM Identity Center console. What's confusing about D is that it does not refer to enabling it.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 812999,
          "date": "Sat 18 Feb 2023 14:21",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "It's D.  You need it s explained in https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks, so its not B. </li><li>Exactly, it's not B.  Additionally, B refers to enabling IAM Identity Center in Organizations, and you would enable that in the IAM Identity Center console. What's confusing about D is that it does not refer to enabling it.</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 814655,
          "date": "Mon 20 Feb 2023 00:19",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Thanks, so its not B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Exactly, it's not B.  Additionally, B refers to enabling IAM Identity Center in Organizations, and you would enable that in the IAM Identity Center console. What's confusing about D is that it does not refer to enabling it.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815554,
          "date": "Mon 20 Feb 2023 17:46",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Exactly, it's not B.  Additionally, B refers to enabling IAM Identity Center in Organizations, and you would enable that in the IAM Identity Center console. What's confusing about D is that it does not refer to enabling it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 811093,
          "date": "Thu 16 Feb 2023 21:58",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "This one is tricky because in order to enable SSO in Organizations you need to enable all features. Thanks @moota for the explanation",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 809669,
          "date": "Wed 15 Feb 2023 15:59",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "just need a feature with AD connector",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 804976,
          "date": "Sat 11 Feb 2023 05:01",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "There are only two feature sets to turn on.<br>All features – The default feature set that is available to AWS Organizations. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization.<br><br>Consolidated billing – This feature set provides shared billing functionality, but doesn't include the more advanced features of AWS Organizations. For example, you can't enable other AWS services to integrate with your organization to work across all of its accounts, or use policies to restrict what users and roles in different accounts can do. To use the advanced AWS Organizations features, you must enable all features in your organization.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The keyword is \\\"Accounts\\\" vs \\\"single account\\\", why is All Features required for a single account?</li><li>Take a look at this link: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html scroll down to \\\"If you've already set up AWS Organizations, make sure that all features are enabled. \\\"</li><li>Because the ONLY OTHER option is to enable Consolidated Billing, which is of no use here, hence All Features must be enabled</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 810287,
          "date": "Thu 16 Feb 2023 06:41",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The keyword is \\\"Accounts\\\" vs \\\"single account\\\", why is All Features required for a single account?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Take a look at this link: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html scroll down to \\\"If you've already set up AWS Organizations, make sure that all features are enabled. \\\"</li><li>Because the ONLY OTHER option is to enable Consolidated Billing, which is of no use here, hence All Features must be enabled</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825368,
          "date": "Tue 28 Feb 2023 23:58",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Take a look at this link: https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqs-considerations.html scroll down to \\\"If you've already set up AWS Organizations, make sure that all features are enabled. \\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819972,
          "date": "Fri 24 Feb 2023 02:16",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "Because the ONLY OTHER option is to enable Consolidated Billing, which is of no use here, hence All Features must be enabled",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795518,
          "date": "Wed 01 Feb 2023 21:50",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "AWS SSO is configured to use AD Connector as an identity source<br>https://controltower.aws-management.tools/aa/sso/ad_connector/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790732,
          "date": "Sat 28 Jan 2023 16:35",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A<br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html<br>https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html<br>Option B is wrong. AD Connector is a directory gateway that can redirect directory requests to your self-managed AD.  Hence you cannot create permission sets and map them with the AD. <br>Option C and D are wrong. There is no need to enable all features in AWS Organizations.<br>https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-sso.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html<br><br>AWS IAM Identity Center (successor to AWS Single Sign-On) \\\"requires a two-way trust\\\" so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to \\\"permission sets\\\" or applications.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791109,
          "date": "Sat 28 Jan 2023 22:26",
          "username": "\t\t\t\tKazr\t\t\t",
          "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html<br><br>AWS IAM Identity Center (successor to AWS Single Sign-On) \\\"requires a two-way trust\\\" so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to \\\"permission sets\\\" or applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776968,
          "date": "Sun 15 Jan 2023 20:09",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775684,
          "date": "Sat 14 Jan 2023 17:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B. <br><br>In this solution, the company creates an organization in AWS Organizations, turns on the IAM Identity Center feature in Organizations, creates and configures an AD Connector to connect to the company's on-premises Active Directory, configures IAM Identity Center and selects the AD Connector as the identity source, and creates permission sets and maps them to the existing groups within the company's Active Directory.<br><br>This solution meets the requirements of the company's IT support workers, as it allows them to use their existing Active Directory credentials to access the AWS Management Console. Additionally, the solution is most cost-effective as it only uses the necessary features of AWS Organizations and IAM Identity Center to achieve the desired functionality without unnecessary costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is correct.. question explicitly says \\\"one account\\\". No need to enable all features in this case.</li><li>Choice A is not the best solution because it creates and configures a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory, which may lead to additional costs and complexity.<br>Choice C and D are not the best solutions as they turn on all features for the organization, which is not necessary to achieve the desired functionality and may lead to additional costs and complexity.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 804244,
          "date": "Fri 10 Feb 2023 12:30",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "B is correct.. question explicitly says \\\"one account\\\". No need to enable all features in this case.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775685,
          "date": "Sat 14 Jan 2023 17:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Choice A is not the best solution because it creates and configures a directory in AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with a two-way trust to the company's on-premises Active Directory, which may lead to additional costs and complexity.<br>Choice C and D are not the best solutions as they turn on all features for the organization, which is not necessary to achieve the desired functionality and may lead to additional costs and complexity.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#71",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A video streaming company recently launched a mobile app for video sharing. The app uploads various files to an Amazon S3 bucket in the us-east-1 Region. The files range in size from 1 GB to 10 GB. <br><br>Users who access the app from Australia have experienced uploads that take long periods of time. Sometimes the files fail to completely upload for these users. A solutions architect must improve the app's performance for these uploads.<br><br>Which solutions will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#71",
          "answers": [
            {
              "choice": "<p>A. Enable S3 Transfer Acceleration on the S3 bucket. Configure the app to use the Transfer Acceleration endpoint for uploads.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure an S3 bucket in each Region to receive the uploads. Use S3 Cross-Region Replication to copy the files to the distribution S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the app to break the video files into chunks. Use a multipart upload to transfer files to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Modify the app to add random prefixes to the files before uploading.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850970,
          "date": "Sun 26 Mar 2023 14:08",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AD all day",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 839772,
          "date": "Wed 15 Mar 2023 11:23",
          "username": "\t\t\t\taqiao\t\t\t",
          "content": "B is not suitable here, since it wants to improve upload experience, not download",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 793312,
          "date": "Mon 30 Jan 2023 21:43",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Transfer Accelerator + Multi-part uploads for files more 500MB",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 793084,
          "date": "Mon 30 Jan 2023 18:13",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I like AD but I am unsure. If the users in US don't complain about issues, it must be because multi-part upload is already enabled, otherwise it would fail 50% of the times. If only Australia users complain, it must be something else... Maybe A+B is a better option, although B is not the most cost efficient certainly.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776971,
          "date": "Sun 15 Jan 2023 20:11",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AD is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775689,
          "date": "Sat 14 Jan 2023 17:14",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/74177-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>The correct answers would be A and D. <br>A.  Enabling S3 Transfer Acceleration on the S3 bucket and configuring the app to use the Transfer Acceleration endpoint for uploads will improve the app's performance for users in Australia by providing a fast and secure way to transfer large files over the Internet.<br>D.  Configuring the app to break the video files into chunks and using a multipart upload to transfer files to Amazon S3, will improve the app's performance for users in Australia by allowing them to upload large files in parallel, which can increase upload speed and reduce the risk of upload failures.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B.  Configuring an S3 bucket in each Region to receive the uploads and using S3 Cross-Region Replication to copy the files to the distribution S3 bucket is not the most cost-effective solution for this specific use case.<br>C.  Setting up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region is not a solution that would improve the performance of the uploads specifically for users in Australia.<br>E.  Modifying the app to add random prefixes to the files before uploading will not improve the app's performance for users in Australia.</li><li>yes, it will. Other options are more important, but sure random (rsp. any hash that distributes well) prefixes improve performance a lot.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 775690,
          "date": "Sat 14 Jan 2023 17:14",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Configuring an S3 bucket in each Region to receive the uploads and using S3 Cross-Region Replication to copy the files to the distribution S3 bucket is not the most cost-effective solution for this specific use case.<br>C.  Setting up Amazon Route 53 with latency-based routing to route the uploads to the nearest S3 bucket Region is not a solution that would improve the performance of the uploads specifically for users in Australia.<br>E.  Modifying the app to add random prefixes to the files before uploading will not improve the app's performance for users in Australia.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>yes, it will. Other options are more important, but sure random (rsp. any hash that distributes well) prefixes improve performance a lot.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831582,
          "date": "Tue 07 Mar 2023 06:26",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "yes, it will. Other options are more important, but sure random (rsp. any hash that distributes well) prefixes improve performance a lot.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#72",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application is using an Amazon RDS for MySQL Multi-AZ DB instance in the us-east-1 Region. After a failover test, the application lost the connections to the database and could not re-establish the connections. After a restart of the application, the application re-established the connections.<br><br>A solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#72",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Aurora MySQL Serverless v1 DB instance. Migrate the RDS DB instance to the Aurora Serverless v1 DB instance. Update the connection settings in the application to point to the Aurora reader endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a two-node Amazon Aurora MySQL DB cluster. Migrate the RDS DB instance to the Aurora DB cluster. Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon S3 bucket. Export the database to Amazon S3 by using AWS Database Migration Service (AWS DMS). Configure Amazon Athena to use the S3 bucket as a data store. Install the latest Open Database Connectivity (ODBC) driver for the application. Update the connection settings in the application to point to the Athena endpoint<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850971,
          "date": "Sun 26 Mar 2023 14:09",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create an RDS proxy.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827647,
          "date": "Fri 03 Mar 2023 05:06",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Amazon RDS Proxy is a fully managed database proxy service for Amazon Relational Database Service (RDS) that makes applications more scalable, resilient, and secure. It allows applications to pool and share connections to an RDS database, which can help reduce database connection overhead, improve scalability, and provide automatic failover and high availability.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809673,
          "date": "Wed 15 Feb 2023 16:03",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "proxy will be a buffer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777081,
          "date": "Sun 15 Jan 2023 22:29",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct.<br>C: Aurora is useless, Proxy is pointing to existing RDS",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775693,
          "date": "Sat 14 Jan 2023 17:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is B.  Create an RDS proxy. Configure the existing RDS endpoint as a target. Update the connection settings in the application to point to the RDS proxy endpoint.<br><br>An RDS proxy is a service that allows you to pool and share connections to an RDS database. By using an RDS proxy, your application can automatically reconnect to the database after a failover event, without the need to restart the application.<br><br>Solution A, migrating to Aurora Serverless, may not solve the problem because Aurora Serverless does not support Multi-AZ.<br>Solution C and D are not the correct solutions because it does not solve the problem of reconnecting to the database after a failover event.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What?? Aurora does not support Multi AZ ? its a blunder !</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827645,
          "date": "Fri 03 Mar 2023 04:59",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "What?? Aurora does not support Multi AZ ? its a blunder !",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#73",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a solution in the AWS Cloud. Thousands or devices will connect to the solution and send data. Each device needs to be able to send and receive data in real time over the MQTT protocol. Each device must authenticate by using a unique X.509 certificate.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#73",
          "answers": [
            {
              "choice": "<p>A. Set up AWS IoT Core. For each device, create a corresponding Amazon MQ queue and provision a certificate. Connect each device to Amazon MQ.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Network Load Balancer (NLB) and configure it with an AWS Lambda authorizer. Run an MQTT broker on Amazon EC2 instances in an Auto Scaling group. Set the Auto Scaling group as the target for the NLConnect each device to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an Amazon API Gateway HTTP API and a Network Load Balancer (NLB). Create integration between API Gateway and the NLB.  Configure a mutual TLS certificate authorizer on the HTTP API. Run an MQTT broker on an Amazon EC2 instance that the NLB targets. Connect each device to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775698,
          "date": "Sat 14 Jan 2023 17:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is C.  Set up AWS IoT Core. For each device, create a corresponding AWS IoT thing and provision a certificate. Connect each device to AWS IoT Core.<br><br>AWS IoT Core is a fully managed service that enables secure, bi-directional communication between internet-connected devices and the AWS Cloud. It supports the MQTT protocol and includes built-in device authentication and access control. By using AWS IoT Core, the company can easily provision and manage the X.509 certificates for each device, and connect the devices to the service with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, setting up Amazon MQ queues and connecting each device to a queue, would require significant operational overhead to manage the queues and ensure that each device is properly authenticated and connected.<br>Option B and D, using a Network Load Balancer (NLB) with a Lambda authorizer or an Amazon API Gateway HTTP API with a mutual TLS certificate authorizer and running an MQTT broker on EC2 instances, would also introduce more operational complexity and overhead compared to using AWS IoT Core.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 775699,
          "date": "Sat 14 Jan 2023 17:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, setting up Amazon MQ queues and connecting each device to a queue, would require significant operational overhead to manage the queues and ensure that each device is properly authenticated and connected.<br>Option B and D, using a Network Load Balancer (NLB) with a Lambda authorizer or an Amazon API Gateway HTTP API with a mutual TLS certificate authorizer and running an MQTT broker on EC2 instances, would also introduce more operational complexity and overhead compared to using AWS IoT Core.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 850973,
          "date": "Sun 26 Mar 2023 14:10",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "I choose C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 842293,
          "date": "Fri 17 Mar 2023 21:54",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/iot/latest/developerguide/attach-to-cert.html<br><br>It is C,- you have to do this through IOT core, for the devices you need an AWS IOT \\\"thing\\\" and then provision a certificate for the thing. from there connect the device.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834481,
          "date": "Thu 09 Mar 2023 23:23",
          "username": "\t\t\t\tforceli\t\t\t",
          "content": "-The AWS IoT Device SDKs support device communications using the MQTT<br>-Device connections to AWS IoT use X.509 client certificates <br>https://docs.aws.amazon.com/iot/latest/developerguide/iot-connect-devices.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry I meant \\\"C\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 834485,
          "date": "Thu 09 Mar 2023 23:30",
          "username": "\t\t\t\tforceli\t\t\t",
          "content": "Sorry I meant \\\"C\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793318,
          "date": "Mon 30 Jan 2023 21:46",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is correct (less op overhead than A)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777096,
          "date": "Sun 15 Jan 2023 22:49",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#74",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running several workloads in a single AWS account. A new company policy states that engineers can provision only approved resources and that engineers must use AWS CloudFormation to provision these resources. A solutions architect needs to create a solution to enforce the new restriction on the IAM role that the engineers use for access.<br><br>What should the solutions architect do to create the solution?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#74",
          "answers": [
            {
              "choice": "<p>A. Upload AWS CloudFormation templates that contain approved resources to an Amazon S3 bucket. Update the IAM policy for the engineers' IAM role to only allow access to Amazon S3 and AWS CloudFormation. Use AWS CloudFormation templates to provision resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision resources in AWS CloudFormation stacks. Update the IAM policy for the engineers' IAM role to only allow access to their own AWS CloudFormation stack.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850974,
          "date": "Sun 26 Mar 2023 14:12",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C.  Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827677,
          "date": "Fri 03 Mar 2023 06:11",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tricky one. Question has a hint -\\\"to enforce the new restriction on the IAM role\\\" (note its not IAM policy as mentioned in option B) Creating a policy with approved resources first and assuming/applying that role to engineers will enforce. So C is correct. (B lacks enforcement, B is incorrect)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 818543,
          "date": "Thu 23 Feb 2023 00:30",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "C IAM policy is allowing to provision of approved resources.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793139,
          "date": "Mon 30 Jan 2023 18:57",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B does not enfore CF, otherwise it would work.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 790740,
          "date": "Sat 28 Jan 2023 16:39",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "C<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#use-iam-to-control-access<br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779391,
          "date": "Wed 18 Jan 2023 00:20",
          "username": "\t\t\t\tNicocacik\t\t\t",
          "content": "You have to use a service role",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778256,
          "date": "Mon 16 Jan 2023 21:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.<br><br>This option is also correct, it is a way to restrict the access of engineers to only be able to perform AWS CloudFormation actions and provision only approved resources. By giving only permissions to the IAM role used by engineers for CloudFormation and creating a separate IAM role with permissions to provision approved resources and then assigning that role to CloudFormation during stack creation, we ensure that engineers can only provision the approved resources using CloudFormation.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Both options B and C are correct.<br><br>Option B: Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.<br><br>Option C: Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.<br><br>Both options will enforce the new restriction on the IAM role that the engineers use for access, by limiting their access only to approved resources and only allowing them to provision resources using AWS CloudFormation. The specif</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778258,
          "date": "Mon 16 Jan 2023 21:59",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Both options B and C are correct.<br><br>Option B: Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.<br><br>Option C: Update the IAM policy for the engineers' IAM role with permissions to only allow AWS CloudFormation actions. Create a new IAM policy with permission to provision approved resources, and assign the policy to a new IAM service role. Assign the IAM service role to AWS CloudFormation during stack creation.<br><br>Both options will enforce the new restriction on the IAM role that the engineers use for access, by limiting their access only to approved resources and only allowing them to provision resources using AWS CloudFormation. The specif",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777089,
          "date": "Sun 15 Jan 2023 22:40",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct<br>A: only allow CF, no approved resources<br>B: role allow approved resources and CF.  User can bypass CF<br>D: CF only<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B: Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation is correct but by itself it does not guarantee that the engineers will use only approved resources or will use AWS CloudFormation to provision them. The solutions architect should also implement additional controls such as using AWS Organizations to centrally manage access policies, using AWS Config to monitor and enforce compliance with the company's policies, or creating a custom resource in the CloudFormation templates to validate the provisioned resources against a predefined list of approved resources.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778254,
          "date": "Mon 16 Jan 2023 21:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B: Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation is correct but by itself it does not guarantee that the engineers will use only approved resources or will use AWS CloudFormation to provision them. The solutions architect should also implement additional controls such as using AWS Organizations to centrally manage access policies, using AWS Config to monitor and enforce compliance with the company's policies, or creating a custom resource in the CloudFormation templates to validate the provisioned resources against a predefined list of approved resources.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775709,
          "date": "Sat 14 Jan 2023 17:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Update the IAM policy for the engineers' IAM role with permissions to only allow provisioning of approved resources and AWS CloudFormation. Use AWS CloudFormation templates to create stacks with approved resources.<br><br>This solution will meet the requirement of enforcing the new restriction on the IAM role that the engineers use for access by only allowing the engineers to use AWS CloudFormation to provision the approved resources. By updating the IAM policy to only allow provisioning of approved resources and AWS CloudFormation, it will restrict the engineers from provisioning any other resources. Engineers will use AWS CloudFormation templates to create stacks with approved resources, which will ensure that only the approved resources are being provisioned.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>it allow provision of approved resources and CF in same time. User can provision resources directly without CF</li><li>Other options are not the correct answer because:<br><br>Option A only allows access to Amazon S3 and AWS CloudFormation, but it doesn't restrict the engineers from provisioning resources other than the approved ones<br>Option C only allows AWS CloudFormation actions, but it doesn't restrict the engineers from provisioning resources other than the approved ones<br>Option D is incomplete, it doesn't specify how to restrict the engineers from provisioning resources other than the approved ones</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777936,
          "date": "Mon 16 Jan 2023 17:36",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "it allow provision of approved resources and CF in same time. User can provision resources directly without CF",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775710,
          "date": "Sat 14 Jan 2023 17:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Other options are not the correct answer because:<br><br>Option A only allows access to Amazon S3 and AWS CloudFormation, but it doesn't restrict the engineers from provisioning resources other than the approved ones<br>Option C only allows AWS CloudFormation actions, but it doesn't restrict the engineers from provisioning resources other than the approved ones<br>Option D is incomplete, it doesn't specify how to restrict the engineers from provisioning resources other than the approved ones",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#75",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing the data storage and retrieval architecture for a new application that a company will be launching soon. The application is designed to ingest millions of small records per minute from devices all around the world. Each record is less than 4 KB in size and needs to be stored in a durable location where it can be retrieved with low latency. The data is ephemeral and the company is required to store the data for 120 days only, after which the data can be deleted.<br><br>The solutions architect calculates that, during the course of a year, the storage requirements would be about 10-15 TB. <br><br>Which storage strategy is the MOST cost-effective and meets the design requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#75",
          "answers": [
            {
              "choice": "<p>A. Design the application to store each incoming record as a single .csv file in an Amazon S3 bucket to allow for indexed retrieval. Configure a lifecycle policy to delete data older than 120 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Design the application to store each incoming record in a single table in an Amazon RDS MySQL database. Run a nightly cron job that runs a query to delete any records older than 120 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Design the application to batch incoming records before writing them to an Amazon S3 bucket. Update the metadata for the object to contain the list of records in the batch and use the Amazon S3 metadata search feature to retrieve the data. Configure a lifecycle policy to delete the data after 120 days.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775714,
          "date": "Sat 14 Jan 2023 17:26",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The most cost-effective and efficient solution that meets the design requirements would be option B, Design the application to store each incoming record in an Amazon DynamoDB table properly configured for the scale. Configure the DynamoDB Time to Live (TTL) feature to delete records older than 120 days.<br><br>DynamoDB is a NoSQL key-value store designed for high scale and performance. It is fully managed by AWS and can easily handle millions of small records per minute. Additionally, with the TTL feature, you can set an expiration time for each record, so that the data can be automatically deleted after the specified time period.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, storing each incoming record as a single .csv file in an Amazon S3 bucket, would not be a good option because it would be difficult to retrieve individual records from the .csv files, and will likely increase the cost of data retrieval.<br><br>Option C, storing each incoming record in a single table in an Amazon RDS MySQL database, would be a more expensive option as RDS is typically more expensive than DynamoDB.  Additionally, running a cron job to delete old data could lead to additional operational overhead.<br><br>Option D, storing incoming records in batches in an S3 bucket, would be a less efficient option as it would require additional processing and parsing of the data to retrieve individual records.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775715,
          "date": "Sat 14 Jan 2023 17:26",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, storing each incoming record as a single .csv file in an Amazon S3 bucket, would not be a good option because it would be difficult to retrieve individual records from the .csv files, and will likely increase the cost of data retrieval.<br><br>Option C, storing each incoming record in a single table in an Amazon RDS MySQL database, would be a more expensive option as RDS is typically more expensive than DynamoDB.  Additionally, running a cron job to delete old data could lead to additional operational overhead.<br><br>Option D, storing incoming records in batches in an S3 bucket, would be a less efficient option as it would require additional processing and parsing of the data to retrieve individual records.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 853731,
          "date": "Tue 28 Mar 2023 23:41",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "B DynamoDB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 850975,
          "date": "Sun 26 Mar 2023 14:13",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Design the application to store each incoming record in an Amazon DynamoDB table",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 784745,
          "date": "Sun 22 Jan 2023 23:34",
          "username": "\t\t\t\tDDONG\t\t\t",
          "content": "B SAP01 #613",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777098,
          "date": "Sun 15 Jan 2023 22:52",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct<br>Dynamodb support 4KB size, low latency and TTL<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>do yo mean B</li><li>https://www.examtopics.com/discussions/amazon/view/28419-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>Option C is using RDS MySQL which is a relational database and will not be able to handle the scale of millions of small records per minute with low latency and it is not designed for automatic deletion of records based on time and it will be more expensive as well.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778291,
          "date": "Mon 16 Jan 2023 22:23",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "do yo mean B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778261,
          "date": "Mon 16 Jan 2023 22:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/28419-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>Option C is using RDS MySQL which is a relational database and will not be able to handle the scale of millions of small records per minute with low latency and it is not designed for automatic deletion of records based on time and it will be more expensive as well.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#76",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company is hosting an ecommerce website on AWS across multiple AWS Regions. The company wants the website to be operational at all times for online purchases. The website stores data in an Amazon RDS for MySQL DB instance.<br><br>Which solution will provide the HIGHEST availability for the database?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#76",
          "answers": [
            {
              "choice": "<p>A. Configure automated backups on Amazon RDS. In the case of disruption, promote an automated backup to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure global tables and read replicas on Amazon RDS. Activate the cross-Region scope. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure global tables and automated backups on Amazon RDS. In the case of disruption, use AWS Lambda to copy the read replicas from one Region to another Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850977,
          "date": "Sun 26 Mar 2023 14:15",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D makes the most sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842392,
          "date": "Sat 18 Mar 2023 01:55",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "This really should be multi-az but you could move to it w/ D. <br>Here is the key to this one though; Highest Availability - the read replica is an asynchronous copy, while backup is a \\\"time\\\". Easier to do the read replica, and flip the switches than to reload from backup. Global Tables relate to DynomoDB https://disaster-recovery.workshop.aws/en/services/databases/dynamodb/dynamo-global-table.html<br>Little handy \\\"DR\\\" guide",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827685,
          "date": "Fri 03 Mar 2023 06:30",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "No global tables concept in RDS, B,C are eliminated. A is wrong in terms of backing up Db copy to a standalone instance ? D provides read replicas for reading and also swtiches as a failiover in times of disruption and becomes primary. this is how HA can be maintained. D is correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 826791,
          "date": "Thu 02 Mar 2023 12:40",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "MySQL - Read Replica. In this case, this is not aurora so not the global table option and hence can not be B and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 826698,
          "date": "Thu 02 Mar 2023 10:28",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "I haven't found any information about a \\\"global table\\\" for RDS.<br>Global tables are for DynamoDB.  For Aurora, it's called \\\"global databases\\\".<br>RDS for MySQL supports cross-region read replicas https://aws.amazon.com/fr/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/, so D has a better availability than A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779550,
          "date": "Wed 18 Jan 2023 04:00",
          "username": "\t\t\t\ticassp\t\t\t",
          "content": "for B,C, Amazon RDS does not support global tables yet. Only Aurora supports.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Is Aurora not part of RDS? You can choose Aurora's compatibility with MySQL and PostreSQL).</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 814077,
          "date": "Sun 19 Feb 2023 13:52",
          "username": "\t\t\t\tAlanKrish\t\t\t",
          "content": "Is Aurora not part of RDS? You can choose Aurora's compatibility with MySQL and PostreSQL).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777099,
          "date": "Sun 15 Jan 2023 22:53",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It is possible that some people may think that option D.  Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source. is the best solution, as it also utilizes read replicas and cross-Region promotion to minimize downtime. However, it is important to consider that while this solution provides high availability, it doesn't provide the same level of automatic replication that global tables do. In case of a disruption, there is a risk of data loss during the manual switchover.<br>and also with option D, you are still working with a single point of failure, the primary database, while in option B you have multiple copies of your data distributed across different regions, so in case of a failure you can switch over to one of the replicas without loss of data.</li><li>Cant be B due to global tables, ReadReplicas are supported with RDS and other options of restoring from backup do not create high availability</li><li>B is not right.Only Aurora has global tables.RDS don't</li><li>https://www.examtopics.com/discussions/amazon/view/69438-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 778268,
          "date": "Mon 16 Jan 2023 22:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "It is possible that some people may think that option D.  Configure read replicas on Amazon RDS. In the case of disruption, promote a cross-Region and read replica to be a standalone DB instance. Direct database traffic to the promoted DB instance. Create a replacement read replica that has the promoted DB instance as its source. is the best solution, as it also utilizes read replicas and cross-Region promotion to minimize downtime. However, it is important to consider that while this solution provides high availability, it doesn't provide the same level of automatic replication that global tables do. In case of a disruption, there is a risk of data loss during the manual switchover.<br>and also with option D, you are still working with a single point of failure, the primary database, while in option B you have multiple copies of your data distributed across different regions, so in case of a failure you can switch over to one of the replicas without loss of data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Cant be B due to global tables, ReadReplicas are supported with RDS and other options of restoring from backup do not create high availability</li><li>B is not right.Only Aurora has global tables.RDS don't</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 809932,
          "date": "Wed 15 Feb 2023 21:07",
          "username": "\t\t\t\tryansmithnz\t\t\t",
          "content": "Cant be B due to global tables, ReadReplicas are supported with RDS and other options of restoring from backup do not create high availability",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797150,
          "date": "Fri 03 Feb 2023 16:28",
          "username": "\t\t\t\tShahul75\t\t\t",
          "content": "B is not right.Only Aurora has global tables.RDS don't",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778270,
          "date": "Mon 16 Jan 2023 22:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/69438-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775724,
          "date": "Sat 14 Jan 2023 17:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option B.  Configuring global tables and read replicas on Amazon RDS with the cross-Region scope enabled provides the highest availability for the database. In case of disruption, the company can use AWS Lambda to copy the read replicas from one Region to another Region, ensuring that the website remains operational at all times. This solution provides automatic failover across multiple regions and allows for fast recovery in case of a disruption.<br><br>Option A involves promoting an automated backup to be a standalone DB instance and creating a replacement read replica that has the promoted DB instance as its source. This solution is less efficient since it requires manual intervention and additional steps to promote the backup and create a replacement read replica.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>If the disruption is an outage that takes the Region offline completely, how could we use Lambda to copy the read replica from the Region that is no longer available to the backup to another Region?</li><li>Option C involves configuring global tables and automated backups on Amazon RDS. This solution is less efficient since it does not provide automatic failover across multiple regions and requires additional steps to copy the read replicas from one Region to another Region using AWS Lambda.<br><br>Option D involves configuring read replicas on Amazon RDS. In the case of disruption, promoting a cross-Region and read replica to be a standalone DB instance. This solution is less efficient than Option B since it does not provide automatic failover across multiple regions and requires manual intervention to promote the read replica to a standalone instance.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 826155,
          "date": "Wed 01 Mar 2023 19:21",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "If the disruption is an outage that takes the Region offline completely, how could we use Lambda to copy the read replica from the Region that is no longer available to the backup to another Region?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775726,
          "date": "Sat 14 Jan 2023 17:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C involves configuring global tables and automated backups on Amazon RDS. This solution is less efficient since it does not provide automatic failover across multiple regions and requires additional steps to copy the read replicas from one Region to another Region using AWS Lambda.<br><br>Option D involves configuring read replicas on Amazon RDS. In the case of disruption, promoting a cross-Region and read replica to be a standalone DB instance. This solution is less efficient than Option B since it does not provide automatic failover across multiple regions and requires manual intervention to promote the read replica to a standalone instance.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#77",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Example Corp. has an on-premises data center and a VPC named VPC A in the Example Corp. AWS account. The on-premises network connects to VPC A through an AWS Site-To-Site VPN. The on-premises servers can properly access VPC A.  Example Corp. just acquired AnyCompany, which has a VPC named VPC B.  There is no IP address overlap among these networks. Example Corp. has peered VPC A and VPC B. <br><br>Example Corp. wants to connect from its on-premise servers to VPC B.  Example Corp. has properly set up the network ACL and security groups.<br><br>Which solution will meet this requirement with the LEAST operational effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#77",
          "answers": [
            {
              "choice": "<p>A. Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a transit gateway. Create a Site-to-Site VPN connection between the on-premises network and VPC B, and connect the VPN connection to the transit gateway. Add a route to direct traffic to the peered VPCs, and add an authorization rule to give clients access to the VPCs A and B. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the route tables for the Site-to-Site VPN and both VPCs for all three networks. Configure BGP propagation for all three networks. Wait for up to 5 minutes for BGP propagation to finish.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Modify the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B.  Split the two routers of the virtual private getaway between the two VPCs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852880,
          "date": "Tue 28 Mar 2023 08:49",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "Solution A is the only one possible solution<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A : the best (and the only one possible) answer : When you have 2 VPC, you have multiple solution to connect to onprem : <br>- Create 2 S2S VPN (1 for each VPC)<br>- or Create a TGW, attach both VPC to it and attach S2S VPN to it too<br>- or Create a third VPC (VPC routing), and peer VPC A with VPC routing, VPC B to VPC routing, attach a S2S VPN to VPC routing and use a NVA on VPC routing to route trafic. NVA can do transitivity.<br>Here, solution A is one of the possible answer</li><li>B is impossible : When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW (it\\\"s a 3 entities). You can however connect a S2S VPN to a TGW (onprem to TGW) (which is solution A).<br>C : Does not work, there is no transitivity on AWS. S2S VPN cannot reach VPC B through VPC A<br>D is impossible :There is no magic, you cannot \\\"split\\\" router (that does not exist). VGW is attach to a single VPC.  A S2S VPN cannot multiplex VPC</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 852882,
          "date": "Tue 28 Mar 2023 08:49",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "A : the best (and the only one possible) answer : When you have 2 VPC, you have multiple solution to connect to onprem : <br>- Create 2 S2S VPN (1 for each VPC)<br>- or Create a TGW, attach both VPC to it and attach S2S VPN to it too<br>- or Create a third VPC (VPC routing), and peer VPC A with VPC routing, VPC B to VPC routing, attach a S2S VPN to VPC routing and use a NVA on VPC routing to route trafic. NVA can do transitivity.<br>Here, solution A is one of the possible answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 852881,
          "date": "Tue 28 Mar 2023 08:49",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "B is impossible : When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW (it\\\"s a 3 entities). You can however connect a S2S VPN to a TGW (onprem to TGW) (which is solution A).<br>C : Does not work, there is no transitivity on AWS. S2S VPN cannot reach VPC B through VPC A<br>D is impossible :There is no magic, you cannot \\\"split\\\" router (that does not exist). VGW is attach to a single VPC.  A S2S VPN cannot multiplex VPC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 850980,
          "date": "Sun 26 Mar 2023 14:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A.  Create a transit gateway. Attach the Site-to-Site VPN",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 844284,
          "date": "Sun 19 Mar 2023 23:00",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A makes sense to me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 843640,
          "date": "Sun 19 Mar 2023 11:30",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "A for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 827700,
          "date": "Fri 03 Mar 2023 07:13",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A has this wierd wording - attaching S-S VPN ? transit gateway attaches to VPCs only not S-S vpn. A is wrong. Since VPC A and VPC B are already peered, the easiest solution to connect from the on-premises servers to VPC B would be to create another Site-to-Site VPN connection between the on-premises data center and VPC B.  This would require minimal operational effort, as the existing VPN connection with VPC A can remain unchanged.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>oops this is wrong..VPN can be attached...</li><li>Moderator, please delete this comment..</li><li>https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html<br>When you create a virtual private gateway, you can specify the private Autonomous System Number (ASN) for the Amazon side of the gateway. If you don't specify an ASN, the virtual private gateway is created with the default ASN (64512). You cannot change the ASN after you've created the virtual private gateway. Due to this reason, So A is not possible (with least effort). Answer should be B. </li><li>THe VGW for VPCA is no more needed on A because you attach the VPCA to the TGW.<br>The ASN will be on the TGW attachment with the S2S VPN.<br>This is the best solution.<br>In the meantime, B is impossible. When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW. You can however connect a S2S VPN to a TGW (onprem to TGW).</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827705,
          "date": "Fri 03 Mar 2023 07:18",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "oops this is wrong..VPN can be attached...<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Moderator, please delete this comment..</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827708,
          "date": "Fri 03 Mar 2023 07:25",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Moderator, please delete this comment..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827707,
          "date": "Fri 03 Mar 2023 07:24",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html<br>When you create a virtual private gateway, you can specify the private Autonomous System Number (ASN) for the Amazon side of the gateway. If you don't specify an ASN, the virtual private gateway is created with the default ASN (64512). You cannot change the ASN after you've created the virtual private gateway. Due to this reason, So A is not possible (with least effort). Answer should be B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>THe VGW for VPCA is no more needed on A because you attach the VPCA to the TGW.<br>The ASN will be on the TGW attachment with the S2S VPN.<br>This is the best solution.<br>In the meantime, B is impossible. When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW. You can however connect a S2S VPN to a TGW (onprem to TGW).</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 852867,
          "date": "Tue 28 Mar 2023 08:41",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "THe VGW for VPCA is no more needed on A because you attach the VPCA to the TGW.<br>The ASN will be on the TGW attachment with the S2S VPN.<br>This is the best solution.<br>In the meantime, B is impossible. When you create a S2S VPN connection, it's between 2 entites (here, the onprem and VPC B). It says that they connect the onprem to VPCB with S2SVPN AND THEN to a TGW, it's not possible to connect a S2S VPN from onprem to VPC to a TGW. You can however connect a S2S VPN to a TGW (onprem to TGW).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 811209,
          "date": "Fri 17 Feb 2023 00:54",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "TGW is the solutions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806859,
          "date": "Sun 12 Feb 2023 23:06",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "D. <br>A - setting up new transit gateway- more operational cost<br>B - new site-to-site - vpn -more operational cost<br>C - updating route tables for site to site vpn and3 VPCs,bgp config update for 3 networks .. more operational cost<br>D - because it requires the least amount of operational effort. By modifying the Site-to-Site VPN's virtual private gateway definition to include both VPC A and VPC B and splitting the two routers of the virtual private gateway between the two VPCs, the on-premises servers can connect to both VPCs with minimal additional effort. This solution leverages the existing Site-to-Site VPN and does not add any additional layers of complexity to the network.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D is not possible. There is no magic, you cannot \\\"split\\\" router (that does not exist). VGW is attach to a single VPC.  A S2S VPN cannot multiplex VPC ;)</li><li>It looks like you understood D.  How can you split two routers of the VGW between two VPCs? The VGW is an object that can be attached to a single VPC at a time.What are the two routers they talk about here? Are there on-prem routers?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 852863,
          "date": "Tue 28 Mar 2023 08:35",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "D is not possible. There is no magic, you cannot \\\"split\\\" router (that does not exist). VGW is attach to a single VPC.  A S2S VPN cannot multiplex VPC ;)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826170,
          "date": "Wed 01 Mar 2023 19:33",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "It looks like you understood D.  How can you split two routers of the VGW between two VPCs? The VGW is an object that can be attached to a single VPC at a time.What are the two routers they talk about here? Are there on-prem routers?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793329,
          "date": "Mon 30 Jan 2023 22:01",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "solution is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778278,
          "date": "Mon 16 Jan 2023 22:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.<br><br>This option will allow you to connect from the on-premises servers to VPC B with the least operational effort, as it utilizes the transit gateway to connect all networks and allows for easy updates to the route tables. BGP propagation is not necessary and the use of transit gateway will simplify the traffic routing.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778276,
          "date": "Mon 16 Jan 2023 22:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A.  Create a transit gateway. Attach the Site-to-Site VPN, VPC A, and VPC B to the transit gateway. Update the transit gateway route tables for all networks to add IP range routes for all other networks.<br><br>This option allows for all three networks (on-premises, VPC A and VPC B) to be connected through the transit gateway, which simplifies the traffic routing and makes it easy to update the route tables for all networks. It also eliminates the need for a separate Site-to-Site VPN connection between the on-premises network and VPC B, which would add unnecessary complexity.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not correct because it would require a separate Site-to-Site VPN connection between the on-premises network and VPC B, which would add unnecessary complexity and effort.<br><br>Option C is not correct because updating the route tables for all three networks and configuring BGP propagation can be a complex process, and waiting for BGP propagation to finish would add an unnecessary delay.<br><br>Option D is not correct because modifying the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B and splitting the two routers of the virtual private gateway between the two VPCs would be overly complex and difficult to manage. It will not be the most efficient solution and adding unnecessary complexity to the existing solution.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778277,
          "date": "Mon 16 Jan 2023 22:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not correct because it would require a separate Site-to-Site VPN connection between the on-premises network and VPC B, which would add unnecessary complexity and effort.<br><br>Option C is not correct because updating the route tables for all three networks and configuring BGP propagation can be a complex process, and waiting for BGP propagation to finish would add an unnecessary delay.<br><br>Option D is not correct because modifying the Site-to-Site VPN's virtual private gateway definition to include VPC A and VPC B and splitting the two routers of the virtual private gateway between the two VPCs would be overly complex and difficult to manage. It will not be the most efficient solution and adding unnecessary complexity to the existing solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777105,
          "date": "Sun 15 Jan 2023 23:06",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct. on-premise is connected to TGW, use TDW to talk to VPC A/B<br>B: too many VPN connections<br>C: VPC B cannot use VPC A to VPN<br>D: one VPN gateway cannot be associated with more than one VPC<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Is correct that option A is the correct answer. Thank for you help.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 778279,
          "date": "Mon 16 Jan 2023 22:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Is correct that option A is the correct answer. Thank for you help.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775732,
          "date": "Sat 14 Jan 2023 17:34",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  This option involves creating a new Site-to-Site VPN connection between the on-premises network and VPC B, and connecting that VPN connection to the transit gateway. This allows the on-premises network to access resources in VPC B through the transit gateway, which already has a connection to VPC A and can route traffic between the two VPCs. This solution requires minimal additional configuration and minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A involves creating a transit gateway, attaching all three networks (the on-premises network, VPC A, and VPC B) to it and updating routing tables for all networks. This solution would require significant additional configuration and would be more complex to set up and maintain.<br><br>Option C involves updating routing tables for all three networks and configuring BGP propagation. This solution is complex and would require additional configuration and maintenance.<br><br>Option D involves modifying the definition of the Site-to-Site VPN to include both VPC A and B and splitting the two VPN routers between the two VPCs. This solution is complex and would require additional configuration and maintenance.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775733,
          "date": "Sat 14 Jan 2023 17:34",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A involves creating a transit gateway, attaching all three networks (the on-premises network, VPC A, and VPC B) to it and updating routing tables for all networks. This solution would require significant additional configuration and would be more complex to set up and maintain.<br><br>Option C involves updating routing tables for all three networks and configuring BGP propagation. This solution is complex and would require additional configuration and maintenance.<br><br>Option D involves modifying the definition of the Site-to-Site VPN to include both VPC A and B and splitting the two VPN routers between the two VPCs. This solution is complex and would require additional configuration and maintenance.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#78",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently completed the migration from an on-premises data center to the AWS Cloud by using a replatforming strategy. One of the migrated servers is running a legacy Simple Mail Transfer Protocol (SMTP) service that a critical application relies upon. The application sends outbound email messages to the company's customers. The legacy SMTP server does not support TLS encryption and uses TCP port 25. The application can use SMTP only.<br><br>The company decides to use Amazon Simple Email Service (Amazon SES) and to decommission the legacy SMTP server. The company has created and validated the SES domain. The company has lifted the SES limits.<br><br>What should the company do to modify the application to send email messages from Amazon SES?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#78",
          "answers": [
            {
              "choice": "<p>A. Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the application to use the SES API to send email messages. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Use the IAM role as a service role for Amazon SES.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the application to use AWS SDKs to send email messages. Create an IAM user for Amazon SES. Generate API access keys. Use the access keys to authenticate with Amazon SES.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850981,
          "date": "Sun 26 Mar 2023 14:18",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Configure the application to connect to Amazon SES by using STARTTLS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 847360,
          "date": "Wed 22 Mar 2023 19:04",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "B , https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 844306,
          "date": "Sun 19 Mar 2023 23:39",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "B is wrong because STARTTLS uses port 25 and EC2 instances can't send outbound traffic through port 25 (you must ask AWS to allow port 25)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 836040,
          "date": "Sat 11 Mar 2023 14:35",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "The key to this question imo is the sentence \\\"The application can use SMTP only\\\". <br>So we cannot go for encryption. <br>Imo there is no TLS wrapper for Mail that supports authentication which is needed for SES, one needs a proxiing mailserver for that(need support for auth and encryption, rewriting mail).<br>With StarttlsSMTP protocol is supported by AWS and the legacy application can send the mail to AWS just as it did to to the legacy mailserver. <br>(Of course: a unix machine has not just one application but a lot of little apps like cron, at ...and low traffic mailserver consumes like no resources, so in real world every unix machine should have a small local smtp, eg a postfix configured to forward all traffic from every tool app, system ...to ses but that real world option is not provided as possible answer: so B. )<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>you may look at https://www.stunnel.org/, if find a way to make auth work with ses: well then go for A.  Afaik it is not possible - but happy to learn if there is a way.</li><li>also have a look at<br>https://hector.dev/2015/01/17/sending-e-mail-via-amazon-ses-over-smtp-with-iam-roles/<br>Using iam roles does not realy work with smpt auth.(I didn't get it to work and it seems no one else either)</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 836119,
          "date": "Sat 11 Mar 2023 15:09",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "you may look at https://www.stunnel.org/, if find a way to make auth work with ses: well then go for A.  Afaik it is not possible - but happy to learn if there is a way.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836138,
          "date": "Sat 11 Mar 2023 15:29",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "also have a look at<br>https://hector.dev/2015/01/17/sending-e-mail-via-amazon-ses-over-smtp-with-iam-roles/<br>Using iam roles does not realy work with smpt auth.(I didn't get it to work and it seems no one else either)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828234,
          "date": "Fri 03 Mar 2023 19:15",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html<br>For new apps, C should be correct. But here, Its re-platforming strategy migrating from SMTP to SES<br>STARTTLS vs TLS Wrapper is being tested here. (A or B) But A sounds 25 port communication which the existing app uses. So B should be correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819996,
          "date": "Fri 24 Feb 2023 02:38",
          "username": "\t\t\t\tscuzzy2010\t\t\t",
          "content": "B is correct.<br>https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html<br>STARTTLS supportsports 25, 587, and 2587<br>TLSWRAPPER supports ports465 and 2465<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>FYI Amazon SES supports STARTTLS encryption over port 587, which is the recommended port for email transmission. But existing port 25 can be configured too as in this case as the migration came from SMTP port 25</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 828237,
          "date": "Fri 03 Mar 2023 19:17",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "FYI Amazon SES supports STARTTLS encryption over port 587, which is the recommended port for email transmission. But existing port 25 can be configured too as in this case as the migration came from SMTP port 25",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793752,
          "date": "Tue 31 Jan 2023 08:07",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "It's B, becuse D is discarded since \\\"The application can use SMTP only.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793750,
          "date": "Tue 31 Jan 2023 08:05",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I doubt between B and D.  Both seem correct to me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790752,
          "date": "Sat 28 Jan 2023 16:45",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "In this scenario, you should use Amazon SES SMTP interface to send emails because the application can use SMTP only.<br>https://docs.aws.amazon.com/ses/latest/dg/send-email-smtp.html<br>https://docs.aws.amazon.com/ses/latest/dg/smtp-credentials.html<br>https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 788794,
          "date": "Thu 26 Jan 2023 15:15",
          "username": "\t\t\t\tboomx\t\t\t",
          "content": "B<br>STARTTLS works over 25, less change. Also SES SMTP interface needs SMTP credentials<br>https://docs.aws.amazon.com/ses/latest/dg/smtp-credentials.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778285,
          "date": "Mon 16 Jan 2023 22:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option A: \\\"Configure the application to connect to Amazon SES by using TLS Wrapper. Create an IAM role that has ses:SendEmail and ses:SendRawEmail permissions. Attach the IAM role to an Amazon EC2 instance.\\\"<br><br>Option B is incorrect as it suggests to use SMTP with STARTTLS to connect to Amazon SES, which is a less secure method than using a secure wrapper such as TLS Wrapper. Option B also suggests using long-term SMTP credentials which could be a security concern.<br><br>Option C is incorrect as it suggests to use the SES API to send email messages, which is not necessary as the application can only use SMTP.<br><br>Option D is incorrect as it suggests to use AWS SDKs to send email messages, which is not necessary as the application can only use SMTP. Also, it suggests to use IAM user for Amazon SES which is also a security concern as it will involve long-term credentials as well.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A) what tls wrapper are you talking about? <br>B) \\\"Starttls is less secure\\\": SES AWS Mailservers support Starttls anf you have no way of reconfigure the AWS severs. <br>(With Starttls the *server* accepts unencrypted and encrypted incomming smtp mail. The client just connects with smpt encrypted or not, the server will accept both. ...)</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 836026,
          "date": "Sat 11 Mar 2023 14:21",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "A) what tls wrapper are you talking about? <br>B) \\\"Starttls is less secure\\\": SES AWS Mailservers support Starttls anf you have no way of reconfigure the AWS severs. <br>(With Starttls the *server* accepts unencrypted and encrypted incomming smtp mail. The client just connects with smpt encrypted or not, the server will accept both. ...)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777245,
          "date": "Mon 16 Jan 2023 02:35",
          "username": "\t\t\t\tAtila50\t\t\t",
          "content": "https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777108,
          "date": "Sun 15 Jan 2023 23:12",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct. <br>https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html<br>TLS support TLS Wrapper or STARTTLS.<br>B: use STARTTLE but use credential, should use role which is in A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can't use a role with SES. https://docs.aws.amazon.com/ses/latest/dg/send-email-concepts-credentials.html</li><li>Yes, you are correct. Option A is the correct answer, as it outlines the steps to configure the application to connect to Amazon SES using a secure method (TLS Wrapper) and authenticate using an IAM role with the necessary permissions (ses:SendEmail and ses:SendRawEmail). This is a more secure and recommended method to authenticate with Amazon SES as it does not involve using long-term credentials such as access keys.</li><li>no - you are assuming the type of SMTP server, it has to be programmatically configured because the \\\"application can use SMTP only\\\".<br><br>SES only support TLS and they already created validated the SES domain. So, if you follow this: https://docs.aws.amazon.com/ses/latest/dg/send-using-smtp-programmatically.html<br><br>you have to configure the app to use starttls to upgrade it, code in your SMTP creds to authenticate, and bang- send email</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 793749,
          "date": "Tue 31 Jan 2023 08:05",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "You can't use a role with SES. https://docs.aws.amazon.com/ses/latest/dg/send-email-concepts-credentials.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778284,
          "date": "Mon 16 Jan 2023 22:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Yes, you are correct. Option A is the correct answer, as it outlines the steps to configure the application to connect to Amazon SES using a secure method (TLS Wrapper) and authenticate using an IAM role with the necessary permissions (ses:SendEmail and ses:SendRawEmail). This is a more secure and recommended method to authenticate with Amazon SES as it does not involve using long-term credentials such as access keys.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>no - you are assuming the type of SMTP server, it has to be programmatically configured because the \\\"application can use SMTP only\\\".<br><br>SES only support TLS and they already created validated the SES domain. So, if you follow this: https://docs.aws.amazon.com/ses/latest/dg/send-using-smtp-programmatically.html<br><br>you have to configure the app to use starttls to upgrade it, code in your SMTP creds to authenticate, and bang- send email</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 842404,
          "date": "Sat 18 Mar 2023 02:21",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "no - you are assuming the type of SMTP server, it has to be programmatically configured because the \\\"application can use SMTP only\\\".<br><br>SES only support TLS and they already created validated the SES domain. So, if you follow this: https://docs.aws.amazon.com/ses/latest/dg/send-using-smtp-programmatically.html<br><br>you have to configure the app to use starttls to upgrade it, code in your SMTP creds to authenticate, and bang- send email",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775745,
          "date": "Sat 14 Jan 2023 17:49",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Configure the application to connect to Amazon SES by using STARTTLS. Obtain Amazon SES SMTP credentials. Use the credentials to authenticate with Amazon SES.<br><br>This solution allows the application to use the Simple Mail Transfer Protocol (SMTP) protocol to send email messages, which the application requires. Using STARTTLS enables the use of Transport Layer Security (TLS) encryption to secure the connection between the application and Amazon SES. Obtaining the Amazon SES SMTP credentials and using them to authenticate with Amazon SES allows the application to use Amazon SES to send email messages.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it uses an IAM role, rather than SMTP credentials, to authenticate with Amazon SES.<br>Option C is incorrect because it uses the SES API to send email messages, which the application may not support.<br>Option D is incorrect because it uses an IAM user, rather than SMTP credentials, to authenticate with Amazon SES.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775747,
          "date": "Sat 14 Jan 2023 17:49",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because it uses an IAM role, rather than SMTP credentials, to authenticate with Amazon SES.<br>Option C is incorrect because it uses the SES API to send email messages, which the application may not support.<br>Option D is incorrect because it uses an IAM user, rather than SMTP credentials, to authenticate with Amazon SES.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#79",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently acquired several other companies. Each company has a separate AWS account with a different billing and reporting method. The acquiring company has consolidated all the accounts into one organization in AWS Organizations. However, the acquiring company has found it difficult to generate a cost report that contains meaningful groups for all the teams.<br><br>The acquiring company's finance team needs a solution to report on costs for all the companies through a self-managed application.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#79",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a table in Amazon Athena. Create an Amazon QuickSight dataset based on the Athena table. Share the dataset with the finance team.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Cost and Usage Report for the organization. Define tags and cost categories in the report. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon QuickSight dataset that receives spending information from the AWS Price List Query API. Share the dataset with the finance team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the AWS Price List Query API to collect account spending information. Create a specialized template in AWS Cost Explorer that the finance department will use to build reports.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850982,
          "date": "Sun 26 Mar 2023 14:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A.  Create an AWS Cost and Usage Report for the organization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 804987,
          "date": "Sat 11 Feb 2023 05:37",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I can customize reporting in Cost Explorer but cannot find how to do templates.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777115,
          "date": "Sun 15 Jan 2023 23:22",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct<br>B: no such template for cost exporer<br>CD: Price List Query API is for list price, not for usage",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775782,
          "date": "Sat 14 Jan 2023 18:33",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is A. <br><br>Creating an AWS Cost and Usage Report for the organization and defining tags and cost categories in the report will allow for detailed cost reporting for the different companies that have been consolidated into one organization. By creating a table in Amazon Athena and an Amazon QuickSight dataset based on the Athena table, the finance team will be able to easily query and generate reports on the costs for all the companies. The dataset can then be shared with the finance team for them to use for their reporting needs.<br><br>Option B is not correct because it does not provide a way to query and generate reports on the costs for all the companies. <br><br>Option C is not correct because it only provides spending information from the AWS Price List Query API and does not provide detailed cost reporting for the different companies. <br><br>Option D is not correct because it only uses the AWS Price List Query API and does not provide a way to query and generate reports on the costs for all the companies.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#80",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an IoT platform on AWS. IoT sensors in various locations send data to the company's Node.js API servers on Amazon EC2 instances running behind an Application Load Balancer. The data is stored in an Amazon RDS MySQL DB instance that uses a 4 TB General Purpose SSD volume.<br><br>The number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. The API servers are consistently overloaded and RDS metrics show high write latency.<br><br>Which of the following steps together will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#80",
          "answers": [
            {
              "choice": "<p>A. Resize the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Re-architect the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and add read replicas.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Leverage Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS X-Ray to analyze and debug application issues and add more API servers to match the load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Re-architect the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775793,
          "date": "Sat 14 Jan 2023 18:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C and E are the correct answers.<br><br>Option C: Leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data would help to resolve the issues with the API servers being consistently overloaded. By using Kinesis, the data can be ingested and processed in real-time, allowing the API servers to handle the increased load. Using Lambda to process the data can also help to improve the overall performance and scalability of the platform.<br><br>Option E: Re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance would help to resolve the issues with high write latency. DynamoDB is a NoSQL database that is designed for high performance and scalability, making it a good fit for this use case. Additionally, DynamoDB supports auto-scaling, which can help to ensure that the database can handle the expected growth in the number of sensors.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I disagree with option E.  Re-architecting the database tier from RDS to DynamoDB is not possible. RDS is a SQL database, and DynamoDB is a NoSQL database. <br><br>The correct one should be C and B</li><li>I agree with you.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html<br>Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.<br><br>Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB).</li><li>Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/<br><br>Plus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB. html<br>This is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale</li><li>Option A, Resizing the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS will not solve the problem, as the problem is not just related to storage size but also high write latency.<br><br>Option B, Re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas would help to improve the read performance, but it won't help in reducing write latency.<br><br>Option D, Using AWS X-Ray to analyze and debug application issues and adding more API servers to match the load, would help in identifying the problem and resolving it, but it will not help in reducing the load on the servers.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 825234,
          "date": "Tue 28 Feb 2023 21:13",
          "username": "\t\t\t\tSuperP43\t\t\t",
          "content": "I disagree with option E.  Re-architecting the database tier from RDS to DynamoDB is not possible. RDS is a SQL database, and DynamoDB is a NoSQL database. <br><br>The correct one should be C and B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I agree with you.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html<br>Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.<br><br>Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB).</li><li>Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/<br><br>Plus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB. html<br>This is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 834714,
          "date": "Fri 10 Mar 2023 08:14",
          "username": "\t\t\t\tkamaro\t\t\t",
          "content": "I agree with you.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html<br>Aurora can deliver up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.<br><br>Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/<br><br>Plus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB. html<br>This is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 842417,
          "date": "Sat 18 Mar 2023 02:48",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Naw, you can migrate: https://aws.amazon.com/blogs/big-data/near-zero-downtime-migration-from-mysql-to-dynamodb/<br><br>Plus, with DynamoDB it scales, don't need to add read replica complexity and it also supports IoT out of the box - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.WhyDynamoDB. html<br>This is for IoT sensors that send data and I don't need to store forever so, DynamoDB for this use case is better and cheaper allowing scale",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775794,
          "date": "Sat 14 Jan 2023 18:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, Resizing the MySQL General Purpose SSD storage to 6 TB to improve the volume's IOPS will not solve the problem, as the problem is not just related to storage size but also high write latency.<br><br>Option B, Re-architecting the database tier to use Amazon Aurora instead of an RDS MySQL DB instance and adding read replicas would help to improve the read performance, but it won't help in reducing write latency.<br><br>Option D, Using AWS X-Ray to analyze and debug application issues and adding more API servers to match the load, would help in identifying the problem and resolving it, but it will not help in reducing the load on the servers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 850986,
          "date": "Sun 26 Mar 2023 14:25",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "CE is the best choice. DynamoDB is the better choice for the IoT sensors growth.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 844319,
          "date": "Mon 20 Mar 2023 00:04",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "BD fit the requirement. (Cost efficient)<br>CE are the opposite of the requirement, DynamoDB and Kinesis more expensive than Aurora for large scale apps .. even DynamoDB alone more expensive than Aurora for large scale apps<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I forgot to mention that DynamoDB is noSQL database, and requires also a big refactor in the NodeJS app. Does not make sense here to choose DynamoDB over Aurora (MySQL)..</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 844321,
          "date": "Mon 20 Mar 2023 00:08",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "I forgot to mention that DynamoDB is noSQL database, and requires also a big refactor in the NodeJS app. Does not make sense here to choose DynamoDB over Aurora (MySQL)..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 841376,
          "date": "Thu 16 Mar 2023 22:48",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/5011-exam-aws-certified-solutions-architect-professional-topic-1/ DynamoDB or other NoSQL options are not the solutions when organizations need to store predictable, structured data. In that case, Amazon Aurora is the best solution with high scalability and best performances.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 838356,
          "date": "Mon 13 Mar 2023 23:00",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "A C for me.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 835732,
          "date": "Sat 11 Mar 2023 09:10",
          "username": "\t\t\t\tlimjieson\t\t\t",
          "content": "B and D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777119,
          "date": "Sun 15 Jan 2023 23:24",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "CE are correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#81",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building an electronic document management system in which users upload their documents. The application stack is entirely serverless and runs on AWS in the eu-central-1 Region. The system includes a web application that uses an Amazon CloudFront distribution for delivery with Amazon S3 as the origin. The web application communicates with Amazon API Gateway Regional endpoints. The API Gateway APIs call AWS Lambda functions that store metadata in an Amazon Aurora Serverless database and put the documents into an S3 bucket.<br>The company is growing steadily and has completed a proof of concept with its largest customer. The company must improve latency outside of Europe.<br><br>Which combination of actions will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#81",
          "answers": [
            {
              "choice": "<p>A. Enable S3 Transfer Acceleration on the S3 bucket. Ensure that the web application uses the Transfer Acceleration signed URLs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an accelerator in AWS Global Accelerator. Attach the accelerator to the CloudFront distribution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Change the API Gateway Regional endpoints to edge-optimized endpoints.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision the entire stack in two other locations that are spread across the world. Use global databases on the Aurora Serverless cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Add an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850990,
          "date": "Sun 26 Mar 2023 14:28",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AC are the best combo",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 836454,
          "date": "Sat 11 Mar 2023 20:41",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "A) well yes, maybe but we do already have cloudfront in front - so questionable if acceleration has a big impact on performance.<br>B) Not possible. Afaik you cannot attach global accelerator to cloudfront.<br>C) this is possible and should five some improvements for api access.<br>D) its more infrastructure but it would indeed improve performance.<br>E)This is AFAIK not possible.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It may be that this is an outdated question when there was no support for D(serverless v1 &lt;-&gt; serverless v2). In which case I indeed would go with AC instead.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 836457,
          "date": "Sat 11 Mar 2023 20:47",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "It may be that this is an outdated question when there was no support for D(serverless v1 <-> serverless v2). In which case I indeed would go with AC instead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829298,
          "date": "Sat 04 Mar 2023 20:17",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "D is wrong because question asked for improvement. nor re-architect.<br>B is not apt here although it improves networking performance because use case here is for S3 uploads.<br>E is wrong because there is no need of RDS proxy which provides DB connection pooling.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Global accelerator vs CloudFront<br>AWS Global Accelerator is designed for non-HTTP(S) traffic and is used when you have a globally distributed set of endpoints that need to be accessed by clients around the world. CloudFront is designed for HTTP(S) traffic and is used when you have web content that needs to be delivered to users around the world</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 829340,
          "date": "Sat 04 Mar 2023 20:53",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Global accelerator vs CloudFront<br>AWS Global Accelerator is designed for non-HTTP(S) traffic and is used when you have a globally distributed set of endpoints that need to be accessed by clients around the world. CloudFront is designed for HTTP(S) traffic and is used when you have web content that needs to be delivered to users around the world",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 805356,
          "date": "Sat 11 Feb 2023 16:43",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I agree with C vs D because the backend behind the API Gateway return metadata, which are cacheable with an edge-optimized API Gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 794658,
          "date": "Tue 31 Jan 2023 23:07",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Thinking better bout the ones that in combintion would help best, I finally vote B and D. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 794646,
          "date": "Tue 31 Jan 2023 22:57",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I think D will help more to reduce latency than A.  Compleexity or cost is not a factor in the request.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 793355,
          "date": "Mon 30 Jan 2023 22:20",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "well explained by masetromain :)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>His answers are from chatGPT :)</li><li>he often changes idea after someone contradicts.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 828432,
          "date": "Fri 03 Mar 2023 22:18",
          "username": "\t\t\t\tPete697989\t\t\t",
          "content": "His answers are from chatGPT :)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 794648,
          "date": "Tue 31 Jan 2023 22:57",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "he often changes idea after someone contradicts.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 775798,
          "date": "Sat 14 Jan 2023 19:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A and C are correct answers.A.  Enable S3 Transfer Acceleration on the S3 bucket and ensure that the web application uses the Transfer Acceleration signed URLs will accelerate the uploads of documents to S3 bucket, this will help to reduce the latency for users outside of Europe.C.  Change the API Gateway Regional endpoints to edge-optimized endpoints will help the company to improve the latency by caching the responses of the API Gateway closer to the users.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B.  Creating an accelerator in AWS Global Accelerator and attaching it to the CloudFront distribution will not help in this scenario as it only helps to route the traffic to the optimal endpoint based on the location of the user.D.  Provisioning the entire stack in two other locations that are spread across the world and using global databases on the Aurora Serverless cluster will help to reduce the latency but it would be more complex to implement and manage.E.  Adding an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database will not help in this scenario because it is only used to improve connection management and load balancing for Amazon RDS databases, but not for Aurora Serverless databases.</li><li>https://www.examtopics.com/discussions/amazon/view/69470-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 775800,
          "date": "Sat 14 Jan 2023 19:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Creating an accelerator in AWS Global Accelerator and attaching it to the CloudFront distribution will not help in this scenario as it only helps to route the traffic to the optimal endpoint based on the location of the user.D.  Provisioning the entire stack in two other locations that are spread across the world and using global databases on the Aurora Serverless cluster will help to reduce the latency but it would be more complex to implement and manage.E.  Adding an Amazon RDS proxy between the Lambda functions and the Aurora Serverless database will not help in this scenario because it is only used to improve connection management and load balancing for Amazon RDS databases, but not for Aurora Serverless databases.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://www.examtopics.com/discussions/amazon/view/69470-exam-aws-certified-solutions-architect-professional-topic-1/</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775801,
          "date": "Sat 14 Jan 2023 19:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/69470-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#82",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An adventure company has launched a new feature on its mobile app. Users can use the feature to upload their hiking and rafting photos and videos anytime. The photos and videos are stored in Amazon S3 Standard storage in an S3 bucket and are served through Amazon CloudFront.<br><br>The company needs to optimize the cost of the storage. A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days. The solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#82",
          "answers": [
            {
              "choice": "<p>A. Configure S3 Intelligent-Tiering on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure an S3 Lifecycle policy to transition image objects and video objects from S3 Standard to S3 Glacier Deep Archive after 30 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace Amazon S3 with an Amazon Elastic File System (Amazon EFS) file system that is mounted on Amazon EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add a Cache-Control: max-age header to the S3 image objects and S3 video objects. Set the header to 30 days.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850992,
          "date": "Sun 26 Mar 2023 14:29",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A - easy question",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 844337,
          "date": "Mon 20 Mar 2023 00:32",
          "username": "\t\t\t\tokladev\t\t\t",
          "content": "A - S3 Intelligent-Tiering can fit the requirement",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829353,
          "date": "Sat 04 Mar 2023 20:59",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "First half of question drags you to answer B but SA found that some media is being used even after downloads. so data is being accessed in unknown patterns. Way to go is Intelligent tier.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>*I meant even after 30 days (not downloads in above comment)</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829356,
          "date": "Sat 04 Mar 2023 21:00",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "*I meant even after 30 days (not downloads in above comment)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814959,
          "date": "Mon 20 Feb 2023 08:42",
          "username": "\t\t\t\tJungMun\t\t\t",
          "content": "This is my open. The question ask us maintains millisecond retrieval ability. It means we can't use cold storage (So, A, B is not answer). EFS is expensive and not durable. If we use client cache (Ignore client's volume), we can reduce network costs(actually s3's storage costs is really cheap). It means that we can reduce costs too.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>There are lots of wrong types. Please forgive me. English is not familiar with me yet.</li><li>The keyword is millisecond retrieval time, which rules everything out except A. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 814962,
          "date": "Mon 20 Feb 2023 08:44",
          "username": "\t\t\t\tJungMun\t\t\t",
          "content": "There are lots of wrong types. Please forgive me. English is not familiar with me yet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The keyword is millisecond retrieval time, which rules everything out except A. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820154,
          "date": "Fri 24 Feb 2023 07:05",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The keyword is millisecond retrieval time, which rules everything out except A. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 809688,
          "date": "Wed 15 Feb 2023 16:32",
          "username": "\t\t\t\tklog\t\t\t",
          "content": "bc A solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. However, some of the uploaded photos and videos are accessed frequently after 30 days.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793358,
          "date": "Mon 30 Jan 2023 22:21",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "typico A S3 Intelligent-Tiering",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 792388,
          "date": "Mon 30 Jan 2023 05:22",
          "username": "\t\t\t\tjhonivy\t\t\t",
          "content": "D it will reduce the cost on retrieval requests",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776247,
          "date": "Sun 15 Jan 2023 09:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A.  Configure S3 Intelligent-Tiering on the S3 bucket.<br><br>Amazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers based on changing access patterns. Objects that are accessed frequently are stored in the frequent access tier and objects that are accessed infrequently are stored in the infrequent access tier. This allows for cost optimization without requiring manual intervention. This makes it an ideal solution for the scenario described, as it can automatically move objects that are infrequently accessed after 30 days to a lower-cost storage tier while still maintaining millisecond retrieval availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not correct as it only moves data to S3 Glacier Deep Archive after 30 days, which would still require additional steps to retrieve the data.<br>Option C is not correct because Amazon Elastic File System (Amazon EFS) is a file storage service for use with Amazon EC2 instances, it does not provide a cost-effective solution for storing and retrieving large amounts of data.<br>Option D is not correct because adding a Cache-Control: max-age header only controls the caching behavior of the objects and does not address the cost optimization requirements.</li><li>Option D works for the reduction cost on retrieval request</li><li>take the test then tell us if your answers are valid, if they are share them with us ;)</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776248,
          "date": "Sun 15 Jan 2023 09:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not correct as it only moves data to S3 Glacier Deep Archive after 30 days, which would still require additional steps to retrieve the data.<br>Option C is not correct because Amazon Elastic File System (Amazon EFS) is a file storage service for use with Amazon EC2 instances, it does not provide a cost-effective solution for storing and retrieving large amounts of data.<br>Option D is not correct because adding a Cache-Control: max-age header only controls the caching behavior of the objects and does not address the cost optimization requirements.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D works for the reduction cost on retrieval request</li><li>take the test then tell us if your answers are valid, if they are share them with us ;)</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 792380,
          "date": "Mon 30 Jan 2023 05:15",
          "username": "\t\t\t\tjhonivy\t\t\t",
          "content": "Option D works for the reduction cost on retrieval request<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>take the test then tell us if your answers are valid, if they are share them with us ;)</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792822,
          "date": "Mon 30 Jan 2023 14:23",
          "username": "\t\t\t\tyoungprinceton\t\t\t",
          "content": "take the test then tell us if your answers are valid, if they are share them with us ;)",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#83",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses Amazon S3 to store files and images in a variety of storage classes. The company's S3 costs have increased substantially during the past year.<br><br>A solutions architect needs to review data trends for the past 12 months and identity the appropriate storage class for the objects.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#83",
          "answers": [
            {
              "choice": "<p>A. Download AWS Cost and Usage Reports for the last 12 months of S3 usage. Review AWS Trusted Advisor recommendations for cost savings.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use S3 storage class analysis. Import data trends into an Amazon QuickSight dashboard to analyze storage trends.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon S3 Storage Lens. Upgrade the default dashboard to include advanced metrics for storage trends.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Access Analyzer for S3. Download the Access Analyzer for S3 report for the last 12 months. Import the .csv file to an Amazon QuickSight dashboard.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 853761,
          "date": "Wed 29 Mar 2023 00:30",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "C - storage lens",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 850994,
          "date": "Sun 26 Mar 2023 14:30",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C - storage lens",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 844059,
          "date": "Sun 19 Mar 2023 18:40",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "C - https://aws.amazon.com/blogs/storage/5-ways-to-reduce-costs-using-amazon-s3-storage-lens/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 842427,
          "date": "Sat 18 Mar 2023 03:17",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Storage class: After you configure a filter, you'll start seeing data analysis based on the filter in the Amazon S3 console in 24 to 48 hours. However, storage class analysis observes the access patterns of a filtered data set for 30 days or longer to gather information for analysis before giving a result<br><br>Storage Lens: All S3 Storage Lens metrics are retained for a period of 15 months. However, metrics are only available for queries for a specific duration, which depends on your metrics selection. This duration can't be modified. Free metrics are available for queries for a 14-day period, and advanced metrics are available for queries for a 15-month period.<br><br>You have to upgrade regardless to query up to 12 months",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830740,
          "date": "Mon 06 Mar 2023 12:08",
          "username": "\t\t\t\tandras\t\t\t",
          "content": "S3 is not among the cost optimization in trusted Advisor:<br>https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814668,
          "date": "Mon 20 Feb 2023 00:46",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C - Storage Lens",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 811068,
          "date": "Thu 16 Feb 2023 21:14",
          "username": "\t\t\t\tryansmithnz\t\t\t",
          "content": "o review data trends and identify the appropriate storage class for the objects, the solutions architect can use the Amazon S3 Storage Lens feature. Amazon S3 Storage Lens provides organization-wide visibility into object storage, access, and usage patterns, making it easier to identify cost optimization opportunities and enforce compliance policies.<br><br>The architect can use the Storage Lens dashboard to view trends and metrics for the past 12 months, such as storage utilization, object size distribution, and access patterns. Based on these insights, the architect can determine the appropriate storage class for each object.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 806880,
          "date": "Sun 12 Feb 2023 23:58",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "not B because there is no cost information from this.<br>not C because it cannot analyze past access trends if you just activate storage lens<br>not D because you are just enabling it <br>choosing A because it will do the job. now.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Storage class analysis only provides recommendations for Standard to Standard IA classes.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817369,
          "date": "Wed 22 Feb 2023 03:03",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Storage class analysis only provides recommendations for Standard to Standard IA classes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798895,
          "date": "Sun 05 Feb 2023 15:34",
          "username": "\t\t\t\tHeer\t\t\t",
          "content": "OPTION C:<br><br>If you have an increasing number of Amazon S3 buckets, spread across tens or even hundreds of accounts, you might be in search of a tool that makes it easier to manage your growing storage footprint and improve cost efficiencies. S3 Storage Lens is an analytics feature built-in to the S3 console to help you gain organization-wide visibility into your object storage usage and activity trends, and to identify cost savings opportunities. S3 Storage Lens is available for all S3 accounts, free of charge. You can also upgrade to advanced metrics to receive additional metrics, insights, and an extended data retention period.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795325,
          "date": "Wed 01 Feb 2023 17:48",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B or C won't analyze anything from before enabling it. You are simply late. If you want to anlayze previous 12 months, go for A. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This is the correct answer.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825275,
          "date": "Tue 28 Feb 2023 22:02",
          "username": "\t\t\t\tSuperP43\t\t\t",
          "content": "This is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795310,
          "date": "Wed 01 Feb 2023 17:40",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "Both B and C can do that, but the question did not mention about cost so I vote for C because It has UI friendly :). <br>Advanced metrics in S3 Storage Lens help Generate metrics that can help you manage and optimize your storage costs, such as lifecycle rule counts for transitions, expirations, and incomplete multipart uploads. Data is available for queries for 15 months",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795304,
          "date": "Wed 01 Feb 2023 17:37",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "It can't be B.  When you upgrade AWS S3 Storage Lens to get advanced metrics, metrics data is only preserved since the date when you upgraded, not for the previous 15 months before you upgraded. The request is to review the previous data for the last 12 months.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790769,
          "date": "Sat 28 Jan 2023 16:53",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Both B and C are good.<br>I guess AWS wants clients to use S3 Storage Lens... Hence I vote C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>agree with u gess aws want us to know about Lens</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793361,
          "date": "Mon 30 Jan 2023 22:24",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "agree with u gess aws want us to know about Lens",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 786626,
          "date": "Tue 24 Jan 2023 15:52",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "Answer B makes more sense as using QS, data can be analyzed further.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785362,
          "date": "Mon 23 Jan 2023 14:04",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I vote C \\\"S3 Storage Lens\\\".<br>free metrics cannot provide recommendation, but upgrade metrics can provide recommendation.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 781213,
          "date": "Thu 19 Jan 2023 15:10",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776251,
          "date": "Sun 15 Jan 2023 09:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  The solution of using S3 storage class analysis and importing data trends into an Amazon QuickSight dashboard will allow the solutions architect to review data trends for the past 12 months and identify the appropriate storage class for the objects. This solution will allow the architect to see which objects are frequently accessed, which objects are infrequently accessed, and the costs associated with the different storage classes.<br><br>Option A, Downloading AWS Cost and Usage Reports for the last 12 months of S3 usage and reviewing AWS Trusted Advisor recommendations for cost savings, will provide information on S3 costs but does not provide data trends that allow the architect to determine the appropriate storage class for the objects.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, Using Amazon S3 Storage Lens and upgrading the default dashboard to include advanced metrics for storage trends, will provide information on S3 usage but does not provide data trends that allow the architect to determine the appropriate storage class for the objects.<br><br>Option D, Using Access Analyzer for S3, and downloading the Access Analyzer for S3 report for the last 12 months and importing the .csv file to an Amazon QuickSight dashboard, is not relevant as it is used to analyze access controls and permissions to S3 resources, and it does not provide data trends that allow the architect to determine the appropriate storage class for the objects.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776252,
          "date": "Sun 15 Jan 2023 09:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C, Using Amazon S3 Storage Lens and upgrading the default dashboard to include advanced metrics for storage trends, will provide information on S3 usage but does not provide data trends that allow the architect to determine the appropriate storage class for the objects.<br><br>Option D, Using Access Analyzer for S3, and downloading the Access Analyzer for S3 report for the last 12 months and importing the .csv file to an Amazon QuickSight dashboard, is not relevant as it is used to analyze access controls and permissions to S3 resources, and it does not provide data trends that allow the architect to determine the appropriate storage class for the objects.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#84",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.<br><br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#84",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850995,
          "date": "Sun 26 Mar 2023 14:31",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use AWS Organizations and AWS CloudFormation StackSets",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793369,
          "date": "Mon 30 Jan 2023 22:28",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The correct answer is C",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776255,
          "date": "Sun 15 Jan 2023 09:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Use AWS Organizations and AWS CloudFormation StackSets.<br>AWS Organizations allows the management of multiple AWS accounts as a single entity and AWS CloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts and regions in an organization. This solution allows creating a single CloudFormation template that can be deployed across multiple accounts and regions, and also allows for the management of access and permissions for the different accounts through the use of IAM roles and policies in the management account.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A and D both use AWS CloudFormation, but do not take into account the management of multiple accounts and regions. Option B uses AWS Organizations but doesn't include the use of CloudFormation StackSets, which is necessary for managing deployments across multiple accounts and regions.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776256,
          "date": "Sun 15 Jan 2023 09:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A and D both use AWS CloudFormation, but do not take into account the management of multiple accounts and regions. Option B uses AWS Organizations but doesn't include the use of CloudFormation StackSets, which is necessary for managing deployments across multiple accounts and regions.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#85",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has its cloud infrastructure on AWS. A solutions architect needs to define the infrastructure as code. The infrastructure is currently deployed in one AWS Region. The company's business expansion plan includes deployments in multiple Regions across multiple AWS accounts.<br><br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#85",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudFormation templates. Add IAM policies to control the various accounts, Deploy the templates across the multiple Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Organizations. Deploy AWS CloudFormation templates from the management account Use AWS Control Tower to manage deployments across accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Organizations and AWS CloudFormation StackSets. Deploy a Cloud Formation template from an account that has the necessary IAM permissions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use nested stacks with AWS CloudFormation templates. Change the Region by using nested stacks.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 795338,
          "date": "Wed 01 Feb 2023 17:58",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "This is repeated :-(",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795312,
          "date": "Wed 01 Feb 2023 17:42",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "Duplicate question with #84",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777137,
          "date": "Sun 15 Jan 2023 23:39",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776257,
          "date": "Sun 15 Jan 2023 09:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "same question of \\\"Questions #84\\\"",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#86",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company plans to refactor a monolithic application into a modern application design deployed on AWS. The CI/CD pipeline needs to be upgraded to support the modern design for the application with the following requirements:<br><br>• It should allow changes to be released several times every hour.<br>• It should be able to roll back the changes as quickly as possible.<br><br>Which design will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#86",
          "answers": [
            {
              "choice": "<p>A. Deploy a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Specify AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application. To deploy, swap the staging and production environment URLs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Systems Manager to re-provision the infrastructure for each deployment. Update the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and use Amazon Route 53 weighted routing to point to the new environment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Roll out the application updates as part of an Auto Scaling event using prebuilt AMIs. Use new versions of the AMIs to add instances. and phase out all instances that use the previous AMI version with the configured termination policy during a deployment event.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850999,
          "date": "Sun 26 Mar 2023 14:32",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Specify AWS Elastic Beanstalk",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 785014,
          "date": "Mon 23 Jan 2023 06:40",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776259,
          "date": "Sun 15 Jan 2023 09:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Specifying AWS Elastic Beanstalk to stage in a secondary environment as the deployment target for the CI/CD pipeline of the application and swapping the staging and production environment URLs. This approach allows the company to deploy updates several times an hour and quickly roll back changes as needed.<br><br>Option A, Deploying a CI/CD pipeline that incorporates AMIs to contain the application and their configurations. Deploy the application by replacing Amazon EC2 instances, while it may provide a way to roll back changes by replacing instances with previous versions, it may not allow for rapid deployment of updates multiple times per hour.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, Using AWS Systems Manager to re-provision the infrastructure for each deployment. Updating the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and using Amazon Route 53 weighted routing to point to the new environment, would require more time-consuming steps and may not be able to roll back changes as quickly.<br><br>Option D, Rolling out the application updates as part of an Auto Scaling event using prebuilt AMIs. Using new versions of the AMIs to add instances and phasing out all instances that use the previous AMI version with the configured termination policy during a deployment event, while it may be a way to roll back changes, it doesn't allow for rapid deployment of updates multiple times per hour.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776260,
          "date": "Sun 15 Jan 2023 09:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C, Using AWS Systems Manager to re-provision the infrastructure for each deployment. Updating the Amazon EC2 user data to pull the latest code artifact from Amazon S3 and using Amazon Route 53 weighted routing to point to the new environment, would require more time-consuming steps and may not be able to roll back changes as quickly.<br><br>Option D, Rolling out the application updates as part of an Auto Scaling event using prebuilt AMIs. Using new versions of the AMIs to add instances and phasing out all instances that use the previous AMI version with the configured termination policy during a deployment event, while it may be a way to roll back changes, it doesn't allow for rapid deployment of updates multiple times per hour.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#87",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that runs on Amazon EC2 instances. A solutions architect is designing VPC infrastructure in an AWS Region where the application needs to access an Amazon Aurora DB Cluster. The EC2 instances are all associated with the same security group. The DB cluster is associated with its own security group.<br><br>The solutions architect needs to add rules to the security groups to provide the application with least privilege access to the DB Cluster.<br><br>Which combination of steps will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#87",
          "answers": [
            {
              "choice": "<p>A. Add an inbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the source over the default Aurora port.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the default Aurora port.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Add an outbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the destination over the ephemeral ports.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776264,
          "date": "Sun 15 Jan 2023 09:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct combination of steps to meet these requirements is B and C. <br>B.  Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port. This allows the instances to make outbound connections to the DB cluster on the default Aurora port.<br>C.  Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port. This allows connections to the DB cluster from the EC2 instances on the default Aurora port.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Adding an inbound rule to the EC2 instances' security group would allow incoming connections to the instances on the default Aurora port, but it would not allow the instances to connect to the DB cluster.<br>D.  Adding an outbound rule to the DB cluster's security group would allow the DB cluster to make outbound connections to the EC2 instances on the default Aurora port, but it would not allow connections to the DB cluster from the instances.<br>E.  Adding an outbound rule to the DB cluster's security group specifying the EC2 instances' security group as the destination over the ephemeral ports would allow the DB cluster to make outbound connections to the instances on ephemeral ports, but it would not allow connections to the DB cluster from the instances on the default Aurora port.</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 776267,
          "date": "Sun 15 Jan 2023 09:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Adding an inbound rule to the EC2 instances' security group would allow incoming connections to the instances on the default Aurora port, but it would not allow the instances to connect to the DB cluster.<br>D.  Adding an outbound rule to the DB cluster's security group would allow the DB cluster to make outbound connections to the EC2 instances on the default Aurora port, but it would not allow connections to the DB cluster from the instances.<br>E.  Adding an outbound rule to the DB cluster's security group specifying the EC2 instances' security group as the destination over the ephemeral ports would allow the DB cluster to make outbound connections to the instances on ephemeral ports, but it would not allow connections to the DB cluster from the instances on the default Aurora port.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851002,
          "date": "Sun 26 Mar 2023 14:36",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "BC gets my vote",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 842430,
          "date": "Sat 18 Mar 2023 03:36",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Look at the traffic - from the instances EC2 -> DB Cluster I need to go to it as the destination and port (outbound, nothing more or less); so that DB responses needs to see my Security group (since they are shared) coming inbound on that port; any other port deny. <br><br>https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/working-with-security-groups.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 822262,
          "date": "Sun 26 Feb 2023 11:46",
          "username": "\t\t\t\tGabehcoud\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/security-group-rules.html<br>By default, security groups contain outbound rules that allow all outbound traffic. So why do we even need a outbound rule? Guys common. lets not confuse each other.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"You can delete these rules...\\\"<br>Practice Security Best Practices - although default why are you leaving all outbound traffic open?<br><br>Besides, to go w/ least privilege access would delete the outbound all rule and only allow outbound to DB cluster.</li><li>That is a really good point, keep in mind that is when you create a security group using the GUI/Console when you use API the SG outbound does not have that allow-all. But again this is not part of the question. If we add that outbound rule, should we need to add others like DNS???</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 842432,
          "date": "Sat 18 Mar 2023 03:45",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "\\\"You can delete these rules...\\\"<br>Practice Security Best Practices - although default why are you leaving all outbound traffic open?<br><br>Besides, to go w/ least privilege access would delete the outbound all rule and only allow outbound to DB cluster.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826221,
          "date": "Wed 01 Mar 2023 20:39",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "That is a really good point, keep in mind that is when you create a security group using the GUI/Console when you use API the SG outbound does not have that allow-all. But again this is not part of the question. If we add that outbound rule, should we need to add others like DNS???",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810331,
          "date": "Thu 16 Feb 2023 07:48",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "To provide the application with least privilege access to the Aurora DB cluster, the solutions architect should add inbound rules to both the security groups.<br><br>For the EC2 instances' security group, an inbound rule should be added that allows traffic from the DB cluster's security group over the default Aurora port. This will allow the EC2 instances to communicate with the Aurora DB cluster.<br><br>For the Aurora DB cluster's security group, an inbound rule should be added that allows traffic from the EC2 instances' security group over the default Aurora port. This will allow the Aurora DB cluster to communicate with the EC2 instances.<br><br>By default all outbound rules are open, it's only the ingress that needs to allow traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B&amp;C after doing a recreate in the AWS Console, stand corrected.</li><li>To provide the application with least privilege access to the Amazon Aurora DB Cluster, the solutions architect should take the following steps:<br><br>Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port (port 3306). This will allow the EC2 instances to connect to the Aurora DB Cluster.<br><br>Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port (port 3306). This will allow the EC2 instances to send traffic to the Aurora DB Cluster.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 817384,
          "date": "Wed 22 Feb 2023 03:15",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "B&C after doing a recreate in the AWS Console, stand corrected.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To provide the application with least privilege access to the Amazon Aurora DB Cluster, the solutions architect should take the following steps:<br><br>Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port (port 3306). This will allow the EC2 instances to connect to the Aurora DB Cluster.<br><br>Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port (port 3306). This will allow the EC2 instances to send traffic to the Aurora DB Cluster.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 818556,
          "date": "Thu 23 Feb 2023 00:54",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "To provide the application with least privilege access to the Amazon Aurora DB Cluster, the solutions architect should take the following steps:<br><br>Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port (port 3306). This will allow the EC2 instances to connect to the Aurora DB Cluster.<br><br>Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port (port 3306). This will allow the EC2 instances to send traffic to the Aurora DB Cluster.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795322,
          "date": "Wed 01 Feb 2023 17:47",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "Flow connection: EC2 -> DB<br>So you need to configure Outbound EC2 and Inbound DB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 793384,
          "date": "Mon 30 Jan 2023 22:37",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "seems logic<br>outbound EC2 and inbound to DB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        }
      ]
    },
    {
      "question_id": "#88",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to change its internal cloud billing strategy for each of its business units. Currently, the cloud governance team shares reports for overall cloud spending with the head of each business unit. The company uses AWS Organizations to manage the separate AWS accounts for each business unit. The existing tagging standard in Organizations includes the application, environment, and owner. The cloud governance team wants a centralized solution so each business unit receives monthly reports on its cloud spending. The solution should also send notifications for any cloud spending that exceeds a set threshold.<br><br>Which solution is the MOST cost-effective way to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#88",
          "answers": [
            {
              "choice": "<p>A. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in each account to create monthly reports for each business unit.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure AWS Budgets in each account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable AWS Cost and Usage Reports in the organization's management account and configure reports grouped by application, environment. and owner. Create an AWS Lambda function that processes AWS Cost and Usage Reports, sends budget alerts, and sends monthly reports to each business unit's email list.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776270,
          "date": "Sun 15 Jan 2023 09:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Configure AWS Budgets in the organization's management account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's management account to create monthly reports for each business unit.<br><br>This option is the most cost-effective because it utilizes the organization's management account to set budgets and configure alerts for all accounts in the organization, rather than having to configure budgets and alerts individually in each account. Additionally, using Cost Explorer in the management account allows the cloud governance team to view the consolidated spending for all accounts in the organization and create reports for each business unit. This eliminates the need to access each individual account to view costs and create reports.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not the most cost-effective solution because it requires configuring budgets and reports in multiple accounts, which increases the complexity and cost of managing the cloud spending for each business unit.<br><br>Option C is not the most cost-effective solution because it requires the cloud governance team to access the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit, which increases the complexity and cost of managing the cloud spending for each business unit.<br><br>Option D is not the most cost-effective solution because it requires creating an AWS Lambda function to process AWS Cost and Usage Reports, which increases the complexity and cost of managing the cloud spending for each business unit.</li></ul>",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776271,
          "date": "Sun 15 Jan 2023 09:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not the most cost-effective solution because it requires configuring budgets and reports in multiple accounts, which increases the complexity and cost of managing the cloud spending for each business unit.<br><br>Option C is not the most cost-effective solution because it requires the cloud governance team to access the AWS Billing and Cost Management dashboard in each account to create monthly reports for each business unit, which increases the complexity and cost of managing the cloud spending for each business unit.<br><br>Option D is not the most cost-effective solution because it requires creating an AWS Lambda function to process AWS Cost and Usage Reports, which increases the complexity and cost of managing the cloud spending for each business unit.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851003,
          "date": "Sun 26 Mar 2023 14:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Configure AWS Budgets in the organization's management account",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#89",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using AWS CloudFormation to deploy its infrastructure. The company is concerned that, if a production CloudFormation stack is deleted, important data stored in Amazon RDS databases or Amazon EBS volumes might also be deleted.<br><br>How can the company prevent users from accidentally deleting data in this way?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#89",
          "answers": [
            {
              "choice": "<p>A. Modify the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure a stack policy that disallows the deletion of RDS and EBS resources.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify IAM policies lo deny deleting RDS and EBS resources that are tagged with an \"aws:cloudformation:stack-name\" tag.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Config rules to prevent deleting RDS and EBS resources.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851006,
          "date": "Sun 26 Mar 2023 14:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 842435,
          "date": "Sat 18 Mar 2023 03:57",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html<br>With the DeletionPolicy attribute you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.<br>Retain<br>CloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted. You can add this deletion policy to any resource type. When CloudFormation completes the stack deletion, the stack will be in Delete_Complete state; however, resources that are retained continue to exist and continue to incur applicable charges until you delete those resource",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833396,
          "date": "Wed 08 Mar 2023 22:58",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "A stack policy is a document that defines the update and deletion actions that can be performed on resources in a CloudFormation stack. By default, all resources in a CloudFormation stack can be deleted by users with appropriate permissions. However, you can use a stack policy to restrict the deletion of certain resources, such as Amazon RDS databases or Amazon EBS volumes.<br><br>In this case, the company can create a stack policy that explicitly disallows the deletion of any RDS or EBS resources in the production CloudFormation stack. This will prevent users from accidentally deleting important data stored in these resources.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 829621,
          "date": "Sun 05 Mar 2023 05:17",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "For RDS instances, you can set the \\\"DeletionPolicy\\\" attribute to \\\"Retain\\\". This will ensure that when the stack is deleted, the RDS instance will not be deleted and its data will be retained. <br><br>For EBS volumes, you can use the \\\"DeletionPolicy\\\" attribute in combination with the \\\"SnapshotId\\\" attribute to create a snapshot of the volume before deleting it. This will allow you to restore the data later if need<br><br>Yaml examples for RDS and EBS :<br><br>Resources:<br>MyDB:<br>Type: AWS::RDS::DBInstance<br>Properties:<br># RDS instance properties go here<br>DeletionPolicy: Retain<br><br>Resources:<br>MyVolume:<br>Type: AWS::EC2::Volume<br>Properties:<br># Volume properties go here<br>DeletionPolicy: Snapshot<br>SnapshotId: my-snapshot-id",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 811253,
          "date": "Fri 17 Feb 2023 01:36",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Clear A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808891,
          "date": "Tue 14 Feb 2023 23:04",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "AC1984 do your homework.<br>Stack policy can protect against deletion but not against actual entire CFN stack template being deleted. DeletionPolicy = if I was to delete the entire CFN stack, the CFN process will delete all elements and skip over RDS and EBS due to protections. 20 second Google search could of confirmed this.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808104,
          "date": "Tue 14 Feb 2023 06:39",
          "username": "\t\t\t\tAC1984\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808102,
          "date": "Tue 14 Feb 2023 06:37",
          "username": "\t\t\t\tAC1984\t\t\t",
          "content": "B.  Configure a stack policy that disallows the deletion of RDS and EBS resources.<br>A stack policy is a JSON-based document that defines the actions that can be performed on a CloudFormation stack, and can be used to prevent users from accidentally deleting critical resources. By configuring a stack policy that disallows the deletion of RDS and EBS resources, the company can prevent users from accidentally deleting important data stored in those resources.<br><br>Option A (adding a DeletionPolicy attribute) does not prevent users from deleting the resources, but rather determines what happens to the resources when the stack is deleted. Option C (modifying IAM policies) is not sufficient because it only affects the permissions of specific users or groups, and does not prevent accidental deletions. Option D (using AWS Config rules) can help detect deletions of RDS and EBS resources, but it does not prevent them from being deleted.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"Option A (adding a DeletionPolicy attribute) does not prevent users from deleting the resources, but rather determines what happens to the resources when the stack is deleted.\\\" This is actually what the question is asking !</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 827767,
          "date": "Fri 03 Mar 2023 09:24",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "\\\"Option A (adding a DeletionPolicy attribute) does not prevent users from deleting the resources, but rather determines what happens to the resources when the stack is deleted.\\\" This is actually what the question is asking !",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 805515,
          "date": "Sat 11 Feb 2023 19:26",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "I go for A because I assume that the CF stack is allowed to be deleted in some deployment scenarios.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793388,
          "date": "Mon 30 Jan 2023 22:45",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 785490,
          "date": "Mon 23 Jan 2023 15:51",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "\\\"DeletionPolicy\\\" : \\\"Retain\\\"　can prevent to delete resource",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 780227,
          "date": "Wed 18 Jan 2023 17:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I switch to A:<br>https://www.examtopics.com/discussions/amazon/view/5233-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>Modifying the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources, is another valid solution to prevent accidental deletion of data in this scenario. By adding a DeletionPolicy attribute of \\\"Retain\\\" to RDS and EBS resources in the CloudFormation templates, the company can ensure that these resources and their data are not deleted when the CloudFormation stack is deleted. This is a way to prevent accidental deletion of data by preserving the resources when the stack is deleted.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, Configuring a stack policy that disallows the deletion of RDS and EBS resources, would also prevent accidental deletion of data by RDS and EBS resources, but it does so by controlling access to the resources rather than preserving the resources as Option A does. Stack policies are a way to set up fine-grained access controls for the CloudFormation stack, so it would prevent users who are not authorized to delete RDS and EBS resources from doing so.<br><br>Option C, Modifying IAM policies to deny deleting RDS and EBS resources that are tagged with an \\\"aws:cloudformation:stack-name\\\" tag, is not a good solution because it only controls who can delete the resources, not whether they are deleted or retained when the stack is deleted.<br><br>Option D, Using AWS Config rules to prevent deleting RDS and EBS resources, is also not a good solution because AWS Config only records and monitors the configuration changes, it does not prevent any action on the resources.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 780228,
          "date": "Wed 18 Jan 2023 17:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B, Configuring a stack policy that disallows the deletion of RDS and EBS resources, would also prevent accidental deletion of data by RDS and EBS resources, but it does so by controlling access to the resources rather than preserving the resources as Option A does. Stack policies are a way to set up fine-grained access controls for the CloudFormation stack, so it would prevent users who are not authorized to delete RDS and EBS resources from doing so.<br><br>Option C, Modifying IAM policies to deny deleting RDS and EBS resources that are tagged with an \\\"aws:cloudformation:stack-name\\\" tag, is not a good solution because it only controls who can delete the resources, not whether they are deleted or retained when the stack is deleted.<br><br>Option D, Using AWS Config rules to prevent deleting RDS and EBS resources, is also not a good solution because AWS Config only records and monitors the configuration changes, it does not prevent any action on the resources.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777146,
          "date": "Sun 15 Jan 2023 23:51",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776274,
          "date": "Sun 15 Jan 2023 09:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Configuring a stack policy that disallows the deletion of RDS and EBS resources will prevent users from accidentally deleting data stored in these resources when a production CloudFormation stack is deleted.<br>A.  Modifying the CloudFormation templates to add a DeletionPolicy attribute to RDS and EBS resources may prevent data from being deleted, but it does not prevent the stack deletion itself, which is the main concern.<br>C.  Modifying IAM policies to deny deleting RDS and EBS resources that are tagged with an \\\"aws:cloudformation:stack-name\\\" tag, would not help to prevent users from accidentally deleting data stored in these resources when a production CloudFormation stack is deleted.<br>D.  Using AWS Config rules to prevent deleting RDS and EBS resources would prevent users from accidentally deleting data stored in these resources when a production CloudFormation stack is deleted, but it does not prevent the stack deletion itself.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Bro, Question is not concerned about stack deletion, only RDS/EBS data.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 829622,
          "date": "Sun 05 Mar 2023 05:20",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Bro, Question is not concerned about stack deletion, only RDS/EBS data.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#90",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has VPC flow logs enabled for Its NAT gateway. The company is seeing Action = ACCEPT for inbound traffic that comes from public IP address 198.51.100.2 destined for a private Amazon EC2 instance.<br><br>A solutions architect must determine whether the traffic represents unsolicited inbound connections from the internet. The first two octets of the VPC CIDR block are 203.0.<br><br>Which set of steps should the solutions architect take to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#90",
          "answers": [
            {
              "choice": "<p>A. Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interlace. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 203.0\" and the source address set as \"like 198.51.100.2\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Open the AWS CloudTrail console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \"like 198.51.100.2\" and the source address set as \"like 203.0\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 786637,
          "date": "Tue 24 Jan 2023 16:16",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "I would go with option B.  Source will be public IP like 198.51.100.2.",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 851014,
          "date": "Sun 26 Mar 2023 14:48",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Option B is better than option D because it filters the logs based on the source address \\\"like 198.51.100.2\\\" (the public IP address) and the destination address \\\"like 203.0\\\" (which corresponds to the VPC CIDR block). By analyzing the traffic between these addresses, you can determine whether the traffic represents unsolicited inbound connections from the internet.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842690,
          "date": "Sat 18 Mar 2023 12:12",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842436,
          "date": "Sat 18 Mar 2023 04:09",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "B is the better option -> we need to understand why and which IPs this is hitting. If it is being allowed through, then we can identify the actual Private internal VPC that may have a bad inbound security group allowing connectivity. <br><br>Once that is identified, we could reverse (or add an 'or') to see all inbound and outbound traffic between the two. <br><br>Identify the actual flow first before flipping it hence statement:<br>Query results show traffic on the NAT gateway private IP from the public IP, but not traffic on other private IPs in the VPC.  These results confirm that the incoming traffic was unsolicited. However, if you do see traffic on the private instance's IP, then follow the steps under Reason #2.<br><br>(this is more security analytics/traffic log review)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837556,
          "date": "Mon 13 Mar 2023 03:30",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "source is internet address",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 833403,
          "date": "Wed 08 Mar 2023 23:11",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/<br><br>Refer Reason 1<br><br>Run the query below.<br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10<br>Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.<br><br>Query results show traffic on the NAT gateway private IP from the public IP, but not traffic on other private IPs in the VPC.  These results confirm that the incoming traffic was unsolicited. However, if you do see traffic on the private instance's IP, then follow the steps under Reason #2.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For those that are choosing D - this is why D is incorrect and needs to be B</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 842437,
          "date": "Sat 18 Mar 2023 04:11",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "For those that are choosing D - this is why D is incorrect and needs to be B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819324,
          "date": "Thu 23 Feb 2023 15:58",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "for me correct answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819285,
          "date": "Thu 23 Feb 2023 15:23",
          "username": "\t\t\t\tmonkeyfish\t\t\t",
          "content": "B because:<br>destination address set as \\\"like 203.0\\\"<br>source address set as \\\"like 198.51.100.2\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 817391,
          "date": "Wed 22 Feb 2023 03:20",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Option D is the correct answer because Amazon CloudWatch Logs Insights can be used to filter the sum of bytes transferred by the source address and the destination address. The solution architect should open the CloudWatch console, select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface, and then run a query to filter with the destination address set as \\\"like 198.51.100.2\\\" and the source address set as \\\"like 203.0\\\". Finally, run the stats command to filter the sum of bytes transferred by the source address and the destination address.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 813287,
          "date": "Sat 18 Feb 2023 17:36",
          "username": "\t\t\t\tgetsat0024\t\t\t",
          "content": "It is B <br>Reference:https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Based in this is D.  <br><br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10</li><li>my bad is B <br><br> Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.</li><li>VPC cidr is SRC \\\"203.0\\\" <br>DST = 195.51.100.2 which is public address<br><br>keep in mind you're tracing the route from the VPC SRC (203.0.) to the Public internet DST 195.51.100.2 using cloudwatch.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 815702,
          "date": "Mon 20 Feb 2023 20:08",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "Based in this is D.  <br><br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>my bad is B <br><br> Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.</li><li>VPC cidr is SRC \\\"203.0\\\" <br>DST = 195.51.100.2 which is public address<br><br>keep in mind you're tracing the route from the VPC SRC (203.0.) to the Public internet DST 195.51.100.2 using cloudwatch.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815707,
          "date": "Mon 20 Feb 2023 20:10",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "my bad is B <br><br> Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>VPC cidr is SRC \\\"203.0\\\" <br>DST = 195.51.100.2 which is public address<br><br>keep in mind you're tracing the route from the VPC SRC (203.0.) to the Public internet DST 195.51.100.2 using cloudwatch.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817396,
          "date": "Wed 22 Feb 2023 03:26",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "VPC cidr is SRC \\\"203.0\\\" <br>DST = 195.51.100.2 which is public address<br><br>keep in mind you're tracing the route from the VPC SRC (203.0.) to the Public internet DST 195.51.100.2 using cloudwatch.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 811256,
          "date": "Fri 17 Feb 2023 01:39",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "I think B - SOurce public IP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 806067,
          "date": "Sun 12 Feb 2023 08:33",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 805529,
          "date": "Sat 11 Feb 2023 19:46",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Trick question. A NAT Gateway is not supposed to accept incoming traffic. There might be a security issue here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/<br><br>basically it accepts the traffic, then dopt it.<br>the idea is check if the sum is more than 0 from NAT to Public Destination</li><li>looks like i am wrongshould be D<br><br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10<br><br>Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.</li><li>No I think you are right. It talks about *inbound* traffic. Meaning source is the public IP. If this is even possible in reality: don't know. Question does say inbound - so....</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 815700,
          "date": "Mon 20 Feb 2023 20:07",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-nat-gateway/<br><br>basically it accepts the traffic, then dopt it.<br>the idea is check if the sum is more than 0 from NAT to Public Destination<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>looks like i am wrongshould be D<br><br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10<br><br>Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.</li><li>No I think you are right. It talks about *inbound* traffic. Meaning source is the public IP. If this is even possible in reality: don't know. Question does say inbound - so....</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 815708,
          "date": "Mon 20 Feb 2023 20:12",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "looks like i am wrongshould be D<br><br>filter (dstAddr like 'xxx.xxx' and srcAddr like 'public IP')<br>| stats sum(bytes) as bytesTransferred by srcAddr, dstAddr<br>| limit 10<br><br>Note: You can use just the first two octets in the search filter to analyze all network interfaces in the VPC.  In the example above, replace xxx.xxx with the first two octets of your VPC classless inter-domain routing (CIDR). Also, replace public IP with the public IP that you're seeing in the VPC flow log entry.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No I think you are right. It talks about *inbound* traffic. Meaning source is the public IP. If this is even possible in reality: don't know. Question does say inbound - so....</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837563,
          "date": "Mon 13 Mar 2023 03:37",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "No I think you are right. It talks about *inbound* traffic. Meaning source is the public IP. If this is even possible in reality: don't know. Question does say inbound - so....",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790785,
          "date": "Sat 28 Jan 2023 17:08",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "This is a trick question.<br>'Action = ACCEPT' response is sent from private EC2 instances (203.0.x.x) to the public IP address (198.51.100.2) via the NAT Gateway. The NAT Gateway can catch only this outbound traffic because the relevant inbound traffic does not pass through the NAT Gateway.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 790070,
          "date": "Sat 28 Jan 2023 00:39",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "should be B no? as the source =198.51.100.2. and dest =203.0<br>I think D is the inverse",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776281,
          "date": "Sun 15 Jan 2023 09:44",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Open the Amazon CloudWatch console. Select the log group that contains the NAT gateway's elastic network interface and the private instance's elastic network interface. Run a query to filter with the destination address set as \\\"like 198.51.100.2\\\" and the source address set as \\\"like 203.0\\\". Run the stats command to filter the sum of bytes transferred by the source address and the destination address.<br><br>This set of steps will allow the solutions architect to determine whether the inbound traffic coming from the public IP address 198.51.100.2 is unsolicited, by filtering the VPC flow logs to show the traffic that is going to the private Amazon EC2 instance, and then checking if the traffic is coming from within the VPC CIDR block (203.0) or from outside of it (198.51.100.2). By using Amazon CloudWatch and filtering the logs with the specified IP addresses, the architect can quickly and efficiently identify any unsolicited traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options are not correct because they are not using the correct service (CloudTrail instead of CloudWatch) or not filtering the logs correctly with the specified IP addresses and the desired information is not available.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776282,
          "date": "Sun 15 Jan 2023 09:44",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options are not correct because they are not using the correct service (CloudTrail instead of CloudWatch) or not filtering the logs correctly with the specified IP addresses and the desired information is not available.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#91",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company consists or two separate business units. Each business unit has its own AWS account within a single organization in AWS Organizations. The business units regularly share sensitive documents with each other. To facilitate sharing, the company created an Amazon S3 bucket in each account and configured low-way replication between the S3 buckets. The S3 buckets have millions of objects.<br><br>Recently, a security audit identified that neither S3 bucket has encryption at rest enabled. Company policy requires that all documents must be stored with encryption at rest. The company wants to implement server-side encryption with Amazon S3 managed encryption keys (SSE-S3).<br><br>What is the MOST operationally efficient solution that meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#91",
          "answers": [
            {
              "choice": "<p>A. Turn on SSE-S3 on both S3 buckets. Use S3 Batch Operations to copy and encrypt the objects in the same location.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Key Management Service (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Encrypt the existing objects by using an S3 copy command in the AWS CLI.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Turn on SSE-S3 on both S3 buckets. Encrypt the existing objects by using an S3 copy command in the AWS CLI.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Key Management Service, (AWS KMS) key in each account. Turn on server-side encryption with AWS KMS keys (SSE-KMS) on each S3 bucket by using the corresponding KMS key in that AWS account. Use S3 Batch Operations to copy the objects into the same location.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851035,
          "date": "Sun 26 Mar 2023 15:10",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A is much more efficient",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 837247,
          "date": "Sun 12 Mar 2023 18:03",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "Answer is A<br>Keyword is \\\"The S3 buckets have millions of objects\\\"<br>If there are million of objects then you should use Batch operations. <br>https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>good point, changing my answer to A</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 844307,
          "date": "Sun 19 Mar 2023 23:46",
          "username": "\t\t\t\tforceli\t\t\t",
          "content": "good point, changing my answer to A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834525,
          "date": "Fri 10 Mar 2023 01:21",
          "username": "\t\t\t\tforceli\t\t\t",
          "content": "A and C seems to be correct but using batch requires more steps.<br>https://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 829668,
          "date": "Sun 05 Mar 2023 06:55",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "C is wrong. How can S3 copy encrypt ? A is correct. Refer how S3 batch operations are used to encrypt here -https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 826260,
          "date": "Wed 01 Mar 2023 21:52",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I guess A and/or C can be because they are pretty close; after reading everything here, they are a lot of good points.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810813,
          "date": "Thu 16 Feb 2023 16:16",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Encrypting existing objects<br>To encrypt your existing Amazon S3 objects, you can use Amazon S3 Batch Operations. You provide S3 Batch Operations with a list of objects to operate on, and Batch Operations calls the respective API to perform the specified operation. You can use the Batch Operations Copy operation to copy existing unencrypted objects and write them back to the same bucket as encrypted objects. A single Batch Operations job can perform the specified operation on billions of objects. For more information, see Performing large-scale batch operations on Amazon S3 objects and the AWS Storage Blog post Encrypting objects with Amazon S3 Batch Operations.<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html?icmpid=docs_s3_hp_create_bucket_default_encryption",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797131,
          "date": "Fri 03 Feb 2023 16:11",
          "username": "\t\t\t\tmmendozaf\t\t\t",
          "content": "To encrypt you need to Re-copy the file and batch is more efficient.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 795402,
          "date": "Wed 01 Feb 2023 19:27",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "As per Romidan's link, it is clear.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793830,
          "date": "Tue 31 Jan 2023 09:20",
          "username": "\t\t\t\tmikeshop\t\t\t",
          "content": "Batch operations are more efficient for millions of objects than running the CLI command.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790161,
          "date": "Sat 28 Jan 2023 02:48",
          "username": "\t\t\t\tromidan\t\t\t",
          "content": "Option A seems efficient as per the blog - <br>https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 786641,
          "date": "Tue 24 Jan 2023 16:24",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "Option A is correct since manual copying (Option C) for millions of objects is time-consuming.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785616,
          "date": "Mon 23 Jan 2023 17:44",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I vote C<br>https://aws.amazon.com/jp/blogs/news/encrypting-existing-amazon-s3-objects-with-the-aws-cli/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As mikeshop said: For S3 buckets with a large number of objects, in the order of millions or billions of objects, using Amazon S3 inventory or Amazon S3 Batch Operations can be a better option than using the AWS CLI instructions in this post.</li><li>If you read further in that post, it says that for large object stores, batch operations are more efficient.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795401,
          "date": "Wed 01 Feb 2023 19:26",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "As mikeshop said: For S3 buckets with a large number of objects, in the order of millions or billions of objects, using Amazon S3 inventory or Amazon S3 Batch Operations can be a better option than using the AWS CLI instructions in this post.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 793828,
          "date": "Tue 31 Jan 2023 09:19",
          "username": "\t\t\t\tmikeshop\t\t\t",
          "content": "If you read further in that post, it says that for large object stores, batch operations are more efficient.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785332,
          "date": "Mon 23 Jan 2023 13:22",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I thought option A might be correct after reading the below blog article because there were millions of objects in the S3 buckets in this scenario.<br>https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3-batch-operations/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776290,
          "date": "Sun 15 Jan 2023 09:50",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option C.  Turn on SSE-S3 on both S3 buckets and encrypt the existing objects by using an S3 copy command in the AWS CLI.<br>This option is the most operationally efficient solution because it uses the built-in SSE-S3 feature of S3, which eliminates the need to create and manage additional KMS keys and encrypting existing objects using S3 copy command is a straight forward process.<br><br>Option A is not the most operationally efficient solution because it requires additional steps to encrypt the objects which might take time as there are millions of objects.<br><br>Option B and D are not the most operationally efficient solution because they require additional steps to create and manage KMS keys. Additionally, they also require additional steps to encrypt the existing objects.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#92",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application in the AWS Cloud. The application collects and stores a large amount of unstructured data in an Amazon S3 bucket. The S3 bucket contains several terabytes of data and uses the S3 Standard storage class. The data increases in size by several gigabytes every day.<br><br>The company needs to query and analyze the data. The company does not access data that is more than 1 year old. However, the company must retain all the data indefinitely for compliance reasons.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#92",
          "answers": [
            {
              "choice": "<p>A. Use S3 Select to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old 10 S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Redshift Spectrum to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Intelligent-Tiering.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 779093,
          "date": "Tue 17 Jan 2023 18:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Use an AWS Glue Data Catalog and Amazon Athena to query the data. Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.<br><br>This solution allows you to use Amazon Athena and the AWS Glue Data Catalog to query and analyze the data in an S3 bucket. Amazon Athena is a serverless, interactive query service that allows you to analyze data in S3 using SQL. The AWS Glue Data Catalog is a managed metadata repository that can be used to store and retrieve table definitions for data stored in S3. Together, these services can provide a cost-effective way to query and analyze large amounts of unstructured data. Additionally, by using an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can retain the data indefinitely for compliance reasons while also reducing storage costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options are not correct because:A.  Using S3 Select is good for filtering data in S3, but it may not be a suitable solution for querying and analyzing large amounts of data.<br>B.  Amazon Redshift Spectrum can be used to query data stored in S3, but it may not be as cost-effective as using Amazon Athena for querying unstructured data<br>D.  Using Amazon Redshift Spectrum with S3 Intelligent-Tiering could be a good solution, but S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns and it would not be the best solution for compliance reasons as S3 Intelligent-Tiering will move data to other storage classes according to access patterns.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779094,
          "date": "Tue 17 Jan 2023 18:01",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options are not correct because:A.  Using S3 Select is good for filtering data in S3, but it may not be a suitable solution for querying and analyzing large amounts of data.<br>B.  Amazon Redshift Spectrum can be used to query data stored in S3, but it may not be as cost-effective as using Amazon Athena for querying unstructured data<br>D.  Using Amazon Redshift Spectrum with S3 Intelligent-Tiering could be a good solution, but S3 Intelligent-Tiering is designed to optimize storage costs based on access patterns and it would not be the best solution for compliance reasons as S3 Intelligent-Tiering will move data to other storage classes according to access patterns.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851039,
          "date": "Sun 26 Mar 2023 15:11",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is the best choice for unstructured data",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 829679,
          "date": "Sun 05 Mar 2023 07:16",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "S3 select only to select few parts of the data and here its lot of unstructured data. So A is wrong. Use Athena console to create Glue crawler as referred here - <br>https://docs.aws.amazon.com/athena/latest/ug/data-sources-glue.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827819,
          "date": "Fri 03 Mar 2023 10:17",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "I think \\\"semi-structured\\\" is the right word here, because unstructured can be videos, images or text that has no schema.<br>Assuming that we want to query semi-structured data :<br>I don't understand why everyone is voting Athena.<br>Athena is fast in certain cases and has more features for aggregation, but we are just asking querying here (and analyzing is very vague).<br>In terms of cost, S3 select is around 2$ by TB scanned, and Athena is 5$.<br>Glue data catalog brings ease of use, but is not required for querying with athena.<br>S3 select is not limited in the amount of scanned data, only in the row size (1MB)<br>Can someone explain ?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 793411,
          "date": "Mon 30 Jan 2023 23:21",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "AWS Glue Data Catalog to convert data to be structuredbefore querying them <br>Amazon Athena to query the data. <br>Create an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 790789,
          "date": "Sat 28 Jan 2023 17:15",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Generally, unstructured data should be converted structured data before querying them. AWS Glue can do that.<br>https://docs.aws.amazon.com/glue/latest/dg/schema-relationalize.html<br>https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777154,
          "date": "Mon 16 Jan 2023 00:00",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct because it is unstructured data. You cannot use S3 select and must use Glue Crawler to generate catalg.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776293,
          "date": "Sun 15 Jan 2023 09:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A is the correct answer. S3 Select allows you to query the data stored in an S3 bucket, which can be useful when you need to retrieve specific subsets of data from a large amount of data. By creating an S3 Lifecycle policy to transition data that is more than 1 year old to S3 Glacier Deep Archive, you can save cost as it is a low-cost storage class for archival data that is infrequently accessed and for which retrieval times of several hours are acceptable. This solution is most cost-effective as it allows you to keep all the data indefinitely for compliance reasons while also reducing storage costs for older data that is not frequently accessed.<br><br>The other options are not as cost-effective as they would require additional costs for data transfer, storage and query in other services.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#93",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A video processing company wants to build a machine learning (ML) model by using 600 TB of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. The company does not have the necessary compute resources on premises for ML experiments and wants to use AWS.<br><br>The company needs to complete the data transfer to AWS within 3 weeks. The data transfer will be a one-time transfer. The data must be encrypted in transit. The measured upload speed of the company's internet connection is 100 Mbps. and multiple departments share the connection.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#93",
          "answers": [
            {
              "choice": "<p>A. Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a VPN connection between the on-premises network attached storage and the nearest AWS Region. Transfer the data over the VPN connection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an AWS Storage Gateway file gateway on premises. Configure the file gateway with a destination S3 bucket. Copy the data to the file gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851041,
          "date": "Sun 26 Mar 2023 15:13",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A obviously",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 830135,
          "date": "Sun 05 Mar 2023 18:32",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Around 8 devices and snowball (actually a Rectangular box)<br>Snowball Edge Storage Optimized device is equipped with up to 80 terabytes (TB) of storage capacity, as well as 40 vCPUs and 80 GB of memory for running compute-intensive applications. It also includes an optional GPU for accelerated computing workloads.<br><br>Built-in security features such as tamper-resistant enclosures, an E Ink shipping label, and 256-bit encryption for data at rest and in transit.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793413,
          "date": "Mon 30 Jan 2023 23:22",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "3 weeks + cost effective ==> Snowball Edge Storage",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776295,
          "date": "Sun 15 Jan 2023 09:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A.  Order several AWS Snowball Edge Storage Optimized devices by using the AWS Management Console. Configure the devices with a destination S3 bucket. Copy the data to the devices. Ship the devices back to AWS.<br><br>This option will meet the requirements to complete the data transfer within 3 weeks, as the Snowball Edge devices can transfer large amounts of data quickly and securely. The data will be encrypted in transit and at rest. The company's internet connection speed is not a bottleneck as the data transfer will happen on the devices and not over the internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not a cost-effective solution, as setting up and maintaining a 10 Gbps Direct Connect connection can be quite expensive, especially if it's only needed for a one-time data transfer.<br><br>Option C is not a cost-effective solution, as creating a VPN connection between the on-premises storage and the nearest AWS region would require significant networking configuration and maintenance, and would likely be more expensive than using Snowball Edge devices.<br><br>Option D is not a cost-effective solution, as deploying an AWS Storage Gateway file gateway on premises would require additional hardware and ongoing maintenance costs, and may not be necessary for a one-time data transfer.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776296,
          "date": "Sun 15 Jan 2023 09:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not a cost-effective solution, as setting up and maintaining a 10 Gbps Direct Connect connection can be quite expensive, especially if it's only needed for a one-time data transfer.<br><br>Option C is not a cost-effective solution, as creating a VPN connection between the on-premises storage and the nearest AWS region would require significant networking configuration and maintenance, and would likely be more expensive than using Snowball Edge devices.<br><br>Option D is not a cost-effective solution, as deploying an AWS Storage Gateway file gateway on premises would require additional hardware and ongoing maintenance costs, and may not be necessary for a one-time data transfer.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#94",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has migrated Its forms-processing application to AWS. When users interact with the application, they upload scanned forms as files through a web application. A database stores user metadata and references to files that are stored in Amazon S3. The web application runs on Amazon EC2 instances and an Amazon RDS for PostgreSQL database.<br><br>When forms are uploaded, the application sends notifications to a team through Amazon Simple Notification Service (Amazon SNS). A team member then logs in and processes each form. The team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an API.<br><br>A solutions architect needs to automate the manual processing of the forms. The solution must provide accurate form extraction. minimize time to market, and minimize tong-term operational overhead.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#94",
          "answers": [
            {
              "choice": "<p>A. Develop custom libraries to perform optical character recognition (OCR) on the forms. Deploy the libraries to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster as an application tier. Use this tier to process the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data into an Amazon DynamoDB table. Submit the data to the target system's APL. Host the new application tier on EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use artificial intelligence and machine learning (AI/ML) models that are trained and hosted on an EC2 instance to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Host a new application tier on EC2 instances. Use this tier to call endpoints that host artificial intelligence and machine teaming (AI/ML) models that are trained and hosted in Amazon SageMaker to perform optical character recognition (OCR) on the forms. Store the output in Amazon ElastiCache. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851045,
          "date": "Sun 26 Mar 2023 15:14",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Amazon Textract..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 830341,
          "date": "Sun 05 Mar 2023 22:23",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Textract can analyze different types of documents such as forms, invoices, receipts, and tables, and can extract information such as text, tables, and key-value pairs.<br><br>Comprehend provides a set of APIs that can be used to analyze text data in real-time. The service can identify the language of the text, extract entities such as people, organizations, and locations, and detect the sentiment expressed in the text. It can also extract key phrases that summarize the meaning of the text, and can classify the text into predefined categories.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827839,
          "date": "Fri 03 Mar 2023 10:42",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "D : Managed AWS Services",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776302,
          "date": "Sun 15 Jan 2023 10:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API.<br><br>This solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fully managed and serverless services that can perform OCR and extract relevant data from the forms, which eliminates the need to develop custom libraries or train and host models. Using AWS Step Functions and Lambda allows for easy automation of the process and the ability to scale as needed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A:<br>This option would require significant development and maintenance effort and would not take advantage of fully managed services, resulting in increased operational overhead.<br><br>Option B:<br>This option is similar to option A in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.<br><br>Option C:<br>This option is similar to option B in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776303,
          "date": "Sun 15 Jan 2023 10:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A:<br>This option would require significant development and maintenance effort and would not take advantage of fully managed services, resulting in increased operational overhead.<br><br>Option B:<br>This option is similar to option A in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.<br><br>Option C:<br>This option is similar to option B in that it would require significant development and maintenance effort to train and host the models, and would not take advantage of fully managed services resulting in increased operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#95",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is refactoring its on-premises order-processing platform in the AWS Cloud. The platform includes a web front end that is hosted on a fleet of VMs, RabbitMQ to connect the front end to the backend, and a Kubernetes cluster to run a containerized backend system to process the orders. The company does not want to make any major changes to the application.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#95",
          "answers": [
            {
              "choice": "<p>A. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a custom AWS Lambda runtime to mimic the web server environment. Create an Amazon API Gateway API to replace the front-end web servers. Set up Amazon MQ to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up Amazon MQ to replace the on-premises messaging queue. Install Kubernetes on a fleet of different EC2 instances to host the order-processing backend.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AMI of the web server VM. Create an Amazon EC2 Auto Scaling group that uses the AMI and an Application Load Balancer. Set up an Amazon Simple Queue Service (Amazon SQS) queue to replace the on-premises messaging queue. Configure Amazon Elastic Kubernetes Service (Amazon EKS) to host the order-processing backend.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776312,
          "date": "Sun 15 Jan 2023 10:10",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is the correct answer. In this solution, the company creates an Amazon Machine Image (AMI) of the web server VM, which can be used to launch EC2 instances that are identical to the on-premises web servers. The company then creates an EC2 Auto Scaling group that uses the AMI and an Application Load Balancer (ALB) to provide automatic scaling and high availability for the web front end. The company also replaces the on-premises messaging queue (RabbitMQ) with Amazon MQ, which is a managed message broker service that is fully compatible with RabbitMQ. Finally, the company uses Amazon Elastic Kubernetes Service (EKS) to host the order-processing backend, which allows them to run their existing Kubernetes cluster in the AWS cloud without making any major changes to the application. This approach allows the company to lift and shift their existing platform with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, using a custom AWS Lambda runtime and Amazon API Gateway, would require significant changes to the application and may not be compatible with the current codebase. <br><br>Option C, installing Kubernetes on a fleet of different EC2 instances, would also require significant changes to the application and may not be compatible with the current codebase. <br><br>Option D, using Amazon Simple Queue Service (Amazon SQS) instead of Amazon MQ, would not provide the same level of messaging capabilities as Amazon MQ and may not be sufficient for the needs of the order-processing platform.</li><li>Your justification for option C is wrong.<br>Option C is valid, as Kubernetes on EC2 is very similar as the existing Kubernetes environment on-premises. But EKS is a safe bet and reduces operational overhead, while keeping the same API as previously. Hence, A is a better choice.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776314,
          "date": "Sun 15 Jan 2023 10:10",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B, using a custom AWS Lambda runtime and Amazon API Gateway, would require significant changes to the application and may not be compatible with the current codebase. <br><br>Option C, installing Kubernetes on a fleet of different EC2 instances, would also require significant changes to the application and may not be compatible with the current codebase. <br><br>Option D, using Amazon Simple Queue Service (Amazon SQS) instead of Amazon MQ, would not provide the same level of messaging capabilities as Amazon MQ and may not be sufficient for the needs of the order-processing platform.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Your justification for option C is wrong.<br>Option C is valid, as Kubernetes on EC2 is very similar as the existing Kubernetes environment on-premises. But EKS is a safe bet and reduces operational overhead, while keeping the same API as previously. Hence, A is a better choice.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 827846,
          "date": "Fri 03 Mar 2023 10:47",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "Your justification for option C is wrong.<br>Option C is valid, as Kubernetes on EC2 is very similar as the existing Kubernetes environment on-premises. But EKS is a safe bet and reduces operational overhead, while keeping the same API as previously. Hence, A is a better choice.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 851051,
          "date": "Sun 26 Mar 2023 15:16",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A is the best choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 815096,
          "date": "Mon 20 Feb 2023 11:13",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Option A is re-hosting or mybe re-platforming. The question says the purpose is re-factoring, then it's B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It says the company does not want to make changes to the application in the problem statement. B would require significant code changes to the application.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 817400,
          "date": "Wed 22 Feb 2023 03:37",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "It says the company does not want to make changes to the application in the problem statement. B would require significant code changes to the application.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#96",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to implement a client-side encryption mechanism for objects that will be stored in a new Amazon S3 bucket. The solutions architect created a CMK that is stored in AWS Key Management Service (AWS KMS) for this purpose.<br><br>The solutions architect created the following IAM policy and attached it to an IAM role:<br><br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-professional-sap-c02/image7.png\"><br><br>During tests, the solutions architect was able to successfully get existing test objects in the S3 bucket. However, attempts to upload a new object resulted in an error message. The error message stated that the action was forbidden.<br><br>Which action must the solutions architect add to the IAM policy to meet all the requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#96",
          "answers": [
            {
              "choice": "<p>A. kms:GenerateDataKey<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. kms:GetKeyPolicy<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. kms:GetPublicKey<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. kms:Sign<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851055,
          "date": "Sun 26 Mar 2023 15:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790792,
          "date": "Sat 28 Jan 2023 17:18",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/kms/latest/cryptographic-details/client-side-encryption.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 786002,
          "date": "Tue 24 Jan 2023 01:21",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I Vote A. <br>https://repost.aws/ja/knowledge-center/s3-large-file-encryption-kms-key<br>Adding kms:GenerateDataKeyis necessary.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776317,
          "date": "Sun 15 Jan 2023 10:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  kms:GenerateDataKey<br><br>The solutions architect needs to add the \\\"kms:GenerateDataKey\\\" action to the IAM policy in order to generate a data key for client-side encryption. Without this action, the IAM role does not have the necessary permissions to generate a data key, which causes the error message when attempting to upload a new object.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options are not correct because they are not required for this use case. kms:GetKeyPolicy allows for the retrieval of the key policy for a CMK but it does not have any relation to client-side encryption of S3 objects, kms:GetPublicKey allows for the retrieval of the public key of a CMK, but it does not have any relation to client-side encryption of S3 objects and kms:Sign allows for signing a message using a CMK but it does not have any relation to client-side encryption of S3 objects.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776320,
          "date": "Sun 15 Jan 2023 10:18",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options are not correct because they are not required for this use case. kms:GetKeyPolicy allows for the retrieval of the key policy for a CMK but it does not have any relation to client-side encryption of S3 objects, kms:GetPublicKey allows for the retrieval of the public key of a CMK, but it does not have any relation to client-side encryption of S3 objects and kms:Sign allows for signing a message using a CMK but it does not have any relation to client-side encryption of S3 objects.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#97",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has developed a web application. The company is hosting the application on a group of Amazon EC2 instances behind an Application Load Balancer. The company wants to improve the security posture of the application and plans to use AWS WAF web ACLs. The solution must not adversely affect legitimate traffic to the application.<br><br>How should a solutions architect configure the web ACLs to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#97",
          "answers": [
            {
              "choice": "<p>A. Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use only rate-based rules in the web ACLs, and set the throttle limit as high as possible. Temporarily block all requests that exceed the limit. Define nested rules to narrow the scope of the rate tracking.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set the action of the web ACL rules to Block. Use only AWS managed rule groups in the web ACLs. Evaluate the rule groups by using Amazon CloudWatch metrics with AWS WAF sampled requests or AWS WAF logs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use only custom rule groups in the web ACLs, and set the action to Allow. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Allow to Block.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851058,
          "date": "Sun 26 Mar 2023 15:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A.  Set the action of the web ACL rules to Count. Enable AWS WAF logging.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 830443,
          "date": "Mon 06 Mar 2023 00:54",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "AWS WAF allows you to create web ACL (Access Control List) rules in \\\"Count\\\" mode, which allows you to monitor traffic without actually blocking it. In Count mode, AWS WAF counts the number of requests that match a particular rule, but doesn't take any action to block those requests.<br><br>Count mode can be useful in several ways:<br><br>Testing new rules: You can create new rules and test them in Count mode before enabling them to block traffic. This allows you to evaluate the effectiveness of your rules without risking false positives or false negatives.<br><br>Analyzing traffic: You can use Count mode to analyze traffic patterns and identify potential security threats. By monitoring the number of requests that match a particular rule, you can detect patterns that may indicate an attack or vulnerability.<br><br>Compliance reporting: Count mode can be used for compliance reporting, where you need to demonstrate that certain rules are being enforced. By counting the number of requests that match a rule, you can provide evidence that your security policies are being followed.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790794,
          "date": "Sat 28 Jan 2023 17:20",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-testing.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776329,
          "date": "Sun 15 Jan 2023 10:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/74273-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>The correct answer is A.  Set the action of the web ACL rules to Count. Enable AWS WAF logging. Analyze the requests for false positives. Modify the rules to avoid any false positive. Over time, change the action of the web ACL rules from Count to Block.<br><br>This approach allows for monitoring of the incoming traffic and its behavior before taking any action that can affect the legitimate traffic. By setting the action to count, the web ACL will only log the requests that match the conditions of the rules, but it will not block them. This way, the company can analyze the requests and check for any false positives. Once they identify and correct any false positives, they can gradually change the action of the web ACL rules from count to block, thus improving the security posture of the application without adversely affecting legitimate traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not correct because using only rate-based rules can lead to false positives and blocking of legitimate traffic. Option C is not correct because using only AWS managed rule groups can limit the flexibility and specificity of the web ACLs. Option D is not correct because using only custom rule groups with action set to allow can lead to security vulnerabilities.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776330,
          "date": "Sun 15 Jan 2023 10:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not correct because using only rate-based rules can lead to false positives and blocking of legitimate traffic. Option C is not correct because using only AWS managed rule groups can limit the flexibility and specificity of the web ACLs. Option D is not correct because using only custom rule groups with action set to allow can lead to security vulnerabilities.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#98",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an organization that has many AWS accounts in AWS Organizations. A solutions architect must improve how the company manages common security group rules for the AWS accounts in the organization.<br><br>The company has a common set of IP CIDR ranges in an allow list in each AWS account to allow access to and from the company's on-premises network. Developers within each account are responsible for adding new IP CIDR ranges to their security groups. The security team has its own AWS account. Currently, the security team notifies the owners of the other AWS accounts when changes are made to the allow list.<br><br>The solutions architect must design a solution that distributes the common set of CIDR ranges across all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#98",
          "answers": [
            {
              "choice": "<p>A. Set up an Amazon Simple Notification Service (Amazon SNS) topic in the security team's AWS account. Deploy an AWS Lambda function in each AWS account. Configure the Lambda function to run every time an SNS topic receives a message. Configure the Lambda function to take an IP address as input and add it to a list of security groups in the account. Instruct the security team to distribute changes by publishing messages to its SNS topic.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create new customer-managed prefix lists in each AWS account within the organization. Populate the prefix lists in each account with all internal CIDR ranges. Notify the owner of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups. Instruct the security team to share updates with each AWS account owner.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM role in each account in the organization. Grant permissions to update security groups. Deploy an AWS Lambda function in the security team's AWS account. Configure the Lambda function to take a list of internal IP addresses as input, assume a role in each organization account, and add the list of IP addresses to the security groups in each account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851126,
          "date": "Sun 26 Mar 2023 16:13",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Prefix lists + RAM",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830468,
          "date": "Mon 06 Mar 2023 02:06",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Prefix lists + Resource Access Manager RAM is the solution.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 795542,
          "date": "Wed 01 Feb 2023 22:42",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Clearly",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793418,
          "date": "Mon 30 Jan 2023 23:38",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Create a new customer-managed prefix list in the security team's AWS account",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 790797,
          "date": "Sat 28 Jan 2023 17:21",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779097,
          "date": "Tue 17 Jan 2023 18:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups.<br><br>This solution meets the requirements with the least amount of operational overhead as it requires the security team to create and maintain a single customer-managed prefix list, and share it with the organization using AWS Resource Access Manager. The owners of each AWS account are then responsible for allowing the prefix list in their security groups, which eliminates the need for the security team to manually notify each account owner when changes are made. This solution also eliminates the need for a separate AWS Lambda function in each account, reducing the overall complexity of the solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not correct because it requires setting up an SNS topic in the security team's AWS account, and deploying an AWS Lambda function in each AWS account. This increases the operational overhead as it requires setting up and maintaining the SNS topic, and deploying and configuring the Lambda function in each account.<br><br>Option B is not correct because it requires creating new customer-managed prefix lists in each AWS account within the organization, which increases the operational overhead as it requires the security team to create and maintain multiple prefix lists.<br><br>Option D is not correct because it requires creating an IAM role in each account in the organization, which increases the operational overhead as it requires the security team to set up and maintain multiple roles. Additionally, it also deploys an AWS Lambda function in the security team's AWS account, which increases complexity and operational overhead.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779098,
          "date": "Tue 17 Jan 2023 18:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not correct because it requires setting up an SNS topic in the security team's AWS account, and deploying an AWS Lambda function in each AWS account. This increases the operational overhead as it requires setting up and maintaining the SNS topic, and deploying and configuring the Lambda function in each account.<br><br>Option B is not correct because it requires creating new customer-managed prefix lists in each AWS account within the organization, which increases the operational overhead as it requires the security team to create and maintain multiple prefix lists.<br><br>Option D is not correct because it requires creating an IAM role in each account in the organization, which increases the operational overhead as it requires the security team to set up and maintain multiple roles. Additionally, it also deploys an AWS Lambda function in the security team's AWS account, which increases complexity and operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777160,
          "date": "Mon 16 Jan 2023 00:11",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct. The prefix list is managed by security team and shared with other accounts. Other accounts can directly use it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776340,
          "date": "Sun 15 Jan 2023 10:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D. <br><br>Option D creates an IAM role in each account in the organization which grants permissions to update security groups. Then, it deploys an AWS Lambda function in the security team's AWS account, this lambda function is able to assume the IAM roles in each account and update the security groups with the new IP CIDR ranges. This solution allows the security team to easily distribute and update the common set of IP CIDR ranges across all accounts with minimal operational overhead.<br><br>Option A, uses an SNS topic, where the security team would need to notify all account owners every time an update is made to the allow list and would require the developers in each account to run a Lambda function which updates the security group. This solution would require a lot of manual work, and is not automated.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, requires the security team to notify the owners of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work.<br><br>Option C, uses a customer-managed prefix list in the security team's AWS account. But, it still requires the owners of each account to allow the new customer-managed prefix list ID in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work.</li><li>Create an IAM role in each account in the organization. this does not add up to operational overhead right.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776341,
          "date": "Sun 15 Jan 2023 10:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B, requires the security team to notify the owners of each AWS account to allow the new customer-managed prefix list IDs in their accounts in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work.<br><br>Option C, uses a customer-managed prefix list in the security team's AWS account. But, it still requires the owners of each account to allow the new customer-managed prefix list ID in their security groups, this solution would not provide a centralized control of the IP CIDR ranges and would require a lot of manual work.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Create an IAM role in each account in the organization. this does not add up to operational overhead right.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 830457,
          "date": "Mon 06 Mar 2023 01:32",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Create an IAM role in each account in the organization. this does not add up to operational overhead right.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#99",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a VPN. The company is hosting internal applications with VPCs in multiple AWS accounts. Currently, the applications are accessible from the company's on-premises office network through an AWS Site-to-Site VPN connection. The VPC in the company's main AWS account has peering connections established with VPCs in other AWS accounts.<br><br>A solutions architect must design a scalable AWS Client VPN solution for employees to use while they work from home.<br><br>What is the MOST cost-effective solution that meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#99",
          "answers": [
            {
              "choice": "<p>A. Create a Client VPN endpoint in each AWS account. Configure required routing that allows access to internal applications.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Client VPN endpoint in the main AWS account. Provision a transit gateway that is connected to each AWS account. Configure required routing that allows access to internal applications.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Client VPN endpoint in the main AWS account. Establish connectivity between the Client VPN endpoint and the AWS Site-to-Site VPN.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851129,
          "date": "Sun 26 Mar 2023 16:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830481,
          "date": "Mon 06 Mar 2023 02:35",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/images/vpn/latest/clientvpn-admin/images/client-vpn-scenario-peer-vpc.png<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Tip - If there is no site-site gateway already and question asks for scalable solution then answer would be C</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830482,
          "date": "Mon 06 Mar 2023 02:39",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tip - If there is no site-site gateway already and question asks for scalable solution then answer would be C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828825,
          "date": "Sat 04 Mar 2023 11:40",
          "username": "\t\t\t\tZek\t\t\t",
          "content": "Support B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795543,
          "date": "Wed 01 Feb 2023 22:50",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "It's B as explained here: https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793420,
          "date": "Mon 30 Jan 2023 23:39",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "should be B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776346,
          "date": "Sun 15 Jan 2023 10:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/80782-exam-aws-certified-solutions-architect-professional-topic-1/<br>B.  Create a Client VPN endpoint in the main AWS account. Configure required routing that allows access to internal applications is the MOST cost-effective solution that meets these requirements. This solution allows employees to connect to the main AWS account using a Client VPN endpoint, and then use peering connections established with other AWS accounts to access the internal applications. This eliminates the need for additional Client VPN endpoints in each AWS account, reducing costs. <br><br>Option A, creating a Client VPN endpoint in each AWS account, would be more expensive as it would require multiple endpoints. <br><br>Option C, creating a transit gateway, would also add unnecessary costs. <br><br>Option D, connecting the Client VPN endpoint to the Site-to-Site VPN, may not provide a scalable solution for remote employees.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#100",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application in the AWS Cloud. Recent application metrics show inconsistent response times and a significant increase in error rates. Calls to third-party services are causing the delays. Currently, the application calls third-party services synchronously by directly invoking an AWS Lambda function.<br><br>A solutions architect needs to decouple the third-party service calls and ensure that all the calls are eventually completed.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#100",
          "answers": [
            {
              "choice": "<p>A. Use an Amazon Simple Queue Service (Amazon SQS) queue to store events and invoke the Lambda function.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an AWS Step Functions state machine to pass events to the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an Amazon EventBridge rule to pass events to the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 857066,
          "date": "Fri 31 Mar 2023 14:32",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "A : SQS QUEUE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 851257,
          "date": "Sun 26 Mar 2023 18:27",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "SQS for decoupling",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818564,
          "date": "Thu 23 Feb 2023 01:16",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "SQS ---> Lambda is the correct option",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793421,
          "date": "Mon 30 Jan 2023 23:41",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "decouple ==> SQS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790808,
          "date": "Sat 28 Jan 2023 17:36",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "The application needs to pass the initiative to the next step. That means the application does not wait the response from the Lambda function, it should have the responsibility only to call the Lambda function. To do so, the application only throw the job information to Amazon SQS queue and finish. After that, AWS Lambda function can pull the job information from SQS queue and start processing actively.<br>https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 788868,
          "date": "Thu 26 Jan 2023 16:40",
          "username": "\t\t\t\tQing\t\t\t",
          "content": "I vote for C - use Step Functions with its callback feature to throttle the third party api call.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776356,
          "date": "Sun 15 Jan 2023 10:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A.  Using an Amazon Simple Queue Service (SQS) queue to store events and invoke the Lambda function is a good solution to decouple the third-party service calls and ensure that all the calls are eventually completed. SQS is a fully managed, reliable, and highly scalable message queuing service that allows applications to send, store, and receive messages between distributed components. By sending the third-party service calls to an SQS queue, it allows the application to continue processing without waiting for the third-party services to respond, which can result in faster response times and lower error rates.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Other options like AWS Step Functions state machine, Amazon EventBridge, and Amazon Simple Notification Service (SNS) topic are not appropriate for this use case. AWS Step Functions is a service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Amazon EventBridge is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated SaaS applications, and AWS services. Amazon SNS is a fully managed messaging service for both application-to-application and application-to-person (A2P) communication. These services are not focused on providing message queues and would not be the best fit for this use case.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776357,
          "date": "Sun 15 Jan 2023 10:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Other options like AWS Step Functions state machine, Amazon EventBridge, and Amazon Simple Notification Service (SNS) topic are not appropriate for this use case. AWS Step Functions is a service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Amazon EventBridge is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated SaaS applications, and AWS services. Amazon SNS is a fully managed messaging service for both application-to-application and application-to-person (A2P) communication. These services are not focused on providing message queues and would not be the best fit for this use case.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#101",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.<br><br>The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#101",
          "answers": [
            {
              "choice": "<p>A. Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sates account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight rote, to create a trust relationship with the new IAM role in the sales account.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 779100,
          "date": "Tue 17 Jan 2023 18:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight role to create a trust relationship with the new IAM role in the sales account.<br><br>This solution meets the requirements by allowing the marketing team to access the data in the S3 bucket in the sales account through assuming an IAM role, which eliminates the need to copy the data or share the KMS key, and also eliminates the need to modify the S3 bucket policy or create a KMS grant. This solution allows to use the same access to the bucket without duplicating data and re-encrypting it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket is not correct because it would create unnecessary data duplication and increased storage costs.<br>B.  Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sales account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because it does not provide a secure way to share the KMS key between accounts and also it would create unnecessary data duplication and increased storage costs.</li><li>C.  Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because the Sales team's S3 bucket is in a different account, so the Marketing team cannot update the policy on the Sales team's S3 bucket.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779101,
          "date": "Tue 17 Jan 2023 18:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket is not correct because it would create unnecessary data duplication and increased storage costs.<br>B.  Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sales account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because it does not provide a secure way to share the KMS key between accounts and also it would create unnecessary data duplication and increased storage costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C.  Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because the Sales team's S3 bucket is in a different account, so the Marketing team cannot update the policy on the Sales team's S3 bucket.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779102,
          "date": "Tue 17 Jan 2023 18:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket is not correct because the Sales team's S3 bucket is in a different account, so the Marketing team cannot update the policy on the Sales team's S3 bucket.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 851282,
          "date": "Sun 26 Mar 2023 18:54",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Since the S3 bucket belongs to the sales account, the marketing team cannot directly update the policy on the sales team's S3 bucket. In that case, option D would be the better option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 843318,
          "date": "Sun 19 Mar 2023 01:57",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "LEAST operational overhead, you could do D and it would work, but honestly it is three steps w/ C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830635,
          "date": "Mon 06 Mar 2023 09:31",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "I just found the official documentation about cross-account s3 access by Quicksight : https://aws.amazon.com/premiumsupport/knowledge-center/quicksight-cross-account-s3/<br>Hence, the answer is C.  No IAM role required because Quicksight uses a service role instead of a service-linked role.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 830525,
          "date": "Mon 06 Mar 2023 04:42",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Same as source bucket to destination bucket copy question in different form. first source should share its data. have a role created for destination bucket and that role will be assumed by dest bucket to get access to data. D is answer. C sounds wrong \\\"Create KMS grant for encryption key\\\" sounds wierd.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 808420,
          "date": "Tue 14 Feb 2023 14:37",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "D is the answer but it does have enough information about KMS<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That is also my point; the other one, which is C, then is missing the S3 bucket permission in the sales account to provide access from the marketing account.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 826352,
          "date": "Thu 02 Mar 2023 02:17",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "That is also my point; the other one, which is C, then is missing the S3 bucket permission in the sales account to provide access from the marketing account.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795549,
          "date": "Wed 01 Feb 2023 23:01",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "The rest of options have errors.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 793424,
          "date": "Mon 30 Jan 2023 23:47",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Initially i thought C and D are correct answers BUT seems C is uncorrrect as The marketing team doesn't have access to data that the sales team stores in the S3 bucket",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777165,
          "date": "Mon 16 Jan 2023 00:18",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct.<br>C is not correct because The marketing team needs access to data that the sates team stores in the S3 bucket",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 776576,
          "date": "Sun 15 Jan 2023 14:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is option C for ME.  (D is also correct but the choice is difficult)<br><br>In this option, the solution architect updates the S3 bucket policy in the marketing account to grant access to the QuickSight role. This allows the QuickSight service to read the data stored in the S3 bucket. Additionally, the solution architect creates a KMS grant for the encryption key that is used in the S3 bucket, granting the QuickSight role the ability to decrypt the data. This will allow the marketing team to access and visualize the data stored in the S3 bucket, while keeping it secure with the use of encryption.<br><br>Option A would require the creation of a new S3 bucket in the marketing account and a replication rule to copy the data. This would increase the operational overhead and could also cause data consistency issues.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>same as zhangyu wrote, C is not correct..wrong account specified..bucket is in the Sales one</li><li>C is only allow marketing bucket. But data is stored on sales bucket</li><li>Option B would require the creation of a service control policy (SCP) in the marketing account, and the use of AWS RAM to share the KMS key, but this option would not grant the QuickSight role the ability to decrypt the data.<br><br>Option D would require creating an IAM role in the sales account and granting access to the S3 bucket. From the marketing account, assuming the IAM role in the sales account to access the S3 bucket. This would also increase operational overhead and complexity.</li><li>Option C:<br><br>- Update the S3 bucket policy in the sales account to grant access to the QuickSight role in the marketing account.<br><br>- Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role in the marketing account.<br><br>- Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.<br><br>This option allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer. The KMS grant allows the marketing team to access the decryption key without having to have access to the key itself.</li><li>Option D:<br><br>- Create an IAM role in the sales account and grant access to the S3 bucket.<br>- From the marketing account, assume the IAM role in the sales account to access the S3 bucket.<br>- Update the QuickSight role, to create a trust relationship with the new IAM role in the sales account.<br><br>This option is similar to option C but instead of using KMS grant, it uses IAM role to access the S3 bucket in the sales account. This option also allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 843448,
          "date": "Sun 19 Mar 2023 05:59",
          "username": "\t\t\t\tignorica\t\t\t",
          "content": "same as zhangyu wrote, C is not correct..wrong account specified..bucket is in the Sales one",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777954,
          "date": "Mon 16 Jan 2023 17:53",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is only allow marketing bucket. But data is stored on sales bucket",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776577,
          "date": "Sun 15 Jan 2023 14:20",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B would require the creation of a service control policy (SCP) in the marketing account, and the use of AWS RAM to share the KMS key, but this option would not grant the QuickSight role the ability to decrypt the data.<br><br>Option D would require creating an IAM role in the sales account and granting access to the S3 bucket. From the marketing account, assuming the IAM role in the sales account to access the S3 bucket. This would also increase operational overhead and complexity.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C:<br><br>- Update the S3 bucket policy in the sales account to grant access to the QuickSight role in the marketing account.<br><br>- Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role in the marketing account.<br><br>- Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.<br><br>This option allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer. The KMS grant allows the marketing team to access the decryption key without having to have access to the key itself.</li><li>Option D:<br><br>- Create an IAM role in the sales account and grant access to the S3 bucket.<br>- From the marketing account, assume the IAM role in the sales account to access the S3 bucket.<br>- Update the QuickSight role, to create a trust relationship with the new IAM role in the sales account.<br><br>This option is similar to option C but instead of using KMS grant, it uses IAM role to access the S3 bucket in the sales account. This option also allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776580,
          "date": "Sun 15 Jan 2023 14:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C:<br><br>- Update the S3 bucket policy in the sales account to grant access to the QuickSight role in the marketing account.<br><br>- Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role in the marketing account.<br><br>- Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.<br><br>This option allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer. The KMS grant allows the marketing team to access the decryption key without having to have access to the key itself.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D:<br><br>- Create an IAM role in the sales account and grant access to the S3 bucket.<br>- From the marketing account, assume the IAM role in the sales account to access the S3 bucket.<br>- Update the QuickSight role, to create a trust relationship with the new IAM role in the sales account.<br><br>This option is similar to option C but instead of using KMS grant, it uses IAM role to access the S3 bucket in the sales account. This option also allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776581,
          "date": "Sun 15 Jan 2023 14:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D:<br><br>- Create an IAM role in the sales account and grant access to the S3 bucket.<br>- From the marketing account, assume the IAM role in the sales account to access the S3 bucket.<br>- Update the QuickSight role, to create a trust relationship with the new IAM role in the sales account.<br><br>This option is similar to option C but instead of using KMS grant, it uses IAM role to access the S3 bucket in the sales account. This option also allows the marketing team to access the data in the S3 bucket in the sales account, while ensuring that the data remains encrypted at rest and during transfer.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#102",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service. A solutions architect must design a heterogeneous database migration on AWS.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#102",
          "answers": [
            {
              "choice": "<p>A. Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851285,
          "date": "Sun 26 Mar 2023 18:56",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use the AWS Schema Conversion Tool",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830535,
          "date": "Mon 06 Mar 2023 04:59",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "For heterogenous DBs, SCT is apt.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 799832,
          "date": "Mon 06 Feb 2023 15:14",
          "username": "\t\t\t\tAppon\t\t\t",
          "content": "https://aws.amazon.com/blogs/database/migrating-a-sql-server-database-to-a-mysql-compatible-database-engine/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797724,
          "date": "Sat 04 Feb 2023 09:05",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "heterogenous -> frmo onee DB engine to another",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 796187,
          "date": "Thu 02 Feb 2023 16:53",
          "username": "\t\t\t\tMasterP007\t\t\t",
          "content": "Straightforward - C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 793426,
          "date": "Mon 30 Jan 2023 23:48",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is the answer",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776585,
          "date": "Sun 15 Jan 2023 14:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.<br><br>AWS Schema Conversion Tool (SCT) can automatically convert the database schema from Microsoft SQL Server to Amazon RDS for MySQL. This allows for a smooth transition of the database schema without any manual intervention.<br><br>AWS DMS can then be used to migrate the data from the on-premises databases to the newly created Amazon RDS for MySQL instance. This service can perform a one-time migration of the data or can set up ongoing replication of data changes to keep the on-premises and AWS databases in sync.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not correct because while Amazon RDS for MySQL supports SQL Server databases, it is not a good fit for migrating business-critical applications. The data model and architecture are different and would require significant re-engineering.<br><br>Option B is not correct because AWS Snowball Edge Storage Optimized devices are used for transferring large amounts of data to and from AWS, but they do not support SQL Server.<br><br>Option D is not correct because AWS DataSync can only transfer files and folders, it does not support SQL Server databases.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776586,
          "date": "Sun 15 Jan 2023 14:25",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not correct because while Amazon RDS for MySQL supports SQL Server databases, it is not a good fit for migrating business-critical applications. The data model and architecture are different and would require significant re-engineering.<br><br>Option B is not correct because AWS Snowball Edge Storage Optimized devices are used for transferring large amounts of data to and from AWS, but they do not support SQL Server.<br><br>Option D is not correct because AWS DataSync can only transfer files and folders, it does not support SQL Server databases.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#103",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.<br><br>After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes.<br><br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#103",
          "answers": [
            {
              "choice": "<p>A. In the production account, create a new IAM policy that allows read and write access to the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the development account, create a new IAM policy that allows read and write access to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the production account, create a role Attach the new policy to the role. Define the development account as a trusted entity.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the development account, create a role. Attach the new policy to the role Define the production account as a trusted entity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. In the development account, create a group that contains all the IAM users of the design team Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851686,
          "date": "Mon 27 Mar 2023 06:35",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ACE is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 843330,
          "date": "Sun 19 Mar 2023 02:22",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Step 1: Create a role in the Production Account; create the role in the Production account and specify the Development account as a trusted entity. You also limit the role permissions to only read and write access to the productionapp bucket. Anyone granted permission to use the role can read and write to the productionapp bucket.<br>Step 2: Grant access to the role Sign in as an administrator in the Development account and allow the AssumeRole action on the UpdateApp role in the Production account.<br><br>So, recap, production account you create the policy for S3, and you set development account as a trusted entity. Then on the development account you allow the sts:assumeRole action on the role in production account. <br>https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 830542,
          "date": "Mon 06 Mar 2023 05:19",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Make Dev account as trusted entity. create a role in prod account. attache IAM policy of prod account and let development account assume this role to access prod s3 bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 797740,
          "date": "Sat 04 Feb 2023 09:31",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I think it's clear",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 795363,
          "date": "Wed 01 Feb 2023 18:36",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "ACE is correct answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 793429,
          "date": "Mon 30 Jan 2023 23:55",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "ACEshould works",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 779103,
          "date": "Tue 17 Jan 2023 18:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A, C, and E. <br><br>A: In the production account, creating a new IAM policy that allows read and write access to the S3 bucket is correct because it allows the design team to upload and update the static assets in the S3 bucket in the production account.<br><br>C: In the production account, creating a role and attaching the new policy to the role, and defining the development account as a trusted entity is correct because it allows the design team from the development account to assume the role and access the S3 bucket in the production account, while limiting their access to only the specific resources and actions defined in the policy.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>E: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account is correct because it allows the users in the group to assume the role created in the production account, which gives them access to the S3 bucket in the production account.<br><br>The other choices are not correct because:<br><br>B: In the development account, creating a new IAM policy that allows read and write access to the S3 bucket is not correct because the design team needs to access the S3 bucket in the production account, not the development account.</li><li>D: In the development account, creating a role, attaching the new policy to the role and defining the production account as a trusted entity is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not create a role in the development account.<br><br>F: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not the development account.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 779105,
          "date": "Tue 17 Jan 2023 18:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "E: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account is correct because it allows the users in the group to assume the role created in the production account, which gives them access to the S3 bucket in the production account.<br><br>The other choices are not correct because:<br><br>B: In the development account, creating a new IAM policy that allows read and write access to the S3 bucket is not correct because the design team needs to access the S3 bucket in the production account, not the development account.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D: In the development account, creating a role, attaching the new policy to the role and defining the production account as a trusted entity is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not create a role in the development account.<br><br>F: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not the development account.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779106,
          "date": "Tue 17 Jan 2023 18:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D: In the development account, creating a role, attaching the new policy to the role and defining the production account as a trusted entity is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not create a role in the development account.<br><br>F: In the development account, creating a group that contains all the IAM users of the design team and attaching a different IAM policy to the group to allow the sts:AssumeRole action on the role in the development account is not correct because the design team needs to assume a role in the production account to access the S3 bucket, not the development account.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777171,
          "date": "Mon 16 Jan 2023 00:24",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ACE is my answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776600,
          "date": "Sun 15 Jan 2023 14:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A, D, and E are the correct steps that would meet the requirements.<br>A.  In the production account, create a new IAM policy that allows read and write access to the S3 bucket. This will allow the design team to read and write to the S3 bucket that holds the assets in the production account.<br>D.  In the development account, create a role. Attach the new policy to the role. Define the production account as a trusted entity. This will allow the design team to assume a role in the development account that has permissions to access the S3 bucket in the production account.<br>E.  In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. This will allow the users in the design team group to assume the role created in step D and access the S3 bucket in the production account.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not required because the design team needs to access the S3 bucket in the production account, not in the development account.<br><br>Option C is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account.<br><br>Option F is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account that is trusted by the production account.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 776601,
          "date": "Sun 15 Jan 2023 14:32",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not required because the design team needs to access the S3 bucket in the production account, not in the development account.<br><br>Option C is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account.<br><br>Option F is not required because the design team needs to access the S3 bucket in the production account and this can be done by assuming a role in the development account that is trusted by the production account.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#104",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.<br><br>A solutions architect must mitigate the performance issues before the company launches the application to production.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#104",
          "answers": [
            {
              "choice": "<p>A. Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851687,
          "date": "Mon 27 Mar 2023 06:36",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Modify the existing environment's capacity configuration to use a load-balanced environment type.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 843332,
          "date": "Sun 19 Mar 2023 02:27",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "You can change your environment type to a single-instance or load-balanced, scalable environment by editing your environment's configuration. In some cases, you might want to change your environment type from one type to another. For example, let's say that you developed and tested an application in a single-instance environment to save costs. When your application is ready for production, you can change the environment type to a load-balanced, scalable environment so that it can scale to meet the demands of your customers.<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830550,
          "date": "Mon 06 Mar 2023 05:36",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A is wrong. no need to re create new EB env when the question is asking to mitigate probable performance issues based on current compute consumption of >=85%",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 808660,
          "date": "Tue 14 Feb 2023 19:01",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797747,
          "date": "Sat 04 Feb 2023 09:41",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "It's C.  https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html#using-features.managing.changetype",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 786727,
          "date": "Tue 24 Jan 2023 18:23",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "A: Elastic Beanstalk environment can not be changed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html<br>Yes they can.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793861,
          "date": "Tue 31 Jan 2023 09:44",
          "username": "\t\t\t\tmikeshop\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html<br>Yes they can.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 786286,
          "date": "Tue 24 Jan 2023 08:54",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "I think AWS wants you to know is the below.<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-types.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 784271,
          "date": "Sun 22 Jan 2023 13:16",
          "username": "\t\t\t\tromidan\t\t\t",
          "content": "I think C does make sense as per the link below - <br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.EditConfig.html<br><br>As per this link, a change would automatically initiate the new instance as per the ASG min attribute.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779110,
          "date": "Tue 17 Jan 2023 18:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A.  Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes.<br>This solution will create a new load-balanced environment which will increase the scalability and availability of the application, which will help mitigate the performance issues. Additionally, by adding a scale-out rule that triggers when the CPU utilization is high, the application will automatically scale to handle increased traffic, which will help alleviate the performance bottlenecks.<br>B.  Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes.<br>This option is not correct because it only directs some traffic to the new environment but it does not scale out the instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C.  Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.<br>This option is not correct because you can't change the existing environment.<br>D.  Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.<br>This option is not correct because it rebuilds the environment but it does not scale out the instances.<br><br>In summary, option A is the correct answer because it creates a new load-balanced environment, which increases scalability and availability, and it also includes a scale-out rule that triggers when CPU utilization is high, which automatically scales the instances to handle increased traffic, thus alleviating performance bottlenecks.</li><li>\\\"you can't change the existing environment.\\\": since when?<br>2 years ago it was possible and I firmly believe AWS didn't change that without updating the documentation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.EditConfig.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779111,
          "date": "Tue 17 Jan 2023 18:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Modify the existing environment's capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes.<br>This option is not correct because you can't change the existing environment.<br>D.  Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes.<br>This option is not correct because it rebuilds the environment but it does not scale out the instances.<br><br>In summary, option A is the correct answer because it creates a new load-balanced environment, which increases scalability and availability, and it also includes a scale-out rule that triggers when CPU utilization is high, which automatically scales the instances to handle increased traffic, thus alleviating performance bottlenecks.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"you can't change the existing environment.\\\": since when?<br>2 years ago it was possible and I firmly believe AWS didn't change that without updating the documentation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.EditConfig.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837532,
          "date": "Mon 13 Mar 2023 02:40",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "\\\"you can't change the existing environment.\\\": since when?<br>2 years ago it was possible and I firmly believe AWS didn't change that without updating the documentation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.EditConfig.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777177,
          "date": "Mon 16 Jan 2023 00:40",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A: You cannot change envrionment<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes, you can. Try on your AWS account. The correct answer is C. </li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 784410,
          "date": "Sun 22 Jan 2023 15:59",
          "username": "\t\t\t\tkeenian\t\t\t",
          "content": "Yes, you can. Try on your AWS account. The correct answer is C. ",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 776607,
          "date": "Sun 15 Jan 2023 14:37",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  This solution will meet the requirements with the least operational overhead because it modifies the existing environment's capacity configuration to use a load-balanced environment type and selects all availability zones. This will allow the application to scale out automatically if the average CPU utilization is over 85% for 5 minutes. This will help alleviate the performance issues without the need to create a new environment or rebuild the existing one.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, creating a new Elastic Beanstalk application, would require more operational overhead as it would involve creating a new environment and configuring it with a load-balanced environment type and selecting all availability zones.<br><br>Option B, creating a second Elastic Beanstalk environment and applying a traffic-splitting deployment policy, would also require more operational overhead as it would involve creating a new environment and configuring it to handle some of the incoming traffic.<br><br>Option D, selecting the Rebuild environment action with the load balancing option, would also require more operational overhead as it would involve rebuilding the existing environment and configuring it with a load-balanced environment type.</li><li>To modify the existing environment's capacity in Elastic Beanstalk, you can use the Elastic Beanstalk management console or the AWS Elastic Beanstalk API.<br><br>To do this using the management console:<br><br>1 - Open the Elastic Beanstalk management console.<br>2 - Select the application and environment that you want to modify.<br>3 - In the navigation pane, choose Configuration.<br>4 - In the Capacity configuration section, you can modify the number of instances in your environment and configure automatic scaling settings.<br><br>To do this using the AWS Elastic Beanstalk API, you can use the UpdateEnvironment API action. The UpdateEnvironment action allows you to change the number of instances in your environment, as well as other settings like the environment name and description.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776608,
          "date": "Sun 15 Jan 2023 14:37",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, creating a new Elastic Beanstalk application, would require more operational overhead as it would involve creating a new environment and configuring it with a load-balanced environment type and selecting all availability zones.<br><br>Option B, creating a second Elastic Beanstalk environment and applying a traffic-splitting deployment policy, would also require more operational overhead as it would involve creating a new environment and configuring it to handle some of the incoming traffic.<br><br>Option D, selecting the Rebuild environment action with the load balancing option, would also require more operational overhead as it would involve rebuilding the existing environment and configuring it with a load-balanced environment type.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To modify the existing environment's capacity in Elastic Beanstalk, you can use the Elastic Beanstalk management console or the AWS Elastic Beanstalk API.<br><br>To do this using the management console:<br><br>1 - Open the Elastic Beanstalk management console.<br>2 - Select the application and environment that you want to modify.<br>3 - In the navigation pane, choose Configuration.<br>4 - In the Capacity configuration section, you can modify the number of instances in your environment and configure automatic scaling settings.<br><br>To do this using the AWS Elastic Beanstalk API, you can use the UpdateEnvironment API action. The UpdateEnvironment action allows you to change the number of instances in your environment, as well as other settings like the environment name and description.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776611,
          "date": "Sun 15 Jan 2023 14:38",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "To modify the existing environment's capacity in Elastic Beanstalk, you can use the Elastic Beanstalk management console or the AWS Elastic Beanstalk API.<br><br>To do this using the management console:<br><br>1 - Open the Elastic Beanstalk management console.<br>2 - Select the application and environment that you want to modify.<br>3 - In the navigation pane, choose Configuration.<br>4 - In the Capacity configuration section, you can modify the number of instances in your environment and configure automatic scaling settings.<br><br>To do this using the AWS Elastic Beanstalk API, you can use the UpdateEnvironment API action. The UpdateEnvironment action allows you to change the number of instances in your environment, as well as other settings like the environment name and description.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#105",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.<br><br>Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#105",
          "answers": [
            {
              "choice": "<p>A. Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851688,
          "date": "Mon 27 Mar 2023 06:38",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Performing a one-time migration",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793433,
          "date": "Tue 31 Jan 2023 00:00",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B is the best solution",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 776617,
          "date": "Sun 15 Jan 2023 14:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.<br><br>This is the optimal solution as migrating the database to Amazon RDS will provide the ability to easily scale read replicas for handling increased read traffic during the end of the month. Additionally, RDS will manage the underlying infrastructure and provide automatic backups, software patching, and monitoring, which will reduce the operational overhead for the company. <br><br>Option A may help but it will not be sufficient to handle the heavy load, option C and D are not efficient solutions to han",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#106",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable. but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.<br><br>Which solution will meet these requirements with the LEAST code changes?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#106",
          "answers": [
            {
              "choice": "<p>A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776623,
          "date": "Sun 15 Jan 2023 14:46",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer would be A, as migrating the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container and storing container images in Amazon Elastic Container Registry (Amazon ECR) would minimize the code changes and administrative overhead required to maintain the servers. This option would allow the company to use the Application Load Balancer (ALB) to interact with the application and the ECS task execution role permission to access the ECR image repository.<br><br>Option B would require the application code to be migrated to a container that runs in AWS Lambda, which would require more code changes. <br><br>Option C would require migrating the application to Amazon Elastic Kubernetes Service (Amazon EKS) which would require more administrative overhead. <br><br>Option D would require configuring Lambda to use an Application Load Balancer (ALB), which is not a native feature of Lambda.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B does not say anything about Lambda. Where have you red that?</li><li>You are right, I mixed A with B</li><li>This solution allows for the existing application code to be packaged into a container, which can then be deployed to ECS on Fargate. The use of AWS App2Container will help automate the containerization process, minimizing the need for code changes. Additionally, by using ECR to store container images, the application can continue to use the same images and dependencies that it currently relies on. The use of an Application Load Balancer (ALB) to interact with the application further simplifies the migration process by allowing the use of the existing application's endpoint.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 797752,
          "date": "Sat 04 Feb 2023 09:52",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B does not say anything about Lambda. Where have you red that?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are right, I mixed A with B</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797756,
          "date": "Sat 04 Feb 2023 09:53",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "You are right, I mixed A with B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776625,
          "date": "Sun 15 Jan 2023 14:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This solution allows for the existing application code to be packaged into a container, which can then be deployed to ECS on Fargate. The use of AWS App2Container will help automate the containerization process, minimizing the need for code changes. Additionally, by using ECR to store container images, the application can continue to use the same images and dependencies that it currently relies on. The use of an Application Load Balancer (ALB) to interact with the application further simplifies the migration process by allowing the use of the existing application's endpoint.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 851695,
          "date": "Mon 27 Mar 2023 06:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 843345,
          "date": "Sun 19 Mar 2023 02:55",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "AWS App2Container (A2C) is a command line tool to help you lift and shift applications that run in your on-premises data centers or on virtual machines, so that they run in containers that are managed by Amazon ECS, Amazon EKS, or AWS App Runner.<br><br>Moving legacy applications to containers is often the starting point toward application modernization. There are many benefits to containerization:<br>• Reduces operational overhead and infrastructure costs<br>• Increases development and deployment agility<br>• Standardizes build and deployment processes across an organization<br>https://docs.aws.amazon.com/app2container/latest/UserGuide/what-is-a2c.html<br>AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).<br>https://aws.amazon.com/fargate/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833759,
          "date": "Thu 09 Mar 2023 10:18",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "least code chansges",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 809366,
          "date": "Wed 15 Feb 2023 11:30",
          "username": "\t\t\t\tkeonlee\t\t\t",
          "content": "Fargate, Modernize stack",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808671,
          "date": "Tue 14 Feb 2023 19:20",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Least code changes",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 805666,
          "date": "Sat 11 Feb 2023 21:47",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "A is much simpler with AWS Copilot. I also don't have to deal with Lambda's cold start time. You also need to do a little bit of coding to interact with Lambda's Runtime API that are part of Lambda's base images - https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 797753,
          "date": "Sat 04 Feb 2023 09:52",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B implies LEAST code changes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry I meant A</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 797754,
          "date": "Sat 04 Feb 2023 09:53",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Sorry I meant A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795791,
          "date": "Thu 02 Feb 2023 07:00",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "B should be correct, why to use EKS, the question does not mention any details or complex design to use it so I will go with an easy and cost Solution<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>can't agree more, however A mentions how to migrate an app on an instance to container images. That seems to be an important step. B does not.</li><li>Also its an app not rest, thats more a job for a loadbalancer.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837502,
          "date": "Mon 13 Mar 2023 01:58",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "can't agree more, however A mentions how to migrate an app on an instance to container images. That seems to be an important step. B does not.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Also its an app not rest, thats more a job for a loadbalancer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837506,
          "date": "Mon 13 Mar 2023 02:00",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Also its an app not rest, thats more a job for a loadbalancer.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#107",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#107",
          "answers": [
            {
              "choice": "<p>A. Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 10 direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 779113,
          "date": "Tue 17 Jan 2023 18:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints. This solution meets the requirement of having a failover to another region by having a copy of the Lambda function and API Gateway endpoint in a different region, and using Route 53's failover routing policy to route traffic between the two regions.<br><br>Option A is not correct because it only creates an additional API Gateway endpoint in us-west-2 and relies on Route 53's failover routing policy to direct traffic to the correct endpoint. But it does not deploy the Lambda function to the new region and this makes the failover incomplete.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You always use ChatGPT to paste answers. Most of the time ChatGPT gives wrong answers do you know this?</li><li>Option B is not correct because it uses a SQS queue as a buffer between the API Gateway and the Lambda function, but this does not provide failover to another region. In addition, it would also increase the latency of the system as the SQS will act as an additional layer.<br><br>Option C is not correct because it deploys the Lambda function to the us-west-2 Region and creates an API Gateway endpoint in the same region. But it uses AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. However, this is not a failover solution as both regions will be active and serving traffic at the same time.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 840773,
          "date": "Thu 16 Mar 2023 16:17",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "You always use ChatGPT to paste answers. Most of the time ChatGPT gives wrong answers do you know this?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779115,
          "date": "Tue 17 Jan 2023 18:13",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not correct because it uses a SQS queue as a buffer between the API Gateway and the Lambda function, but this does not provide failover to another region. In addition, it would also increase the latency of the system as the SQS will act as an additional layer.<br><br>Option C is not correct because it deploys the Lambda function to the us-west-2 Region and creates an API Gateway endpoint in the same region. But it uses AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints. However, this is not a failover solution as both regions will be active and serving traffic at the same time.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 851698,
          "date": "Mon 27 Mar 2023 06:41",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 843354,
          "date": "Sun 19 Mar 2023 03:13",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Currently, the default API endpoint type in API Gateway is the edge-optimized API endpoint, which enables clients to access an API through an Amazon CloudFront distribution. This typically improves connection time for geographically diverse clients. By default, a custom domain name is globally unique and the edge-optimized API endpoint would invoke a Lambda function in a single region in the case of Lambda integration. You can't use this type of endpoint with a Route 53 active-active setup and fail-over.<br><br>The new regional API endpoint in API Gateway moves the API endpoint into the region and the custom domain name is unique per region. This makes it possible to run a full copy of an API in each region and then use Route 53 to use an active-active setup and failover.<br>https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 830560,
          "date": "Mon 06 Mar 2023 06:21",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "B is wrong, cannot direct traffic to SQS Queue ? it does not even mention posting messages to queue.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 793436,
          "date": "Tue 31 Jan 2023 00:09",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The correct answer is D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777194,
          "date": "Mon 16 Jan 2023 01:03",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct<br>A is not because the Lambda is in us-ease-1 but api gateway is in us-west-2. cannot cross regions",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 776639,
          "date": "Sun 15 Jan 2023 15:01",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A. <br><br>In this solution, an API Gateway endpoint is created in the us-west-2 Region. This new endpoint is configured to direct traffic to the Lambda function in us-east-1. If a failure occurs in the us-east-1 Region, Amazon Route 53's failover routing policy automatically routes traffic to the us-west-2 Region. This ensures that traffic is directed to a healthy endpoint, providing failover support for the application.<br><br>B, C and D does not meet the requirement of having failover routing policy.<br><br>In B, SQS is not a failover mechanism, it is a messaging service and it does not provide failover routing.<br><br>In C, Global Accelerator and Application Load Balancer does not provide failover routing.<br><br>In D, While creating a second endpoint in the us-west-2 Region and using Amazon Route 53 to route traffic to it, it still does not provide failover routing.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#108",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.<br><br>The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department will install the new application on this account. The HR department wants to make sure that other departments cannot share the RI discounts.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#108",
          "answers": [
            {
              "choice": "<p>A. In the AWS Billing and Cost Management console for the HR department's production account turn off RI sharing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the HR department's production AWS account from the organization. Add the account 10 the consolidating billing configuration only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the AWS Billing and Cost Management console. use the organization's management account 10 turn off RI Sharing for the HR departments production AWS account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851703,
          "date": "Mon 27 Mar 2023 06:43",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is the way to go",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 833758,
          "date": "Thu 09 Mar 2023 10:18",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Management account -->Billing Dashboard --> Billing preferences, this option is there to choose enable/disable RI discounts sharing",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830847,
          "date": "Mon 06 Mar 2023 14:40",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830576,
          "date": "Mon 06 Mar 2023 07:01",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Management account -->Billing Dashboard --> Billing preferences, this option is there to choose enable/disable RI discounts sharing <br>https://us-east-1.console.aws.amazon.com/billing/home#/preferences",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827251,
          "date": "Thu 02 Mar 2023 19:43",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "How can you restrict access from AWS billing console? Can you show me please??<br>Option D is the correct solution because an SCP (Service Control Policy) can be created in the AWS Organizations service to restrict access to specific resources or actions across the entire organization or specific OUs. In this case, an SCP can be created to restrict other departments from accessing the RIs purchased by the HR department's production account. This ensures that the discounts are not shared with other departments.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Bro, Go to Management account --&gt;Billing Dashboard --&gt; Billing preferences, this option is there to choose enable/disable RI discounts sharing <br>https://us-east-1.console.aws.amazon.com/billing/home#/preferences</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 830577,
          "date": "Mon 06 Mar 2023 07:01",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Bro, Go to Management account -->Billing Dashboard --> Billing preferences, this option is there to choose enable/disable RI discounts sharing <br>https://us-east-1.console.aws.amazon.com/billing/home#/preferences",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795795,
          "date": "Thu 02 Feb 2023 07:07",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "The correct answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776699,
          "date": "Sun 15 Jan 2023 16:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C. <br><br>In this solution, the organization's management account can be used to turn off RI sharing for the HR department's production AWS account in the AWS Billing and Cost Management console. This will ensure that the other departments cannot share the RI discounts and the HR department can use the RIs for their new system without any interruption.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A, B and D does not meet the requirement of turning off RI sharing for the HR department's production AWS account.<br><br>In A, Turning off RI sharing in the HR department's production account will not prevent other departments from sharing the RI discounts.<br><br>In B, Removing the HR department's production AWS account from the organization may cause issues in consolidated billing and it does not prevent other departments from sharing the RI discounts.<br><br>In D, Creating an SCP in the organization to restrict access to the RIs is not necessary because the management account can directly turn off the RI sharing, it also does not prevent other departments from sharing the RI discounts.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776700,
          "date": "Sun 15 Jan 2023 16:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A, B and D does not meet the requirement of turning off RI sharing for the HR department's production AWS account.<br><br>In A, Turning off RI sharing in the HR department's production account will not prevent other departments from sharing the RI discounts.<br><br>In B, Removing the HR department's production AWS account from the organization may cause issues in consolidated billing and it does not prevent other departments from sharing the RI discounts.<br><br>In D, Creating an SCP in the organization to restrict access to the RIs is not necessary because the management account can directly turn off the RI sharing, it also does not prevent other departments from sharing the RI discounts.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#109",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is configured, and AWS Systems Manager Agent is running on all the EC2 instances.<br><br>The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive.<br><br>How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#109",
          "answers": [
            {
              "choice": "<p>A. Suspend the Auto Scaling group's HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Suspend the Auto Scaling group's Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851706,
          "date": "Mon 27 Mar 2023 06:45",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Suspend the Auto Scaling group's Terminate process.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 843365,
          "date": "Sun 19 Mar 2023 03:32",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Amazon EC2 Auto Scaling stops marking instances unhealthy as a result of EC2 and Elastic Load Balancing health checks. Your custom health checks continue to function properly. After you suspend HealthCheck, if you need to, you can manually set the health state of instances in your group and have ReplaceUnhealthy replace them.<br>Suspending the Terminate process doesn't prevent the successful termination of instances using the force delete option with the delete-auto-scaling-group command.<br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html <br>https://docs.aws.amazon.com/systems-manager/latest/userguide/incident-manager.html<br>We want the health checks to continue failing, just stop terminating to identify root cause",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831060,
          "date": "Mon 06 Mar 2023 18:24",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Disabling health check wont let SA know which instance is un healthy. So A is certainly wrong. D is correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827334,
          "date": "Thu 02 Mar 2023 20:36",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "Answer is A<br>If you do not want instances to be replaced, we recommend that you suspend the ReplaceUnhealthy and HealthCheck process for individual Auto Scaling groups. For more information, see Suspend and resume a process for an Auto Scaling group. <br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-health-checks.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That does not solve, it removes the healthcheck process, but also removes the ones that are being marked as unhealthy. The issue now is that one it is tagged as unhealthy they are being terminated. So, any that are already marked get terminated and you just removed the health checks to find remaining. you can't troubleshoot what you don't know.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 843369,
          "date": "Sun 19 Mar 2023 03:34",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "That does not solve, it removes the healthcheck process, but also removes the ones that are being marked as unhealthy. The issue now is that one it is tagged as unhealthy they are being terminated. So, any that are already marked get terminated and you just removed the health checks to find remaining. you can't troubleshoot what you don't know.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793437,
          "date": "Tue 31 Jan 2023 00:12",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The correct answer is D. ",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776711,
          "date": "Sun 15 Jan 2023 16:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/51249-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>The correct answer is D. <br><br>In this solution, the architect can suspend the Auto Scaling group's Terminate process, which will prevent the instances marked as unhealthy from being terminated. This will allow the architect to log in to the instance using Session Manager and troubleshoot the issue without losing access to the instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because suspending the HealthCheck scaling process will not prevent instances from being terminated.<br><br>Option B is incorrect because enabling EC2 instance termination protection will not prevent instances from being terminated by Auto Scaling group.<br><br>Option C is incorrect because setting the termination policy to OldestInstance on the Auto Scaling group will not prevent instances marked as unhealthy from being terminated.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776712,
          "date": "Sun 15 Jan 2023 16:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because suspending the HealthCheck scaling process will not prevent instances from being terminated.<br><br>Option B is incorrect because enabling EC2 instance termination protection will not prevent instances from being terminated by Auto Scaling group.<br><br>Option C is incorrect because setting the termination policy to OldestInstance on the Auto Scaling group will not prevent instances marked as unhealthy from being terminated.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#110",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations.<br><br>Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#110",
          "answers": [
            {
              "choice": "<p>A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 776719,
          "date": "Sun 15 Jan 2023 16:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is A. <br><br>In this solution, AWS Firewall Manager is used to manage AWS WAF rules across accounts in the organization. An AWS Systems Manager Parameter Store parameter is used to store account numbers and OUs to manage. This parameter can be updated as needed to add or remove accounts or OUs. An Amazon EventBridge rule is used to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This solution allows for easy management of AWS WAF rules across multiple accounts with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B does not meet the requirement of being able to add or remove accounts or OUs from managed AWS WAF rule sets as needed.<br><br>Option C is not the best approach as it requires manual configuration of the cross-account IAM roles and assume-role calls in the Lambda function, increasing the operational overhead.<br><br>Option D does not meet the requirement of providing a centralized management console to manage the WAF rules across multiple accounts.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 776720,
          "date": "Sun 15 Jan 2023 16:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B does not meet the requirement of being able to add or remove accounts or OUs from managed AWS WAF rule sets as needed.<br><br>Option C is not the best approach as it requires manual configuration of the cross-account IAM roles and assume-role calls in the Lambda function, increasing the operational overhead.<br><br>Option D does not meet the requirement of providing a centralized management console to manage the WAF rules across multiple accounts.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851707,
          "date": "Mon 27 Mar 2023 06:46",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use AWS Firewall Manager to manage AWS WAF rules",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831219,
          "date": "Mon 06 Mar 2023 21:04",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Not D, KMS to store account numbers ?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793439,
          "date": "Tue 31 Jan 2023 00:13",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "The correct answer is A. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 787287,
          "date": "Wed 25 Jan 2023 06:12",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#111",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is auditing the security setup or an AWS Lambda function for a company. The Lambda function retrieves, the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC.  Lambda environment variables are providing the database credentials to the Lambda function.<br><br>The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is configured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise.<br><br>What should the solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#111",
          "answers": [
            {
              "choice": "<p>A. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851709,
          "date": "Mon 27 Mar 2023 06:48",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A for sure due to VPC endpoints.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833782,
          "date": "Thu 09 Mar 2023 10:39",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "I had a strong opinion about D but after reading and doing some research convience about A<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 831226,
          "date": "Mon 06 Mar 2023 21:14",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Key is - Data must not travel on the internet. Only S3 VPC Endpoints have this feature.<br><br>A VPC endpoint allows you to connect privately to S3 from within your Amazon Virtual Private Cloud (VPC) without the need for an internet gateway, NAT device, or VPN connection. Instead, the endpoint provides a direct and secure connection between your VPC and S3 over the Amazon network backbone.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 805672,
          "date": "Sat 11 Feb 2023 22:03",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": [<br>\\\"rds-db:connect\\\"<br>],<br>\\\"Resource\\\": [<br>\\\"arn:aws:rds-db:<region>:<account-id>:dbuser:<DbiResourceId>/<db_user_name>\\\"<br>]<br>}<br>]<br>}<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Source: https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 805673,
          "date": "Sat 11 Feb 2023 22:04",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Source: https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 803911,
          "date": "Fri 10 Feb 2023 03:42",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "Accessing S3 from Lambda does not use the internet. It's the foundation of AWS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>postscript<br>All communication uses the AWS private network</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 803913,
          "date": "Fri 10 Feb 2023 03:42",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "postscript<br>All communication uses the AWS private network",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793442,
          "date": "Tue 31 Jan 2023 00:17",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "a little bit confused between A and D but as said by others members D doesn't adress the The question of \\\"data must not travel across the Internet\\\"==> A is the answer",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790818,
          "date": "Sat 28 Jan 2023 17:46",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 786759,
          "date": "Tue 24 Jan 2023 18:58",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "A: the critical point here - is \\\"The data must not travel across the Internet\\\". VPC endpoint for Amazon S3 help in this.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779117,
          "date": "Tue 17 Jan 2023 18:16",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "You are correct. Option A: Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC is the best solution.<br><br>It is a combination of measures that work together to meet the requirements:<br><br>- IAM database authentication for the Aurora DB cluster allows for secure and centralized management of access to the database, and eliminates the need to store user credentials in the database.<br><br>- Deploying a gateway VPC endpoint for Amazon S3 ensures that data does not travel across the internet and is protected by VPC security.<br>Changing the IAM role for the Lambda function allows it to access the database securely via IAM database authentication.<br><br>- By implementing the above steps, you can ensure that the data is protected in transit and at rest, and that the impact of a compromise of the database credentials is minimized.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B: Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br>This option only covers the encryption of data in transit and doesn't address the security concerns of the data at rest.<br><br>Option C: Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. <br>This option addresses the security concern of rotating the credentials but doesn't cover the secure authentication to the database.</li><li>Option D: Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br>This option addresses the security concern of rotating the credentials but doesn't cover the secure authentication to the database and doesn't address the security concerns of the data at rest.<br><br>Option A is the best choice as it covers all the security concern in the question :<br><br>- secure authentication to the database<br>- encryption of data in transit<br>- protection of the data at rest.<br><br>It also provides centralised management of access to the database via IAM and protection of data while stored in S3 via a VPC endpoint.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779118,
          "date": "Tue 17 Jan 2023 18:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B: Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br>This option only covers the encryption of data in transit and doesn't address the security concerns of the data at rest.<br><br>Option C: Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC. <br>This option addresses the security concern of rotating the credentials but doesn't cover the secure authentication to the database.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D: Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br>This option addresses the security concern of rotating the credentials but doesn't cover the secure authentication to the database and doesn't address the security concerns of the data at rest.<br><br>Option A is the best choice as it covers all the security concern in the question :<br><br>- secure authentication to the database<br>- encryption of data in transit<br>- protection of the data at rest.<br><br>It also provides centralised management of access to the database via IAM and protection of data while stored in S3 via a VPC endpoint.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779120,
          "date": "Tue 17 Jan 2023 18:17",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option D: Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers.<br>This option addresses the security concern of rotating the credentials but doesn't cover the secure authentication to the database and doesn't address the security concerns of the data at rest.<br><br>Option A is the best choice as it covers all the security concern in the question :<br><br>- secure authentication to the database<br>- encryption of data in transit<br>- protection of the data at rest.<br><br>It also provides centralised management of access to the database via IAM and protection of data while stored in S3 via a VPC endpoint.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777201,
          "date": "Mon 16 Jan 2023 01:20",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct answer - https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776726,
          "date": "Sun 15 Jan 2023 16:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D. <br><br>In this solution, the solutions architect would recommend saving the database credentials in AWS Secrets Manager. Secrets Manager allows for password rotation on the credentials, which means that the credentials can be automatically rotated on a schedule to minimize the impact of a compromise. Additionally, the IAM role for the Lambda function can be changed to allow the function to access Secrets Manager, so that the function can retrieve the credentials securely.<br><br>Enforcing HTTPS on the connection to Amazon S3 during data transfers also ensures that the data is transmitted securely and does not travel across the internet.<br><br>Option A does not include a way to securely store and rotate the database credentials, and Option B does not include a way to securely retrieve the credentials from the Lambda function. Option C does not include a way to encrypt the data transfer to S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#112",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.<br><br>While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several large instance types account for a high proportion of the costs. The solutions architect finds out that the company's developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types.<br><br>The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#112",
          "answers": [
            {
              "choice": "<p>A. Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers' IAM accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851712,
          "date": "Mon 27 Mar 2023 06:49",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "IAM policy..",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793443,
          "date": "Tue 31 Jan 2023 00:20",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "answer is C",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776729,
          "date": "Sun 15 Jan 2023 16:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C. <br><br>In this solution, a new IAM policy is created that specifies the allowed instance types. This policy is then attached to an IAM group that contains the IAM accounts for the developers. This will ensure that the developers can only launch instances of the specified types, thus limiting the costs associated with the creation and termination of large instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Creating a desired-instance-type managed rule in AWS Config is not a sufficient solution, as it only identifies when an instance is launched with an unauthorized type, it does not prevent it.<br>B.  Creating a launch template that specifies the instance types that are allowed is not a sufficient solution, because it limits the instances types that can be launched in the EC2 console, but it does not prevent the launch of instances through the AWS SDK, AWS CLI, or other AWS services.<br>D.  Using EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image is not a direct solution to the problem of limiting the instance types that only the developers can launch. It can be useful for creating standardize images for the developers, but it does not provide the necessary control mechanism to limit the instance types.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776730,
          "date": "Sun 15 Jan 2023 16:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Creating a desired-instance-type managed rule in AWS Config is not a sufficient solution, as it only identifies when an instance is launched with an unauthorized type, it does not prevent it.<br>B.  Creating a launch template that specifies the instance types that are allowed is not a sufficient solution, because it limits the instances types that can be launched in the EC2 console, but it does not prevent the launch of instances through the AWS SDK, AWS CLI, or other AWS services.<br>D.  Using EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image is not a direct solution to the problem of limiting the instance types that only the developers can launch. It can be useful for creating standardize images for the developers, but it does not provide the necessary control mechanism to limit the instance types.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#113",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.<br><br>Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ABE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#113",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Config rule in each account to find resources with missing tags.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Inspector in the organization to find resources with missing tags.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851715,
          "date": "Mon 27 Mar 2023 06:51",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ABE is the better choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 842117,
          "date": "Fri 17 Mar 2023 17:14",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "what's the value of A and E together- it's either or ? the outcome is the same - thoughts?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831430,
          "date": "Tue 07 Mar 2023 01:33",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "If config rule is added (A) it can be seen in AWS Config aggregator (E) Using SCP in as aws organization is used here in question. So, A,B,E<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>If there are no organizations used, D can be used to prevent EC2 run instances too,<br>C is for vulnerabilities checking..F for all security issues consolidated..</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 831431,
          "date": "Tue 07 Mar 2023 01:34",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "If there are no organizations used, D can be used to prevent EC2 run instances too,<br>C is for vulnerabilities checking..F for all security issues consolidated..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816824,
          "date": "Tue 21 Feb 2023 17:20",
          "username": "\t\t\t\tjaysparky\t\t\t",
          "content": "ABE makes sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807334,
          "date": "Mon 13 Feb 2023 12:41",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Config, SCP and IAM policy may not require in each account but it says to select three optionsso going with ABE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 797795,
          "date": "Sat 04 Feb 2023 10:57",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "BE makes sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 790538,
          "date": "Sat 28 Jan 2023 12:00",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "the best way to deploy config rules accross accounts= SCP",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 788814,
          "date": "Thu 26 Jan 2023 15:45",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "In adding tag, the keywords are config, scp, aggreagator.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 787324,
          "date": "Wed 25 Jan 2023 07:29",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "A and E are correct. But the below is the best way to deploy config rules accross accounts.<br>https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html<br>https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html<br>B is correct.<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_tagging.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 786939,
          "date": "Tue 24 Jan 2023 21:37",
          "username": "\t\t\t\tccort\t\t\t",
          "content": "ABE for me<br><br>D I am sure it is not, it would be too much trouble putting the policy in each account",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ABE"
        },
        {
          "id": 777204,
          "date": "Mon 16 Jan 2023 01:23",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "BDE are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776742,
          "date": "Sun 15 Jan 2023 16:33",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is BDE. <br><br>B: Creating an SCP (Service Control Policy) in the organization with a deny action for ec2:RunInstances if the Project tag is missing will prevent developers from launching instances without the necessary tag. This is a good option because it will prevent the problem from happening again in the future.<br><br>D: Creating an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing will also prevent developers from launching instances without the necessary tag. This is a good option because it will prevent the problem from happening again in the future.<br><br>E: Creating an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag will help the team identify which instances are missing the tag, so they can take action to add the tag. This is a good option because it will help resolve the problem that has already happened and also help the team identify any instances that are not compliant with the company's tagging policy.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options, A and C are not appropriate for this scenario, because they would only identify the instances that are missing the tag, but not prevent the problem from happening again. Using option F is also not appropriate for this scenario, because AWS Security Hub is not used for cost allocation.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BDE"
        },
        {
          "id": 776744,
          "date": "Sun 15 Jan 2023 16:34",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options, A and C are not appropriate for this scenario, because they would only identify the instances that are missing the tag, but not prevent the problem from happening again. Using option F is also not appropriate for this scenario, because AWS Security Hub is not used for cost allocation.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#114",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage.<br><br>The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes:<br>• Managed AWS services to minimize operational complexity.<br>• A buffer that automatically scales to match the throughput of data and requires no ongoing administration.<br>• A visualization tool to create dashboards to observe events in near-real time.<br>• Support for semi-structured JSON data and dynamic schemas.<br><br>Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#114",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851718,
          "date": "Mon 27 Mar 2023 06:53",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AD is my vote",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 831526,
          "date": "Tue 07 Mar 2023 04:26",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Amazon Kinesis Data Firehose (A) allows you to buffer events in two ways: through buffering size or buffering time. With buffering size, you can configure the maximum size of the buffer in MB or the maximum number of records in the buffer. Once the buffer is full, it will automatically deliver the data to the destination<br><br>Amazon ES (D) has its ability to receive events from various sources in real-time. Amazon ES can ingest data from a variety of sources, such as Amazon Kinesis Data Firehose, Amazon CloudWatch Logs, and Amazon S3, making it a powerful tool for organizations looking to analyze and visualize real-time streaming data. (Kibana dashboards)",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 829807,
          "date": "Sun 05 Mar 2023 10:51",
          "username": "\t\t\t\tZek\t\t\t",
          "content": "A,D seem correct. https://www.examtopics.com/discussions/amazon/view/47625-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777207,
          "date": "Mon 16 Jan 2023 01:24",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AD are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776749,
          "date": "Sun 15 Jan 2023 16:38",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The combination of components that will enable the company to create a monitoring solution that will satisfy these requirements is:<br>A.  Use Amazon Kinesis Data Firehose to buffer events. This service can automatically scale to match the throughput of data, and it requires no ongoing administration. With Firehose, it's possible to use a Lambda function to process and transform events as well as to store them in other services like S3 or Redshift.<br>D.  Configure Amazon Elasticsearch Service (Amazon ES) to receive events. With Amazon Elasticsearch Service, it's possible to create an index for the events, making them searchable and queryable. This service is a fully managed service so it minimizes operational complexity. Also, it's possible to use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B: Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events. is incorrect because Kinesis Data Stream is a different service than Kinesis Data Firehose and does not have the buffer feature.<br><br>Option C: Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Aurora is a relational database service and does not support JSON data or dynamic schemas.<br><br>Option E: Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Neptune is a graph database service and does not support JSON data or dynamic schemas.</li><li>We use the Kinesis data stream specifically for its capability to store data \\\"aka buffer events\\\". Firehouse also has some resemblance of this feature but is more of a transportation service.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 776750,
          "date": "Sun 15 Jan 2023 16:38",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B: Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events. is incorrect because Kinesis Data Stream is a different service than Kinesis Data Firehose and does not have the buffer feature.<br><br>Option C: Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Aurora is a relational database service and does not support JSON data or dynamic schemas.<br><br>Option E: Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards. is incorrect because Amazon Neptune is a graph database service and does not support JSON data or dynamic schemas.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>We use the Kinesis data stream specifically for its capability to store data \\\"aka buffer events\\\". Firehouse also has some resemblance of this feature but is more of a transportation service.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826986,
          "date": "Thu 02 Mar 2023 15:21",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "We use the Kinesis data stream specifically for its capability to store data \\\"aka buffer events\\\". Firehouse also has some resemblance of this feature but is more of a transportation service.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#115",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company's applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets.<br><br>A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category.<br><br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#115",
          "answers": [
            {
              "choice": "<p>A. Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add an interface VPC endpoint for Kinesis Data Streams to the VPC.  Ensure that applications have the correct IAM permissions to use the interface VPC endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add an interface VPC endpoint for Kinesis Data Streams to the VPC.  Ensure that the VPC endpoint policy allows traffic from the applications.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851721,
          "date": "Mon 27 Mar 2023 06:54",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D is the best choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831540,
          "date": "Tue 07 Mar 2023 04:58",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "VPC endpoints to mitigate NAT gateway huge data transfer costs especially in Kinesis usecase where large data is passed thru<br><br>With a VPC endpoint policy, you can define rules to control access to the VPC endpoint. You can specify the source IP address or IP address range that is allowed to access the endpoint, as well as the type of traffic that is allowed, such as HTTP, HTTPS, or custom TCP ports. You can also specify the resources that can be accessed through the VPC endpoint, such as an Amazon S3 bucket or an Amazon DynamoDB table.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 827001,
          "date": "Thu 02 Mar 2023 15:33",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "If this is a cost-saving question is very hard to answer, you pay for both, and depending on the region one can be cheaper than the other. There is a cost for a NAT GW and also for a VPCendpoint per AZ plus the traffic you generate over them. In my experience, because you need a VPCendpoint for each service NAT-GW is cheaper.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818741,
          "date": "Thu 23 Feb 2023 04:27",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Allowing traffic from the application using the VPC endpoint is key to bypassing NAT Gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 805702,
          "date": "Sat 11 Feb 2023 22:50",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Which is which?<br><br>A VPC endpoint policy is an IAM resource policy that you attach to a VPC endpoint. It determines which principals can use the VPC endpoint to access the endpoint service. The default VPC endpoint policy allows all actions by all principals on all resources over the VPC endpoint.https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html#vpc-endpoints-policies",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 797843,
          "date": "Sat 04 Feb 2023 11:49",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B seems correct too.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777209,
          "date": "Mon 16 Jan 2023 01:26",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D: by pass internet to save cost on NAT GW",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776753,
          "date": "Sun 15 Jan 2023 16:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Adding an interface VPC endpoint for Kinesis Data Streams to the VPC will allow the applications to access the service without the need for a NAT gateway. This will reduce the cost associated with NatGateway-Bytes charges, which are increasing the cost in the EC2-Other category.<br><br>Option A is not correct because enabling VPC Flow Logs and reviewing the logs for traffic that can be removed is not a direct solution for reducing NatGateway-Bytes charges. Additionally, security groups are used to control access to resources, not to optimize network traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not correct because it does not address the specific issue of high NatGateway-Bytes charges. Additionally, ensuring that applications have the correct IAM permissions is a best practice but it is not directly related to reducing costs.<br><br>Option C is not correct because while reviewing Detective findings for traffic that is not related to Kinesis Data Streams can help identify potential issues, it does not directly address the issue of high NatGateway-Bytes charges. Additionally, Configuring security groups to block that traffic is not a solution for reducing costs associated with NatGateway-Bytes charges.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776754,
          "date": "Sun 15 Jan 2023 16:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B is not correct because it does not address the specific issue of high NatGateway-Bytes charges. Additionally, ensuring that applications have the correct IAM permissions is a best practice but it is not directly related to reducing costs.<br><br>Option C is not correct because while reviewing Detective findings for traffic that is not related to Kinesis Data Streams can help identify potential issues, it does not directly address the issue of high NatGateway-Bytes charges. Additionally, Configuring security groups to block that traffic is not a solution for reducing costs associated with NatGateway-Bytes charges.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#116",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffic from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffic that is routed directly between VPCs in those Regions. No single points of failure can exist on the network.<br><br>The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is configured to route all inter-VPC traffic within that Region.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#116",
          "answers": [
            {
              "choice": "<p>A. Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-8 connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross-Region routing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Configure the Direct Connect gateway to route traffic between the transit gateways.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851722,
          "date": "Mon 27 Mar 2023 06:57",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 843478,
          "date": "Sun 19 Mar 2023 07:24",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "This model is constructed of the following:<br>• Multi AWS Regions<br>• Dual Direct Connect connections to independent DX locations<br>• Single on-premises data center with dual connections to AWS<br>• AWS DXGW with AWS Transit Gateway<br>• High scale of VPCs per Region<br><br>https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/aws-dx-dxgw-with-aws-transit-gateway-multi-regions-and-aws-public-peering.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831563,
          "date": "Tue 07 Mar 2023 05:52",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/images/whitepapers/latest/hybrid-connectivity/images/dx-dxgw-transit-gateway-multi-region-public-vif.png<br>B is wrong as it says, two DX Gateways contradictory<br>C is wrong as it says to configure DXG to route traffic. infact Transit gateway peering need to be done between two transit gateways of each reigon.<br>A is wrong because Private VIF is not apt in mentioned config of the question. Public VIF is correct (Transit public VIF)<br>If you are using a single DX Gateway<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Whichever option has this text is correct - \\\"Peer the transit gateways with each other to support cross-Region routing\\\"</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831566,
          "date": "Tue 07 Mar 2023 06:00",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Whichever option has this text is correct - \\\"Peer the transit gateways with each other to support cross-Region routing\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827010,
          "date": "Thu 02 Mar 2023 15:42",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Yeah, a single DX-GW tied to TGW on different regions that further connect to the VPCs on those regions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 817429,
          "date": "Wed 22 Feb 2023 04:27",
          "username": "\t\t\t\tYowie351\t\t\t",
          "content": "Multiple dynamically routed AWS Direct Connect connections are necessary to support high availability.<br>Refer to the second diagram:<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 808944,
          "date": "Wed 15 Feb 2023 00:35",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "D Seems Correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 795867,
          "date": "Thu 02 Feb 2023 08:58",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "D: <br>https://aws.amazon.com/blogs/networking-and-content-delivery/aws-transit-gateway-now-supports-intra-region-peering/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Maybe you meant this one: https://aws.amazon.com/blogs/aws/new-for-aws-transit-gateway-build-global-networks-and-centralize-monitoring-using-network-manager/</li><li>and this for connect two transit gateways with one direct connect gateway :<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 798178,
          "date": "Sat 04 Feb 2023 17:54",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Maybe you meant this one: https://aws.amazon.com/blogs/aws/new-for-aws-transit-gateway-build-global-networks-and-centralize-monitoring-using-network-manager/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795869,
          "date": "Thu 02 Feb 2023 09:01",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "and this for connect two transit gateways with one direct connect gateway :<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793448,
          "date": "Tue 31 Jan 2023 00:33",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "answer is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 787378,
          "date": "Wed 25 Jan 2023 09:00",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html<br>https://docs.aws.amazon.com/directconnect/latest/UserGuide/high_resiliency.html<br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777212,
          "date": "Mon 16 Jan 2023 01:28",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D use DX GW for multi region to on-premise, direct TGW peer for cross regions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776760,
          "date": "Sun 15 Jan 2023 16:52",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/91771-exam-aws-certified-solutions-architect-professional-topic-1/<br><br>The correct answer is D. <br><br>In this solution, two transit VIFs are created - one from the DX-A connection and one from the DX-B connection - into the same Direct Connect gateway for high availability. Both the eu-west-1 and us-east-1 transit gateways are then associated with this Direct Connect gateway. The transit gateways are then peered with each other to support cross-Region routing.<br><br>This solution meets the requirements of the company by creating a highly available connection between the on-premises data center and the VPCs in both the eu-west-1 and us-east-1 regions, and by enabling direct traffic routing between VPCs in those regions.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because a private VIF does not support inter-VPC traffic and cross-Region routing.<br><br>Option B is incorrect because it separates the two Direct Connect connections into separate Direct Connect gateways, which would not provide high availability.<br><br>Option C is incorrect because it does not mention how to peer the transit gateways to support cross-Region routing.</li><li>I think the reason why B is wrong is because there is no need to have 2 Direct Connect Gateways. DX-GW is a global object, separating the regions with 2 DX-GW only creates fragmentation of the routing, which can be good in some cases.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 776761,
          "date": "Sun 15 Jan 2023 16:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because a private VIF does not support inter-VPC traffic and cross-Region routing.<br><br>Option B is incorrect because it separates the two Direct Connect connections into separate Direct Connect gateways, which would not provide high availability.<br><br>Option C is incorrect because it does not mention how to peer the transit gateways to support cross-Region routing.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think the reason why B is wrong is because there is no need to have 2 Direct Connect Gateways. DX-GW is a global object, separating the regions with 2 DX-GW only creates fragmentation of the routing, which can be good in some cases.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 827009,
          "date": "Thu 02 Mar 2023 15:40",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I think the reason why B is wrong is because there is no need to have 2 Direct Connect Gateways. DX-GW is a global object, separating the regions with 2 DX-GW only creates fragmentation of the routing, which can be good in some cases.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#117",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user. The company has a multi-Region AWS CloudTrail trail in the AWS account.<br><br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ADE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#117",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Invoke an AWS Step Functions state machine to remove access.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Use Amazon Pinpoint to notify the security team.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851724,
          "date": "Mon 27 Mar 2023 06:59",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ADE is right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 832258,
          "date": "Tue 07 Mar 2023 21:10",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Event Bus (EventBridge) system to receive event notification (Option A). Step function can get triggered with workflow of doing steps like removing access and sending email etc..(Option D, E)<br><br>EventBridge enables you to create event rules that match events from different sources, such as AWS services, SaaS applications, custom applications, and other AWS accounts. Once an event rule is triggered, EventBridge can route the event to one or more targets, such as AWS Lambda functions, Amazon SNS topics, Amazon SQS queues, or custom HTTP endpoints.<br><br>AWS Step Functions supports several AWS services, such as AWS Lambda, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS). You can use these services to trigger actions and pass data between steps in your state machine.<br><br>Pinpoint is chat system which question did not ask, F is wrong. Not C as<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>this explanation makes sense to me.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 837449,
          "date": "Sun 12 Mar 2023 23:17",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "this explanation makes sense to me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804676,
          "date": "Fri 10 Feb 2023 20:32",
          "username": "\t\t\t\tCloudguy594\t\t\t",
          "content": "ADE Step Functions works.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 798246,
          "date": "Sat 04 Feb 2023 19:20",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I like ACE better. I am not sure Step Functions would work.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>According to ChatGPT, AWS Step Functions can interact with AWS APIs in a few different ways. One example is below.<br><br>Directly invoking AWS APIs using the \\\"Task\\\" state in Step Functions. This state type allows you to run an AWS Lambda function, which can interact with AWS APIs as part of its logic.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACE"
        },
        {
          "id": 805721,
          "date": "Sat 11 Feb 2023 23:32",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT, AWS Step Functions can interact with AWS APIs in a few different ways. One example is below.<br><br>Directly invoking AWS APIs using the \\\"Task\\\" state in Step Functions. This state type allows you to run an AWS Lambda function, which can interact with AWS APIs as part of its logic.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777213,
          "date": "Mon 16 Jan 2023 01:31",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ADE are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776763,
          "date": "Sun 15 Jan 2023 16:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This is the correct answer because it follows these steps:<br><br>- A: The first step is to create an EventBridge rule that listens for the specific API call to create a new IAM user. This will trigger the next step in the process.<br><br>- D: The next step is to use an AWS Step Functions state machine to remove access for the new IAM user. This ensures that access is removed automatically, as required by the security team.<br><br>- E: Finally, use Amazon SNS to notify the security team that a new user has been created and access has been removed. This allows the security team to review and approve the user as necessary.<br><br>Option B is not correct because CloudTrail alone is not able to remove access for the new user.<br><br>Option C is not correct because it is not specified in the question that the company is using Amazon Elastic Container Service and AWS Fargate technology.<br><br>Option F is not correct because the question specifies that the company should use Amazon SNS to notify the security team, not Amazon Pinpoint.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"the question specifies that the company should use Amazon SNS \\\" -&gt; no, it does not specify anything like that. <br>\\\"because it is not specified in the question that the company is using Amazon Elastic Container\\\"-&gt; so?is it specified that they use step function., can't find that either.<br>The question must have changed, it does not match yourexplanations.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ADE"
        },
        {
          "id": 837448,
          "date": "Sun 12 Mar 2023 23:15",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "\\\"the question specifies that the company should use Amazon SNS \\\" -> no, it does not specify anything like that. <br>\\\"because it is not specified in the question that the company is using Amazon Elastic Container\\\"-> so?is it specified that they use step function., can't find that either.<br>The question must have changed, it does not match yourexplanations.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#118",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.<br><br>The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other.<br><br>Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#118",
          "answers": [
            {
              "choice": "<p>A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognoto user pools and Identity pools to manage access to accounts and between accounts.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851726,
          "date": "Mon 27 Mar 2023 07:01",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ACD are the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 808946,
          "date": "Wed 15 Feb 2023 00:39",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "By Elimination Rule",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 777218,
          "date": "Mon 16 Jan 2023 01:37",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ACD are correct.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 776771,
          "date": "Sun 15 Jan 2023 17:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer would be options A, C and D, because they address the requirements outlined in the question.<br>A.  Deploying a landing zone environment using AWS Control Tower and enrolling accounts in an organization in AWS Organizations allows for a centralized management of access to all accounts and applications.<br>C.  Creating transit gateways and transit gateway VPC attachments in each account and configuring appropriate route tables allows for private network traffic, and ensures that the production account and shared network account have connectivity to all accounts, while the development and staging accounts have access only to each other.<br>D.  Setting up and enabling AWS IAM Identity Center (AWS Single Sign-On) and creating appropriate permission sets with required MFA for existing accounts allows for multi-factor authentication at login and specific roles to be assigned to user groups.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options are not correct because:<br>B.  Enabling AWS Security Hub in all accounts to manage cross-account access and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.<br>E.  Enabling AWS Control Tower in all accounts to manage routing between accounts and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.</li><li>F.  Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 776773,
          "date": "Sun 15 Jan 2023 17:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options are not correct because:<br>B.  Enabling AWS Security Hub in all accounts to manage cross-account access and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.<br>E.  Enabling AWS Control Tower in all accounts to manage routing between accounts and collecting findings through AWS CloudTrail to force MFA login is not enough to meet the requirement of creating separate accounts for development, staging, production, and shared network. It can be used in addition to the other steps, but not as a standalone solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>F.  Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776775,
          "date": "Sun 15 Jan 2023 17:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "F.  Creating IAM users and groups and configuring MFA for all users and setting up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts does not address the requirement of creating separate accounts for development, staging, production, and shared network. Additionally, it does not address the requirement of keeping the traffic on a private network.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#119",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size.<br><br>The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key.<br><br>What should a solutions architect do to reduce costs with the LEAST operational effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#119",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or slops instances based on me tag, day, and time.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that starts instances based on the tag.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates, instances based on the lag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule lo invoke another Lambda function that restores the instances from their last backup based on the tag.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag. day, and time.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851728,
          "date": "Mon 27 Mar 2023 07:04",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the easy choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 798946,
          "date": "Sun 05 Feb 2023 16:39",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "this is easy. I wish I'll have several of this in the exam.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777220,
          "date": "Mon 16 Jan 2023 01:41",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct. Stop the instance that preserver all data. <br>C: is incorrect because it terminate instance that will loss data",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 776863,
          "date": "Sun 15 Jan 2023 18:23",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort.<br><br>This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.<br><br>Option A would require the instances to be stopped and started once a day, which could result in instances being stopped while they are in use or not being stopped when they are not in use.<br><br>Option C would terminate instances during non-business hours and restore them again in the morning, which could lead to data loss or longer start up times.<br><br>Option D would terminate or restore instances every hour, which could lead to unnecessary costs as well as data loss or longer start up times.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#120",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account.<br><br>The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked.<br><br>What could be the cause of the error messages for these customers?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#120",
          "answers": [
            {
              "choice": "<p>A. The Lambda function reached its concurrency limit.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The Lambda function its Region limit for concurrency.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The company reached its API Gateway account limit for calls per second.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. The company reached its API Gateway default per-method limit for calls per second.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851729,
          "date": "Mon 27 Mar 2023 07:05",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Company reached its limit",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831658,
          "date": "Tue 07 Mar 2023 09:20",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "API Gateway has a limit of 10k requests per second, per account, per region <br>https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793460,
          "date": "Tue 31 Jan 2023 00:46",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777221,
          "date": "Mon 16 Jan 2023 01:43",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776869,
          "date": "Sun 15 Jan 2023 18:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  The company reached its API Gateway account limit for calls per second. This is because Amazon API Gateway has a default account-level limit of 10,000 requests per second (RPS) and a default per-method limit of 5,000 RPS. If the company's premium tier customers are making more than 10,000 requests per second in total across all API methods and regions, they would be receiving the error message of 429 Too Many Requests. This indicates that the API Gateway account is reaching its capacity limit, and the Lambda function is not being invoked because API Gateway is blocking the requests before they reach the Lambda function. <br><br>The other choices are not correct because the Lambda function's concurrency limit and region limit for concurrency would not affect the API Gateway's request rate limit, and the API Gateway's default per-method limit is 5,000 RPS which is less than the premium tier's 3,000 calls per second.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because the error message is not related to the Lambda function reaching its concurrency limit.<br><br>Option B is incorrect because the error message is not related to the Lambda function reaching its region limit for concurrency.<br><br>Option D is incorrect because the error message is not related to the company reaching its API Gateway default per-method limit for calls per second, but it's related to the account level limit.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 776871,
          "date": "Sun 15 Jan 2023 18:28",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because the error message is not related to the Lambda function reaching its concurrency limit.<br><br>Option B is incorrect because the error message is not related to the Lambda function reaching its region limit for concurrency.<br><br>Option D is incorrect because the error message is not related to the company reaching its API Gateway default per-method limit for calls per second, but it's related to the account level limit.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#121",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology.<br><br>The company plans to deploy the application migration to AWS on Amazon EC2 instances. The EC2 instances will run in an Auto Scaling group in a dedicated VPC.  The company needs to use the security tool to inspect all packets that come in and out of the VPC.  This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#121",
          "answers": [
            {
              "choice": "<p>A. Deploy the security tool on EC2 instances m a new Auto Scaling group in the existing VPC<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the web application behind a Network Load Balancer<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy an Application Load Balancer in front of the security tool instances<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Provision a transit gateway to facilitate communication between VPCs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851734,
          "date": "Mon 27 Mar 2023 07:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AD for me. DE would be more relevant if there were multiple VPCs or hybrid architectures involved.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 851137,
          "date": "Sun 26 Mar 2023 16:22",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "I don't know why you chose the D. <br>GWLB is using GENEVE, nowhere it says that the sec tool is compatible with this protocol.<br>And most of all, the tool as no cloud solutions (we can suppose that it does not support GENEVE).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's a tricky question, but my reflexion is :<br>We need to build an HA solution. They told us that web application will be in ASG but not in multi AZ, so I assume that we need to put an ELB if front of the web app to be HA, so I choose B. <br>Then, where to put the sec tool ? In EC2, regarding the answer we have. So I choose A. <br><br>So : AB</li><li>https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/gateway-load-balancers.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851150,
          "date": "Sun 26 Mar 2023 16:31",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "It's a tricky question, but my reflexion is :<br>We need to build an HA solution. They told us that web application will be in ASG but not in multi AZ, so I assume that we need to put an ELB if front of the web app to be HA, so I choose B. <br>Then, where to put the sec tool ? In EC2, regarding the answer we have. So I choose A. <br><br>So : AB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851139,
          "date": "Sun 26 Mar 2023 16:22",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/gateway-load-balancers.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 842474,
          "date": "Sat 18 Mar 2023 06:05",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "A.  Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPCD.  Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 839374,
          "date": "Tue 14 Mar 2023 23:41",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "D E for me. https://catalog.workshops.aws/networking/en-US/gwlb you could find this solution<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Change to A D , because we need to deploy this solution first. But from architecture perspective will be better to deploy this security solution in the separate vpc and peer this vpc with required vpcs through transit gateway. And I have question could we run any on prem solution on ec2 instances?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 840063,
          "date": "Wed 15 Mar 2023 17:23",
          "username": "\t\t\t\tDimidrol\t\t\t",
          "content": "Change to A D , because we need to deploy this solution first. But from architecture perspective will be better to deploy this security solution in the separate vpc and peer this vpc with required vpcs through transit gateway. And I have question could we run any on prem solution on ec2 instances?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837426,
          "date": "Sun 12 Mar 2023 22:43",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "We need two things: <br>a) something to intercept any traffic in an out of a vpc. <br>b) redirect traffic to asecurity tool.<br>E) the transit gateway is needed to redirect traffic from one vpc to the dedicated vpc.<br>D) the gateway loadbalancer is needed to intercept traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>quote: \\\"the tool has no cloud solutions available from its vendor. \\\"<br>In my book this means you cannot run it in the cloud. That eliminates A. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 837430,
          "date": "Sun 12 Mar 2023 22:45",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "quote: \\\"the tool has no cloud solutions available from its vendor. \\\"<br>In my book this means you cannot run it in the cloud. That eliminates A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832524,
          "date": "Wed 08 Mar 2023 05:25",
          "username": "\t\t\t\tandras\t\t\t",
          "content": "the question already said: The EC2 instances will run in an Auto Scaling group in a dedicated VPC.  I think there is no need to \\\"repeat\\\" that in the \\\"answer.<br>It's DE",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 832458,
          "date": "Wed 08 Mar 2023 03:14",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "On premises should ring network load balancers which the Gateway load balancer is.<br>Have the security tool install on ec2 and direct traffic to it so it can sniff in packets of traffic received as the existing solution. A, D<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You cannot really sniff in the aws virtual network. <br>\\\"On pemises\\\" contradicts \\\"Have the security tool install on ec2\\\".</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 837432,
          "date": "Sun 12 Mar 2023 22:47",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "You cannot really sniff in the aws virtual network. <br>\\\"On pemises\\\" contradicts \\\"Have the security tool install on ec2\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807899,
          "date": "Tue 14 Feb 2023 00:03",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "A and D are the closest choice - GLB and Sec Tool on EC2 ASG",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 793462,
          "date": "Tue 31 Jan 2023 00:49",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "answer A and D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 777769,
          "date": "Mon 16 Jan 2023 15:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Yes, options A and D are a good combination of steps for the solutions architect to take to meet the requirements.<br><br>Option A, Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, allows the company to use its existing security tool while still running it within the AWS environment. This ensures that all packets coming in and out of the VPC are inspected by the security tool in real time.<br><br>Option D, Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows for high availability within an AWS Region. By provisioning a Gateway Load Balancer for each Availability Zone, the traffic is redirected to the security tool in the event of any failures or outages. This ensures that the security tool is always available to inspect the traffic, even in the event of a failure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B and C are not correct because the security tool is not compatible with the AWS load balancer services and deploying it behind or in front of the load balancer would not provide the desired functionality.<br>Option E is not correct because it is not required for this scenario and does not relate to the requirement of integrating the security tool with the VPC. <br><br>I am not aware of any specific information that the security tool is not compatible with the AWS load balancer. In this scenario, it is stated that the company has used the security tool for the last 15 years and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology. So it is unclear if the security tool is compatible with the AWS load balancer or not.</li><li>Option A and D seem like a good solution because it allows the company to deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, which allows the company to continue using its existing security tool while also taking advantage of the scalability and high availability of the Auto Scaling group. Additionally, Provisioning a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows the company to ensure that the traffic is being inspected by the security tool in real time, and that the inspection does not affect the performance of the application.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 777772,
          "date": "Mon 16 Jan 2023 15:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B and C are not correct because the security tool is not compatible with the AWS load balancer services and deploying it behind or in front of the load balancer would not provide the desired functionality.<br>Option E is not correct because it is not required for this scenario and does not relate to the requirement of integrating the security tool with the VPC. <br><br>I am not aware of any specific information that the security tool is not compatible with the AWS load balancer. In this scenario, it is stated that the company has used the security tool for the last 15 years and the tool has no cloud solutions available from its vendor. The company's security team is concerned about how to integrate the security tool with AWS technology. So it is unclear if the security tool is compatible with the AWS load balancer or not.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A and D seem like a good solution because it allows the company to deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, which allows the company to continue using its existing security tool while also taking advantage of the scalability and high availability of the Auto Scaling group. Additionally, Provisioning a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows the company to ensure that the traffic is being inspected by the security tool in real time, and that the inspection does not affect the performance of the application.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777774,
          "date": "Mon 16 Jan 2023 15:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A and D seem like a good solution because it allows the company to deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, which allows the company to continue using its existing security tool while also taking advantage of the scalability and high availability of the Auto Scaling group. Additionally, Provisioning a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows the company to ensure that the traffic is being inspected by the security tool in real time, and that the inspection does not affect the performance of the application.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777670,
          "date": "Mon 16 Jan 2023 14:12",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AE: classical case for third party appliance deployed with gateway load balancer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ignore, should be AD</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777966,
          "date": "Mon 16 Jan 2023 18:06",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ignore, should be AD",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#122",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis.<br><br>The company needs to design a new data analysis solution that can deliver faster and optimize costs.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#122",
          "answers": [
            {
              "choice": "<p>A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon. S3 Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshlft for analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851735,
          "date": "Mon 27 Mar 2023 07:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Connect the IoT sensors to AWS IoT Core.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832490,
          "date": "Wed 08 Mar 2023 04:09",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "IOT Core communication supports protocols MQTT, HTTPS, MQTT over WSS, and LoRaWAN (but not FTP/SFTP ) so C should be wrong.<br><br>Rules Engine: AWS IoT Core provides a rules engine that allows users to define and execute business logic on the data generated by their IoT devices. This enables users to automate actions such as sending notifications, triggering alarms, or updating device settings based on real-time data.<br><br>Integration with other AWS Services: AWS IoT Core integrates with other AWS services such as AWS Lambda, AWS Kinesis, and AWS S3, allowing users to easily process and store their IoT data, as well as build complex IoT applications using a range of AWS services.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808953,
          "date": "Wed 15 Feb 2023 00:50",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "A by Elimination rule",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 798983,
          "date": "Sun 05 Feb 2023 17:28",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I m not convinced about A.  It kind of requires changes in the sensors to be compatible with AWS IoT Core.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792153,
          "date": "Mon 30 Jan 2023 00:17",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "i'll go for A",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777779,
          "date": "Mon 16 Jan 2023 15:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis.<br><br>This solution meets the requirement of faster analysis and cost optimization by using AWS IoT Core to collect data from the IoT sensors in real-time and then using AWS Glue and Amazon Athena for efficient data analysis.<br><br>Option B and D do not optimize the cost of data analysis as they involve use of expensive services like AWS Fargate and Snowball Edge respectively. Option C does not make use of real-time data collection and may not be optimal for faster analysis.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777676,
          "date": "Mon 16 Jan 2023 14:16",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct.<br>B: it is appliance, impossible to install on Fargate<br>C: device not use FTP protocol<br>D: snowball is not real time<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>In B, we don't try to port appliances to Fargate, but only the app that parses the informtion from the appliances into JSON.<br>I am doubting about A.  Unless you would reprogrm the sensors they would not know how to connect to AWS IoT Core.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 798982,
          "date": "Sun 05 Feb 2023 17:27",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "In B, we don't try to port appliances to Fargate, but only the app that parses the informtion from the appliances into JSON.<br>I am doubting about A.  Unless you would reprogrm the sensors they would not know how to connect to AWS IoT Core.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#123",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account.<br><br>The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center.<br><br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BDF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#123",
          "answers": [
            {
              "choice": "<p>A. Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Provision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Share the transit gateway with other accounts. Attach VPCs to the transit gateway.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Provision VPC peering as necessary.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851737,
          "date": "Mon 27 Mar 2023 07:21",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "BDF is the right combo",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 832508,
          "date": "Wed 08 Mar 2023 04:56",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "VPC Peering does not work as there are hundreds of VPCs, transit gateway is easy to configure and practical.<br>https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 793468,
          "date": "Tue 31 Jan 2023 00:54",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B D and F",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 792155,
          "date": "Mon 30 Jan 2023 00:23",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "I agree with BD&F",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 777783,
          "date": "Mon 16 Jan 2023 15:34",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B and D and F are correct.<br>B: Creating a Direct Connect gateway and a transit gateway in the central network account will allow the company to connect its on-premises data center to the resources in AWS.<br>D: Sharing the transit gateway with other accounts will allow the company to communicate with all the VPCs in multiple accounts.<br>F: Provisioning only private subnets and opening necessary routes on the transit gateway and customer gateway will allow the company to route its cloud resources to the internet through its on-premises data center.<br><br>A is incorrect because it would be redundant to use both a Direct Connect gateway and a transit gateway.<br>C is incorrect because it is not necessary to provision an internet gateway, since the company wants to route traffic through their on-premises data center.<br>E is incorrect because VPC peering may not be necessary if the company is using a transit gateway to connect all the VPCs.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BDF"
        },
        {
          "id": 777680,
          "date": "Mon 16 Jan 2023 14:18",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "BDF are correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#124",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously.<br><br>A solutions architect needs to enforce the new process in the most secure way possible.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#124",
          "answers": [
            {
              "choice": "<p>A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851743,
          "date": "Mon 27 Mar 2023 07:24",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AD easy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 792157,
          "date": "Mon 30 Jan 2023 00:26",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and D",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 777799,
          "date": "Mon 16 Jan 2023 15:51",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A and D are the correct answer.<br>A: By ensuring all AWS accounts are part of an organization in AWS Organizations, it allows for centralized management and control of the accounts. This can help enforce the new purchasing process by giving a dedicated team the ability to manage and enforce policies across all accounts.<br>D: By creating an SCP (Service Control Policy) that denies access to the ec2:PurchaseReservedInstancesOffering and ec2:ModifyReservedInstances actions, it enforces the new centralized purchasing process. Attaching the SCP to each OU (organizational unit) within the organization ensures that all business units are adhering to the new process.<br><br>B and C are not the correct answer, because AWS Config and IAM policies are used for monitoring and managing access to resources in an account, respectively. They don't enforce the new process for purchasing reserved instances.<br>E is not the correct answer as this is not related to the new process for purchasing reserved instances.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 777683,
          "date": "Mon 16 Jan 2023 14:19",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AD are correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#125",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode.<br><br>A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CDE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#125",
          "answers": [
            {
              "choice": "<p>A. Use Amazon ElastiCache for Memcached in front of the database<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon ElastiCache for Redis in front of the database<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use RDS Proxy in front of the database.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the database to Amazon Aurora MySQL.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an Amazon Aurora Replica.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create an RDS for MySQL read replica<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851744,
          "date": "Mon 27 Mar 2023 07:25",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "CDE is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 845441,
          "date": "Tue 21 Mar 2023 02:59",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "CDE.  I would have said F, but the question asks for a combination of steps, so its looking for the Aurora replica and not the MySQL RDS replica",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 833428,
          "date": "Wed 08 Mar 2023 23:57",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "C for sure as connection pooling helps quick re connect. There is no preference for A or B cache solution based on the question. So, A,B are eliminated. so three correct options should be in others. If you choose Aurora only, three answers will be met :-) C,D,E",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 793473,
          "date": "Tue 31 Jan 2023 00:57",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C D andE<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A and B are incorrect options because Amazon ElastiCache is a caching service, not a failover solution. F is also incorrect because RDS read replicas are asynchronous, which means that there may be a delay in replication, leading to the potential loss of data. Additionally, creating a read replica does not improve the failover time.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 811526,
          "date": "Fri 17 Feb 2023 07:01",
          "username": "\t\t\t\tnyxs_19\t\t\t",
          "content": "A and B are incorrect options because Amazon ElastiCache is a caching service, not a failover solution. F is also incorrect because RDS read replicas are asynchronous, which means that there may be a delay in replication, leading to the potential loss of data. Additionally, creating a read replica does not improve the failover time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781884,
          "date": "Fri 20 Jan 2023 05:36",
          "username": "\t\t\t\tAjayD123\t\t\t",
          "content": "RDS read replica auto failover takes approx 35 seconds hence, BCF does not satisfy under 20 seconds failover requirement.<br>https://aws.amazon.com/rds/features/multi-az/#:~:text=Amazon%20RDS%20Multi%2DAZ%20with%20two%20readable%20standbys,-Automatically%20fail%20over&text=Automatically%20failover%20in%20typically%20under,and%20with%20no%20manual%20intervention.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>thanks for the information about RDS read replica</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 792164,
          "date": "Mon 30 Jan 2023 00:34",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "thanks for the information about RDS read replica",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779124,
          "date": "Tue 17 Jan 2023 18:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D, E and C:<br><br>Migrate the database to Amazon Aurora MySQL.<br>- Create an Amazon Aurora Replica.<br>- Use RDS Proxy in front of the database.<br>- These options are correct because they address the requirement of reducing the failover time to less than 20 seconds.<br><br>Migrating to Amazon Aurora MySQL and creating an Aurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerant storage system that can automatically detect and repair failures. Additionally, Aurora has a feature called \\\"Aurora Global Database\\\" which allows you to create read-only replicas across multiple AWS regions which can further help to reduce the failover time.<br><br>Creating an Aurora replica can also help to reduce the failover time as it can take over as the primary DB instance in case of a failure.<br><br>Using RDS proxy can also help to reduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance the load across multiple DB instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A and B, Use Amazon ElastiCache for Memcached and Redis in front of the database, are not correct as ElastiCache is a caching service, it doesn't provide a high availability solution for the underlying database.<br><br>Option F, Create an RDS for MySQL read replica, is not correct as a read replica can only be used to offload read traffic from the primary instance, it doesn't provide a high availability solution for the underlying database.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CDE"
        },
        {
          "id": 779125,
          "date": "Tue 17 Jan 2023 18:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A and B, Use Amazon ElastiCache for Memcached and Redis in front of the database, are not correct as ElastiCache is a caching service, it doesn't provide a high availability solution for the underlying database.<br><br>Option F, Create an RDS for MySQL read replica, is not correct as a read replica can only be used to offload read traffic from the primary instance, it doesn't provide a high availability solution for the underlying database.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777809,
          "date": "Mon 16 Jan 2023 15:56",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B, C and F. <br><br>Using Amazon ElastiCache for Redis in front of the database (Option B) will help to reduce the failover time by caching the frequently-used data, so that it can be quickly served from the cache rather than having to be retrieved from the database during a failover.<br><br>Using RDS Proxy in front of the database (Option C) will help to reduce the failover time by managing the connections to the RDS DB instance, so that it can quickly route traffic to the new primary instance during a failover.<br><br>Creating an RDS for MySQL read replica (Option F) will help to reduce the failover time by having a read-only copy of the database running in parallel with the primary instance, so that it can take over as the primary instance in the event of a failover.<br><br>Option A and D are not relevant in this case as the question is asking specifically about reducing failover time for an RDS for MySQL database.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C,D and E Correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BCF"
        },
        {
          "id": 807902,
          "date": "Tue 14 Feb 2023 00:11",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C,D and E Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777696,
          "date": "Mon 16 Jan 2023 14:26",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "CDE are correct",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#126",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account.<br><br>What is the MOST secure way to allow org1 to access resources in org2?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#126",
          "answers": [
            {
              "choice": "<p>A. The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN) when requesting access to perform the required tasks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN), including the external ID in the IAM role's trust policy, when requesting access to perform the required tasks.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851748,
          "date": "Mon 27 Mar 2023 07:27",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "With the external ID. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 833437,
          "date": "Thu 09 Mar 2023 00:15",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": {<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Principal\\\": {<br>\\\"AWS\\\": \\\"Example Corp's AWS Account ID\\\"<br>},<br>\\\"Action\\\": \\\"sts:AssumeRole\\\",<br>\\\"Condition\\\": {<br>\\\"StringEquals\\\": {<br>\\\"sts:ExternalId\\\": \\\"1122334455-The ID that only Third party and customer knows\\\"<br>}<br>}<br>}<br>}",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799037,
          "date": "Sun 05 Feb 2023 18:46",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Easy. The external ID is for sure the winner.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 792166,
          "date": "Mon 30 Jan 2023 00:39",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "D seems the correct answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 788442,
          "date": "Thu 26 Jan 2023 07:36",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777815,
          "date": "Mon 16 Jan 2023 15:58",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  This is the most secure way to allow org1 to access resources in org2 because it allows for least privilege security access. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN) and include the external ID in the IAM role's trust policy when requesting access to perform the required tasks. This ensures that the partner company can only access the resources that it needs and only from the specific customer account.<br><br>Option A and B both involve providing the partner company with credentials, which can be easily compromised and could lead to a security breach. Option C also provides the partner company with an IAM role, but it doesn't have any restrictions on when and where the partner company can access the resources in customer account, it could be a security risk.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777705,
          "date": "Mon 16 Jan 2023 14:30",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#127",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map.<br><br>The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.<br><br>The company needs the ability to allocate resources cost-effectively based on the number of running containers.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#127",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851749,
          "date": "Mon 27 Mar 2023 07:30",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D is easier",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842477,
          "date": "Sat 18 Mar 2023 06:15",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "I vote for D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 834417,
          "date": "Thu 09 Mar 2023 22:32",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "i still think is B<br>\\\"each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 833858,
          "date": "Thu 09 Mar 2023 11:48",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "choosing D based on below tagging information<br>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 833551,
          "date": "Thu 09 Mar 2023 04:06",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "EKS with Fargate is a more complex platform than ECS with Fargate. Kubernetes has a steeper learning curve than ECS, and requires more expertise to manage. ECS with Fargate is designed to be simple and easy to use, making it a good choice for organizations that want to quickly deploy containerized applications without having to manage the complexity of Kubernetes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 823941,
          "date": "Mon 27 Feb 2023 17:22",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 822263,
          "date": "Sun 26 Feb 2023 11:47",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Option B and D are both valid solutions to meet the requirements, but option B is the better choice for the following reasons:<br><br>It uses AWS Fargate, which is a serverless compute engine for containers, eliminating the need for managing EC2 instances.<br>It allows for more efficient resource allocation as Fargate automatically scales the containers based on demand, reducing operational overhead.<br>It allows for tagging resources directly in the EKS cluster, simplifying management and reducing the potential for errors.<br>Therefore, option B is the best solution for the delivery company's requirements.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 815209,
          "date": "Mon 20 Feb 2023 13:29",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "tag-resource is the right option. I don't think --tags is a valid option for that<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Now I am not sure. I believe it's D</li><li>You tag the Task, not the pods. That's the difference between B and D. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 815227,
          "date": "Mon 20 Feb 2023 13:44",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Now I am not sure. I believe it's D<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You tag the Task, not the pods. That's the difference between B and D. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820679,
          "date": "Fri 24 Feb 2023 16:47",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "You tag the Task, not the pods. That's the difference between B and D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792173,
          "date": "Mon 30 Jan 2023 00:46",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "ECS and Fragate cost effective",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777826,
          "date": "Mon 16 Jan 2023 16:03",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D.  Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task.<br><br>AWS Fargate is a serverless compute engine for containers that eliminates the need to provision and manage servers, which reduces operational overhead. Additionally, Fargate automatically scales resources based on the number of running containers, providing cost-effective resource allocation. Using the AWS CLI run-task command and setting enableECSManagedTags to true allows for easy tagging of resources for organization and cost tracking.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options, A and C, are using Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon Elastic Container Service (Amazon ECS) on Amazon EC2, which require provisioning and management of servers and may not provide the same cost-effective resource allocation as Fargate. Option B is using EKS on Fargate but it's not recommended because EKS is intended for more complex and advanced use cases, whereas ECS is a more simple service for running Docker container.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777828,
          "date": "Mon 16 Jan 2023 16:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options, A and C, are using Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon Elastic Container Service (Amazon ECS) on Amazon EC2, which require provisioning and management of servers and may not provide the same cost-effective resource allocation as Fargate. Option B is using EKS on Fargate but it's not recommended because EKS is intended for more complex and advanced use cases, whereas ECS is a more simple service for running Docker container.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777711,
          "date": "Mon 16 Jan 2023 14:36",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct<br>https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#128",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure.<br><br>Which factors could cause this error? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#128",
          "answers": [
            {
              "choice": "<p>A. The IPv4 CIDR ranges of the two VPCs overlap<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. The VPCs are not in the same Region<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. One or both accounts do not have access to an Internet gateway<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. One of the VPCs was not shared through AWS Resource Access Manager<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. The IAM role in the peer accepter account does not have the correct permissions<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851753,
          "date": "Mon 27 Mar 2023 07:34",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AE is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 833570,
          "date": "Thu 09 Mar 2023 04:35",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "FYI, Other reasons for issue : <br>If the IAM role in the accepter account doesn't have the right permissions<br><br>If the PeerRoleArn property isn't passed correctly when you create a VPC peering connection between VPCs in different accounts<br><br>If the PeerRegion property isn't passed correctly when you're creating a VPC peering connection between VPCs in different AWS Regions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 801006,
          "date": "Tue 07 Feb 2023 15:38",
          "username": "\t\t\t\tAppon\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-vpc-peering-error/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 792176,
          "date": "Mon 30 Jan 2023 00:48",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 777878,
          "date": "Mon 16 Jan 2023 16:40",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A is correct because the IPv4 CIDR ranges of the two VPCs overlap. The two VPCs have an IP range of 10.10.0.0/16 and 10.10.10.0/24, which means that they share the same 10.10.0.0 network. This causes a conflict in routing and will prevent the VPCs from being able to communicate with each other.<br><br>E is correct because the IAM role in the peer accepter account does not have the correct permissions. The role must have permissions to create, modify, and delete VPC peering connections in order for the peering to be established.<br><br>B, C, and D are not correct. The VPCs are in the same region, both accounts have access to an internet gateway and both VPCs are not shared through AWS Resource Access Manager.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 777713,
          "date": "Mon 16 Jan 2023 14:37",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AE is correct<br>D is not correct because you cannot share VPC via RAM, subnet can",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#129",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An external audit of a company's serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task.<br><br>A solutions architect must determine which permissions each Lambda function needs.<br><br>What should the solutions architect do to meet this requirement with the LEAST amount of effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#129",
          "answers": [
            {
              "choice": "<p>A. Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company's business requirements.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company's business requirements.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851758,
          "date": "Mon 27 Mar 2023 07:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B - Identity and Access Management Access Analyzer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832543,
          "date": "Wed 08 Mar 2023 06:02",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Access Analyzer uses automated reasoning to analyze resource policies and detect issues such as overly permissive access or violations of organizational security policies. It works by examining the policies attached to AWS resources, such as S3 buckets, IAM roles, and KMS keys, and identifying any potential security risks or policy violations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>fyi<br>ML tool - CodeGuru has two main components: CodeGuru Reviewer and CodeGuru Profiler.<br><br>CodeGuru Reviewer is a code review service that uses machine learning to identify code quality issues and security vulnerabilities in your application's source code. It analyzes the code and provides recommendations for improvements based on best practices, industry standards, and AWS experience.<br><br>CodeGuru Profiler is a profiling tool that uses machine learning to identify performance issues in your application code at runtime. It continuously analyzes the performance characteristics of your application code and provides recommendations for optimization.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832545,
          "date": "Wed 08 Mar 2023 06:03",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "fyi<br>ML tool - CodeGuru has two main components: CodeGuru Reviewer and CodeGuru Profiler.<br><br>CodeGuru Reviewer is a code review service that uses machine learning to identify code quality issues and security vulnerabilities in your application's source code. It analyzes the code and provides recommendations for improvements based on best practices, industry standards, and AWS experience.<br><br>CodeGuru Profiler is a profiling tool that uses machine learning to identify performance issues in your application code at runtime. It continuously analyzes the performance characteristics of your application code and provides recommendations for optimization.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792178,
          "date": "Mon 30 Jan 2023 00:51",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "Identity and Access Management Access Analyzer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777892,
          "date": "Mon 16 Jan 2023 16:48",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  Turn on AWS CloudTrail logging for the AWS account, and use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements.<br><br>This is the least amount of effort as it makes use of AWS services that can automatically analyze the CloudTrail logs, generate the IAM policies, and provide a report for the review process. <br>Option A and D both involve additional steps such as running scripts or using Amazon EMR, which would take more effort to set up and maintain. <br>Option C is similar to option A and D but doesn't use any AWS services to help with the process.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777714,
          "date": "Mon 16 Jan 2023 14:38",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#130",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect must analyze a company's Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern.<br><br>The solutions architect must analyze the environment and take action based on the findings.<br><br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#130",
          "answers": [
            {
              "choice": "<p>A. Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851761,
          "date": "Mon 27 Mar 2023 07:42",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C - cost optimizer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>*Compute</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851762,
          "date": "Mon 27 Mar 2023 07:43",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "*Compute",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832551,
          "date": "Wed 08 Mar 2023 06:15",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "AWS Compute Optimize helps analyze the usage patterns of AWS resources, such as EC2 instances and Auto Scaling groups, and makes recommendations on how to optimize them for performance and cost using machine learning algorithms.It then generates recommendations that can be used to adjust instance types, purchase options, and other parameters.It provides two types of recommendations:<br>Recommended instance types - recommends instance types that are more cost-effective and better suited to the workload requirements.<br>Recommended purchase options - recommends purchasing options, such as Reserved Instances or Savings Plans, that can help customers save money on their compute resources.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>fyi Pricing looks cheap too - https://aws.amazon.com/compute-optimizer/pricing/</li><li>A is wrong. <br>OpsCenter, a capability of AWS Systems Manager, provides a central location where operations engineers and IT professionals can manage operational work items (OpsItems) related to AWS resources. An OpsItem is any operational issue or interruption that needs investigation and remediation. Using OpsCenter, you can view contextual investigation data about each OpsItem, including related OpsItems and related resources. You can also run Systems Manager Automation runbooks to resolve OpsItems.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832556,
          "date": "Wed 08 Mar 2023 06:22",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "fyi Pricing looks cheap too - https://aws.amazon.com/compute-optimizer/pricing/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832565,
          "date": "Wed 08 Mar 2023 06:33",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A is wrong. <br>OpsCenter, a capability of AWS Systems Manager, provides a central location where operations engineers and IT professionals can manage operational work items (OpsItems) related to AWS resources. An OpsItem is any operational issue or interruption that needs investigation and remediation. Using OpsCenter, you can view contextual investigation data about each OpsItem, including related OpsItems and related resources. You can also run Systems Manager Automation runbooks to resolve OpsItems.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829002,
          "date": "Sat 04 Mar 2023 14:46",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C is correct - Optimzer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822280,
          "date": "Sun 26 Feb 2023 12:03",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Option C may be a good solution to rightsize the EC2 instances but may incur additional cost for installing the Amazon CloudWatch agent on each of the EC2 instances.<br><br>The MOST cost-effective solution to analyze the company's Amazon EC2 instances and Amazon EBS volumes is to create a dashboard using AWS Systems Manager OpsCenter. The OpsCenter dashboard can be configured to visualize the Amazon CloudWatch metrics associated with the EC2 instances and their EBS volumes. By reviewing the dashboard periodically, usage patterns can be identified, and EC2 instances can be right-sized based on the peaks in the metrics.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Bro, install cost is 0. Simple linux command &gt; sudo yum install amazon-cloudwatch-agent</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832561,
          "date": "Wed 08 Mar 2023 06:29",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Bro, install cost is 0. Simple linux command > sudo yum install amazon-cloudwatch-agent",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778000,
          "date": "Mon 16 Jan 2023 18:27",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Installing the Amazon CloudWatch agent on each of the EC2 instances and turning on AWS Compute Optimizer allows the solutions architect to analyze the environment and make recommendations on the sizing of the EC2 instances in a cost-effective way. AWS Compute Optimizer analyzes the utilization of the instances and recommends the optimal instance types for the workloads. This solution is more cost-effective than creating a dashboard and reviewing it periodically, or signing up for the AWS Enterprise Support plan and waiting for Trusted Advisor recommendations.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777715,
          "date": "Mon 16 Jan 2023 14:40",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct, with computer optimizer",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#131",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts.<br><br>In an AWS application account, the company's application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed m the application account.<br><br>The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#131",
          "answers": [
            {
              "choice": "<p>A. Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851765,
          "date": "Mon 27 Mar 2023 07:45",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 846112,
          "date": "Tue 21 Mar 2023 16:19",
          "username": "\t\t\t\tDWsk\t\t\t",
          "content": "Has to be B because C is not possible.<br>I get that you can't share access to the default KMS key, but how does it work to share access through a cross account role? How does the role in the DBA account decrypt the secrets that are encrypted by the default key if the role doesn't have permissions to that key?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 834301,
          "date": "Thu 09 Mar 2023 20:11",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "cross account assume role",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832654,
          "date": "Wed 08 Mar 2023 08:55",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "Cross account assumerole is needed. You can't directly grant access to the secret from the DBA account to the application account because the key policy for the default KMS key is not modifiable.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832589,
          "date": "Wed 08 Mar 2023 07:18",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2020/09/17/PatternSecretsManager2.png<br>App account has the RDS and Secrets manager. So, first, app team should allow to share the secret with DBA account thru \\\"DBA-Secret\\\" IAM role. and DBA (thru DBA-Admin role) should assume that role to access secret. This is common design pattern. So option which has DBA-Secret IAM role is the answer which is B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>* I meant option which says DBA-Secret role in app account (owner account) is the answer</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832590,
          "date": "Wed 08 Mar 2023 07:20",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "* I meant option which says DBA-Secret role in app account (owner account) is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822299,
          "date": "Sun 26 Feb 2023 12:20",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Option B is the correct answer because it creates an IAM role named DBA-Secret in the application account and grants the required permissions to access the secrets. In the DBA account, it creates an IAM role named DBA-Admin, grants the required permissions to assume the DBA-Secret role in the application account, and attaches the DBA-Admin role to the EC2 instance for access to the cross-account secrets. This eliminates the need to manually share the secrets and provides access to the database administrators to the database.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819414,
          "date": "Thu 23 Feb 2023 17:47",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "Follow below link. It has both option to be used for this scenarios. But default kms key can not be used so B<br>https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 815988,
          "date": "Mon 20 Feb 2023 23:56",
          "username": "\t\t\t\tryansmithnz\t\t\t",
          "content": "https://aws.amazon.com/blogs/database/design-patterns-to-access-cross-account-secrets-stored-in-aws-secrets-manager/ the diagram here is pretty much exactly the scenario described in this question. B for the win",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809784,
          "date": "Wed 15 Feb 2023 18:16",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Option B is the correct solution to meet the requirements.<br><br>In this solution, an IAM role named DBA-Secret is created in the application account, and the required permissions to access the secrets are granted to this role. In the DBA account, an IAM role named DBA-Admin is created, and the required permissions to assume the DBA-Secret role in the application account are granted to this role. The DBA-Admin role is then attached to the EC2 instance to access the cross-account secrets.<br><br>This solution follows the principle of least privilege, where the IAM roles have only the necessary permissions to access the secrets. Also, it eliminates the need for manual sharing of secrets and provides a secure way to access the secrets by leveraging cross-account IAM roles.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 807904,
          "date": "Tue 14 Feb 2023 00:25",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "It can not be B - How one role assume to other role ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why not? It's called role chaining and been available since cross-accounts IAM permissions. Used it numerous times. The userY&gt;Acct1:RoleA&gt;Acct2:RoleB.  Acct2:RoleB permissions is only valid for 1hr on CLI/API.</li><li>Thanks, it should be B then</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 811003,
          "date": "Thu 16 Feb 2023 19:34",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "Why not? It's called role chaining and been available since cross-accounts IAM permissions. Used it numerous times. The userY>Acct1:RoleA>Acct2:RoleB.  Acct2:RoleB permissions is only valid for 1hr on CLI/API.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks, it should be B then</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 814427,
          "date": "Sun 19 Feb 2023 20:26",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Thanks, it should be B then",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 800562,
          "date": "Tue 07 Feb 2023 06:17",
          "username": "\t\t\t\tSignup_Nickname\t\t\t",
          "content": "You must use the full AWS KMS key ARN to access a secret from another AWS account.<br>https://aws.amazon.com/premiumsupport/knowledge-center/secrets-manager-share-between-accounts/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Not C.  In the link you shared, there is this big note.<br><br>Note: You can't use the AWS KMS default key for the account. The AWS KMS default key is created, managed, and used on your behalf by an AWS service that runs on AWS Key Management Service. The AWS KMS default key is unique to your AWS account and Region. Only the service that created the AWS managed key can use it.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805751,
          "date": "Sun 12 Feb 2023 00:38",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Not C.  In the link you shared, there is this big note.<br><br>Note: You can't use the AWS KMS default key for the account. The AWS KMS default key is created, managed, and used on your behalf by an AWS service that runs on AWS Key Management Service. The AWS KMS default key is unique to your AWS account and Region. Only the service that created the AWS managed key can use it.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 799086,
          "date": "Sun 05 Feb 2023 19:34",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "B makes complete sense. Other options have issues.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795435,
          "date": "Wed 01 Feb 2023 20:10",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "I voted for B, because: <br>SCPs are a type of policy that is used to set fine-grained permissions at the root level of an AWS organization. Using SCPs in this scenario could result in overly permissive access, which may not meet the organization's security and compliance requirements.<br>Additionally, using SCPs to manage access to the secrets could make it more difficult to track who has access to the secrets, as SCPs are applied at the organization level and may not be tied to specific IAM roles or users.<br>Option B provides a more secure and controlled solution, as it uses cross-account role assumption and IAM roles to manage access to the secrets, which is a more secure and controlled way to manage access to AWS resources in a multi-account environment.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792195,
          "date": "Mon 30 Jan 2023 01:10",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "SCP will provide more granular permissions for DBA<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>SCP do not provide permissions. It just removes permissions.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799085,
          "date": "Sun 05 Feb 2023 19:33",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "SCP do not provide permissions. It just removes permissions.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791642,
          "date": "Sun 29 Jan 2023 13:56",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "A is incorrect.Because aRAM cannot share secrets manager resouce.<br>https://docs.aws.amazon.com/ja_jp/ram/latest/userguide/shareable.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Good catch</li><li>RAM can share secret manager ressours bu as masetromainsaid \\\"Option A is incorrect because while using AWS Resource Access Manager (AWS RAM) to share the secrets would provide the DBA account access to the secrets, it does not eliminate the need for manual sharing of the secrets as the DBA team would still need to manually access the shared secrets in the DBA account.\\\"</li><li>I don't think it can as per this list: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 799087,
          "date": "Sun 05 Feb 2023 19:34",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Good catch",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792194,
          "date": "Mon 30 Jan 2023 01:09",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "RAM can share secret manager ressours bu as masetromainsaid \\\"Option A is incorrect because while using AWS Resource Access Manager (AWS RAM) to share the secrets would provide the DBA account access to the secrets, it does not eliminate the need for manual sharing of the secrets as the DBA team would still need to manually access the shared secrets in the DBA account.\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I don't think it can as per this list: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 799089,
          "date": "Sun 05 Feb 2023 19:35",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I don't think it can as per this list: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 786490,
          "date": "Tue 24 Jan 2023 13:13",
          "username": "\t\t\t\tNicocacik\t\t\t",
          "content": "It can't be D because SCP doesn't give access. A permission is needed in the application account. The correct answer has to be B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 781375,
          "date": "Thu 19 Jan 2023 18:10",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I swith for D<br> option D, using an SCP to allow access to the secrets from the DBA account, is a more appropriate solution for the requirements given in the problem. Using an SCP allows for more granular control over cross-account access, and ensures that the DBA-Admin role in the DBA account is only able to perform the actions that are explicitly allowed by the SCP, rather than being granted all permissions to access the secrets. Additionally, using an SCP is more secure than using IAM roles and policies because SCP uses a deny-all by default approach while IAM policies use an allow-all by default approach.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#132",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs: Research and DataOps.<br><br>Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types.<br><br>A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance.<br><br>Which combination of steps will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#132",
          "answers": [
            {
              "choice": "<p>A. Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance type.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851767,
          "date": "Mon 27 Mar 2023 07:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "SCP's are the most efficient here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 795438,
          "date": "Wed 01 Feb 2023 20:14",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "With AWS Org, consider SCP first.<br>In this scenario, Only C,D,E are mention about SCP, but D apply for all, not only the DataOps OU",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 778018,
          "date": "Mon 16 Jan 2023 18:36",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct options are C and E. <br><br>Option C: Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.<br><br>This option is correct because it allows the company to restrict access to all AWS regions except for ap-northeast-1. This ensures that all resources deployed in the organization must reside in the ap-northeast-1 region. By applying the SCP to the root OU, it ensures that all accounts and OUs under the root will be affected.<br><br>Option E: Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.<br><br>This option is correct because it allows the company to restrict access to specific instance types, which is required for the DataOps OU. By applying the SCP to the DataOps OU, it ensures that only resources deployed in the DataOps OU will be affected by the restriction.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it only restricts access to specific instance types, but it does not restrict access to a specific region.<br><br>Option B is incorrect because it is applied to IAM users rather than OUs, which would not effectively apply the restriction to all resources in the organization.<br><br>Option D is incorrect because it uses the ec2:Region condition key which would not allow to restrict the instances types only in the DataOps OU.<br><br>By creating an SCP that uses the aws:RequestedRegion condition key and restricting access to all regions except ap-northeast-1 and applying it to the root OU, this ensures that all resources deployed in the organization will reside in the ap-northeast-1 Region.<br><br>By creating an SCP that uses the ec2:InstanceType condition key and restricts access to specific instance types and applying it to the DataOps OU, this ensures that all EC2 instances deployed in the DataOps OU will use the predefined list of instance types.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 778021,
          "date": "Mon 16 Jan 2023 18:39",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because it only restricts access to specific instance types, but it does not restrict access to a specific region.<br><br>Option B is incorrect because it is applied to IAM users rather than OUs, which would not effectively apply the restriction to all resources in the organization.<br><br>Option D is incorrect because it uses the ec2:Region condition key which would not allow to restrict the instances types only in the DataOps OU.<br><br>By creating an SCP that uses the aws:RequestedRegion condition key and restricting access to all regions except ap-northeast-1 and applying it to the root OU, this ensures that all resources deployed in the organization will reside in the ap-northeast-1 Region.<br><br>By creating an SCP that uses the ec2:InstanceType condition key and restricts access to specific instance types and applying it to the DataOps OU, this ensures that all EC2 instances deployed in the DataOps OU will use the predefined list of instance types.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777721,
          "date": "Mon 16 Jan 2023 14:44",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "CE is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#133",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket.<br><br>The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region.<br><br>Which combination of changes will produce multi-Region deployment that meets these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#133",
          "answers": [
            {
              "choice": "<p>A. Deploy the SQS queue with the Lambda function to other Regions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Subscribe the SNS topic in each Region to the SQS queue.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Subscribe the SQS queue in each Region to the SNS topic.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the SQS queue to publish URLs to SNS topics in each Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Deploy the SNS topic and the Lambda function to other Regions.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851769,
          "date": "Mon 27 Mar 2023 07:50",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "AC - SQS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 831721,
          "date": "Tue 07 Mar 2023 10:54",
          "username": "\t\t\t\tZek\t\t\t",
          "content": "support A,C.  https://www.examtopics.com/discussions/amazon/view/74009-exam-aws-certified-solutions-architect-professional-topic-1/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798211,
          "date": "Sat 04 Feb 2023 18:32",
          "username": "\t\t\t\tMasterP007\t\t\t",
          "content": "A & C - Deploy & Subscribe SQS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792200,
          "date": "Mon 30 Jan 2023 01:21",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and C",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 778025,
          "date": "Mon 16 Jan 2023 18:40",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is correct because deploying the SQS queue with the Lambda function to other regions will allow the application to process URLs in those regions and compare differences in site localization.<br><br>Option C is correct because subscribing the SQS queue in each region to the SNS topic in the existing region will allow the application to publish URLs to the existing SNS topic and have those URLs processed in other regions.<br><br>Option B is incorrect because subscribing the SNS topic in each region to the SQS queue in the existing region would not allow URLs to be processed in other regions.<br><br>Option D is incorrect because configuring the SQS queue to publish URLs to SNS topics in each region would not ensure that the URLs are processed in those regions.<br><br>Option E is incorrect because deploying the SNS topic and Lambda function to other regions without the SQS queue would not allow the application to process URLs in those regions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 777725,
          "date": "Mon 16 Jan 2023 14:46",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "AC is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#134",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution.<br><br>Which strategy should the solutions architect use?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#134",
          "answers": [
            {
              "choice": "<p>A. Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 777726,
          "date": "Mon 16 Jan 2023 14:49",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct. only eventbridge can run scheduled task",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 851772,
          "date": "Mon 27 Mar 2023 07:51",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C - Fargate is the best choice here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778029,
          "date": "Mon 16 Jan 2023 18:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.<br><br>AWS Fargate is a serverless compute engine for containers that allows running containerized workloads without managing the underlying EC2 instances. This eliminates the need to provision, configure, and scale clusters of virtual machines to run containers.<br><br>Amazon EventBridge (formerly CloudWatch Events) allows scheduling tasks using cron or rate expressions, which can be used to invoke the Fargate task every 4 hours. This will allow for cost-effective and scalable solution, as the infrastructure is managed by AWS and the application can run in a serverless fashion, only incurring costs when the task is running.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other options are not appropriate in this scenario:<br><br>Option A: Running the application on AWS Lambda would not be appropriate, as Lambda is designed to run event-driven, short-lived functions, and not CPU-intensive, long-running tasks.<br>Option B: AWS Batch is a service for running batch jobs, and it may not be the most appropriate service for this scenario, as the application is not a batch job but a long running task.<br>Option D: Using Amazon EC2 Spot Instances would not be the best option for this scenario because the application is running for up to 20 minutes and EC2 Spot instances can be terminated at any time.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778034,
          "date": "Mon 16 Jan 2023 18:44",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other options are not appropriate in this scenario:<br><br>Option A: Running the application on AWS Lambda would not be appropriate, as Lambda is designed to run event-driven, short-lived functions, and not CPU-intensive, long-running tasks.<br>Option B: AWS Batch is a service for running batch jobs, and it may not be the most appropriate service for this scenario, as the application is not a batch job but a long running task.<br>Option D: Using Amazon EC2 Spot Instances would not be the best option for this scenario because the application is running for up to 20 minutes and EC2 Spot instances can be terminated at any time.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#135",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region:<br><br>• Amazon S3 bucket that stores game assets<br>• Amazon DynamoDB table that stores player scores<br><br>A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement.<br><br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#135",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create another S3 bucket in the sine Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851773,
          "date": "Mon 27 Mar 2023 07:53",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792203,
          "date": "Mon 30 Jan 2023 01:28",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "DynamoDB global tables+ S3 replication+Cloudfront",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778044,
          "date": "Mon 16 Jan 2023 18:49",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option C is the correct answer because it meets the requirements of reducing latency, improving reliability and requiring minimal effort to implement.<br><br>By creating another S3 bucket in a new Region, and configuring S3 Cross-Region Replication between the buckets, the game assets will be replicated to the new Region, reducing latency for users accessing the assets from that region. Additionally, by creating an Amazon CloudFront distribution and configuring origin failover with two origins accessing the S3 buckets in each Region, it ensures that the game assets will be served to users even if one of the regions becomes unavailable.<br><br>Configuring DynamoDB global tables by enabling Amazon DynamoDB Streams, and adding a replica table in a new Region, will also improve reliability by allowing the player scores to be replicated and updated in multiple regions, ensuring that the scores are available even in the event of a regional failure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not correct because using the new table as a replica target for DynamoDB global tables will not improve reliability. The same applies for Option D, which only uses S3 Same-Region Replication, which will not reduce latency for users in other regions.<br><br>Option B is not correct because configuring asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) is not the best solution for this use case. It would require additional configuration and management effort.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778047,
          "date": "Mon 16 Jan 2023 18:49",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is not correct because using the new table as a replica target for DynamoDB global tables will not improve reliability. The same applies for Option D, which only uses S3 Same-Region Replication, which will not reduce latency for users in other regions.<br><br>Option B is not correct because configuring asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC) is not the best solution for this use case. It would require additional configuration and management effort.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777727,
          "date": "Mon 16 Jan 2023 14:51",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct. S3 cross replicate, CloudFront, Dynamodb global database and origin failover",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#136",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.<br><br>The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#136",
          "answers": [
            {
              "choice": "<p>A. Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851777,
          "date": "Mon 27 Mar 2023 07:59",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C - there is no on-demand capacity mode.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 846633,
          "date": "Wed 22 Mar 2023 04:22",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Amazon DocumentDB best practice to choose an instance type with enough RAM to fit your working set (i.e., data and indexes) in memory. Having properly sized instances will help optimize for overall performance and potentially minimize I/O cost.<br> https://docs.aws.amazon.com/documentdb/latest/developerguide/best_practices.html<br><br>Also, you would already need to have it as on-demand; first thing is to size it appropriately",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834317,
          "date": "Thu 09 Mar 2023 20:27",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "No on-demand capacity mode for DocumentDB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832671,
          "date": "Wed 08 Mar 2023 09:15",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "No on-demand capacity mode for DocumentDB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832616,
          "date": "Wed 08 Mar 2023 07:56",
          "username": "\t\t\t\tandras\t\t\t",
          "content": "https://dynobase.dev/dynamodb-vs-documentdb/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 827572,
          "date": "Fri 03 Mar 2023 02:02",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Is C, DocumentDB On-Demand is not a thing. You need to create On-Demand instances as part of the cluster, but nothing like DynamoDB.  The cluster can either be Instance base or Elastic.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 813683,
          "date": "Sun 19 Feb 2023 04:06",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "On-demand capacity mode is there for document DB<br>https://aws.amazon.com/blogs/database/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That blog is for Dynamodb, not document DB.  Nowhere is mentioned capacity mode for documentDB, there's on-demand https://aws.amazon.com/documentdb/pricing/.</li><li>Is this ref for DocumentDB ?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 818859,
          "date": "Thu 23 Feb 2023 07:25",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "That blog is for Dynamodb, not document DB.  Nowhere is mentioned capacity mode for documentDB, there's on-demand https://aws.amazon.com/documentdb/pricing/.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814442,
          "date": "Sun 19 Feb 2023 20:36",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Is this ref for DocumentDB ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 811019,
          "date": "Thu 16 Feb 2023 19:53",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "There is no on-demand capacity mode. FAQ itself states capacity is reference to CPU and not Storage. C is right in my eyes.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807912,
          "date": "Tue 14 Feb 2023 00:35",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "No on-demand - so its C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807690,
          "date": "Mon 13 Feb 2023 19:24",
          "username": "\t\t\t\tCloudguy594\t\t\t",
          "content": "It's D.  On Demand Instances with DocumentDB let you provision instances without knowing the capacity. https://aws.amazon.com/documentdb/pricing",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804685,
          "date": "Fri 10 Feb 2023 20:39",
          "username": "\t\t\t\tIlk\t\t\t",
          "content": "There is no on-demand mode for DocumentDB.  It is C. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 799203,
          "date": "Sun 05 Feb 2023 22:48",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "\\\"DocumentDB in on-demand capacity mode\\\" is an invented thing",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 798656,
          "date": "Sun 05 Feb 2023 07:55",
          "username": "\t\t\t\tExamTopix01\t\t\t",
          "content": "C<br>On-demand capacity mode is the function of Dynamodb. <br>https://aws.amazon.com/blogs/news/running-spiky-workloads-and-optimizing-costs-by-more-than-90-using-amazon-dynamodb-on-demand-capacity-mode/<br><br>Amazon DocumentDB Elastic Clusters<br>https://aws.amazon.com/blogs/news/announcing-amazon-documentdb-elastic-clusters/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Both these URLs are not found. Are you sure?</li><li>Very good catch</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 813681,
          "date": "Sun 19 Feb 2023 03:59",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "Both these URLs are not found. Are you sure?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799201,
          "date": "Sun 05 Feb 2023 22:46",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Very good catch",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778050,
          "date": "Mon 16 Jan 2023 18:51",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D is correct because it uses Amazon DocumentDB with MongoDB compatibility, which allows for a seamless migration of the subscriber data from the on-premises MongoDB database to the cloud. Additionally, by using DocumentDB in on-demand capacity mode, the company can easily scale the database based on the actual load and usage of the application, without the need to provision instances in advance.<br><br>Option A is not a good fit because Aurora is a relational database and not compatible with MongoDB. <br>Option B is not a good fit because it uses MongoDB on a single EC2 instance, which does not provide high availability for the subscriber data.<br>Option C is not a good fit because it uses Amazon DocumentDB with MongoDB compatibility, but it is deployed on instances, which may not be able to handle the load and usage of the application.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777731,
          "date": "Mon 16 Jan 2023 14:56",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct with Document on demand mode because it is new deployment. load is unknown<br>A: Aurora is not compatiable with Mongodb<br>B: Mongodb on single EC2 not support HA<br>C: DocumentDB on instance may not support real load",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#137",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company's marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects.<br><br>A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error.<br><br>The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#137",
          "answers": [
            {
              "choice": "<p>A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851788,
          "date": "Mon 27 Mar 2023 08:19",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ACF is the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 842490,
          "date": "Sat 18 Mar 2023 06:36",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "A.  Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account.C.  Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role.F.  Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 833649,
          "date": "Thu 09 Mar 2023 07:30",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "B wrong - full permissions ? when question asks for minimum permissions.<br>D wrong - anonymous user ? anonymous does not work<br>E wrong - encrypt permissions ? No Strategy account needs decrypt permissions<br>So, A,C,F<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>first the source bucket needs to give grant access thru bucket policy and KMS key policy (A,C options)<br>Secondly, Strategy IAM role needs to give access to read from S3 bucket and also KMS key (Option F)</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 833652,
          "date": "Thu 09 Mar 2023 07:33",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "first the source bucket needs to give grant access thru bucket policy and KMS key policy (A,C options)<br>Secondly, Strategy IAM role needs to give access to read from S3 bucket and also KMS key (Option F)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792215,
          "date": "Mon 30 Jan 2023 01:49",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A C AND F",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 789348,
          "date": "Fri 27 Jan 2023 06:46",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://repost.aws/knowledge-center/cross-account-access-denied-error-s3",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 781389,
          "date": "Thu 19 Jan 2023 18:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A, C, and F are the correct options.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ACF"
        },
        {
          "id": 778055,
          "date": "Mon 16 Jan 2023 18:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A, C, and F are the correct options.<br><br>Option A creates a bucket policy that includes read permissions for the S3 bucket and sets the principal of the bucket policy to the account ID of the Strategy account. This ensures that users in the Strategy account have the necessary permissions to access the S3 bucket.<br><br>Option C updates the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. This ensures that the users in the Strategy account have the necessary permissions to decrypt the objects stored in the S3 bucket.<br><br>Option F updates the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. This ensures that the users in the Strategy account have the necessary permissions to read the objects in the S3 bucket and to decrypt them using the custom KMS key.<br><br>The other options are not correct because they either grant unnecessary permissions (B, D) or grant permissions in the wrong way (E).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777736,
          "date": "Mon 16 Jan 2023 14:59",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ACF is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#138",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days.<br><br>The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day.<br><br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#138",
          "answers": [
            {
              "choice": "<p>A. Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 854351,
          "date": "Wed 29 Mar 2023 13:59",
          "username": "\t\t\t\tEshu2009\t\t\t",
          "content": "Cannot be D.  S3 events cannot trigger Batch jobs. Only Eventbridge can trigger but thats not an option in D.  Both Storage FileGW and Datasync dont support SAN. File GS supports NAS vis NFS/SMB.  DataSync NAS vis NFS/SMB. <br>Data Pipeline can be an option.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 854216,
          "date": "Wed 29 Mar 2023 11:28",
          "username": "\t\t\t\tAsagumo\t\t\t",
          "content": "正解はCです。<br>SANについての記載がありますが、それはあくまで現状の説明であって、次期の仕組みの話ではないです。<br>また、S3イベントで起動できるものにAWS Batchはありません。",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 848695,
          "date": "Thu 23 Mar 2023 22:45",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "For me, none of these answer are correct.<br>C: DataSync is not working with SAN<br>D: Storage gateway have multiple gateway type. Answer is talking about \\\"file gateway\\\" which is not compatible with SAN. The gateway compatible would be \\\"Volume gateway\\\".<br>B: Data Pipeline, i'm not sure it's working with SAN.<br>A: snow is not a solution for regular and automatic process ..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847103,
          "date": "Wed 22 Mar 2023 14:33",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D because of the SAN. Its more efficient to use Storage Gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842492,
          "date": "Sat 18 Mar 2023 06:37",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 842403,
          "date": "Sat 18 Mar 2023 02:21",
          "username": "\t\t\t\tDamijo\t\t\t",
          "content": "https://aws.amazon.com/blogs/storage/how-to-move-and-store-your-genomics-sequencing-data-with-aws-datasync/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 836932,
          "date": "Sun 12 Mar 2023 12:14",
          "username": "\t\t\t\tkrishccie\t\t\t",
          "content": "https://docs.aws.amazon.com/filegateway/latest/filefsxw/using-dx.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 835235,
          "date": "Fri 10 Mar 2023 17:41",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834243,
          "date": "Thu 09 Mar 2023 18:52",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Guys its Tricky one between C and D and answer is D! (Modernization question)<br>Look at this two below blogs : <br>https://aws.amazon.com/blogs/storage/using-aws-storage-gateway-to-modernize-next-generation-sequencing-workflows/<br><br>Thanks to tinyflame who made me do my research on this :-)<br>Yes, SAN -> Storage Gateway Only<br>NAS -> Data Sync or Storage Gateway<br>https://aws.amazon.com/blogs/storage/from-on-premises-to-aws-hybrid-cloud-architecture-for-network-file-shares/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>On Premise NAS and file servers toS3.--&gt; Use DataSync solution<br>On Premise SMB or NFS file share to S3 --&gt; Use Storage/File Gateway solution</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 834248,
          "date": "Thu 09 Mar 2023 18:59",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "On Premise NAS and file servers toS3.--> Use DataSync solution<br>On Premise SMB or NFS file share to S3 --> Use Storage/File Gateway solution",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822317,
          "date": "Sun 26 Feb 2023 12:44",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Option D uses an AWS Storage Gateway file gateway, which is not a good fit for the high-speed Direct Connect connection, and it introduces additional complexity with an extra gateway layer.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 818873,
          "date": "Thu 23 Feb 2023 07:38",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Option C would be the best solution to meet the requirements.<br><br>AWS DataSync can be used to transfer the sequencing data to Amazon S3, which will make the data accessible to the processing applications. S3 events can then be used to trigger an AWS Lambda function, which starts an AWS Step Functions workflow. The Docker images can be stored in Amazon Elastic Container Registry (Amazon ECR), which can then trigger AWS Batch to run the containers and process the sequencing data. This approach makes the system scalable, and the Docker containers can be run on multiple EC2 instances to handle the workload.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 816137,
          "date": "Tue 21 Feb 2023 03:50",
          "username": "\t\t\t\ttinyflame\t\t\t",
          "content": "SAN -> Storage Gateway Only<br>NAS -> Data Sync or Storage Gateway",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 792218,
          "date": "Mon 30 Jan 2023 01:52",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is the correct solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778058,
          "date": "Mon 16 Jan 2023 18:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct solution is C. <br><br>AWS DataSync can be used to transfer the sequencing data to Amazon S3, which is a more efficient and faster method than using Snowball Edge devices. Once the data is in S3, S3 events can trigger an AWS Lambda function that starts an AWS Step Functions workflow. The Docker images can be stored in Amazon Elastic Container Registry (Amazon ECR) and AWS Batch can be used to run the container and process the sequencing data.<br><br>Option A is not the best solution because it would take a long time to transfer the data to AWS and process the data, and AWS Snowball Edge is not ideal for high-speed data transfer.<br>Option B is not the best solution because EC2 Auto Scaling group is not a cost-effective solution for running short-lived jobs<br>Option D is not the best solution because AWS Batch is not the best service for running short-lived jobs and it may not be cost-effective",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777739,
          "date": "Mon 16 Jan 2023 15:01",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#139",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones.<br><br>A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time.<br><br>Which solution will meet these requirements with the LEAST management overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#139",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 847112,
          "date": "Wed 22 Mar 2023 14:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "EFS and Windows is not straight forward. C is the best solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 846655,
          "date": "Wed 22 Mar 2023 05:03",
          "username": "\t\t\t\tzejou1\t\t\t",
          "content": "Amazon FSx is built on Windows Server... Access Control Lists (ACLs)... To control user access, Amazon FSx integrates with your on-premises Microsoft Active Directory as well as with AWS Microsoft Managed AD. <br>https://aws.amazon.com/fsx/windows/features/?nc=sn&loc=2<br><br>All others don't work - forget about the \\\"least management\\\" statement - it says \\\"implement Windows ACLS to control...\\\" all others are thrown out.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834504,
          "date": "Fri 10 Mar 2023 00:07",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "EFS is Linux/Mac based, So, A,D are out.<br>Lustre stands for Linux cluster, So B is out. Left is C which is correct (Amazon FSx for Windows )",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822326,
          "date": "Sun 26 Feb 2023 12:57",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Option D suggests using an EFS file system, which is a shared file system that can be mounted on multiple EC2 instances, but this requires additional configuration to keep the content in sync across all instances.<br><br>Option C is the optimal choice because Amazon FSx for Windows File Server supports Windows ACLs and seamlessly integrates with Active Directory to join instances to a domain. This option minimizes management overhead by reducing the complexity of managing multiple EFS file shares or writing scripts to synchronize content across EC2 instances.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 800124,
          "date": "Mon 06 Feb 2023 19:39",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "FSX for WIndows is the only option. The rest of options are not supported.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 796839,
          "date": "Fri 03 Feb 2023 08:34",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "FSx for Lustre can only be used by Linux-based instances.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792226,
          "date": "Mon 30 Jan 2023 01:58",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "good answer are C or D but as it says LEAST management overhead ==> D as in C we will need a user data script<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>sorry D is uncorrect as it use Elastic File System (Amazon EFS) itch is not windows so Iswitch to C</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 792227,
          "date": "Mon 30 Jan 2023 02:00",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "sorry D is uncorrect as it use Elastic File System (Amazon EFS) itch is not windows so Iswitch to C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 789855,
          "date": "Fri 27 Jan 2023 18:52",
          "username": "\t\t\t\tARLV\t\t\t",
          "content": "@masetromain is this a good exam study guide? Like how many questions were from here. Any help would be appreciated. Thank you",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 789372,
          "date": "Fri 27 Jan 2023 07:37",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html<br>https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779136,
          "date": "Tue 17 Jan 2023 18:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I switch for C: Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This solution meets the requirements with the least management overhead because it utilizes Amazon FSx for Windows File Server, which is a fully managed service that allows you to easily set up a highly available and scalable file server. The Auto Scaling group ensures that the application is running on at least three instances across multiple Availability Zones, providing high availability and fault tolerance. The user data script can be used to automate the setup and configuration of the instances when they are launched, and it can be used to join the instances to the AD domain, so that the instances can be managed and access to the file contents can be controlled using Windows ACLs.</li><li>The other choices are not correct because:<br><br>Option A: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>Option B: Amazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, it is not a windows file system and it does not support Windows ACLs.<br><br>Option D: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>In both cases, creating a new AMI from the current EC2 instance that is running it doesn't help to solve the problem as it won't provide a scalable solution that runs on at least three instances across multiple Availability Zones.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779138,
          "date": "Tue 17 Jan 2023 18:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "This solution meets the requirements with the least management overhead because it utilizes Amazon FSx for Windows File Server, which is a fully managed service that allows you to easily set up a highly available and scalable file server. The Auto Scaling group ensures that the application is running on at least three instances across multiple Availability Zones, providing high availability and fault tolerance. The user data script can be used to automate the setup and configuration of the instances when they are launched, and it can be used to join the instances to the AD domain, so that the instances can be managed and access to the file contents can be controlled using Windows ACLs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other choices are not correct because:<br><br>Option A: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>Option B: Amazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, it is not a windows file system and it does not support Windows ACLs.<br><br>Option D: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>In both cases, creating a new AMI from the current EC2 instance that is running it doesn't help to solve the problem as it won't provide a scalable solution that runs on at least three instances across multiple Availability Zones.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779139,
          "date": "Tue 17 Jan 2023 18:30",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other choices are not correct because:<br><br>Option A: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>Option B: Amazon FSx for Lustre is a high-performance file system optimized for compute-intensive workloads, it is not a windows file system and it does not support Windows ACLs.<br><br>Option D: An Amazon Elastic File System (Amazon EFS) file share is not a windows file system and it does not support Windows ACLs.<br><br>In both cases, creating a new AMI from the current EC2 instance that is running it doesn't help to solve the problem as it won't provide a scalable solution that runs on at least three instances across multiple Availability Zones.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778061,
          "date": "Mon 16 Jan 2023 18:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D, as it meets all of the requirements with the least management overhead.<br><br>In this solution, an Amazon Elastic File System (Amazon EFS) file system is created and an Auto Scaling group is created that extends across three Availability Zones and maintains a minimum size of three instances. A new AMI is created from the current EC2 instance that is running, and the instances in the Auto Scaling group are then launched from this new AMI.<br><br>A seamless domain join is then performed to join the instances to the AD domain, and the Amazon EFS file system is mounted on the instances. This solution uses an existing EC2 instance, so there is no need to use a user data script to install the application or join the instances to the AD domain, which reduces the management overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The other choices are not correct because they either require a user data script to install the application or to join the instances to the AD domain, which increases the management overhead, or they use a different file system that may not be compatible with the application or the AD domain.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778062,
          "date": "Mon 16 Jan 2023 18:57",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The other choices are not correct because they either require a user data script to install the application or to join the instances to the AD domain, which increases the management overhead, or they use a different file system that may not be compatible with the application or the AD domain.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777758,
          "date": "Mon 16 Jan 2023 15:19",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is only one make sense<br>A: No AMI creation, have to use user data to install app, more complex<br>B: need user data<br>C: need user data<br>D: has least management overhead<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D: EFS does not work for WIndows.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 800123,
          "date": "Mon 06 Feb 2023 19:38",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "D: EFS does not work for WIndows.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#140",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A software as a service (SaaS) based company provides a case management solution to customers A3 part of the solution. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer.<br><br>The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#140",
          "answers": [
            {
              "choice": "<p>A. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851791,
          "date": "Mon 27 Mar 2023 08:24",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Template - easy one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 834622,
          "date": "Fri 10 Mar 2023 05:02",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "SendTemplatedEmail<br>SendEmail<br>SendRawEmail are email api methods used in SES",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 792231,
          "date": "Mon 30 Jan 2023 02:07",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "D should be the answer",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778069,
          "date": "Mon 16 Jan 2023 19:02",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is D. <br><br>In this solution, the company can use Amazon SES to send email messages, which will minimize operational overhead as SES is a fully managed service that handles sending and receiving email messages. The company can store the email template on Amazon SES with parameters for the customer data and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination. This solution eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and time-consuming.<br><br>Option A and B are not correct because it requires to set up an SMTP server on EC2 instances, which is not necessary and will increase operational overhead.<br>Option C is not correct because it stores the email template in Amazon SES with parameters for the customer data which is not possible.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777765,
          "date": "Mon 16 Jan 2023 15:22",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct - https://docs.aws.amazon.com/ses/latest/APIReference/API_SendTemplatedEmail.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#141",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.<br><br>The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.<br><br>Several times during the day. the development team receives notification that messages are in the dead-letter queue and that videos have not been processed property. An investigation finds no errors m the application logs.<br><br>How can the company solve this problem?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#141",
          "answers": [
            {
              "choice": "<p>A. Turn on termination protection tor the EC2 Instances<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Update the visibility timeout for the SQS queue to 3 hours<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure scale-in protection for the instances during processing<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the redrive policy and set maxReceiveCount to 0.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851794,
          "date": "Mon 27 Mar 2023 08:27",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Scale-in protection<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>he visibility timeout might still need to be adjusted, but the scale-in protection is the primary solution to prevent instances from being terminated during processing, which would cause the messages to end up in the dead-letter queue.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851798,
          "date": "Mon 27 Mar 2023 08:30",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "he visibility timeout might still need to be adjusted, but the scale-in protection is the primary solution to prevent instances from being terminated during processing, which would cause the messages to end up in the dead-letter queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822796,
          "date": "Sun 26 Feb 2023 19:15",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Setting maxReceiveCount to 0 in the redrive policy of an Amazon SQS queue means that if a message is not successfully processed by any of the consumers after one attempt, the message will be deleted from the queue immediately instead of being moved to the dead-letter queue.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 814460,
          "date": "Sun 19 Feb 2023 20:46",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C Correct Answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 812080,
          "date": "Fri 17 Feb 2023 16:40",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "I'm conflicted on this question, D updating the redrive sounds like the best solution because it's addressing the root cause. C is a workaround, not solving the problem of processing the videos.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>moderator dont approve, I figured it out.</li><li>B is the correct option.<br><br>The issue seems to be that the videos are taking longer than the visibility timeout to process, so they are being sent to the dead-letter queue even though they are still being processed. By updating the visibility timeout for the SQS queue to 3 hours, the videos will have more time to process before being sent to the dead-letter queue, which should solve the problem.</li><li>Interesting point, I understood the problem in a different way. I think the problem is that while an Ec2 Instance is still working on the video, there was a scale-in event and that instance was selected for termination. I will use personally lifecycle hooks, option C defeats the purpose of AutoScaling in some way like you said is a workaround.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 812090,
          "date": "Fri 17 Feb 2023 16:50",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "moderator dont approve, I figured it out.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is the correct option.<br><br>The issue seems to be that the videos are taking longer than the visibility timeout to process, so they are being sent to the dead-letter queue even though they are still being processed. By updating the visibility timeout for the SQS queue to 3 hours, the videos will have more time to process before being sent to the dead-letter queue, which should solve the problem.</li><li>Interesting point, I understood the problem in a different way. I think the problem is that while an Ec2 Instance is still working on the video, there was a scale-in event and that instance was selected for termination. I will use personally lifecycle hooks, option C defeats the purpose of AutoScaling in some way like you said is a workaround.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812094,
          "date": "Fri 17 Feb 2023 16:52",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "B is the correct option.<br><br>The issue seems to be that the videos are taking longer than the visibility timeout to process, so they are being sent to the dead-letter queue even though they are still being processed. By updating the visibility timeout for the SQS queue to 3 hours, the videos will have more time to process before being sent to the dead-letter queue, which should solve the problem.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Interesting point, I understood the problem in a different way. I think the problem is that while an Ec2 Instance is still working on the video, there was a scale-in event and that instance was selected for termination. I will use personally lifecycle hooks, option C defeats the purpose of AutoScaling in some way like you said is a workaround.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 827964,
          "date": "Fri 03 Mar 2023 13:43",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "Interesting point, I understood the problem in a different way. I think the problem is that while an Ec2 Instance is still working on the video, there was a scale-in event and that instance was selected for termination. I will use personally lifecycle hooks, option C defeats the purpose of AutoScaling in some way like you said is a workaround.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792235,
          "date": "Mon 30 Jan 2023 02:11",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "for me, should be C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778073,
          "date": "Mon 16 Jan 2023 19:04",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  The company can solve the problem by configuring scale-in protection for the instances during processing. This will ensure that the instances are not terminated while they are processing videos. This will prevent the messages from moving to the dead-letter queue and ensure that videos are processed properly.<br><br>Option A is incorrect because turning on termination protection for the EC2 instances will not solve the problem as it will impact the ability of the Auto Scaling group to scale instances in and out based on the number of videos in the queue.<br><br>Option B is incorrect because the company has specified a visibility timeout of 1 hour, which is enough time for the instances to process a video and there is no need to update the timeout to 3 hours.<br><br>Option D is incorrect because the company has set the maxReceiveCount to 1 and changing it to 0 will not solve the problem. maxReceiveCount allowed range is 1 to 1000.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777793,
          "date": "Mon 16 Jan 2023 15:45",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct<br>A: termination protection of EC2 will impact ASG<br>B: only take 30 minutes, no need for 3 hour<br>C: ASG is based on queue depth, ASG will scale in when queue length is 0. But maxreceivecount is set to 1.<br>D: maxreceivecount allowed range is 1 to 1000",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#142",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.<br><br>The solutions architect must design a solution to make the set of APIs accessible only from a VPC.  All APIs need to be called with an authenticated user<br><br>Which solution will meet these requirements with the LEAST amount of effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#142",
          "answers": [
            {
              "choice": "<p>A. Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPCreate a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the Lambda functions inside the VPC Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 792240,
          "date": "Mon 30 Jan 2023 02:19",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "should be C as on the question has said 'no need for public IP\\\" ==> private in API gateway = VPC endpoint",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851799,
          "date": "Mon 27 Mar 2023 08:31",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C.  Update the API endpoint from Regional to private in API Gateway.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778075,
          "date": "Mon 16 Jan 2023 19:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is C.  Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC.  Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC. <br>This solution will meet the requirements with the least amount of effort because it utilizes the built-in features of API Gateway and VPC to restrict access to the API. With this method, no additional infrastructure or configurations are necessary.<br>A and B are not correct because they would require additional infrastructure and configurations.<br>D is not correct because it would require provisioning an EC2 instance and installing an Apache server, introducing additional complexity and management overhead.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777804,
          "date": "Mon 16 Jan 2023 15:54",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C iscorrect",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#143",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.<br><br>The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.<br><br>Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#143",
          "answers": [
            {
              "choice": "<p>A. Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 853902,
          "date": "Wed 29 Mar 2023 04:03",
          "username": "\t\t\t\tEshu2009\t\t\t",
          "content": "BE- global accelerators improve performance by providing edge location for onboarding traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Q: Can I use AWS Global Accelerator for object storage with Amazon S3?<br><br>A: You can use Amazon S3 Multi-Region Access Points to get the benefits of Global Accelerator for object storage. S3 Multi-Region Access Points use Global Accelerator transparently to provide a single global endpoint to access a data set that spans multiple S3 buckets in different AWS Regions. This allows you to build multi-region applications with the same simple architecture used in a single region, and then to run those applications anywhere in the world. Application requests made to an S3 Multi-Region Access Point's global endpoint automatically route over the AWS global network to the S3 bucket with the lowest network latency. This allows applications to automatically avoid congested network segments on the public internet, improving application performance and reliability.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 853906,
          "date": "Wed 29 Mar 2023 04:13",
          "username": "\t\t\t\tEshu2009\t\t\t",
          "content": "Q: Can I use AWS Global Accelerator for object storage with Amazon S3?<br><br>A: You can use Amazon S3 Multi-Region Access Points to get the benefits of Global Accelerator for object storage. S3 Multi-Region Access Points use Global Accelerator transparently to provide a single global endpoint to access a data set that spans multiple S3 buckets in different AWS Regions. This allows you to build multi-region applications with the same simple architecture used in a single region, and then to run those applications anywhere in the world. Application requests made to an S3 Multi-Region Access Point's global endpoint automatically route over the AWS global network to the S3 bucket with the lowest network latency. This allows applications to automatically avoid congested network segments on the public internet, improving application performance and reliability.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851805,
          "date": "Mon 27 Mar 2023 08:37",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Ill go with BD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 832753,
          "date": "Wed 08 Mar 2023 10:19",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "A: Global Accelerator can't have an s3 bucket as endpoint<br>C: People are complaining about time to retreive maps. Transfert acceleration is used to accelerate PUT requests to an s3 bucket located in a distant region.<br>E: An accelerator as cloudfront origin does not make much sense, because cloudfront is already using the AWS network. Global Accelerator is usually for Layer 4 networking and/or static anycast IPs",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 822808,
          "date": "Sun 26 Feb 2023 19:22",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Since only one additional region we dont need global accelerators",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 817913,
          "date": "Wed 22 Feb 2023 15:25",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "S3 transfer acceleration is more efficient",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 792247,
          "date": "Mon 30 Jan 2023 02:22",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A and E are not correct as there isn't a need to use aws global accel",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 778077,
          "date": "Mon 16 Jan 2023 19:06",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is correct because it involves creating a new S3 bucket in the us-east-1 region and configuring cross-Region replication to synchronize from the existing S3 bucket in eu-west-1. This will allow users in us-east-1 to access the weather maps from a closer location, improving performance.<br><br>D is correct because it involves using Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1. This will also allow users in us-east-1 to access the weather maps from a closer location, improving performance.<br><br>A and E are not correct because they do not involve creating a new S3 bucket in us-east-1, which is necessary for improving performance for the users in that region. C is not correct because it involves using the S3 Transfer Acceleration endpoint, which is a different service and not necessary for this scenario.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 777805,
          "date": "Mon 16 Jan 2023 15:55",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "BD is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#144",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon Workspaces. An initial analysis indicates that the issue involves user profiles. The Amazon Workspaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.<br><br>The solutions architect discovers that the file system has reached Its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#144",
          "answers": [
            {
              "choice": "<p>A. Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851808,
          "date": "Mon 27 Mar 2023 08:39",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 834651,
          "date": "Fri 10 Mar 2023 05:53",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/cli/latest/reference/fsx/update-file-system.html<br>EventBridge invoking lambda to update settings will prevent too from occurring again",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792249,
          "date": "Mon 30 Jan 2023 02:25",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B seems to be the correct answer.<br>the unique possible solution is to add storage capacity using CLI",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 787509,
          "date": "Wed 25 Jan 2023 11:34",
          "username": "\t\t\t\tpitakk\t\t\t",
          "content": "To increase the storage capacity for an FSx for Windows File Server file system, use the AWS CLI command update-file-system. https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-capacity.html It's B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778078,
          "date": "Mon 16 Jan 2023 19:07",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B is correct. It can prevent the issue from happening again by monitoring the file system with the FreeStorageCapacity metric in Amazon CloudWatch and using Amazon EventBridge to invoke an AWS Lambda function to increase the capacity as required. This ensures that the file system always has enough free space to store user profiles and avoids reaching maximum capacity.<br>A: Removing old user profiles may not be sufficient to create enough space and does not prevent the problem from happening again.<br>C: AWS Step Functions cannot be used to increase capacity, it is a service for creating and running workflows that stitch together multiple AWS services.<br>D: Creating an additional FSx for Windows File Server file system and updating user profile redirection for a portion of the users may not be sufficient to prevent the problem from happening again and does not address the current capacity issue.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777825,
          "date": "Mon 16 Jan 2023 16:03",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct. It can prevent issue happen again with EventBridge and Lambda<br>A: not make sense at all<br>C: Cannot use Step Function to increase capacity<br>D: not prevent happen again",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#145",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient's signature or a photo of the package with the recipient. The driver's handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the file name matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving.<br><br>As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated.<br><br>A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. The handheld devices cannot be modified, so the company cannot deploy a new application.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#145",
          "answers": [
            {
              "choice": "<p>A. Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851809,
          "date": "Mon 27 Mar 2023 08:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C is the most efficient",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792253,
          "date": "Mon 30 Jan 2023 02:30",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778082,
          "date": "Mon 16 Jan 2023 19:10",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C is correct. Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and using S3 event notifications through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance. <br><br>Option A and B still use EC2 instance, which is the source of the problem. Option D requires modification to the handheld devices which is not possible.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777829,
          "date": "Mon 16 Jan 2023 16:04",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#146",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application in the AWS Cloud. The application runs on containers m an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.<br><br>Which solution will meet these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#146",
          "answers": [
            {
              "choice": "<p>A. Provision an Aurora Replica in a different Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up AWS DataSync for continuous replication of the data to a different Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851811,
          "date": "Mon 27 Mar 2023 08:41",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Replica",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835677,
          "date": "Sat 11 Mar 2023 06:15",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "B,C are on premises usecase solutions. D is wrong because 5 minute worth of data could be lost against the requirement. So A is correct. In fact replica works as standby if primary DB fails.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 792259,
          "date": "Mon 30 Jan 2023 02:34",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "A is correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778084,
          "date": "Mon 16 Jan 2023 19:11",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A is correct. Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.<br>B.  AWS DataSync can replicate data, but it is not a fully managed service and requires more configuration and management.<br>C.  AWS DMS is a fully managed service for migrating data between databases, but it may require additional configuration and management to continuously replicate data in real-time.<br>D.  Amazon DLM can be used for scheduling snapshots, but it does not provide real-time replication and may not meet the requirement of no data loss in case of a failure.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777839,
          "date": "Mon 16 Jan 2023 16:08",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A is correct<br>B: cannot use DataSync for Aurora backup<br>C: too complex<br>D: DLM is for EBS backup. Here use managed Aurora server, no access to EBS",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#147",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.<br><br>Which solutions will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#147",
          "answers": [
            {
              "choice": "<p>A. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 847127,
          "date": "Wed 22 Mar 2023 14:56",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C makes the most sense.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 835712,
          "date": "Sat 11 Mar 2023 08:00",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Extract Data from S3 + mask + Send to another S3 + Transform/Process + Load into S3<br>All these are ETL, ELT tasks which should ring Glue<br><br>EMR is more focused on big data processing frameworks such as Hadoop and Spark, <br>while Glue is more focused on ETL, More over 5000 records every 15 minutes is not soo big data..So I choose C",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 800159,
          "date": "Mon 06 Feb 2023 20:05",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "The question is at what point Athena and EMR are a better choice because it is a lot of data to store and process<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That, I agree. Honestly, I will use it from day one, regardless.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828081,
          "date": "Fri 03 Mar 2023 16:26",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "That, I agree. Honestly, I will use it from day one, regardless.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792262,
          "date": "Mon 30 Jan 2023 02:40",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "C is correct.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778089,
          "date": "Mon 16 Jan 2023 19:15",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C is correct. It will process the data in batch mode using Glue ETL job which can handle large amount of data and can be scheduled to run periodically. This solution is also easily expandable for future feeds.<br><br>A: It uses multiple Lambda functions, SQS queue and S3 temporary location which will increase operational overhead.<br>B: Using Fargate may not be the most cost-effective solution and also it may not handle large amount of data.<br>D: Athena and EMR both are powerful tools but they are more complex and can be more costly than Glue.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777844,
          "date": "Mon 16 Jan 2023 16:10",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#148",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.<br><br>Which solution will achieve the company's goal with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#148",
          "answers": [
            {
              "choice": "<p>A. Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 847129,
          "date": "Wed 22 Mar 2023 14:59",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B has less operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 843708,
          "date": "Sun 19 Mar 2023 12:49",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "B, tricky",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835727,
          "date": "Sat 11 Mar 2023 08:54",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Tricky one. This is not an on premise migration use case which prompts for answer C.  Its a current situation of on premise application which the company wants to continue its state in the requirement of using AWS as DR solution.<br>https://docs.aws.amazon.com/images/drs/latest/userguide/images/drs-failback-arc.png<br>https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Moreover, B has least operational over head of just initiating DR solution with replicating agents. C has operational overhead with DMS , SCT ,CDC,migration etc</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835729,
          "date": "Sat 11 Mar 2023 08:58",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Moreover, B has least operational over head of just initiating DR solution with replicating agents. C has operational overhead with DMS , SCT ,CDC,migration etc",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822851,
          "date": "Sun 26 Feb 2023 19:43",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://aws.amazon.com/disaster-recovery/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818461,
          "date": "Wed 22 Feb 2023 22:32",
          "username": "\t\t\t\tYowie351\t\t\t",
          "content": "The answer is definitely B.  Database recovery is included as a feature with EDR.<br>https://aws.amazon.com/blogs/storage/achieving-data-consistency-with-aws-elastic-disaster-recovery/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 813846,
          "date": "Sun 19 Feb 2023 08:56",
          "username": "\t\t\t\tMahakali\t\t\t",
          "content": "Disaster recovery solution should be B , this option mentions AWS replication agent with reference to context of Elastic Disaster Recovery",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 807935,
          "date": "Tue 14 Feb 2023 01:05",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Selecting C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805888,
          "date": "Sun 12 Feb 2023 03:25",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "It should be B.  The frequent failover and failback should be mostly a drill like here https://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html#drill-recover-instance-faq<br><br>The sentence does not make sense. CDC is not with SCT.<br><br>> Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802468,
          "date": "Wed 08 Feb 2023 20:41",
          "username": "\t\t\t\tMasterP007\t\t\t",
          "content": "If you Frequently perform failover and fallback... isn't that an operational overhead ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Nope, it's a quick push button exercise.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 805884,
          "date": "Sun 12 Feb 2023 03:21",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "Nope, it's a quick push button exercise.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795563,
          "date": "Wed 01 Feb 2023 23:43",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "C is correct.<br>Option B, Initializing AWS Elastic Disaster Recovery in the target AWS Region, requires frequent failover and fallback from the most recent point in time which would increase operational overhead for the company. The goal is to have the solution with the LEAST operational overhead.<br>Option C, using AWS DMS and SCT, provides a more efficient and less manual process for replicating and synchronizing the database, reducing the operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's more of a drill like this https://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html#drill-recover-instance-faq</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805887,
          "date": "Sun 12 Feb 2023 03:23",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "It's more of a drill like this https://docs.aws.amazon.com/drs/latest/userguide/failback-overview.html#drill-recover-instance-faq",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792264,
          "date": "Mon 30 Jan 2023 02:42",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "I select C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 792199,
          "date": "Mon 30 Jan 2023 01:20",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "this is asking about application and not data alone. So option B with DRS is good. C is concentrating only on DB migration.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 790295,
          "date": "Sat 28 Jan 2023 05:47",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html<br>https://docs.aws.amazon.com/drs/latest/userguide/recovery-workflow-gs.html<br>Option C is wrong. That just mentions the migration method. I think this question asks us the DR architecture between on-premises and AWS cloud.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 785387,
          "date": "Mon 23 Jan 2023 14:29",
          "username": "\t\t\t\tpitakk\t\t\t",
          "content": "C is intuitive and makes sense apart from the fact that there's no CDC in SCT... I guess they thought about B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CDC would be referenced as part of DMS<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC. html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 801516,
          "date": "Tue 07 Feb 2023 23:48",
          "username": "\t\t\t\tSignup_Nickname\t\t\t",
          "content": "CDC would be referenced as part of DMS<br>https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC. html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778180,
          "date": "Mon 16 Jan 2023 20:51",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI.<br><br>This option would allow the company to use the AWS DMS and SCT tools, which are specifically designed for migrating and replicating databases, to migrate their MySQL database to an Amazon Aurora MySQL DB cluster in the target AWS Region. This would reduce the operational overhead compared to option A and B, as it would automate the process of replicating and synchronizing the data. Option D is also a viable solution but it would have more operational overhead compared to option C as it involves more manual steps like taking regular snapshot and restoring them.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777850,
          "date": "Mon 16 Jan 2023 16:18",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#149",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#149",
          "answers": [
            {
              "choice": "<p>A. In the company's AWS account, create resource policies for all resources in the account to grant access to the auditors' AWS account. Assign a unique external ID to the resource policy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. In the company's AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. In the company's AWS account, create an IAM group that has the required permissions. Create an IAM user in the company's account for each auditor. Add the IAM users to the IAM group.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851818,
          "date": "Mon 27 Mar 2023 08:46",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "In the company's AWS account, create an IAM role that trusts the auditors' AWS account.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795565,
          "date": "Wed 01 Feb 2023 23:47",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "Option B is the best solution. This solution creates an IAM role that trusts the auditors' AWS account and attaches the required IAM policies to the role. This ensures that the auditors have read-only access to the company's AWS account while ensuring that the company's AWS account is secure and complies with AWS security best practices. Additionally, the unique external ID assigned to the role's trust policy adds an extra layer of security.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792266,
          "date": "Mon 30 Jan 2023 02:45",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "B seems to be the right answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778181,
          "date": "Mon 16 Jan 2023 20:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "The correct answer is B.  In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.<br><br>This solution meets the requirement of providing the external auditors with secure, read-only access to the company's AWS account while also complying with AWS security best practices. In this solution, an IAM role is created that trusts the auditors' AWS account and has an IAM policy with the required permissions attached to it. The role's trust policy should include a unique external ID for added security. This allows the external auditors to assume the role and access the resources with the permissions specified in the policy, without the need to share access keys or create individual IAM users for each auditor.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it grants access to all resources in the company's AWS account and does not provide a way to restrict the permissions that the external auditors have.<br><br>Option C is incorrect because it creates an IAM user in the company's account and shares the API access keys with the external auditors, which is not secure and does not comply with AWS security best practices.<br><br>Option D is incorrect because it creates an IAM user in the company's account for each auditor, which would be tedious and difficult to manage for the company. It would be more secure and efficient to use an IAM role that trusts the auditors' AWS account instead of creating individual users for each auditor.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778183,
          "date": "Mon 16 Jan 2023 20:54",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A is incorrect because it grants access to all resources in the company's AWS account and does not provide a way to restrict the permissions that the external auditors have.<br><br>Option C is incorrect because it creates an IAM user in the company's account and shares the API access keys with the external auditors, which is not secure and does not comply with AWS security best practices.<br><br>Option D is incorrect because it creates an IAM user in the company's account for each auditor, which would be tedious and difficult to manage for the company. It would be more secure and efficient to use an IAM role that trusts the auditors' AWS account instead of creating individual users for each auditor.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777852,
          "date": "Mon 16 Jan 2023 16:20",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#150",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.<br><br>Which solution will meet these requirements with the LEAST latency?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#150",
          "answers": [
            {
              "choice": "<p>A. Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 790302,
          "date": "Sat 28 Jan 2023 06:06",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "3 nodes are required for a DAX cluster to be fault-tolerant.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluster.html",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 851820,
          "date": "Mon 27 Mar 2023 08:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 836566,
          "date": "Sat 11 Mar 2023 23:01",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluster.html<br>2 node is not fault tolerant and in fact more nodes less latency. If there's an option with > 3nodes, I'd go for that instead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822866,
          "date": "Sun 26 Feb 2023 19:51",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that can significantly improve read performance. In this scenario, since the platform is latency-sensitive, the goal is to reduce read latency.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821841,
          "date": "Sat 25 Feb 2023 22:25",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "As per below link B is best option.<br><br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 817990,
          "date": "Wed 22 Feb 2023 16:42",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The write-through behavior of DAX is appropriate for many application patterns. However, there are some application patterns where a write-through model might not be appropriate.<br><br>For applications that are sensitive to latency, writing through DAX incurs an extra network hop. So a write to DAX is a little slower than a write directly to DynamoDB.  If your application is sensitive to write latency, you can reduce the latency by writing directly to DynamoDB instead. For more information, see Write-around.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html#DAX.consistency.strategies-for-writes.write-around",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807941,
          "date": "Tue 14 Feb 2023 01:11",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Going with B due tomore latency with Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795392,
          "date": "Wed 01 Feb 2023 19:18",
          "username": "\t\t\t\tirene7\t\t\t",
          "content": "B, writing with DAX can have more latency and needs three nodes for HA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792271,
          "date": "Mon 30 Jan 2023 02:51",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "I\\\"ll go for A (enven if it is less in HA) but because it is the unique option where it it saied that read and write is done thru DAX<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It said least latency. Writing via DAX has higher latency than writing directly to dynamodb.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 794004,
          "date": "Tue 31 Jan 2023 11:36",
          "username": "\t\t\t\tmikeshop\t\t\t",
          "content": "It said least latency. Writing via DAX has higher latency than writing directly to dynamodb.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791055,
          "date": "Sat 28 Jan 2023 21:20",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "A DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.A DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778185,
          "date": "Mon 16 Jan 2023 20:55",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX.<br><br>DynamoDB Accelerator (DAX) is a service that provides a fully managed, in-memory cache for DynamoDB tables. By creating a two-node DAX cluster, the solution will be able to handle a large number of read and write requests with low latency. Configuring the application to read and write data by using DAX will enable the trading platform to take advantage of DAX's in-memory cache, resulting in improved performance.<br><br>Option B, C and D are not correct because they all recommend using DAX only for reading data or writing data which will reduce the performance improvement, also having a single node DAX cluster will not ensure high availability for the trading platform as it has a single point of failure.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778016,
          "date": "Mon 16 Jan 2023 18:35",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "A: two nodes with R/W through DAX",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#151",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB.  The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application.<br><br>The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day.<br><br>A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability.<br><br>Which combination of steps will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#151",
          "answers": [
            {
              "choice": "<p>A. Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Move the application frontend to a static website that is hosted on Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Change all the backend EC2 instances to Spot Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851841,
          "date": "Mon 27 Mar 2023 09:11",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "BE makes the most sense here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 836583,
          "date": "Sat 11 Mar 2023 23:32",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Burstable because peak performance is needed at lunch time and its cost effective based on this - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html<br>S3 static website hosting is cost effective",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 822874,
          "date": "Sun 26 Feb 2023 19:57",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Burstable EC2 instances, also known as T instances, provide a baseline level of CPU performance with the ability to burst CPU usage when additional cycles are available. They are designed for workloads that do not require sustained high CPU performance but occasionally need more CPU power. Burstable instances can be a cost-effective option for workloads that have moderate CPU requirements but still require flexibility to handle occasional spikes in demand.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 795570,
          "date": "Thu 02 Feb 2023 00:01",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "The correct answer is B, E. <br>Option B of moving the frontend to a static website hosted on Amazon S3 will reduce the cost of running the frontend, as S3 is a lower cost storage option than EC2 instances.<br>Option E of deploying the backend Python application to general purpose burstable EC2 instances will ensure that the backend EC2 instances have the capacity to handle spikes in usage, as burstable instances are designed to handle unpredictable workloads. This will help to optimize the cost of running the backend, as burstable instances are less expensive than On-Demand instances and more cost-effective than Spot instances.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 790327,
          "date": "Sat 28 Jan 2023 06:37",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "B and E. <br>Option D is wrong. A spot instance is not appropriate for a production server.<br>By the way, I would like another option that mentions changing the backend Python API Gateway and Lambda because Option B mentions changing the frontend serverless. I think this question is a typical use case of the serverless architecture.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 787192,
          "date": "Wed 25 Jan 2023 03:07",
          "username": "\t\t\t\tvsk12\t\t\t",
          "content": "Correct answers are<br>B & E<br>Option B as S3 is a cost-effective storage solution for static websites.<br>Option E as burstable general-purpose instances provides a cost-effective solution for this kind of workload.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 778188,
          "date": "Mon 16 Jan 2023 21:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Move the application frontend to a static website that is hosted on Amazon S3.D.  Change all the backend EC2 instances to Spot Instances.<br><br>Step 1: Moving the application frontend to a static website that is hosted on Amazon S3 will reduce the cost and increase the scalability of the application. S3 is a highly scalable object storage service that can handle large amounts of data and traffic at a lower cost than running EC2 instances.<br><br>Step 2: Changing the backend EC2 instances to Spot Instances can help reduce cost without negatively affecting the application availability. Spot Instances allow customers to bid on unused Amazon EC2 capacity, which can result in significant cost savings. You can also use AWS Auto Scaling to automatically increase or decrease the number of Spot Instances based on the application's traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, C: Changing to compute optimized instances or using Elastic Beanstalk will not help reducing the cost, it will only change the instances type and not helping the cost optimization.<br>Option E: Deploying the backend Python application to general purpose burstable EC2 instances will not help reducing the cost, as it still using On-Demand instances.<br><br>It is important to note that using spot instances comes with the risk of instances being terminated when the spot price goes up. To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 778189,
          "date": "Mon 16 Jan 2023 21:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A, C: Changing to compute optimized instances or using Elastic Beanstalk will not help reducing the cost, it will only change the instances type and not helping the cost optimization.<br>Option E: Deploying the backend Python application to general purpose burstable EC2 instances will not help reducing the cost, as it still using On-Demand instances.<br><br>It is important to note that using spot instances comes with the risk of instances being terminated when the spot price goes up. To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777864,
          "date": "Mon 16 Jan 2023 16:28",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "BE are correct<br>A: Compute optimized instance is expensive than burstable instance<br>B: S3 hosted static web server is cheaper<br>C: Not save money<br>D: Spot instance affect availibility<br>E: Burstable EC2 is cheaper<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779144,
          "date": "Tue 17 Jan 2023 18:33",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "To mitigate this risk, you could use the EC2 Auto Scaling group with a combination of on-demand and spot instances. This way, if a spot instance is terminated, the Auto Scaling group can automatically replace it with an on-demand instance to ensure the application is always available.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#152",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate.<br><br>The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.<br><br>Which solution will provide the MOST cost-effective setup for the platform?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#152",
          "answers": [
            {
              "choice": "<p>A. Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale up the DB instance manually during peaks.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852616,
          "date": "Tue 28 Mar 2023 02:31",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "capacity reservations do not offer discounts. D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 851845,
          "date": "Mon 27 Mar 2023 09:14",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 836653,
          "date": "Sun 12 Mar 2023 02:14",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Out of some research initially against B, had to choose B because of this - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html<br>On Demand capacity reservations can be done anytime, so before events they can reserve and after events they can release to save costs<br>From above link - <br><br>Events — you can create Capacity Reservations before your business-critical events to ensure that you can scale when you need to.<br><br>\\\"You can create Capacity Reservations at any time, without entering into a one-year or three-year term commitment. The capacity becomes available and billing starts as soon as the Capacity Reservation is provisioned in your account. When you no longer need the capacity assurance, cancel the Capacity Reservation to release the capacity and to stop incurring charges. You can also use the billing discounts offered by Savings Plans and Regional Reserved Instances to reduce the cost of a Capacity Reservation.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 834391,
          "date": "Thu 09 Mar 2023 21:43",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "spot instances never a good answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 816237,
          "date": "Tue 21 Feb 2023 06:38",
          "username": "\t\t\t\tryansmithnz\t\t\t",
          "content": "Surely B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 808971,
          "date": "Wed 15 Feb 2023 01:36",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "agree with B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 805902,
          "date": "Sun 12 Feb 2023 03:41",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "A is not good because the DB will be underutilized (1yr RI to meet the _predicted peak_). You need a reliable on-demand on event dates. There is little incentive but more downside of unreliability if you choose Spots on event dates.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795575,
          "date": "Thu 02 Feb 2023 00:08",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "Agree with zhangyu20000",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792302,
          "date": "Mon 30 Jan 2023 03:14",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "I agree with zhangyu20000",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 790342,
          "date": "Sat 28 Jan 2023 07:05",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Option A, C and D are wrong. They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.<br>https://aws.amazon.com/savingsplans/compute-pricing/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 784003,
          "date": "Sun 22 Jan 2023 08:04",
          "username": "\t\t\t\tpitakk\t\t\t",
          "content": "I think it's D.  They use Savings Plans for the predicted base load. They scale on Spot during peaks. They manually scale up DB as they know the event dates - scaling out read replicas only won't help with writes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>never build real time system on spot instance, spot instance is good for no-real time data processing, not good for web server</li><li>The company is developing new application features to run on Amazon EKS with AWS Fargate.<br>So any solution that deal with EC2 for EKS are not correct. ACD all deal with EC2.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 788223,
          "date": "Thu 26 Jan 2023 01:08",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "never build real time system on spot instance, spot instance is good for no-real time data processing, not good for web server",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784591,
          "date": "Sun 22 Jan 2023 19:44",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "The company is developing new application features to run on Amazon EKS with AWS Fargate.<br>So any solution that deal with EC2 for EKS are not correct. ACD all deal with EC2.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778190,
          "date": "Mon 16 Jan 2023 21:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year.<br><br>This solution provides the most cost-effective setup for the platform by combining the cost savings of reserved instances with the flexibility of spot instances. By purchasing Standard Reserved Instances for the baseline load of the EKS cluster, the company can save money on the cost of running the instances. For the infrequent high peaks in demand, the company can scale the cluster with Spot Instances, which are significantly cheaper than on-demand instances. Finally, purchasing 1-year All Upfront Reserved Instances for the database will help meet the predicted peak load for the year and will help save more cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No, the another key point is \\\"The company is developing new application features to run on Amazon EKS with AWS Fargate\\\" -&gt; It mean the do not want manage EC2. B is correct</li><li>The company is developing new application features to run on Amazon EKS with AWS Fargate. This means customer should not manage EC2 at all. So ACD all incorrect</li><li>Option B, C and D all use a combination of reserved instances, savings plans and spot instances but they don't fully utilize the cost savings of reserved instances and savings plans.<br>Option B uses On-demand Capacity Reservations which are not cost-effective as they are more expensive than spot instances.<br>Option C and D both use manually scaling the DB instance during peaks which is a labor-intensive process and can cause delays in handling the peak demand.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 795573,
          "date": "Thu 02 Feb 2023 00:07",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "No, the another key point is \\\"The company is developing new application features to run on Amazon EKS with AWS Fargate\\\" -> It mean the do not want manage EC2. B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779214,
          "date": "Tue 17 Jan 2023 19:42",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "The company is developing new application features to run on Amazon EKS with AWS Fargate. This means customer should not manage EC2 at all. So ACD all incorrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778192,
          "date": "Mon 16 Jan 2023 21:05",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option B, C and D all use a combination of reserved instances, savings plans and spot instances but they don't fully utilize the cost savings of reserved instances and savings plans.<br>Option B uses On-demand Capacity Reservations which are not cost-effective as they are more expensive than spot instances.<br>Option C and D both use manually scaling the DB instance during peaks which is a labor-intensive process and can cause delays in handling the peak demand.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777871,
          "date": "Mon 16 Jan 2023 16:38",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B is correct. Compute saving plan will also cover Fargate<br>A: use spot instance is not reliable<br>CD: manually scale up DB",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#153",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application.<br><br>Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message.<br><br>A solutions architect creates an Amazon S3 bucket as the first step in the process.<br><br>Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: ACD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#153",
          "answers": [
            {
              "choice": "<p>A. Upload static informational content to the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new CloudFront distribution. Set the S3 bucket as the origin.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. During the weekly maintenance, configure Elastic Beanstalk to serve traffic from the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 778197,
          "date": "Mon 16 Jan 2023 21:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Upload static informational content to the S3 bucket.C.  Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI).D.  During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete.<br><br>Step 1: The solutions architect should upload static informational content to the S3 bucket, this content will be shown to the users when the application is down for maintenance.<br><br>Step 2: The solutions architect should set the S3 bucket as a second origin in the original CloudFront distribution. To keep the S3 bucket secure, the solutions architect should configure the distribution and the S3 bucket to use an origin access identity (OAI). This will ensure that only CloudFront has access to the S3 bucket.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Step 3: During the weekly maintenance, the solutions architect should edit the default cache behavior of the CloudFront distribution to use the S3 origin. This will redirect all incoming traffic to the S3 bucket and show the static informational content to the users. Once the maintenance is complete, the solutions architect should revert the change back to the original Elastic Beanstalk origin.<br><br>Option B: Creating a new CloudFront distribution and setting the S3 bucket as the origin is unnecessary and could cause confusion for the users.<br>Option E: During the weekly maintenance, creating a cache behavior for the S3 origin on the new distribution is unnecessary, it is more complex and prone to human error.<br>Option F: Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not necessary because CloudFront is already being used as the web request server.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 778198,
          "date": "Mon 16 Jan 2023 21:12",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Step 3: During the weekly maintenance, the solutions architect should edit the default cache behavior of the CloudFront distribution to use the S3 origin. This will redirect all incoming traffic to the S3 bucket and show the static informational content to the users. Once the maintenance is complete, the solutions architect should revert the change back to the original Elastic Beanstalk origin.<br><br>Option B: Creating a new CloudFront distribution and setting the S3 bucket as the origin is unnecessary and could cause confusion for the users.<br>Option E: During the weekly maintenance, creating a cache behavior for the S3 origin on the new distribution is unnecessary, it is more complex and prone to human error.<br>Option F: Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not necessary because CloudFront is already being used as the web request server.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 851852,
          "date": "Mon 27 Mar 2023 09:20",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "ACD is the best fit",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 801306,
          "date": "Tue 07 Feb 2023 20:00",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "About E, the lowest possible value for the \\\"Origin Priority\\\" field in AWS CloudFront is 1",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 792307,
          "date": "Mon 30 Jan 2023 03:17",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "ACD is correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: ACD"
        },
        {
          "id": 777883,
          "date": "Mon 16 Jan 2023 16:43",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ABD is correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ACD is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779234,
          "date": "Tue 17 Jan 2023 19:56",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "ACD is correct",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#154",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN.<br><br>The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users.<br><br>A solutions architect needs to simplify this process to minimize disruption to users.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#154",
          "answers": [
            {
              "choice": "<p>A. Directly modify the environment variables of the published Lambda function version. Use the SLATEST version to test image processing parameters.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851855,
          "date": "Mon 27 Mar 2023 09:22",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create a Lambda function alias.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 795578,
          "date": "Thu 02 Feb 2023 00:20",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "D is correct<br>By using a function alias, the custom application invokes the latest version of the Lambda function without the need to modify the application code every time the company updates the image processing parameters. This reduces the risk of causing interruptions for users.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778200,
          "date": "Mon 16 Jan 2023 21:15",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D.  Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing.<br><br>Creating a Lambda function alias allows the solutions architect to change the version of the Lambda function that the alias points to without modifying the client application. This eliminates the need for frequent updates to the custom application and minimizes disruption to users. The solutions architect can test different parameters by using different versions of the function and reconfigure the alias to point to the new version after validating results. This allows the company to update the image processing parameters without affecting the users.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A: Directly modifying the environment variables of the published Lambda function version would cause all clients to use the updated environment variables immediately and would not allow for testing.<br>Option B: Using DynamoDB to store image processing parameters increases complexity and operational overhead, and it would not eliminate the need for updating the custom application.<br>Option C: Directly coding the image processing parameters within the Lambda function and publishing new versions would not eliminate the need for updating the custom application.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778201,
          "date": "Mon 16 Jan 2023 21:15",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Option A: Directly modifying the environment variables of the published Lambda function version would cause all clients to use the updated environment variables immediately and would not allow for testing.<br>Option B: Using DynamoDB to store image processing parameters increases complexity and operational overhead, and it would not eliminate the need for updating the custom application.<br>Option C: Directly coding the image processing parameters within the Lambda function and publishing new versions would not eliminate the need for updating the custom application.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777891,
          "date": "Mon 16 Jan 2023 16:47",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#155",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain.<br><br>Which solution will meet these requirements with the LEAST effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#155",
          "answers": [
            {
              "choice": "<p>A. Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB.  Use a geolocation routing policy to route traffic based on user location.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Place a Network Load Balancer (NLB) in front of the ALMigrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocation routing policy to route traffic based on user location.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851859,
          "date": "Mon 27 Mar 2023 09:23",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 838501,
          "date": "Tue 14 Mar 2023 03:29",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "No, an apex domain cannot use CNAME records in AWS. This is because of the way DNS resolution works. A CNAME record specifies an alias for a domain name, which points to the canonical name of another domain. However, the DNS standard does not allow CNAME records for apex domains, as they should only have A or AAAA records.<br><br>When you try to create a CNAME record for an apex domain in AWS Route 53, you will receive an error message indicating that the record set type is not valid for the apex domain. To work around this limitation, you can use an alias record instead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778204,
          "date": "Mon 16 Jan 2023 21:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "C.  Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.<br><br>This solution meets the requirements with the least effort because it uses AWS Global Accelerator, which automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies. It also eliminates the need to create a CNAME record for the apex domain to point to the ALB or NLB's IP address, which can be less efficient and less reliable.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB.  Use a geolocation routing policy to route traffic based on user location.<br>While this solution uses Route 53 and geolocation routing, it requires manual configuration and maintenance of the routing policy and could introduce additional latency as traffic is routed through the ALB first.<br>B.  Place a Network Load Balancer (NLB) in front of the ALB.  Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocation routing policy to route traffic based on user location.<br>This solution is similar to the first one, but it uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB). It has the same downsides as the first solution.</li><li>D.  Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.<br><br>This solution uses Amazon API Gateway and AWS Lambda to route traffic, but the round-robin method is not the best way to ensure optimal performance and availability for a multi-region deployment. Additionally, routing traffic through a Lambda function can introduce additional latency.<br><br>AWS Global Accelerator is a more efficient solution that automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778206,
          "date": "Mon 16 Jan 2023 21:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB.  Use a geolocation routing policy to route traffic based on user location.<br>While this solution uses Route 53 and geolocation routing, it requires manual configuration and maintenance of the routing policy and could introduce additional latency as traffic is routed through the ALB first.<br>B.  Place a Network Load Balancer (NLB) in front of the ALB.  Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB's static IP address. Use a geolocation routing policy to route traffic based on user location.<br>This solution is similar to the first one, but it uses a Network Load Balancer (NLB) instead of an Application Load Balancer (ALB). It has the same downsides as the first solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D.  Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.<br><br>This solution uses Amazon API Gateway and AWS Lambda to route traffic, but the round-robin method is not the best way to ensure optimal performance and availability for a multi-region deployment. Additionally, routing traffic through a Lambda function can introduce additional latency.<br><br>AWS Global Accelerator is a more efficient solution that automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778207,
          "date": "Mon 16 Jan 2023 21:21",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D.  Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API's URL.<br><br>This solution uses Amazon API Gateway and AWS Lambda to route traffic, but the round-robin method is not the best way to ensure optimal performance and availability for a multi-region deployment. Additionally, routing traffic through a Lambda function can introduce additional latency.<br><br>AWS Global Accelerator is a more efficient solution that automatically routes traffic to the optimal endpoint based on health and geography, eliminating the need for manual configuration or additional routing policies.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778035,
          "date": "Mon 16 Jan 2023 18:44",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "C is correct<br>ABD all have CNAME record that is not allowed for apex domain",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#156",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes.<br><br>A solutions architect needs to simplify the deployment of the solution and optimize for code reuse.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#156",
          "answers": [
            {
              "choice": "<p>A. Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 811101,
          "date": "Thu 16 Feb 2023 22:05",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "Don't understand why so many people are choosing B.  Read up. A container image cannot be used with Lambda layers. That means A B C are out instantly. Its literally one of the first things they mention about Lamba layers. Answer is D and ABC simply impossible to configure.<br><br>https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/</li><li>B suggests deploying the shared libraries and custom classes to a Docker image, uploading it to Amazon Elastic Container Registry (Amazon ECR), creating a Lambda layer that uses the Docker image as the source, and deploying the API's Lambda functions as Zip packages. Configuring the packages to use the Lambda layer simplifies deployment, and the Docker image allows for code reuse. This option takes advantage of the built-in features provided by AWS API Gateway and Lambda, making it the optimal solution.</li><li>The requirement is code reuse: <br>https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/<br>Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 835242,
          "date": "Fri 10 Mar 2023 17:50",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812151,
          "date": "Fri 17 Feb 2023 18:06",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "B suggests deploying the shared libraries and custom classes to a Docker image, uploading it to Amazon Elastic Container Registry (Amazon ECR), creating a Lambda layer that uses the Docker image as the source, and deploying the API's Lambda functions as Zip packages. Configuring the packages to use the Lambda layer simplifies deployment, and the Docker image allows for code reuse. This option takes advantage of the built-in features provided by AWS API Gateway and Lambda, making it the optimal solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The requirement is code reuse: <br>https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/<br>Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 819290,
          "date": "Thu 23 Feb 2023 15:30",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The requirement is code reuse: <br>https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/<br>Lambda functions packaged as container images do not support adding Lambda layers to the function configuration. However, there are a number of solutions to use the functionality of Lambda layers with container images. You take on the responsible for packaging your preferred runtimes and dependencies as a part of the container image during the build process.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 857174,
          "date": "Fri 31 Mar 2023 16:19",
          "username": "\t\t\t\tAsagumo\t\t\t",
          "content": "This page is in Japanese.<br>https://michimani.net/post/aws-create-lambda-layers-with-docker/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 854501,
          "date": "Wed 29 Mar 2023 16:07",
          "username": "\t\t\t\tfabu\t\t\t",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 851866,
          "date": "Mon 27 Mar 2023 09:31",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D seems a better choice. Docker images can be used to package and deploy Lambda functions directly, but not for Lambda layers.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842716,
          "date": "Sat 18 Mar 2023 12:40",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "B.  Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837339,
          "date": "Sun 12 Mar 2023 20:35",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Lambda layers to package the common code and save it in ECR as a docker image and refer it from actual lambda function.<br>By using Lambda Layers, the shared libraries and custom classes can be reused across multiple Lambda functions, simplifying the deployment and management of the serverless API. The Lambda Layer can also be versioned, making it easy to update and manage changes to the shared code.<br><br>Additionally, the use of Lambda Layers can help reduce the size of the Lambda function packages, which can result in faster deployment times and lower costs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837126,
          "date": "Sun 12 Mar 2023 15:51",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "AFAIK it is either-or. Either one uses a docker images or Lambda Layers as zip files. I may overlook something but I cannot see a way to have Lambda as zip together with libraries in a container.<br>To put it simple: a Lambda Layer is a zip file and not a docker container. <br>You can use a container instead of Lambda Layers - have a container image created from a cascade ofdifferent docker layers(image from ... image from ... image from).<br>That makes D the only valid option .",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 836019,
          "date": "Sat 11 Mar 2023 14:14",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "D is correct, <br>B is a overhead and is not supported in aws console",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834745,
          "date": "Fri 10 Mar 2023 09:21",
          "username": "\t\t\t\tsambb\t\t\t",
          "content": "From my understanding, a lambda layer is a zip, not a docker image or a container. This exludes A, B and C.  The D is valid because the whole package with the shared libs and the code is a single docker image, this would work.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 822890,
          "date": "Sun 26 Feb 2023 20:26",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "An AWS Lambda Layer is a distribution mechanism for libraries, custom runtimes, or other function dependencies. With Lambda Layers, you can manage in a central place your in-house or third-party code that you want to share across multiple functions or even multiple teams. By providing this functionality, Lambda Layers allow you to reduce the size of your deployment package, simplify your development process, and enable code reuse across multiple functions.<br><br>When you create a layer, you can specify runtime dependencies that your functions require, and then attach those dependencies to your functions at runtime. This allows you to write less code and reuse the same code across multiple functions, and simplifies the process of managing and updating the dependencies of your functions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 797760,
          "date": "Sat 04 Feb 2023 09:59",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/lambda-layer-simulated-docker/#:~:text=(Optional)%20To%20use%20the%20Docker,package%20without%20creating%20a%20layer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795586,
          "date": "Thu 02 Feb 2023 00:37",
          "username": "\t\t\t\ttatdatpham\t\t\t",
          "content": "\\\"use several shared libraries and custom classes\\\" => Use Lambda layer to optimize code reuse.<br>=> A & B are matched but A is saving Image into S3, not good. Should use ECR.<br>So the answer is B",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792319,
          "date": "Mon 30 Jan 2023 03:30",
          "username": "\t\t\t\tzozza2023\t\t\t",
          "content": "After reading comment , I guess B makes more sens than D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 790645,
          "date": "Sat 28 Jan 2023 15:07",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "Option A, B and C are wrong. An AWS Lambda Layer does not support a Docker image or a deployed container as the source.<br>https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html<br>https://aws.amazon.com/blogs/compute/working-with-lambda-layers-and-extensions-in-container-images/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778210,
          "date": "Mon 16 Jan 2023 21:23",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "B.  Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.<br><br>This solution will simplify the deployment of the solution and optimize for code reuse as it utilizes Lambda Layers to share common code and dependencies. By using Amazon Elastic Container Registry (ECR) to store the Docker image, it allows for easy management and versioning of the shared libraries and custom classes. This way the common code can be reused across multiple Lambda functions and are only deployed once.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A.  Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.<br><br>This solution is similar to option B, but it uses S3 bucket to store the Docker image instead of Amazon Elastic Container Registry (ECR). Using S3 for storing the image may be less secure and less manageable than using ECR.<br>C.  Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.<br><br>This solution is using a more complex service (ECS) and launch type (Fargate) which is not necessary, it will also increase the complexity of the deployment process and make it harder to manage.</li><li>D.  Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.<br><br>This solution is using a single image to contain all the shared libraries, custom classes, and the code for the API's Lambda functions, which makes it harder to manage and update the shared libraries and custom classes. It also increases the size of the image and make it harder to reuse the common code.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778212,
          "date": "Mon 16 Jan 2023 21:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "A.  Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the Lambda layer.<br><br>This solution is similar to option B, but it uses S3 bucket to store the Docker image instead of Amazon Elastic Container Registry (ECR). Using S3 for storing the image may be less secure and less manageable than using ECR.<br>C.  Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API's Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer.<br><br>This solution is using a more complex service (ECS) and launch type (Fargate) which is not necessary, it will also increase the complexity of the deployment process and make it harder to manage.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D.  Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.<br><br>This solution is using a single image to contain all the shared libraries, custom classes, and the code for the API's Lambda functions, which makes it harder to manage and update the shared libraries and custom classes. It also increases the size of the image and make it harder to reuse the common code.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778213,
          "date": "Mon 16 Jan 2023 21:24",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "D.  Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.<br><br>This solution is using a single image to contain all the shared libraries, custom classes, and the code for the API's Lambda functions, which makes it harder to manage and update the shared libraries and custom classes. It also increases the size of the image and make it harder to reuse the common code.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 777228,
          "date": "Mon 16 Jan 2023 01:58",
          "username": "\t\t\t\tzhangyu20000\t\t\t",
          "content": "B: is correct. use Lambda layer. Layer source is docker images on ECR",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#157",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images.<br><br>The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory's internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers.<br><br>How should the company deploy the ML model to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#157",
          "answers": [
            {
              "choice": "<p>A. Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851881,
          "date": "Mon 27 Mar 2023 09:40",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "The ML model is run locally, so it can still provide feedback when the internet is down.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837347,
          "date": "Sun 12 Mar 2023 20:45",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Offline operation: AWS IoT Greengrass supports offline operation by enabling devices to continue processing data even when they are disconnected from the internet.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837105,
          "date": "Sun 12 Mar 2023 15:20",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Quote \\\"The company must be able to provide this feedback even if the factory's internet connectivity is down\\\"<br>So everything that needs internet can be ignored. Leaves D. <br>While there is a lot of garbage text about how they process date with SargeMaker, the question only asks for a solution to detect failures in the equipment. Amazon Monitron does this plus it can work even when internet is down.<br><br>All other options provide solutions for things, the question didn't ask for and/or already in place and need internet.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 804173,
          "date": "Fri 10 Feb 2023 10:28",
          "username": "\t\t\t\tAppon\t\t\t",
          "content": "https://aws.amazon.com/blogs/machine-learning/anomaly-detection-with-amazon-sagemaker-edge-manager-using-aws-iot-greengrass-v2/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 803990,
          "date": "Fri 10 Feb 2023 06:20",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "The point is how to offload ML workloads to the local.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802373,
          "date": "Wed 08 Feb 2023 19:16",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Monitron is something different",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796232,
          "date": "Thu 02 Feb 2023 17:34",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "this is taking about detecting defects from an image that is taken from a camera. I would go for running a ML model on IoT greengras pc and transfer it to IoT core, then store it in s3 bucket, which can be called by api function via lambda to send it to users. <br>option D would monitor only sensor data of machines.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796193,
          "date": "Thu 02 Feb 2023 16:57",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "Amazon Monitron is a machine-learning based end-to-end condition monitoring system that detects potential failures within equipment. You can use it to implement a predictive maintenance program and reduce lost productivity from unplanned machine downtime. Amazon Monitron includes purpose-built sensors to capture vibration and temperature data, as well as gateways to automatically transfer data to the AWS Cloud. It also comes with an application in two versions. The mobile application handles system setup, analytics, and notiﬁcation when tracking equipment conditions. The web application provides all the same functions as the mobile app except setup. Reliability managers can quickly deploy Amazon Monitron to track the machine health of industrial equipment, such as such as bearings, motors, gearboxes, and pumps, without any development work or specialized training.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is wrong, D is correct.</li><li>B is correct.<br>AWS IoT Greengrass enables ML inference locally using models that are created, trained, and optimized in the cloud using Amazon SageMaker, AWS Deep Learning AMI, or AWS Deep Learning Containers, and deployed on the edge devices</li><li>when do you take the exam man i would like to see if everything is still valid after you test</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799892,
          "date": "Mon 06 Feb 2023 16:19",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "B is wrong, D is correct.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 797416,
          "date": "Fri 03 Feb 2023 22:52",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "B is correct.<br>AWS IoT Greengrass enables ML inference locally using models that are created, trained, and optimized in the cloud using Amazon SageMaker, AWS Deep Learning AMI, or AWS Deep Learning Containers, and deployed on the edge devices<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>when do you take the exam man i would like to see if everything is still valid after you test</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 800044,
          "date": "Mon 06 Feb 2023 19:02",
          "username": "\t\t\t\tyoungprinceton\t\t\t",
          "content": "when do you take the exam man i would like to see if everything is still valid after you test",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#158",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case.<br><br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#158",
          "answers": [
            {
              "choice": "<p>A. Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Application Discovery Service to import the CMDB data to perform an analysis.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 799502,
          "date": "Mon 06 Feb 2023 08:46",
          "username": "\t\t\t\tZZ5\t\t\t",
          "content": "B<br>https://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/<br>Build a business case with AWS Migration Evaluator<br>The foundation for a successful migration starts with a defined business objective (for example, growth or new offerings). In order to enable the business drivers, the established business case must then be aligned to a technical capability (increased security and elasticity). AWS Migration Evaluator (formerly known as TSO Logic) can help you meet these objectives.<br><br>To get started, you can choose to upload exports from third-party tools such as Configuration Management Database (CMDB) or install a collector agent to monitor. You will receive an assessment after data collection, which includes a projected cost estimate and savings of running your on-premises workloads in the AWS Cloud. This estimate will provide a summary of the projected costs to re-host on AWS based on usage patterns. It will show the breakdown of costs by infrastructure and software licenses. With this information, you can make the business case and plan next steps.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 851884,
          "date": "Mon 27 Mar 2023 09:44",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B is the best fit",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837392,
          "date": "Sun 12 Mar 2023 22:11",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "The AWS Migration Evaluator works by analyzing data about your current on-premises environment, including servers, storage, networking, and applications. It then provides a report that outlines the recommended AWS services and configurations that best match your existing infrastructure and applications. This report includes a detailed cost analysis that estimates the total cost of running your applications in the AWS cloud.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822892,
          "date": "Sun 26 Feb 2023 20:39",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Migration Evaluator is a complimentary service to create data-driven assessments and business cases for AWS cloud planning and migration.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821860,
          "date": "Sat 25 Feb 2023 23:01",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "B is right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809281,
          "date": "Wed 15 Feb 2023 09:27",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "B <br>Free service, focus on cost of migration",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 808315,
          "date": "Tue 14 Feb 2023 12:38",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "B - Evaluator",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 805922,
          "date": "Sun 12 Feb 2023 04:23",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "The big hint is business case. So Migration Evaluator.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802388,
          "date": "Wed 08 Feb 2023 19:25",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I think it's B, which is free, while D requires servers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796197,
          "date": "Thu 02 Feb 2023 17:02",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "D<br>https://aws.amazon.com/application-discovery/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is correct.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799359,
          "date": "Mon 06 Feb 2023 04:11",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#159",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL.<br><br>The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#159",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL's deny list.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy AWS Shield Advanced in addition to AWS WAF.  Add the ALB as a protected resource.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server's subnet route table for any IP addresses that activate the alarm.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851886,
          "date": "Mon 27 Mar 2023 09:45",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Deploy AWS Shield Advanced in addition to AWS WAF. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837431,
          "date": "Sun 12 Mar 2023 22:46",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "AWS Shield Advanced is focused on protecting against DDoS attacks, while AWS WAF is focused on protecting against web exploits. However, both services can be used together to provide comprehensive protection for your applications.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818481,
          "date": "Wed 22 Feb 2023 22:51",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "as long as i know or think to know, shield advanced, does nothing by default and needs to be configured.<br><br>https://docs.aws.amazon.com/waf/latest/developerguide/enable-ddos-prem.html<br>https://docs.aws.amazon.com/waf/latest/developerguide/getting-started-ddos.html<br>Note<br>Shield Advanced doesn't automatically protect your resources after you subscribe. You must specify the resources you want Shield Advanced to protect configure the protections.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 805927,
          "date": "Sun 12 Feb 2023 04:31",
          "username": "\t\t\t\tmoota\t\t\t",
          "content": "According to ChatGPT, the ff are what you get with Advanced over Basic.<br><br>AWS Shield Advanced is a paid version of the service that provides additional protection against large scale and sophisticated DDoS attacks. This version includes all the features of the Basic version, but with additional capabilities such as 24/7 availability, a dedicated DDoS response team, and advanced attack analytics and reporting. Additionally, AWS Shield Advanced provides access to advanced DDoS protection and mitigation capabilities, such as the ability to customize protections for specific application requirements, and to mitigate attacks more quickly and effectively.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802584,
          "date": "Wed 08 Feb 2023 22:49",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Reading more about option B, I pick B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802583,
          "date": "Wed 08 Feb 2023 22:45",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Not sure. With WAF you get Shield, which hs DDoS. Not sure the the Shield dvnced gives you much more.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796198,
          "date": "Thu 02 Feb 2023 17:04",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "AWS Shield is a managed distributed denial of service (DDoS) protection service that safeguards applications running on AWS. It provides dynamic detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield: Standard and Advanced.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#160",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions.<br><br>Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.<br><br>Which combination of steps will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#160",
          "answers": [
            {
              "choice": "<p>A. Add another Region to the Aurora MySQL DB cluster<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add another Region to each table in the Aurora MySQL DB cluster<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Convert the existing DynamoDB table to a global table by adding another Region to its configuration<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 842516,
          "date": "Sat 18 Mar 2023 07:25",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "A.  Add another Region to the Aurora MySQL DB clusterD.  Convert the existing DynamoDB table to a global table by adding another Region to its configuration",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 841193,
          "date": "Thu 16 Mar 2023 17:49",
          "username": "\t\t\t\ttestingaws123\t\t\t",
          "content": "Badly written question: <br>\\\"The RTO and RPO must be no more than a few minutes each.\\\"<br>What is few minutes mean? May be it is 2-3 min for me, may be it is 9-10 min for you.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837564,
          "date": "Mon 13 Mar 2023 03:41",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "A solves multi region for DB layer. but question also asks for minimum RPO and RTO which means quick uptime of application in case of failure which is possible with backups.<br>https://aws.amazon.com/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide<br>/CrossRegionAccountCopyAWS.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Hint given is - Aurora MySQL engine version supports a global database which makes this possible - https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2021/03/08/Aurora-Global-database-2.jpg</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 837565,
          "date": "Mon 13 Mar 2023 03:43",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Hint given is - Aurora MySQL engine version supports a global database which makes this possible - https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2021/03/08/Aurora-Global-database-2.jpg",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796444,
          "date": "Thu 02 Feb 2023 22:08",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "A and D",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 796309,
          "date": "Thu 02 Feb 2023 19:07",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "you can create only db's not global tables, hence A and D",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AD"
        }
      ]
    },
    {
      "question_id": "#161",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB.  The company has multiple target groups and uses path-based routing to forward requests based on the URL path.<br><br>The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#161",
          "answers": [
            {
              "choice": "<p>A. Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB.  Add the ALB IP addresses to the firewall appliance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB.  Update the clients to connect to the NLB.  Delete the ALB Add the NLB IP addresses to the firewall appliance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB.  Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 797799,
          "date": "Sat 04 Feb 2023 11:02",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "it uses path-based routing to forward requests based on the URL path",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837590,
          "date": "Mon 13 Mar 2023 04:26",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/<br>Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. This decreases potential points of failure in your network and increases availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://youtu.be/-j2smz_VCH4?t=1270<br>ALB (L7)- HTTP, HTTPS<br>NLB (L4)- TCP, UDP, TLS traffic<br>GWLB(L3)- IP traffic and 3rd party Appliances</li><li>AWS Gateway Load Balancer (GWLB) can terminate TLS traffic. GWLB supports SSL/TLS offloading, which means that it can terminate SSL/TLS connections from clients and then forward the decrypted traffic to backend servers over HTTP or HTTPS.</li><li>I think main question is can it support static IP address which is needed by the firmware to waitlist it?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 837603,
          "date": "Mon 13 Mar 2023 04:43",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://youtu.be/-j2smz_VCH4?t=1270<br>ALB (L7)- HTTP, HTTPS<br>NLB (L4)- TCP, UDP, TLS traffic<br>GWLB(L3)- IP traffic and 3rd party Appliances<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Gateway Load Balancer (GWLB) can terminate TLS traffic. GWLB supports SSL/TLS offloading, which means that it can terminate SSL/TLS connections from clients and then forward the decrypted traffic to backend servers over HTTP or HTTPS.</li><li>I think main question is can it support static IP address which is needed by the firmware to waitlist it?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837621,
          "date": "Mon 13 Mar 2023 05:06",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "AWS Gateway Load Balancer (GWLB) can terminate TLS traffic. GWLB supports SSL/TLS offloading, which means that it can terminate SSL/TLS connections from clients and then forward the decrypted traffic to backend servers over HTTP or HTTPS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think main question is can it support static IP address which is needed by the firmware to waitlist it?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 854199,
          "date": "Wed 29 Mar 2023 11:06",
          "username": "\t\t\t\tMickey321\t\t\t",
          "content": "I think main question is can it support static IP address which is needed by the firmware to waitlist it?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829404,
          "date": "Sat 04 Mar 2023 21:51",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "The question is confusing. If I understood this question correctly, I do this almost every day, and I don't use those terms. Basically, the solution is inserting an NLB in front of the existing ALB, so traffic is Client->FW->NLB->ALB->EC2. Another point is that fixing the public IP address makes a lot of sense, but not the private one, like in this case. Every time you create an ALB 2 or more ENI are created and you have the IP addresses there.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 816902,
          "date": "Tue 21 Feb 2023 18:10",
          "username": "\t\t\t\tdummy1777\t\t\t",
          "content": "B.  Create a Network Load Balancer (NLB). Associate the NLB with one static IP address in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALB.  Add the NLB IP address to the firewall appliance. Update the clients to connect to the NLB. <br><br>In this solution, the company would create a new Network Load Balancer (NLB) and associate it with a single static IP address in multiple Availability Zones. The NLB would then be configured with an ALB-type target group and the existing ALB would be added to this target group. The IP address of the NLB would be added to the on-premises firewall appliance, and the clients would be updated to connect to the NLB. <br><br>This solution allows the on-premises firewall to whitelist the IP address of the NLB, which is a fixed, predictable address that can be easily identified and managed by the firewall appliance. Additionally, the NLB provides higher network throughput and lower latency than an ALB, which may be beneficial for the application's performance.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804034,
          "date": "Fri 10 Feb 2023 07:27",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "The background is the below.<br>- The company is using ALB features and must keep them.<br>- The new on-premise firewall needs a static IP address of the ALB as the next hop.<br>- However, ALB cannot have a static IP address.<br>So the point is how ALB can have a static IP address endpoint.<br><br>Solution<br>https://aws.amazon.com/premiumsupport/knowledge-center/alb-static-ip/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 803689,
          "date": "Thu 09 Feb 2023 22:28",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "Sure about this one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 798032,
          "date": "Sat 04 Feb 2023 15:31",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "https://aws.amazon.com/jp/premiumsupport/knowledge-center/alb-static-ip/<br>ALB cannnot use static ip, so it must be set NLB in front of ALB. <br>Correct answer is C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>miss type.<br>correct answer is B. <br>There is no need to delete ALB. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820222,
          "date": "Fri 24 Feb 2023 08:45",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "miss type.<br>correct answer is B. <br>There is no need to delete ALB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796310,
          "date": "Thu 02 Feb 2023 19:09",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "ALB's cannot use static IP's. NLB's have static IP's<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No should be B cause it uses path-based routing to forward requests based on the URL path</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797798,
          "date": "Sat 04 Feb 2023 11:01",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "No should be B cause it uses path-based routing to forward requests based on the URL path",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#162",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.<br><br>The company needs a solution that will prevent internet traffic from directly accessing the ALB. <br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#162",
          "answers": [
            {
              "choice": "<p>A. Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Associate the existing web ACL with the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add a security group rule to the ALB to allow only the various CloudFront IP address ranges.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851905,
          "date": "Mon 27 Mar 2023 10:05",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C.  Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798038,
          "date": "Sat 04 Feb 2023 15:34",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/LocationsOfEdgeServers.html<br>AWS managed prefix list is more recommended.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797839,
          "date": "Sat 04 Feb 2023 11:47",
          "username": "\t\t\t\tExamTopix01\t\t\t",
          "content": "C https://aws.amazon.com/blogs/news/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 797807,
          "date": "Sat 04 Feb 2023 11:11",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#163",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.<br><br>A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#163",
          "answers": [
            {
              "choice": "<p>A. Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851916,
          "date": "Mon 27 Mar 2023 10:16",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Create an AUTH token. Store the token in AWS Secrets Manager.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 838581,
          "date": "Tue 14 Mar 2023 06:29",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Redis CLI has AUTH command as a feature to SET/ROTATE strategies<br>https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 832841,
          "date": "Wed 08 Mar 2023 11:51",
          "username": "\t\t\t\tZek\t\t\t",
          "content": "B seems right. <br>To enable authentication on an existing Redis server, call the ModifyReplicationGroup API operation. Call ModifyReplicationGroup with the --auth-token parameter as the new token and the --auth-token-update-strategy with the value ROTATE. <br><br>After the modification is complete, the cluster supports the AUTH token specified in the auth-token parameter in addition to supporting connecting without authentication. Enabling authentication is only supported on Redis servers with encryption in transit (TLS) enabled.<br><br>https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 808330,
          "date": "Tue 14 Feb 2023 12:51",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "As per https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 799120,
          "date": "Sun 05 Feb 2023 20:38",
          "username": "\t\t\t\tharleydog\t\t\t",
          "content": "You have to create a new cluster, otherwise the the cluster supports the AUTH token specified and supports connecting without authentication.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797822,
          "date": "Sat 04 Feb 2023 11:23",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "Previously, you needed to set up authentication for ElastiCache for Redis clusters using Redis user passwords or store the password in AWS Secrets Manager or on a third-party secrets management tool. However, in large organizations that host many applications, passwords can often become out of sync when it comes time to rotate the password. IAM authentication provides a streamlined security posture by allowing access management from a centralized service. With IAM authentication, ElastiCache users can use their IAM identities when connecting to their Redis clusters",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796326,
          "date": "Thu 02 Feb 2023 19:28",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#164",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.<br><br>Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.<br><br>Which solution will meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#164",
          "answers": [
            {
              "choice": "<p>A. Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the launch template Auto Scaling group to increase the number of placement groups.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Update the launch template to use a larger instance type.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851919,
          "date": "Mon 27 Mar 2023 10:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B.  Create a new launch template version that uses attribute-based instance type selection.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 838587,
          "date": "Tue 14 Mar 2023 06:44",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-type-requirements.html#use-attribute-based-instance-type-selection-prerequisites",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822918,
          "date": "Sun 26 Feb 2023 21:12",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "Confused between B and D , will choose B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821867,
          "date": "Sat 25 Feb 2023 23:23",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "b is correct <br><br>https://aws.amazon.com/blogs/aws/new-attribute-based-instance-type-selection-for-ec2-auto-scaling-and-ec2-fleet/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 804018,
          "date": "Fri 10 Feb 2023 07:02",
          "username": "\t\t\t\tetechsystem_ts\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 796364,
          "date": "Thu 02 Feb 2023 20:24",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "launch config is replaced by launch template hence is not advisible, option A rulled out. C is wrong because launch template cannot be updated. D is also wrong for the same reason",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#165",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.<br><br>During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.<br><br>Which solution will meet these requirements with the LEAST amount of effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#165",
          "answers": [
            {
              "choice": "<p>A. Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 796336,
          "date": "Thu 02 Feb 2023 19:43",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "C: <br>Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance, scalable storage for compute workloads. Powered by Lustre, the world's most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851924,
          "date": "Mon 27 Mar 2023 10:25",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "I think b is a better choice its far easier to implement.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 848553,
          "date": "Thu 23 Mar 2023 19:33",
          "username": "\t\t\t\tArnaud92\t\t\t",
          "content": "I think it's B. <br>C can be a good solution but at the end of the answer, there is a mention to mount with NFS. Lustre is not using NFS. So I'll go for B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I agree with this.<br>FSx is going to be mounted as ”Lusture\\\"<br>https://docs.aws.amazon.com/fsx/latest/LustreGuide/mount-fs-auto-mount-onreboot.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 850192,
          "date": "Sat 25 Mar 2023 15:22",
          "username": "\t\t\t\tmarcoforexam\t\t\t",
          "content": "I agree with this.<br>FSx is going to be mounted as ”Lusture\\\"<br>https://docs.aws.amazon.com/fsx/latest/LustreGuide/mount-fs-auto-mount-onreboot.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 839517,
          "date": "Wed 15 Mar 2023 03:55",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "I choose C between B and C after reading the feature of seamless linking the S3 file system with Lustre - https://aws.amazon.com/fsx/lustre/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839497,
          "date": "Wed 15 Mar 2023 03:30",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Option C (Configure Amazon FSx for Lustre with an import and export policy) may be overkill for the given workload and may require additional management overhead, resulting in additional effort and cost.On the other hand, Option B (Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store) provides a straightforward solution that requires minimal changes to the processing server. By using the S3 File Gateway, the processing server can continue to generate and modify files on the local file system, and changes can be automatically synced to S3 via the gateway. Customers can then access the files directly from S3 within 30 minutes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Above may be wrong-<br>Option C could be the solution that will meet the requirements with the least amount of effort. By linking the new file system to an S3 bucket, the company can store, retrieve, and modify documents with fast local access. The Lustre client can be installed on an Amazon EC2 instance, which will provide the processing server with the fast local access it requires. This solution requires minimal changes to the processing server and will enable the files to be available to the public for download within 30 minutes.<br><br>Option B may not be a good solution because it involves setting up an S3 File Gateway and configuring a file share, which would require additional infrastructure and configuration. This solution also involves using NFS, which may not provide the level of performance required for the processing server.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 839506,
          "date": "Wed 15 Mar 2023 03:42",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Above may be wrong-<br>Option C could be the solution that will meet the requirements with the least amount of effort. By linking the new file system to an S3 bucket, the company can store, retrieve, and modify documents with fast local access. The Lustre client can be installed on an Amazon EC2 instance, which will provide the processing server with the fast local access it requires. This solution requires minimal changes to the processing server and will enable the files to be available to the public for download within 30 minutes.<br><br>Option B may not be a good solution because it involves setting up an S3 File Gateway and configuring a file share, which would require additional infrastructure and configuration. This solution also involves using NFS, which may not provide the level of performance required for the processing server.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834453,
          "date": "Thu 09 Mar 2023 22:44",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://aws.amazon.com/fsx/lustre/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 829422,
          "date": "Sat 04 Mar 2023 22:21",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "If I understand the question correctly, the processing server needs direct access to the files and is located on-premises. In this case, you would need to use a file storage solution that provides local access to the files from the processing server.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818708,
          "date": "Thu 23 Feb 2023 03:52",
          "username": "\t\t\t\tYowie351\t\t\t",
          "content": "Fast access and linking to S3. Answer is C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 815357,
          "date": "Mon 20 Feb 2023 15:20",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "D might be leasat amount of effort...",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#166",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.<br><br>The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#166",
          "answers": [
            {
              "choice": "<p>A. Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 804114,
          "date": "Fri 10 Feb 2023 09:19",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "The trigger is that the central user service deletes a user in the DynamoDB table. The DynamoDB Streams meets the requirement.<br>https://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/<br>Option B is wrong. There is no feature named DynamoDB event notifications.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Correct, the point they want to make is central user service is system of record. You should not be deleting from other services until you delete from DynamoDB. </li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 853331,
          "date": "Tue 28 Mar 2023 16:10",
          "username": "\t\t\t\tAmac1979\t\t\t",
          "content": "Correct, the point they want to make is central user service is system of record. You should not be deleting from other services until you delete from DynamoDB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809388,
          "date": "Wed 15 Feb 2023 11:53",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "C seems correct; SQS is one queue to one microservice, could not find anything on dynamodb event notifications.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 851930,
          "date": "Mon 27 Mar 2023 10:34",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "I think A is the best fit here due to the phrasing of the question around who is deleting users.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 842531,
          "date": "Sat 18 Mar 2023 07:55",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "C.  Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839557,
          "date": "Wed 15 Mar 2023 06:14",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "C,D are wrong. Flaw is \\\"Company deletes user? \\\" Nope, Its central service does deletion.<br>B is wrong because, there is no such thing as DynamoDB events feature, Only Streams can enable CRUD operations and eventually could trigger lambda, Refer here : https://stackoverflow.com/questions/53857304/how-to-get-notified-when-a-aws-dynamo-db-entry-are-updated<br>https://dynobase.dev/dynamodb-triggers/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 834458,
          "date": "Thu 09 Mar 2023 22:48",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "A and B require changes to all the existing microservices",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830485,
          "date": "Mon 06 Mar 2023 02:43",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "I believe the question is looking for the fan-out pattern. Basically is one event that notify multiple endpoints. SQS cannot be used for that because only one micro-service in this case will get the message and the rest will never delete the user. EventBridge could potentially do it too but you need to targets all micro services. The method I see used commonly for this is SNS. I don't like how B is written, in theory we can do DynamoDB Streams=>Lambda=>SNS=>micro-services.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822926,
          "date": "Sun 26 Feb 2023 21:19",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "This solution doesnt require continous polling of the queue",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 810340,
          "date": "Thu 16 Feb 2023 08:00",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "DynamoDB stream can trigger the change of specific item, so there is no noise.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808336,
          "date": "Tue 14 Feb 2023 13:00",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "C looks correct - EventBridge rule has the ability to inform multiple services",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805487,
          "date": "Sat 11 Feb 2023 18:57",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "C seems correct, the others don't. The ones that refer to SQS would not work because the SQS notification can be consumed by a single microservice. With A, you would be getting all changes through DynamoDB Streams, which is too much noise.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805462,
          "date": "Sat 11 Feb 2023 18:28",
          "username": "\t\t\t\tIlk\t\t\t",
          "content": "SQS delivers the queue item to only one service. Only SNS can do it.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 799122,
          "date": "Sun 05 Feb 2023 20:45",
          "username": "\t\t\t\tharleydog\t\t\t",
          "content": "We can't use a queue, the message will only get processed once. I don't think we can generate event notifications on DynamoDB table updates.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>So the only option is C</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 841838,
          "date": "Fri 17 Mar 2023 10:17",
          "username": "\t\t\t\taqiao\t\t\t",
          "content": "So the only option is C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#167",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a web application in a VPC.  The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF. <br><br>An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#167",
          "answers": [
            {
              "choice": "<p>A. Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Allocate an Elastic IP address. Assign the Elastic IP address to the ALProvide the Elastic IP address to the customer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution's DNS name to determine the distribution's public IP address. Provide the IP address to the customer.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 851934,
          "date": "Mon 27 Mar 2023 10:38",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C.  Create an AWS Global Accelerator standard accelerator.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839583,
          "date": "Wed 15 Mar 2023 07:08",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).<br>https://stackoverflow.com/questions/55236806/how-to-assign-elastic-ip-to-application-load-balancer-in-aws<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This feature allows you to migrate your applications to AWS without requiring your partners and customers to change their IP address whitelists. (which could be used in WAF)<br>BYOIP - Bring your own IP https://aws.amazon.com/blogs/networking-and-content-delivery/using-bring-your-own-ip-addresses-byoip-with-global-accelerator/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839584,
          "date": "Wed 15 Mar 2023 07:14",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "This feature allows you to migrate your applications to AWS without requiring your partners and customers to change their IP address whitelists. (which could be used in WAF)<br>BYOIP - Bring your own IP https://aws.amazon.com/blogs/networking-and-content-delivery/using-bring-your-own-ip-addresses-byoip-with-global-accelerator/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822932,
          "date": "Sun 26 Feb 2023 21:24",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/alb-static-ip/<br><br>Can assisng Static IP to ALB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 804223,
          "date": "Fri 10 Feb 2023 11:42",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html<br>Option A is wrong. AWS WAF does not support associating with NLB. <br>https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html<br>Option B is wrong. An ALB does not support an Elastic IP address.<br>https://aws.amazon.com/elasticloadbalancing/features/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 798332,
          "date": "Sat 04 Feb 2023 21:04",
          "username": "\t\t\t\tjojom19980\t\t\t",
          "content": "......<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C<br>WAF cannot be assoicated with NLB</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 799777,
          "date": "Mon 06 Feb 2023 14:43",
          "username": "\t\t\t\tCloudInfrastructures\t\t\t",
          "content": "C<br>WAF cannot be assoicated with NLB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798077,
          "date": "Sat 04 Feb 2023 15:59",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "NLB cannot be used when WAF is used",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798073,
          "date": "Sat 04 Feb 2023 15:55",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "static IP can made below method.<br>・NLB (replace NLB from ALB)<br>・NLB + ALB<br>・global accelarator + ALB<br>・original load balancer (ex. made by EC2 + nginx)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 797861,
          "date": "Sat 04 Feb 2023 12:14",
          "username": "\t\t\t\tExamTopix01\t\t\t",
          "content": "A<br>https://aws.amazon.com/jp/premiumsupport/knowledge-center/alb-static-ip/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry C<br>https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797864,
          "date": "Sat 04 Feb 2023 12:18",
          "username": "\t\t\t\tExamTopix01\t\t\t",
          "content": "Sorry C<br>https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.alb-accelerator.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796450,
          "date": "Thu 02 Feb 2023 22:19",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "This solution meets the requirement with the least operational overhead, as it only requires the allocation of an Elastic IP address, assignment to the ALB, and providing the address to the customer. The other options involve configuring additional services, which can increase operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796376,
          "date": "Thu 02 Feb 2023 20:36",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "this option has the least admin effort. A has more admin effort, B is not possible, D will not give static IP address",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 796136,
          "date": "Thu 02 Feb 2023 15:47",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "B will works",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#168",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.<br><br>Which combination of steps will meet these requirements? (Choose three.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CDF</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#168",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs. respectively.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a guardrail from the management account to detect EBS encryption.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>F. Create a guardrail for the production OU to detect EBS encryption.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852043,
          "date": "Mon 27 Mar 2023 13:17",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "CDF seems the best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CDF"
        },
        {
          "id": 840512,
          "date": "Thu 16 Mar 2023 03:26",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "When you enable controls on an organizational unit (OU) that is registered with AWS Control Tower, preventive controls apply to all member accounts under the OU, enrolled and unenrolled. Detective controls apply to enrolled accounts only.<br>https://docs.aws.amazon.com/controltower/latest/userguide/controls.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CDF"
        },
        {
          "id": 816924,
          "date": "Tue 21 Feb 2023 18:26",
          "username": "\t\t\t\tdummy1777\t\t\t",
          "content": "B.  Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively.D.  Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.F.  Create a control for the production OU to detect EBS encryption.<br><br>By creating a new AWS Control Tower landing zone, the company can create OUs for accounts and add them to the appropriate production and development OUs. This will enable centralized governance and enforce consistent policies and best practices. The company can then invite existing accounts to join the organization in AWS Organizations and create SCPs to ensure compliance. Finally, the company can create a control for the production OU to detect EBS encryption, ensuring that encryption at rest is enforced in production accounts.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 806703,
          "date": "Sun 12 Feb 2023 20:01",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Answer is CDF<br>https://docs.aws.amazon.com/controltower/latest/userguide/controls.html<br>https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The artifact for this control is AWS Config rule and AWS Config rules cannot be deployed using AWS CloudFormation StackSets.</li><li>moderator, delete above as the statement is incorrect that I posted, don't approve post.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CDF"
        },
        {
          "id": 812364,
          "date": "Fri 17 Feb 2023 21:53",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The artifact for this control is AWS Config rule and AWS Config rules cannot be deployed using AWS CloudFormation StackSets.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>moderator, delete above as the statement is incorrect that I posted, don't approve post.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812371,
          "date": "Fri 17 Feb 2023 21:59",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "moderator, delete above as the statement is incorrect that I posted, don't approve post.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806072,
          "date": "Sun 12 Feb 2023 08:38",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "In F, guardrails are proposed to detect. Guardrails don't detect but prevent.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I found this, and after further reading I vote for CDF: https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: ABD"
        },
        {
          "id": 815366,
          "date": "Mon 20 Feb 2023 15:26",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I found this, and after further reading I vote for CDF: https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804872,
          "date": "Sat 11 Feb 2023 00:59",
          "username": "\t\t\t\toatif\t\t\t",
          "content": "CloudTower and guard rails are custom built for this kind of situation",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CDF"
        },
        {
          "id": 804234,
          "date": "Fri 10 Feb 2023 12:11",
          "username": "\t\t\t\tUntamables\t\t\t",
          "content": "https://docs.aws.amazon.com/controltower/latest/userguide/controls.html<br>https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#ebs-enable-encryption<br>AWS is now transitioning the previous term 'guardrail' new term 'control'.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: CDF"
        },
        {
          "id": 797869,
          "date": "Sat 04 Feb 2023 12:24",
          "username": "\t\t\t\tExamTopix01\t\t\t",
          "content": "CDF<br>https://docs.aws.amazon.com/controltower/latest/userguide/guardrails.html",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#169",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application.<br><br>The solution must meet the following objectives:<br><br>• Application tier: RPO of 2 minutes. RTO of 30 minutes<br>• Database tier: RPO of 5 minutes. RTO of 30 minutes<br><br>The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#169",
          "answers": [
            {
              "choice": "<p>A. Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB.  Update DNS records to point to CloudFront.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 840539,
          "date": "Thu 16 Mar 2023 04:56",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "DRS includes EC2 instances as well not just data related as offered by DLM or Backup<br><br>Q: What operating systems and applications are supported by AWS DRS?<br>A: You can use AWS DRS to recover all of your applications and databases that run on supported Windows and Linux operating system versions. This includes critical databases such as Oracle, MySQL, and SQL Server, and enterprise applications such as SAP.<br><br>AWS Elastic Disaster Recovery (DRS) vs AWS DLM vs AWS Backup <br><br>You should use DLM when you want to automate the creation, retention, and deletion of EBS snapshots. You should use AWS Backup to manage and monitor backups across the AWS services you use, including EBS volumes, from a single place.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806080,
          "date": "Sun 12 Feb 2023 08:50",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I agree it's A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 796380,
          "date": "Thu 02 Feb 2023 20:44",
          "username": "\t\t\t\tbititan\t\t\t",
          "content": "its understood that others cannot meet the RTO and RPO requirements, because restore from back can take time based on the size of the data",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 796135,
          "date": "Thu 02 Feb 2023 15:44",
          "username": "\t\t\t\tschalke04\t\t\t",
          "content": "DRS should fulfill the requirements",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#170",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#170",
          "answers": [
            {
              "choice": "<p>A. Purchase AWS Business Support or AWS Enterprise Support for the account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Turn on AWS Trusted Advisor and review any “Low Utilization Amazon EC2 Instances” recommendations.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852047,
          "date": "Mon 27 Mar 2023 13:23",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "CD is right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 840541,
          "date": "Thu 16 Mar 2023 05:02",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Not B because, Trusted Advisor is available for Enterprise support only which is not cheap and the SA needs to cost optimize here. CPU, memory, and network relate to Compute so D for sure. C will enable to know how much actual memory/CPU is needed for instances and SA can provision based on cw logs",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 821885,
          "date": "Sat 25 Feb 2023 23:57",
          "username": "\t\t\t\tsaurabh1805\t\t\t",
          "content": "trusted advisor does not take memory in consideration hence CD is right answer.<br><br>https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 810964,
          "date": "Thu 16 Feb 2023 18:48",
          "username": "\t\t\t\tCloudFloater\t\t\t",
          "content": "D,OK..but, why not B trusted advisor rather than C cloudwatch ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Memory taken by the os is almost always 100% - but most of it caches, buffers. To get you need the actually used memory by applications. This is number is os specific(need to ask the os how the memory is used: only caches or actual use?) and as such can't be gathered from the virtualizer. So you need an agent for that.</li><li>seems like you need cloud watch agent installed in order to check memory parameter<br>Note:<br>To have Compute Optimizer analyze the memory utilization of your instances, install the CloudWatch agent on your instances. Enabling Compute Optimizer to analyze memory utilization data for your instances provides an additional measurement of data that further improves Compute Optimizer's recommendations<br>https://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836836,
          "date": "Sun 12 Mar 2023 10:15",
          "username": "\t\t\t\thobokabobo\t\t\t",
          "content": "Memory taken by the os is almost always 100% - but most of it caches, buffers. To get you need the actually used memory by applications. This is number is os specific(need to ask the os how the memory is used: only caches or actual use?) and as such can't be gathered from the virtualizer. So you need an agent for that.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819468,
          "date": "Thu 23 Feb 2023 18:16",
          "username": "\t\t\t\trtgfdv3\t\t\t",
          "content": "seems like you need cloud watch agent installed in order to check memory parameter<br>Note:<br>To have Compute Optimizer analyze the memory utilization of your instances, install the CloudWatch agent on your instances. Enabling Compute Optimizer to analyze memory utilization data for your instances provides an additional measurement of data that further improves Compute Optimizer's recommendations<br>https://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810032,
          "date": "Wed 15 Feb 2023 23:03",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "CD according to https://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 808352,
          "date": "Tue 14 Feb 2023 13:13",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "For Memory - CLoudwatch and <br>Compute Optimizer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What about the other metrics?<br>CPU and network metrics.</li><li>CD is correct, cloudwatch agents supports the metrics mentioned. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 809661,
          "date": "Wed 15 Feb 2023 15:52",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "What about the other metrics?<br>CPU and network metrics.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CD is correct, cloudwatch agents supports the metrics mentioned. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812387,
          "date": "Fri 17 Feb 2023 22:15",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "CD is correct, cloudwatch agents supports the metrics mentioned. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#171",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#171",
          "answers": [
            {
              "choice": "<p>A. Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852048,
          "date": "Mon 27 Mar 2023 13:25",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "C for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 840547,
          "date": "Thu 16 Mar 2023 05:26",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822940,
          "date": "Sun 26 Feb 2023 21:38",
          "username": "\t\t\t\tkiran15789\t\t\t",
          "content": "https://www.automat-it.com/post/backup-aws-codecommit",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 812431,
          "date": "Fri 17 Feb 2023 22:57",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "C is correct, AWS Backup does not backup code commit as a source.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 812184,
          "date": "Fri 17 Feb 2023 18:53",
          "username": "\t\t\t\tlunt\t\t\t",
          "content": "B is wrong > AWS Backup does not support CodeCommit as source.<br>A is out.<br>C is right.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 810046,
          "date": "Wed 15 Feb 2023 23:30",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809767,
          "date": "Wed 15 Feb 2023 17:55",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "It says backup so I think B is the answer:<br>B.  Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Changing to C, thanks.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 812411,
          "date": "Fri 17 Feb 2023 22:40",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Changing to C, thanks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808358,
          "date": "Tue 14 Feb 2023 13:19",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-code-in-multiple-aws-regions-using-aws-codepipeline-aws-codecommit-and-aws-codebuild.html<br><br>https://medium.com/geekculture/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-39f6b8fcefd2",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#172",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company's marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#172",
          "answers": [
            {
              "choice": "<p>A. Instruct each business unit to add a unique secondary CIDR range to the business unit's VPC.  Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account's VPC.  Create an AWS Site-to-Site VPN connection between the marketing team and each business unit's VPC.  Perform NAT where necessary.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB.  Activate IAM authorization for the API. Grant access to the accounts of the other business units.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852049,
          "date": "Mon 27 Mar 2023 13:26",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Private link",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 840568,
          "date": "Thu 16 Mar 2023 06:24",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Networking & Content Delivery blog -<br>https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-with-overlapping-ip-ranges/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809765,
          "date": "Wed 15 Feb 2023 17:54",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "With AWS PrivateLink, the marketing team can create an endpoint service to share their internal application with other accounts securely using private IP addresses. They can grant permission to specific AWS accounts to connect to the service and create interface VPC endpoints in the other accounts to access the application by using private IP addresses. This option does not require any changes to the network of the other business units, and it does not require peering or NATing. This solution is both scalable and secure.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 808359,
          "date": "Tue 14 Feb 2023 13:20",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "Private link is the solution for IP Overlapping and Securely access the app between accounts",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#173",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to audit the security posture of a newly acquired AWS account. The company's data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#173",
          "answers": [
            {
              "choice": "<p>A. Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type “Access Analyzer Finding” with a filter for “isPublic: true.” Select the SNS topic as the EventBridge rule target.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EventBridge rule for the event type “Bucket-Level API Call via CloudTrail” with a filter for “PutBucketPolicy.” Select the SNS topic as the EventBridge rule target.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type “Config Rules Re-evaluation Status” with a filter for “NON_COMPLIANT.” Select the SNS topic as the EventBridge rule target.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852051,
          "date": "Mon 27 Mar 2023 13:28",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "B eventbirdge and access analyser",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 841530,
          "date": "Fri 17 Mar 2023 03:36",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Click on the \\\"Create rule\\\" button.<br><br>Enter a name for the rule and a brief description, if desired.<br><br>Under \\\"Define pattern\\\", select \\\"Event pattern\\\".<br><br>Select \\\"Custom pattern\\\".<br><br>In the \\\"Event pattern\\\" field, enter the following code:<br><br>{<br>\\\"source\\\": [\\\"aws.securityhub\\\"],<br>\\\"detail-type\\\": [\\\"Access Analyzer Finding\\\"],<br>\\\"detail\\\": {<br>\\\"findings\\\": [<br>{<br>\\\"isPublic\\\": [<br>true<br>]<br>}<br>]<br>}<br>}<br><br>This code will match all Access Analyzer Finding events where the \\\"isPublic\\\" field is set to \\\"true\\\".</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 841533,
          "date": "Fri 17 Mar 2023 03:39",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Click on the \\\"Create rule\\\" button.<br><br>Enter a name for the rule and a brief description, if desired.<br><br>Under \\\"Define pattern\\\", select \\\"Event pattern\\\".<br><br>Select \\\"Custom pattern\\\".<br><br>In the \\\"Event pattern\\\" field, enter the following code:<br><br>{<br>\\\"source\\\": [\\\"aws.securityhub\\\"],<br>\\\"detail-type\\\": [\\\"Access Analyzer Finding\\\"],<br>\\\"detail\\\": {<br>\\\"findings\\\": [<br>{<br>\\\"isPublic\\\": [<br>true<br>]<br>}<br>]<br>}<br>}<br><br>This code will match all Access Analyzer Finding events where the \\\"isPublic\\\" field is set to \\\"true\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814731,
          "date": "Mon 20 Feb 2023 03:07",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "B is the correct solution because it uses AWS Identity and Access Management Access Analyzer to continuously monitor access control configurations and detect whether any S3 buckets have been configured to be publicly accessible. When a publicly accessible bucket is detected, an Amazon EventBridge rule is triggered, and the SNS topic is notified with the finding.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 810413,
          "date": "Thu 16 Feb 2023 09:03",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "Access Analyzer is to assess the access policy.<br>https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/access-control-block-public-access.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809824,
          "date": "Wed 15 Feb 2023 19:03",
          "username": "\t\t\t\tCloudguy594\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/how-to-use-aws-iam-access-analyzer-api-to-automate-detection-of-public-access-to-aws-kms-keys/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809683,
          "date": "Wed 15 Feb 2023 16:25",
          "username": "\t\t\t\tmdijoux25\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-analyzer.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 808366,
          "date": "Tue 14 Feb 2023 13:26",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "D by elimination rule",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#174",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to assess a newly acquired company's portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.<br><br>The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#174",
          "answers": [
            {
              "choice": "<p>A. Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 808382,
          "date": "Tue 14 Feb 2023 13:47",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "First need to evaluate",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 852052,
          "date": "Mon 27 Mar 2023 13:32",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use migration evaluator",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809762,
          "date": "Wed 15 Feb 2023 17:49",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "C.  Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#175",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.<br><br>Which solution will meet these requirements while providing the FASTEST storage performance?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#175",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 808389,
          "date": "Tue 14 Feb 2023 13:54",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "EFS = Fastest storage performance compare to S3/EBS<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I vote B. <br>I think EBS is faster than S3/EBS.<br>https://www.msp360.com/resources/blog/amazon-s3-vs-ebs-vs-efs/</li><li>typo.<br>EBS faster than S3/EFS.</li><li>I just read the question refers to multiple AZs, so B is not an option.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810426,
          "date": "Thu 16 Feb 2023 09:19",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "I vote B. <br>I think EBS is faster than S3/EBS.<br>https://www.msp360.com/resources/blog/amazon-s3-vs-ebs-vs-efs/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>typo.<br>EBS faster than S3/EFS.</li><li>I just read the question refers to multiple AZs, so B is not an option.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810427,
          "date": "Thu 16 Feb 2023 09:20",
          "username": "\t\t\t\tmasssa\t\t\t",
          "content": "typo.<br>EBS faster than S3/EFS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I just read the question refers to multiple AZs, so B is not an option.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810937,
          "date": "Thu 16 Feb 2023 17:59",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I just read the question refers to multiple AZs, so B is not an option.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 852106,
          "date": "Mon 27 Mar 2023 14:46",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 851281,
          "date": "Sun 26 Mar 2023 18:52",
          "username": "\t\t\t\tramyaram\t\t\t",
          "content": "Keyword here is multiple small files and shared between multiple clusters",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 841555,
          "date": "Fri 17 Mar 2023 04:47",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "In the past, EBS can be attached only to one ec2 instance but not anymore but there are limitations like - it works only on io1/io2 instance types and many others as described here. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html<br>EFS has shareable storage <br><br>In terms of performance, Amazon EFS is optimized for workloads that require high levels of aggregate throughput and IOPS, whereas EBS is optimized for low-latency, random access I/O operations. Amazon EFS is designed to scale throughput and capacity automatically as your storage needs grow, while EBS volumes can be resized on demand.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 833838,
          "date": "Thu 09 Mar 2023 11:23",
          "username": "\t\t\t\tZek\t\t\t",
          "content": "I support A since their is a multi-AZ requirement.<br><br>https://repost.aws/questions/QUK2RANw1QTKCwpDUwCCI72A/efs-vs-ebs-mult-attach<br><br>EFS is also designed for high availability and high durability. To achieve these levels of availability and durability, EFS automatically replicates data within and across 3 Availability Zones, with no single points of failure. EBS multi-attach volumes can be used for clients within a single Availability Zone.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 830798,
          "date": "Mon 06 Mar 2023 13:45",
          "username": "\t\t\t\tSarutobi\t\t\t",
          "content": "When you have an EKS cluster and use the EBS that is local to the node, only Pods running on that node have access to the storage. If the node starts on any other Pod, it will potentially break. There are ways to fix this, but they are beyond this question. I believe we need shared fast storage here, so it should be S3 vs EFS the decision.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810936,
          "date": "Thu 16 Feb 2023 17:58",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "I've been reding here and there, and B does not seem that feasible, although if supported it would be faster than A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809760,
          "date": "Wed 15 Feb 2023 17:48",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Explanation: Amazon EFS provides shared file storage that is highly available and durable. It is an ideal solution to share files between containers running on multiple instances in a cluster. Mounting an Amazon EFS file system on each subnet provides a shared file system for multiple instances running in different Availability Zones. Additionally, AWS Backup provides automated backup and recovery of Amazon EFS file systems.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#176",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.<br><br>Which solution will meet these requirements with the LEAST ongoing operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#176",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852107,
          "date": "Mon 27 Mar 2023 14:47",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 841559,
          "date": "Fri 17 Mar 2023 04:57",
          "username": "\t\t\t\tGod_Is_Love\t\t\t",
          "content": "Amazon Connect is a cloud-based contact center service that allows you to set up a virtual call center for your business. It provides an easy-to-use interface for managing customer interactions through voice and chat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, to help you collect, store, and analyze customer data for insights into customer behavior and trends.<br><br>On the other hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engage with your customers across different channels, such as email, SMS, push notifications, and voice. It helps you create personalized campaigns based on user behavior and enables you to track user engagement and retention.<br><br>While both services allow you to communicate with your customers, they serve different purposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focused on marketing and engagement.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 809757,
          "date": "Wed 15 Feb 2023 17:48",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "The solution that will meet the company's requirements with the LEAST ongoing operational overhead and send two-way experience survey is to use Amazon Connect to replace the old call center hardware and use Amazon Pinpoint to send text message surveys to customers. Amazon Connect is a fully managed, cloud-based contact center service that is easy to set up and configure, while Amazon Pinpoint can be used to send text message surveys and gather responses. By using these services, the company can offload the operational overhead of running and maintaining the call center hardware and survey system to AWS.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808394,
          "date": "Tue 14 Feb 2023 14:02",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "https://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-two-way.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#177",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a call center by using Amazon Connect. The company's operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.<br><br>Which solution will provide DR with the LOWEST RTO?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#177",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 852518,
          "date": "Tue 28 Mar 2023 00:09",
          "username": "\t\t\t\tEshu2009\t\t\t",
          "content": "why not C?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 852109,
          "date": "Mon 27 Mar 2023 14:49",
          "username": "\t\t\t\tmfsec\t\t\t",
          "content": "D.  Provision a new Amazon Connect instance with all existing users and contact flows in a second Region.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 814510,
          "date": "Sun 19 Feb 2023 21:32",
          "username": "\t\t\t\tspd\t\t\t",
          "content": "D looks most appropriate",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 812448,
          "date": "Fri 17 Feb 2023 23:23",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "D is the better solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 811431,
          "date": "Fri 17 Feb 2023 05:42",
          "username": "\t\t\t\tnyxs_19\t\t\t",
          "content": "The solution that will provide DR with the LOWEST RTO (Recovery Time Objective) is option D. <br><br>Option D provisions a new Amazon Connect instance with all existing users and contact flows in a second Region. It also sets up an Amazon Route 53 health check for the URL of the Amazon Connect instance, an Amazon CloudWatch alarm for failed health checks, and an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. This option allows for the fastest recovery time because all the necessary components are already provisioned and ready to go in the second Region. In the event of a disaster, the failed health check will trigger the AWS Lambda function to deploy the CloudFormation template to provision the claimed phone numbers, which is the only missing component.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 809749,
          "date": "Wed 15 Feb 2023 17:41",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "B.  Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region will provide disaster recovery with the LOWEST Recovery Time Objective.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks for pointing that out,D is the better solution.</li><li>With D you can have a quicker reaction if you use high-resolution CloudWatch alarms that alert as soon as 10-second or 30-second periods. Additionally, contact flows are alredy there so you don't need to deploy when the error occurs.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 812447,
          "date": "Fri 17 Feb 2023 23:22",
          "username": "\t\t\t\tc73bf38\t\t\t",
          "content": "Thanks for pointing that out,D is the better solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810994,
          "date": "Thu 16 Feb 2023 19:14",
          "username": "\t\t\t\tMusk\t\t\t",
          "content": "With D you can have a quicker reaction if you use high-resolution CloudWatch alarms that alert as soon as 10-second or 30-second periods. Additionally, contact flows are alredy there so you don't need to deploy when the error occurs.",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    }
  ]
}
