var SAA_C03_Exam_001_100 = {
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": "#1",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB.  Each site has a high-speed Internet connection.<br>The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#1",
          "answers": [
            {
              "choice": "<p>A. Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690908,
          "date": "Mon 10 Oct 2022 10:55",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "S3 Transfer Acceleration is the best solution cz it's faster , good for high speed,Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I thoughtS3 Transfer Acceleration is based on Cross Region Repilication, I made a mistake.</li></ul>",
          "upvote_count": "26",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 692565,
          "date": "Wed 12 Oct 2022 04:16",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I thoughtS3 Transfer Acceleration is based on Cross Region Repilication, I made a mistake.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834836,
          "date": "Fri 10 Mar 2023 11:02",
          "username": "\t\t\t\tchen_0707\t\t\t",
          "content": "S3 Transfer Acceleration utilize AWS local entry point and internal network to optimize upload route and speed.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821028,
          "date": "Sat 25 Feb 2023 00:45",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "Transfer Acceleration works with s3 services and on site premise. It allowed faster speeds however it does add cost",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817873,
          "date": "Wed 22 Feb 2023 14:45",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "S3 Transfer Acceleration is the best solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 811923,
          "date": "Fri 17 Feb 2023 14:35",
          "username": "\t\t\t\tVictorn\t\t\t",
          "content": "A is correct.<br>B is close but it adds quite a lot of complexities.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807015,
          "date": "Mon 13 Feb 2023 04:25",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "A is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 795486,
          "date": "Wed 01 Feb 2023 21:14",
          "username": "\t\t\t\tacts268\t\t\t",
          "content": "Correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778413,
          "date": "Tue 17 Jan 2023 01:35",
          "username": "\t\t\t\tG3\t\t\t",
          "content": "ChatGPT concurs with B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The solution must minimize operational complexity.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785141,
          "date": "Mon 23 Jan 2023 09:50",
          "username": "\t\t\t\teltomon\t\t\t",
          "content": "The solution must minimize operational complexity.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773008,
          "date": "Thu 12 Jan 2023 01:56",
          "username": "\t\t\t\tTung234\t\t\t",
          "content": "A is the answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 771935,
          "date": "Wed 11 Jan 2023 02:12",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "A is the answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 769556,
          "date": "Sun 08 Jan 2023 16:08",
          "username": "\t\t\t\tknocs12\t\t\t",
          "content": "The answer is A.  I had a similar question from Tutorials dojo material which contains the correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766655,
          "date": "Thu 05 Jan 2023 14:26",
          "username": "\t\t\t\tshirleyson123\t\t\t",
          "content": "TESTING 123",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764689,
          "date": "Tue 03 Jan 2023 14:34",
          "username": "\t\t\t\tatwale44\t\t\t",
          "content": "i came on exam topics last minute before my exam and saw some questions here. i wished i had studied exam topics earlier i would have passed my exam i had 670. this question was in my exam which i wrote on dec 26th<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I studied these questions twice &amp; took the exam a few hours ago. I don't remember any questions from this material. Amazon must have updated the questions. I wish to pass.</li><li>Yes, very few questions were from this dump.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765268,
          "date": "Wed 04 Jan 2023 06:22",
          "username": "\t\t\t\tCarlosephy\t\t\t",
          "content": "I studied these questions twice & took the exam a few hours ago. I don't remember any questions from this material. Amazon must have updated the questions. I wish to pass.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes, very few questions were from this dump.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788888,
          "date": "Thu 26 Jan 2023 16:59",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Yes, very few questions were from this dump.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764378,
          "date": "Tue 03 Jan 2023 10:55",
          "username": "\t\t\t\tBharatGundubilli\t\t\t",
          "content": "S3 TA is best option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763753,
          "date": "Mon 02 Jan 2023 14:54",
          "username": "\t\t\t\tjainparag1\t\t\t",
          "content": "@goodmail has nailed it nicely.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759640,
          "date": "Wed 28 Dec 2022 10:36",
          "username": "\t\t\t\tdb23\t\t\t",
          "content": "S3 Transfer Acceleration is the best solution",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750243,
          "date": "Mon 19 Dec 2022 22:13",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "One solution that could meet these requirements is Option B: Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.<br><br>Option B involves uploading the data from each site to an S3 bucket in the closest Region, which can help to minimize transfer times and improve the speed of data aggregation. You can then use S3 Cross-Region Replication to replicate the objects from the origin bucket to the destination bucket in a different Region. Once the data is replicated, you can remove the data from the origin bucket to reduce storage costs.<br><br>Sincerely,<br>ChatGPT<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is exactly what transfer-acceleration (in Option A) does.<br>transfer-acceleration:<br>1. Transfer files to edge location<br>2. Forward the data to the S3 bucket in the target region.<br>* Is compatible with multipart upload, which is required for files &gt; 5GByte<br><br>Therefore Option A is the correct answer.</li><li>Question says - The solution must minimize operational complexity.<br>Answer is A</li><li>Option A involves turning on S3 Transfer Acceleration on the destination bucket and using multipart uploads to directly upload site data to the destination bucket. While this option could potentially work, it may not be the most efficient solution, as it would require transferring all of the data over the internet, which could be time-consuming and may not provide the fastest data aggregation.<br><br>Option C involves scheduling AWS Snowball Edge Storage Optimized device jobs to transfer the data from each site to the closest Region and then using S3 Cross-Region Replication to copy the objects to the destination bucket. While this option could potentially work, it may require more operational complexity to set up and manage the Snowball Edge devices.</li><li>S3 Transfer Acceleration service itself will make use of globally distributed edge locations in Amazon CloudFront. So this has similar meaning as \\\"Upload the data from each site to an S3 bucket in the closest Region\\\" of option B.  <br>Moreover, option A is a single step action (local --&gt; destination, middle step handled by AWS), while B is 2-step action (local --&gt; closest region bucket --&gt; destination), so option A shall be more appropriate.</li><li>Option D involves uploading the data from each site to an EC2 instance in the closest Region and storing it in an EBS volume. At regular intervals, you would take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. This option may not be the most efficient solution, as it would require transferring data over the internet multiple times and may require more operational complexity to set up and manage the EC2 instances and EBS volumes.<br><br>Overall, Option B is a simple and efficient solution for aggregating data from multiple global sites in a single S3 bucket.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 808022,
          "date": "Tue 14 Feb 2023 04:17",
          "username": "\t\t\t\tShlomiM\t\t\t",
          "content": "Option B is exactly what transfer-acceleration (in Option A) does.<br>transfer-acceleration:<br>1. Transfer files to edge location<br>2. Forward the data to the S3 bucket in the target region.<br>* Is compatible with multipart upload, which is required for files > 5GByte<br><br>Therefore Option A is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765991,
          "date": "Wed 04 Jan 2023 19:35",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Question says - The solution must minimize operational complexity.<br>Answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750255,
          "date": "Mon 19 Dec 2022 22:29",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A involves turning on S3 Transfer Acceleration on the destination bucket and using multipart uploads to directly upload site data to the destination bucket. While this option could potentially work, it may not be the most efficient solution, as it would require transferring all of the data over the internet, which could be time-consuming and may not provide the fastest data aggregation.<br><br>Option C involves scheduling AWS Snowball Edge Storage Optimized device jobs to transfer the data from each site to the closest Region and then using S3 Cross-Region Replication to copy the objects to the destination bucket. While this option could potentially work, it may require more operational complexity to set up and manage the Snowball Edge devices.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>S3 Transfer Acceleration service itself will make use of globally distributed edge locations in Amazon CloudFront. So this has similar meaning as \\\"Upload the data from each site to an S3 bucket in the closest Region\\\" of option B.  <br>Moreover, option A is a single step action (local --&gt; destination, middle step handled by AWS), while B is 2-step action (local --&gt; closest region bucket --&gt; destination), so option A shall be more appropriate.</li><li>Option D involves uploading the data from each site to an EC2 instance in the closest Region and storing it in an EBS volume. At regular intervals, you would take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. This option may not be the most efficient solution, as it would require transferring data over the internet multiple times and may require more operational complexity to set up and manage the EC2 instances and EBS volumes.<br><br>Overall, Option B is a simple and efficient solution for aggregating data from multiple global sites in a single S3 bucket.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 761779,
          "date": "Fri 30 Dec 2022 09:40",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "S3 Transfer Acceleration service itself will make use of globally distributed edge locations in Amazon CloudFront. So this has similar meaning as \\\"Upload the data from each site to an S3 bucket in the closest Region\\\" of option B.  <br>Moreover, option A is a single step action (local --> destination, middle step handled by AWS), while B is 2-step action (local --> closest region bucket --> destination), so option A shall be more appropriate.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 750258,
          "date": "Mon 19 Dec 2022 22:30",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D involves uploading the data from each site to an EC2 instance in the closest Region and storing it in an EBS volume. At regular intervals, you would take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. This option may not be the most efficient solution, as it would require transferring data over the internet multiple times and may require more operational complexity to set up and manage the EC2 instances and EBS volumes.<br><br>Overall, Option B is a simple and efficient solution for aggregating data from multiple global sites in a single S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#2",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.<br>What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#2",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Athena directly with Amazon S3 to run the queries as needed.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690161,
          "date": "Sun 09 Oct 2022 14:21",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "Answer: C<br>https://docs.aws.amazon.com/athena/latest/ug/what-is.html<br>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I agree C is the answer</li><li>C is right.</li></ul>",
          "upvote_count": "23",
          "selected_answers": ""
        },
        {
          "id": 692564,
          "date": "Wed 12 Oct 2022 04:14",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I agree C is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 690542,
          "date": "Sun 09 Oct 2022 23:04",
          "username": "\t\t\t\ttt79\t\t\t",
          "content": "C is right.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828792,
          "date": "Sat 04 Mar 2023 10:40",
          "username": "\t\t\t\tRuhi02\t\t\t",
          "content": "Answer c",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817947,
          "date": "Wed 22 Feb 2023 15:50",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Answer is C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 811926,
          "date": "Fri 17 Feb 2023 14:40",
          "username": "\t\t\t\tVictorn\t\t\t",
          "content": "Amazon Athena<br><br>Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC.  You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 807017,
          "date": "Mon 13 Feb 2023 04:26",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "Answer: C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807016,
          "date": "Mon 13 Feb 2023 04:25",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "C is right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 766277,
          "date": "Thu 05 Jan 2023 07:00",
          "username": "\t\t\t\tenvest\t\t\t",
          "content": "IMO: on-demand with least overhead would mean automated serverless (e.g. schedule). Answer A lacks Spectrum, Answer C lacks Glue, but D has all necessary components & services (Glue, Spark, EMR serverless). But for simple log queries it takes a lot of serverless know how thought for big data and not logs. Considering this, I go with D. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You missed this part \\\"minimal changes to the existing architecture.\\\" There is a lot you have to implement for D. </li><li>low Oh with the use of EMR serverless (e.g. Athena): https://aws.amazon.com/athena/faqs/?nc=sn&amp;loc=6#:~:text=eliminate%20the%20operational%20overhead</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 802197,
          "date": "Wed 08 Feb 2023 16:48",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "You missed this part \\\"minimal changes to the existing architecture.\\\" There is a lot you have to implement for D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766296,
          "date": "Thu 05 Jan 2023 07:33",
          "username": "\t\t\t\tenvest\t\t\t",
          "content": "low Oh with the use of EMR serverless (e.g. Athena): https://aws.amazon.com/athena/faqs/?nc=sn&loc=6#:~:text=eliminate%20the%20operational%20overhead",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 754610,
          "date": "Sat 24 Dec 2022 01:10",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "It's C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750246,
          "date": "Mon 19 Dec 2022 22:19",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Answered by ChatGPT<br><br>The correct solution that the solutions architect should do to meet these requirements with the least amount of operational overhead is Option C: Use Amazon Athena directly with Amazon S3 to run the queries as needed.<br><br>Option C involves using Amazon Athena, which is a fully managed, serverless query service that allows you to analyze data stored in Amazon S3 using SQL. Athena is particularly well suited for analyzing JSON-formatted data, such as the log files in this case. You can use Athena to run on-demand queries against the log data in S3, without the need to set up any infrastructure or perform any data ingestion or transformation tasks.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Overall, Option C is the most straightforward and least operationally complex solution for analyzing the log files using SQL.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750247,
          "date": "Mon 19 Dec 2022 22:20",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Overall, Option C is the most straightforward and least operationally complex solution for analyzing the log files using SQL.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749506,
          "date": "Mon 19 Dec 2022 06:57",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "Athena analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC.  You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747424,
          "date": "Fri 16 Dec 2022 17:45",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Amazon Athena is an interactive query service that makes it easy to analyze data directly.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747116,
          "date": "Fri 16 Dec 2022 12:47",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "Its C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 739819,
          "date": "Fri 09 Dec 2022 06:40",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 735296,
          "date": "Sun 04 Dec 2022 18:33",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723653,
          "date": "Mon 21 Nov 2022 15:57",
          "username": "\t\t\t\tDrekorig\t\t\t",
          "content": "Athena allows query data stored in S3 with SQL",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723457,
          "date": "Mon 21 Nov 2022 13:19",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 711943,
          "date": "Sat 05 Nov 2022 19:18",
          "username": "\t\t\t\tpm2229\t\t\t",
          "content": "Serverless query service to perform analytics on S3.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#3",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.<br>Which solution meets these requirements with the LEAST amount of operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#3",
          "answers": [
            {
              "choice": "<p>A. Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690855,
          "date": "Mon 10 Oct 2022 10:05",
          "username": "\t\t\t\tude\t\t\t",
          "content": "aws:PrincipalOrgID Validates if the principal accessing the resource belongs to an account in your organization.<br>https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>the condition key aws:PrincipalOrgID can prevent the members who don't belong to your organization to access the resource</li></ul>",
          "upvote_count": "29",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 692567,
          "date": "Wed 12 Oct 2022 04:25",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "the condition key aws:PrincipalOrgID can prevent the members who don't belong to your organization to access the resource",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 711246,
          "date": "Fri 04 Nov 2022 16:38",
          "username": "\t\t\t\tNaneyerocky\t\t\t",
          "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html<br>Condition keys: AWS provides condition keys that you can query to provide more granular control over certain actions. <br>The following condition keys are especially useful with AWS Organizations:<br><br>aws:PrincipalOrgID – Simplifies specifying the Principal element in a resource-based policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization. Instead of listing all of the accounts that are members of an organization, you can specify the organization ID in the Condition element.<br><br>aws:PrincipalOrgPaths – Use this condition key to match members of a specific organization root, an OU, or its children. The aws:PrincipalOrgPaths condition key returns true when the principal (root user, IAM user, or role) making the request is in the specified organization path. A path is a text representation of the structure of an AWS Organizations entity.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 807018,
          "date": "Mon 13 Feb 2023 04:26",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 767811,
          "date": "Fri 06 Jan 2023 15:52",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "This is the least operationally overhead solution because it requires only a single configuration change to the S3 bucket policy, which will allow access to the bucket for all users within the organization. The other options require ongoing management and maintenance. Option B requires the creation and maintenance of organizational units for each department. Option C requires monitoring of specific CloudTrail events and updates to the S3 bucket policy based on those events. Option D requires the creation and maintenance of tags for each user that needs access to the bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750250,
          "date": "Mon 19 Dec 2022 22:24",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Answered by ChatGPT with an explanation.<br><br>The correct solution that meets these requirements with the least amount of operational overhead is Option A: Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.<br><br>Option A involves adding the aws:PrincipalOrgID global condition key to the S3 bucket policy, which allows you to specify the organization ID of the accounts that you want to grant access to the bucket. By adding this condition to the policy, you can limit access to the bucket to only users of accounts within the organization.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B involves creating organizational units (OUs) for each department and adding the aws:PrincipalOrgPaths global condition key to the S3 bucket policy. This option would require more operational overhead, as it involves creating and managing OUs for each department.<br><br>Option C involves using AWS CloudTrail to monitor certain events and updating the S3 bucket policy accordingly. While this option could potentially work, it would require ongoing monitoring and updates to the policy, which could increase operational overhead.</li><li>Option D involves tagging each user that needs access to the S3 bucket and adding the aws:PrincipalTag global condition key to the S3 bucket policy. This option would require you to tag each user, which could be time-consuming and could increase operational overhead.<br><br>Overall, Option A is the most straightforward and least operationally complex solution for limiting access to the S3 bucket to only users of accounts within the organization.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750251,
          "date": "Mon 19 Dec 2022 22:24",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B involves creating organizational units (OUs) for each department and adding the aws:PrincipalOrgPaths global condition key to the S3 bucket policy. This option would require more operational overhead, as it involves creating and managing OUs for each department.<br><br>Option C involves using AWS CloudTrail to monitor certain events and updating the S3 bucket policy accordingly. While this option could potentially work, it would require ongoing monitoring and updates to the policy, which could increase operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D involves tagging each user that needs access to the S3 bucket and adding the aws:PrincipalTag global condition key to the S3 bucket policy. This option would require you to tag each user, which could be time-consuming and could increase operational overhead.<br><br>Overall, Option A is the most straightforward and least operationally complex solution for limiting access to the S3 bucket to only users of accounts within the organization.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750252,
          "date": "Mon 19 Dec 2022 22:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D involves tagging each user that needs access to the S3 bucket and adding the aws:PrincipalTag global condition key to the S3 bucket policy. This option would require you to tag each user, which could be time-consuming and could increase operational overhead.<br><br>Overall, Option A is the most straightforward and least operationally complex solution for limiting access to the S3 bucket to only users of accounts within the organization.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749519,
          "date": "Mon 19 Dec 2022 07:14",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "use a new condition key, aws:PrincipalOrgID, in these policies to require all principals accessing the resource to be from an account (including the master account) in the organization. For example, let's say you have an Amazon S3 bucket policy and you want to restrict access to only principals from AWS accounts inside of your organization. To accomplish this, you can define the aws:PrincipalOrgID condition and set the value to your organization ID in the bucket policy. Your organization ID is what sets the access control on the S3 bucket. Additionally, when you use this condition, policy permissions apply when you add new accounts to this organization without requiring an update to the policy.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747427,
          "date": "Fri 16 Dec 2022 17:48",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "aws:PrincipalOrgID – Simplifies specifying the Principal element in a resource-based policy. This global key provides an alternative to listing all the account IDs for all AWS accounts in an organization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747119,
          "date": "Fri 16 Dec 2022 12:49",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "I think that LEAST is the key. So A!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 732676,
          "date": "Thu 01 Dec 2022 15:01",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "A is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723459,
          "date": "Mon 21 Nov 2022 13:20",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714289,
          "date": "Wed 09 Nov 2022 05:28",
          "username": "\t\t\t\tVTI_Training\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 713904,
          "date": "Tue 08 Nov 2022 15:41",
          "username": "\t\t\t\tSaiofy\t\t\t",
          "content": ".... and it's A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 711924,
          "date": "Sat 05 Nov 2022 18:54",
          "username": "\t\t\t\tpm2229\t\t\t",
          "content": "It's A, IAM now makes it easier for you to control access to your AWS resources by using the AWS organization of IAM principals (users and roles). You can use the aws:PrincipalOrgID condition key in your resource-based policies to more easily restrict access to IAM principals from accounts in your AWS organization.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706769,
          "date": "Fri 28 Oct 2022 23:50",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ans is A.  The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations.<br>https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704068,
          "date": "Tue 25 Oct 2022 19:39",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-organization-of-iam-principals/ Answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696440,
          "date": "Sun 16 Oct 2022 20:49",
          "username": "\t\t\t\tqueen101\t\t\t",
          "content": "AAAAAAAAAA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695607,
          "date": "Sat 15 Oct 2022 20:28",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A requires the LEAST effort",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#4",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application runs on an Amazon EC2 instance in a VPC.  The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet.<br>Which solution will provide private network connectivity to Amazon S3?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#4",
          "answers": [
            {
              "choice": "<p>A. Create a gateway VPC endpoint to the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an instance profile on Amazon EC2 to allow S3 access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon API Gateway API with a private link to access the S3 endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690928,
          "date": "Mon 10 Oct 2022 11:27",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "VPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 824083,
          "date": "Mon 27 Feb 2023 20:37",
          "username": "\t\t\t\tFolayinka\t\t\t",
          "content": "A VPC endpoint allows you to connect from the VPC to other AWS services outside of the VPC without the use of the internet.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819185,
          "date": "Thu 23 Feb 2023 13:35",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "VPC endpoint enables creation of a private connection between VPC to supported AWS services and VPC endpoint services powered by PrivateLink using its private IP address. Traffic between VPC and AWS service does not leave the Amazon network.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 807019,
          "date": "Mon 13 Feb 2023 04:28",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "A is correct, VPCendpoint is a connection between your VPC and an AWS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 803944,
          "date": "Fri 10 Feb 2023 04:48",
          "username": "\t\t\t\tdvoaviarison\t\t\t",
          "content": "VPC endpoint allows you to connect to AWS services using a private network instead of using the public Internet",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 775082,
          "date": "Sat 14 Jan 2023 05:25",
          "username": "\t\t\t\tvishwa10\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767812,
          "date": "Fri 06 Jan 2023 15:54",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "A gateway VPC endpoint is a connection between your VPC and an AWS service that enables private connectivity to the service. A gateway VPC endpoint for S3 allows the EC2 instance to access the S3 bucket without requiring internet connectivity.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757059,
          "date": "Mon 26 Dec 2022 05:37",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "You can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints (using AWS PrivateLink).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751127,
          "date": "Tue 20 Dec 2022 16:35",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT ANSWER***<br>The correct solution that will provide private network connectivity to Amazon S3 is Option A: Create a gateway VPC endpoint to the S3 bucket.<br><br>***EXPLANATION***<br>Option A involves creating a gateway VPC endpoint, which is a network interface in a VPC that allows you to privately connect to a service over the Amazon network. You can create a gateway VPC endpoint for Amazon S3, which will allow the EC2 instance in the VPC to access the S3 bucket without connectivity to the internet.<br><br>Option B involves streaming the logs to Amazon CloudWatch Logs and then exporting the logs to the S3 bucket. This option would not provide private network connectivity to S3, as the logs would need to be exported over the internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C involves creating an instance profile on the EC2 instance to allow S3 access. While this option could potentially work, it would not provide private network connectivity to S3, as the EC2 instance would still need to access S3 over the internet.<br><br>Option D involves creating an Amazon API Gateway API with a private link to access the S3 endpoint. This option would not provide private network connectivity to S3, as the API Gateway API is not a network interface that can be used to privately connect to S3.<br><br>Overall, Option A is the correct solution for providing private network connectivity to Amazon S3 from an EC2 instance in a VPC. </li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751129,
          "date": "Tue 20 Dec 2022 16:35",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C involves creating an instance profile on the EC2 instance to allow S3 access. While this option could potentially work, it would not provide private network connectivity to S3, as the EC2 instance would still need to access S3 over the internet.<br><br>Option D involves creating an Amazon API Gateway API with a private link to access the S3 endpoint. This option would not provide private network connectivity to S3, as the API Gateway API is not a network interface that can be used to privately connect to S3.<br><br>Overall, Option A is the correct solution for providing private network connectivity to Amazon S3 from an EC2 instance in a VPC. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750461,
          "date": "Tue 20 Dec 2022 04:50",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749528,
          "date": "Mon 19 Dec 2022 07:24",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "Gateway endpoints :https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html<br>A gateway endpoint targets specific IP routes in an Amazon VPC route table, in the form of a prefix-list, used for traffic destined to Amazon DynamoDB orAmazon S3. Gateway endpoints do not enable AWS PrivateLink.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747137,
          "date": "Fri 16 Dec 2022 13:01",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "Correct Answer: A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 746309,
          "date": "Thu 15 Dec 2022 17:17",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "A VPC endpoint allows to connect AWS services andand you don't need to use public network.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742383,
          "date": "Mon 12 Dec 2022 05:00",
          "username": "\t\t\t\tstepman\t\t\t",
          "content": "Took the exam today and this question was there.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 739986,
          "date": "Fri 09 Dec 2022 11:17",
          "username": "\t\t\t\tsanjay3x1\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734969,
          "date": "Sun 04 Dec 2022 10:32",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724969,
          "date": "Wed 23 Nov 2022 08:35",
          "username": "\t\t\t\tDrekorig\t\t\t",
          "content": "To provide connectivity the answer is \\\"A\\\". To authorize the connection we can use the instance profile.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#5",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time.<br>What should a solutions architect propose to ensure users see all of their documents at once?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#5",
          "answers": [
            {
              "choice": "<p>A. Copy the data so both EBS volumes contain all the documents<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the Application Load Balancer to direct a user to the server with the documents<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690945,
          "date": "Mon 10 Oct 2022 11:39",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "Concurrent or at the same time key word for EFS",
          "upvote_count": "17",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 719165,
          "date": "Tue 15 Nov 2022 23:26",
          "username": "\t\t\t\tmikey2000\t\t\t",
          "content": "Ebs doesnt support cross az only reside in one Az but Efs does, that why it's c",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 819193,
          "date": "Thu 23 Feb 2023 13:44",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "EFS automatically scales as users upload and delete files. EBS volumes can scale vertically by reconfiguring volume types and horizontally by managing additional EC2 volumes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807020,
          "date": "Mon 13 Feb 2023 04:28",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "Correct answer: C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 803947,
          "date": "Fri 10 Feb 2023 04:49",
          "username": "\t\t\t\tdvoaviarison\t\t\t",
          "content": "EFS allows to share storage",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797549,
          "date": "Sat 04 Feb 2023 03:38",
          "username": "\t\t\t\tSaiPavan10\t\t\t",
          "content": "option C makes sense.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767815,
          "date": "Fri 06 Jan 2023 15:56",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Amazon Elastic File System (EFS) is a fully managed file storage service that enables users to store and access data in the Amazon cloud. EFS is accessible over the network and can be mounted on multiple Amazon EC2 instances. By copying the data from both EBS volumes to EFS and modifying the application to save new documents to EFS, users will be able to access all of their documents at the same time.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750528,
          "date": "Tue 20 Dec 2022 06:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To ensure that users see all of their documents at once, the solutions architect should propose Option C: Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS.<br><br>Option C involves copying the data from both EBS volumes to Amazon Elastic File System (EFS), and modifying the application to save new documents to EFS. Amazon EFS is a fully managed, scalable file storage service that allows you to store and access files from multiple EC2 instances concurrently. By moving the data to EFS and modifying the application to save new documents to EFS, the application will be able to access all of the documents from a single, centralized location, ensuring that users see all of their documents at once.<br><br>Overall, Option C is the most effective solution for ensuring that users see all of their documents at once.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A involves copying the data so both EBS volumes contain all the documents. This option would not solve the issue, as the data is still stored on two separate EBS volumes, and the application would still need to read from both volumes to retrieve all of the documents.<br><br>Option B involves configuring the Application Load Balancer to direct a user to the server with the documents. This option would not solve the issue, as the user may not always be directed to the server that has the documents they are looking for.<br><br>Option D involves configuring the Application Load Balancer to send the request to both servers and return each document from the correct server. This option would not be an efficient solution, as it would require the application to send requests to both servers and receive and process the responses from both servers, which could increase the load on the application.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750531,
          "date": "Tue 20 Dec 2022 06:24",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A involves copying the data so both EBS volumes contain all the documents. This option would not solve the issue, as the data is still stored on two separate EBS volumes, and the application would still need to read from both volumes to retrieve all of the documents.<br><br>Option B involves configuring the Application Load Balancer to direct a user to the server with the documents. This option would not solve the issue, as the user may not always be directed to the server that has the documents they are looking for.<br><br>Option D involves configuring the Application Load Balancer to send the request to both servers and return each document from the correct server. This option would not be an efficient solution, as it would require the application to send requests to both servers and receive and process the responses from both servers, which could increase the load on the application.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749809,
          "date": "Mon 19 Dec 2022 13:27",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "EFS is useful to store files from multiple AZs to a single storage. On the other hand, for EBS files must be within the same AZ as the EBS volume",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749542,
          "date": "Mon 19 Dec 2022 07:37",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://aws.amazon.com/efs/when-to-choose-efs/<br>Amazon EFS provides shared file storage for use with compute instances in the AWS Cloud and on-premises servers. Applications that require shared file access can use Amazon EFS for reliable file storage delivering high aggregate throughput to thousands of clients simultaneously.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747430,
          "date": "Fri 16 Dec 2022 17:52",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "EFS can be mounted to multiple EC2 instances across AZs. The Performance is higher latency & throughput.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747140,
          "date": "Fri 16 Dec 2022 13:03",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "Correct answer: C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734970,
          "date": "Sun 04 Dec 2022 10:33",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "c is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 724365,
          "date": "Tue 22 Nov 2022 15:06",
          "username": "\t\t\t\tcheese929\t\t\t",
          "content": "C is the only solution that make sense.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723462,
          "date": "Mon 21 Nov 2022 13:22",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714178,
          "date": "Wed 09 Nov 2022 01:58",
          "username": "\t\t\t\tAzeeza\t\t\t",
          "content": "Amazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic and concurrency. Answer is C",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 711949,
          "date": "Sat 05 Nov 2022 19:31",
          "username": "\t\t\t\tpm2229\t\t\t",
          "content": "It's C,EFS can be mounted to multiple EC2 instances across AZs. The Performance is higher latency & throughput.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#6",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB.  The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#6",
          "answers": [
            {
              "choice": "<p>A. Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 695468,
          "date": "Sat 15 Oct 2022 16:32",
          "username": "\t\t\t\tGatt\t\t\t",
          "content": "Let's analyse this:<br>B.  On a Snowball Edge device you can copy files with a speed of up to 100Gbps. 70TB will take around 5600 seconds, so very quickly, less than 2 hours. The downside is that it'll take between 4-6 working days to receive the device and then another 2-3 working days to send it back and for AWS to move the data onto S3 once it reaches them. Total time: 6-9 working days. Bandwidth used: 0.<br>C.  File Gateway uses the Internet, so maximum speed will be at most 1Gbps, so it'll take a minimum of 6.5 days and you use 70TB of Internet bandwidth.<br>D.  You can achieve speeds of up to 10Gbps with Direct Connect. Total time 15.5 hours and you will use 70TB of bandwidth. However, what's interesting is that the question does not specific what type of bandwidth? Direct Connect does not use your Internet bandwidth, as you will have a dedicate peer to peer connectivity between your on-prem and the AWS Cloud, so technically, you're not using your \\\"public\\\" bandwidth. <br><br>The requirements are a bit too vague but I think that B is the most appropriate answer, although D might also be correct if the bandwidth usage refers strictly to your public connectivity.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D is a viable solution but to setup D it can take weeks or months and the question does say as soon as possible.</li><li>Time Calc Clarification:<br><br>Data: 70TB<br>=70TB*8b/B=560Tb<br>=560Tb*1000G/1T=560000Gb<br>Speed: 100Gb/s<br><br>Time=Data:Speed=56000Gb:100Gb/s=5600s<br>Time=5600s:3600s/hour=~1.5 hours (in case always on max speed)</li><li>But it said \\\"as soon as possible\\\" It takes about 4-6 weeks to provision a direct connect.</li><li>You missed the first part of the question \\\"The company must migrate the video files as soon as possible...\\\" hence C would be the best answer.</li></ul>",
          "upvote_count": "29",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 810700,
          "date": "Thu 16 Feb 2023 14:37",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "D is a viable solution but to setup D it can take weeks or months and the question does say as soon as possible.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 809069,
          "date": "Wed 15 Feb 2023 04:47",
          "username": "\t\t\t\tShlomiM\t\t\t",
          "content": "Time Calc Clarification:<br><br>Data: 70TB<br>=70TB*8b/B=560Tb<br>=560Tb*1000G/1T=560000Gb<br>Speed: 100Gb/s<br><br>Time=Data:Speed=56000Gb:100Gb/s=5600s<br>Time=5600s:3600s/hour=~1.5 hours (in case always on max speed)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771680,
          "date": "Tue 10 Jan 2023 19:34",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "But it said \\\"as soon as possible\\\" It takes about 4-6 weeks to provision a direct connect.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 768584,
          "date": "Sat 07 Jan 2023 14:31",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "You missed the first part of the question \\\"The company must migrate the video files as soon as possible...\\\" hence C would be the best answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 690292,
          "date": "Sun 09 Oct 2022 17:02",
          "username": "\t\t\t\ttuloveu\t\t\t",
          "content": "As using the least possible network bandwidth.",
          "upvote_count": "25",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830792,
          "date": "Mon 06 Mar 2023 13:30",
          "username": "\t\t\t\tStuMoz\t\t\t",
          "content": "B.  File Gateway shouldnt primarily be used for migration, only extending on-prem capacity. DataSync should be used for Migration. Least possible bandwidth is Snowmobile Edge since it doesn't use network bandwidth.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825512,
          "date": "Wed 01 Mar 2023 05:05",
          "username": "\t\t\t\tpyae\t\t\t",
          "content": "using the least possible network bandwidth. That is the main point of question. (B) is the best choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821471,
          "date": "Sat 25 Feb 2023 13:32",
          "username": "\t\t\t\takira90\t\t\t",
          "content": "definitely B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819235,
          "date": "Thu 23 Feb 2023 14:19",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "The basic difference between Snowball and Snowball Edge is the capacity they provide. Snowball provides a total of 50 TB or 80 TB, out of which 42 TB or 72 TB is available, while Amazon Snowball Edge provides 100 TB, out of which 83 TB is available.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 813018,
          "date": "Sat 18 Feb 2023 14:36",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "C.  is the correct answer. the keys are \\\"asap\\\" and \\\"using the least possible network bandwidth\\\". with B there is no network at all.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 810852,
          "date": "Thu 16 Feb 2023 16:51",
          "username": "\t\t\t\tAviDen\t\t\t",
          "content": "As @Gatt explained so accurately",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 807023,
          "date": "Mon 13 Feb 2023 04:30",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "C, keywork: least possible network bandwidth , D high bandwidth -> wrong",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 803905,
          "date": "Fri 10 Feb 2023 03:31",
          "username": "\t\t\t\tTUANHA2312\t\t\t",
          "content": "withrequirement that as soon as possible, the answer is B.  <br>C is fine, but you need time to order device and transfer device to aws",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 802706,
          "date": "Thu 09 Feb 2023 01:57",
          "username": "\t\t\t\tEmesias\t\t\t",
          "content": "Answer is C, the Snowball Edge To transport the data, these are sent to the devices through a regional carrier, it does not do so over the internet, in this case the question indicates \\\"Use the least possible bandwidth\\\" the other word The key is \\\"Enterprise uses NFS\\\" which refers to Amazon FSx File Gateway for compatibility.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 802704,
          "date": "Thu 09 Feb 2023 01:55",
          "username": "\t\t\t\tEmesias\t\t\t",
          "content": "Answer is C, the Snowball Edge To transport the data, these are sent to the devices through a regional carrier, it does not do so over the internet, in this case the question indicates \\\"Use the least possible bandwidth\\\" the other word The key is \\\"Enterprise uses NFS\\\" which refers to Amazon FSx File Gateway for compatibility.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer is C, the Snowball Edge To transport the data, these are sent to the devices through a regional carrier, it does not do so over the internet, in this case the question indicates \\\"Use the least possible bandwidth\\\" the other word The key is \\\"Enterprise uses NFS\\\" which refers to Amazon FSx File Gateway for compatibility.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 802705,
          "date": "Thu 09 Feb 2023 01:55",
          "username": "\t\t\t\tEmesias\t\t\t",
          "content": "Answer is C, the Snowball Edge To transport the data, these are sent to the devices through a regional carrier, it does not do so over the internet, in this case the question indicates \\\"Use the least possible bandwidth\\\" the other word The key is \\\"Enterprise uses NFS\\\" which refers to Amazon FSx File Gateway for compatibility.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 802667,
          "date": "Thu 09 Feb 2023 00:45",
          "username": "\t\t\t\tK0nAn\t\t\t",
          "content": "Hey guys ,I also think the option B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799587,
          "date": "Mon 06 Feb 2023 10:59",
          "username": "\t\t\t\tRONNYC\t\t\t",
          "content": "using the least possible network bandwidth<br>And File Gateway is for onging file sync.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 797007,
          "date": "Fri 03 Feb 2023 13:05",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Does not use bandwidth and each snowball edge can store up to 80TB. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 793787,
          "date": "Tue 31 Jan 2023 08:50",
          "username": "\t\t\t\tRoshantheDon\t\t\t",
          "content": "https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html#device-options",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 780104,
          "date": "Wed 18 Jan 2023 15:41",
          "username": "\t\t\t\tmartin451\t\t\t",
          "content": "you can do 1- or two-day shipping on snowball edge.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#7",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#7",
          "answers": [
            {
              "choice": "<p>A. Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB.  Configure the consumer applications to read from DynamoDB to process the messages.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689305,
          "date": "Sat 08 Oct 2022 14:44",
          "username": "\t\t\t\trein_chau\t\t\t",
          "content": "D makes more sense to me.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D.  Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues.<br><br>This solution uses Amazon SNS and SQS to publish and subscribe to messages respectively, which decouples the system and enables scalability by allowing multiple consumer applications to process the messages in parallel. Additionally, using Amazon SQS with multiple subscriptions can provide increased resiliency by allowing multiple copies of the same message to be processed in parallel.</li><li>By default, an SQS queue can handle a maximum of 3,000 messages per second. However, you can request higher throughput by contacting AWS Support. AWS can increase the message throughput for your queue beyond the default limits in increments of 300 messages per second, up to a maximum of 10,000 messages per second.<br><br>It's important to note that the maximum number of messages per second that a queue can handle is not the same as the maximum number of requests per second that the SQS API can handle. The SQS API is designed to handle a high volume of requests per second, so it can be used to send messages to your queue at a rate that exceeds the maximum message throughput of the queue.</li><li>The limit that you're mentioning apply to FIFO queues. Standard queues are unlimited in throughput (https://aws.amazon.com/sqs/features/). Do you think that the use case require FIFO queue ?</li><li>of course, the answer is D</li></ul>",
          "upvote_count": "26",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 794802,
          "date": "Wed 01 Feb 2023 02:46",
          "username": "\t\t\t\tdaizy\t\t\t",
          "content": "D.  Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues.<br><br>This solution uses Amazon SNS and SQS to publish and subscribe to messages respectively, which decouples the system and enables scalability by allowing multiple consumer applications to process the messages in parallel. Additionally, using Amazon SQS with multiple subscriptions can provide increased resiliency by allowing multiple copies of the same message to be processed in parallel.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767830,
          "date": "Fri 06 Jan 2023 16:21",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "By default, an SQS queue can handle a maximum of 3,000 messages per second. However, you can request higher throughput by contacting AWS Support. AWS can increase the message throughput for your queue beyond the default limits in increments of 300 messages per second, up to a maximum of 10,000 messages per second.<br><br>It's important to note that the maximum number of messages per second that a queue can handle is not the same as the maximum number of requests per second that the SQS API can handle. The SQS API is designed to handle a high volume of requests per second, so it can be used to send messages to your queue at a rate that exceeds the maximum message throughput of the queue.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The limit that you're mentioning apply to FIFO queues. Standard queues are unlimited in throughput (https://aws.amazon.com/sqs/features/). Do you think that the use case require FIFO queue ?</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 772677,
          "date": "Wed 11 Jan 2023 16:34",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "The limit that you're mentioning apply to FIFO queues. Standard queues are unlimited in throughput (https://aws.amazon.com/sqs/features/). Do you think that the use case require FIFO queue ?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 732687,
          "date": "Thu 01 Dec 2022 15:05",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "of course, the answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 712728,
          "date": "Mon 07 Nov 2022 01:52",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "D.  SNS Fan Out Pattern https://docs.aws.amazon.com/sns/latest/dg/sns-common-scenarios.html(A is wrong Kinesis Analysis does not 'persist' by itself.)",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 827419,
          "date": "Thu 02 Mar 2023 21:51",
          "username": "\t\t\t\tkhetran\t\t\t",
          "content": "A because key point here is that there are multipe consumer which consume the data immidiately , and kenesis fan out is the option we can use to scale out",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821124,
          "date": "Sat 25 Feb 2023 03:56",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "\\\"messages varies drastically and sometimes increases suddenly to 100,000 each second\\\"<br>meaning it can increase even more than 100,000 A would make the most sense based on this.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819273,
          "date": "Thu 23 Feb 2023 15:12",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Kinesis Streams differ from SNS in many ways: Lambda polls Kinesis for records up to 5 times a second, whereas SNS would push messages to Lambda. records are received in batches (up to your specified maximum), SNS invokes your function with one message.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 803906,
          "date": "Fri 10 Feb 2023 03:33",
          "username": "\t\t\t\tTUANHA2312\t\t\t",
          "content": "keypoint is: The company wants to decouple the solution and increase scalability.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 800921,
          "date": "Tue 07 Feb 2023 14:20",
          "username": "\t\t\t\tawscerts023\t\t\t",
          "content": "Fan Out pattern , can also filter messages to different consumers if needed.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 793303,
          "date": "Mon 30 Jan 2023 21:35",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "C.  Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB.  Configure the consumer applications to read from DynamoDB to process the messages.<br><br>While using Amazon SNS and SQS can also decouple the solution and provide a level of scalability, using Kinesis Data Streams with Lambda and DynamoDB provides a more flexible and scalable solution for ingesting and processing large amounts of data in near real-time. With Kinesis, the ingestion application can write messages to a stream that can scale horizontally to handle increased traffic, while the Lambda function provides the ability to preprocess the messages before storing them in a scalable NoSQL database like DynamoDB.  This setup also allows for better control and optimization of the processing pipeline.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This answer is also valid, however do you think it would benefit to pay more by being with Kineses which is real time (Unnecessary) and writing an SQL custom code to process this when you can simply have SNS with SQS which process more data, sever-less, cheaper.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 802552,
          "date": "Wed 08 Feb 2023 22:02",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "This answer is also valid, however do you think it would benefit to pay more by being with Kineses which is real time (Unnecessary) and writing an SQL custom code to process this when you can simply have SNS with SQS which process more data, sever-less, cheaper.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 789182,
          "date": "Fri 27 Jan 2023 01:02",
          "username": "\t\t\t\tSTRELOK\t\t\t",
          "content": "Answer is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 787744,
          "date": "Wed 25 Jan 2023 15:38",
          "username": "\t\t\t\tMourner\t\t\t",
          "content": "Company needs to decouple and improve scalabilty.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 787252,
          "date": "Wed 25 Jan 2023 04:28",
          "username": "\t\t\t\tm2khp\t\t\t",
          "content": "Answer is D.  A is an overkill.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 772679,
          "date": "Wed 11 Jan 2023 16:35",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "I think it is D because the limits mentioned are valid for FIFO queues",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 771686,
          "date": "Tue 10 Jan 2023 19:41",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Anytime you see the word \\\"decouple\\\" expressly or Implied ...... think SQS that's it's primary purpose !",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 767820,
          "date": "Fri 06 Jan 2023 16:07",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "By writing the incoming messages to Kinesis Data Streams and using a Lambda function to preprocess and store the messages in DynamoDB, the company can decouple the ingestion application from the consumer applications and increase scalability. Kinesis Data Streams can automatically scale to handle the sudden increases in the number of messages, and the Lambda function can be triggered to process the messages as they arrive. The consumer applications can then read and process the messages from DynamoDB. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>After much thought, I think D is the right answer.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 767835,
          "date": "Fri 06 Jan 2023 16:28",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "After much thought, I think D is the right answer.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 765594,
          "date": "Wed 04 Jan 2023 13:05",
          "username": "\t\t\t\tPuppetMaster2k\t\t\t",
          "content": "The correct answer should be D: Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SQS) subscriptions. Configure the consumer applications to process the messages from the queues.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765326,
          "date": "Wed 04 Jan 2023 07:53",
          "username": "\t\t\t\tstudystudy999\t\t\t",
          "content": "D is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 758110,
          "date": "Tue 27 Dec 2022 04:26",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is correct<br>SNS can not handle 100K per second message throughput. As per the documentation it can only handle upto 300 msgs/sec.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#8",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.<br>How should a solutions architect design the architecture to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#8",
          "answers": [
            {
              "choice": "<p>A. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689313,
          "date": "Sat 08 Oct 2022 14:52",
          "username": "\t\t\t\trein_chau\t\t\t",
          "content": "A - incorrect: Schedule scaling policy doesn't make sense.<br>C, D - incorrect: Primary server should not be in same Auto Scaling group with compute nodes.<br>B is correct.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</li></ul>",
          "upvote_count": "36",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 714820,
          "date": "Thu 10 Nov 2022 01:25",
          "username": "\t\t\t\tWilson_S\t\t\t",
          "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 689123,
          "date": "Sat 08 Oct 2022 10:07",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "The answer seems to be B for me:<br>A: doesn't make sense to schedule auto-scaling<br>C: Not sure how CloudTrail would be helpful in this case, at all.<br>D: EventBridge is not really used for this purpose, wouldn't be very reliable",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 836710,
          "date": "Sun 12 Mar 2023 05:08",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "I wondered why the correct answer is C.  Is it possible the key is \\\"Configure EC2 Auto Scaling based on the load on the primary server.\\\"?<br>Because - <br>1. all traffics go to the primary firstly. <br>2. there is one primary server \\\"a primary server\\\" if this server goes down then the whole solution is down.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836704,
          "date": "Sun 12 Mar 2023 04:37",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "I don't know why the correct answer is C.  Question - if I meet the question in the test, what should I select? Select C?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 833661,
          "date": "Thu 09 Mar 2023 07:45",
          "username": "\t\t\t\tmhmud12393\t\t\t",
          "content": "even chat gpt agrees",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830160,
          "date": "Sun 05 Mar 2023 19:11",
          "username": "\t\t\t\tTo_mi\t\t\t",
          "content": "Also agree with most of you",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821050,
          "date": "Sat 25 Feb 2023 01:27",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "C doesn't make sense. Cloudtrail does not assist in resiliency",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819279,
          "date": "Thu 23 Feb 2023 15:16",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Amazon SQS is a fully managed message queue service that enables you to decouple and scale microservices, distributed systems, and serverless applications. By using SQS as the destination for the jobs, you can decouple the primary server from the compute nodes, which will increase resiliency and scalability.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 807025,
          "date": "Mon 13 Feb 2023 04:33",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774041,
          "date": "Fri 13 Jan 2023 03:43",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "B.  Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771937,
          "date": "Wed 11 Jan 2023 02:16",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 767838,
          "date": "Fri 06 Jan 2023 16:31",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Amazon SQS is a fully managed message queue service that enables you to decouple and scale microservices, distributed systems, and serverless applications. By using SQS as the destination for the jobs, you can decouple the primary server from the compute nodes, which will increase resiliency and scalability.<br><br>Amazon EC2 Auto Scaling is a service that automatically increases or decreases the number of EC2 instances in your application based on demand. By configuring EC2 Auto Scaling to scale based on the size of the SQS queue, you can ensure that the number of compute nodes is sufficient to handle the workload.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 765567,
          "date": "Wed 04 Jan 2023 12:37",
          "username": "\t\t\t\tavidanov\t\t\t",
          "content": "My guess was B and the ChatGPT has the same opinion here:<br><br>To meet the requirements for resiliency and scalability in this situation, a solutions architect could design the architecture as follows:<br><br>Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. This will allow the primary server to send jobs to the queue, which can then be processed asynchronously by the compute nodes.<br>Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. This will allow the application to automatically scale the number of compute nodes up or down in response to changes in workload.<br>Configure EC2 Auto Scaling to use Amazon SQS as a metric. This will allow EC2 Auto Scaling to automatically adjust the number of compute nodes based on the size of the queue. This will ensure that the application has sufficient capacity to process the jobs in a timely manner, while also maximizing resiliency and scalability.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762355,
          "date": "Sat 31 Dec 2022 00:19",
          "username": "\t\t\t\tFayzal\t\t\t",
          "content": "It is a little confusing, most of the voted answers are not correct. How do they select the correct answer?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As per previous comments you need to follow most voted answers</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763088,
          "date": "Sun 01 Jan 2023 07:49",
          "username": "\t\t\t\tawseisa\t\t\t",
          "content": "As per previous comments you need to follow most voted answers",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 761743,
          "date": "Fri 30 Dec 2022 08:10",
          "username": "\t\t\t\thaysamof\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759108,
          "date": "Tue 27 Dec 2022 23:16",
          "username": "\t\t\t\tSoheila\t\t\t",
          "content": "manage plan so cloudtrail",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 758976,
          "date": "Tue 27 Dec 2022 20:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution that meets these requirements is Option B: Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because scheduled scaling is not based on the demand for the jobs, and may not be sufficient to handle variable workloads.<br><br>Option C is incorrect because AWS CloudTrail is a service for auditing and monitoring API activity, and is not a suitable destination for the jobs.<br><br>Option D is incorrect because Amazon EventBridge (formerly Amazon CloudWatch Events) is a service for routing real-time data streams, and is not a suitable destination for the jobs.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 758977,
          "date": "Tue 27 Dec 2022 20:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because scheduled scaling is not based on the demand for the jobs, and may not be sufficient to handle variable workloads.<br><br>Option C is incorrect because AWS CloudTrail is a service for auditing and monitoring API activity, and is not a suitable destination for the jobs.<br><br>Option D is incorrect because Amazon EventBridge (formerly Amazon CloudWatch Events) is a service for routing real-time data streams, and is not a suitable destination for the jobs.",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#9",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed.<br>The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#9",
          "answers": [
            {
              "choice": "<p>A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon FSx for Windows File Server file system to extend the company's storage space.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689126,
          "date": "Sat 08 Oct 2022 10:11",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "Answer directly points towards file gateway with lifecycles, https://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html<br><br>D is wrong because utility function is vague and there is no need for flexible storage.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes it might be vague but how do we keep the low-latency access that only flexible can offer?</li></ul>",
          "upvote_count": "30",
          "selected_answers": ""
        },
        {
          "id": 731229,
          "date": "Wed 30 Nov 2022 09:24",
          "username": "\t\t\t\tUdoyen\t\t\t",
          "content": "Yes it might be vague but how do we keep the low-latency access that only flexible can offer?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 734975,
          "date": "Sun 04 Dec 2022 10:46",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "B answwer is correct. low latency is only needed for newer files. Additionally, File GW provides low latency access by caching frequently accessed files locally so answer is B",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 836713,
          "date": "Sun 12 Mar 2023 05:19",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "Why B is not correct? <br>Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.<br>I think “ S3 Glacier Deep Archive” is not correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 833401,
          "date": "Wed 08 Mar 2023 23:10",
          "username": "\t\t\t\tsunnyninja\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 826537,
          "date": "Thu 02 Mar 2023 07:15",
          "username": "\t\t\t\tsuraj2045\t\t\t",
          "content": "B IS RIGHT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821126,
          "date": "Sat 25 Feb 2023 04:02",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "It's B, but half the answers incorrect. you have to hold the files for 30 days prior to transitioning any s3 life cycle policy<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You don't have to. 30 days is duration of storage of particular object after moving it. You're confirming, that you're aware of storing object for minimum 30 days in the new storage class. In this case is respective only to Storage IA and Storage IA One-Zone</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 829275,
          "date": "Sat 04 Mar 2023 19:57",
          "username": "\t\t\t\tjakubzajac\t\t\t",
          "content": "You don't have to. 30 days is duration of storage of particular object after moving it. You're confirming, that you're aware of storing object for minimum 30 days in the new storage class. In this case is respective only to Storage IA and Storage IA One-Zone",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819291,
          "date": "Thu 23 Feb 2023 15:30",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Without losing low-latency access to the most recently accessed files. is the key point",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 814922,
          "date": "Mon 20 Feb 2023 08:14",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "B can't be a solution because it takes 12-48 hours for retrieval for glacier deep dive archive.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>after 7 days rarely accessed .</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 826529,
          "date": "Thu 02 Mar 2023 06:54",
          "username": "\t\t\t\ttellmenowwwww\t\t\t",
          "content": "after 7 days rarely accessed .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807137,
          "date": "Mon 13 Feb 2023 08:22",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "B answwer is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 806370,
          "date": "Sun 12 Feb 2023 14:34",
          "username": "\t\t\t\tMichaelCarrasco\t\t\t",
          "content": "Letter B<br>A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues.<br>https://docs.aws.amazon.com/filegateway/latest/files3/CreatingAnSMBFileShare.html<br><br>Dont make sense each user upload the file if you already have a server, the latency is the same.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801163,
          "date": "Tue 07 Feb 2023 18:10",
          "username": "\t\t\t\tdevg198\t\t\t",
          "content": "without losing low-latency access to the most recently accessed files. is the key point<br><br>And Option: D is wrong because installing utility on each Amazon S3 doesn't make sense.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 795833,
          "date": "Thu 02 Feb 2023 08:17",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "B is the most viable solution. But still the second half of the answer is wrong, because you need at least to store for 30 days data in S3 before transitioningto S3 Glacier Deep Archive<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's why I was also confused</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 821055,
          "date": "Sat 25 Feb 2023 01:32",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "That's why I was also confused",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773956,
          "date": "Fri 13 Jan 2023 00:54",
          "username": "\t\t\t\tMoMoCh4n\t\t\t",
          "content": "D is Dead wrong<br>\\\"install a utility on each user's computer\\\" doesn't seem logical...what if there are 99999999 users?",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 771939,
          "date": "Wed 11 Jan 2023 02:19",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 766659,
          "date": "Thu 05 Jan 2023 14:29",
          "username": "\t\t\t\tshirleyson123\t\t\t",
          "content": "I AM QUITE SUSPICIOUS WHY EXAMTOPICS ARE FIELDING OUT WRONG ANSWERS!???<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I SUSPECT IT'S A LEGAL ISSUE, SOME SO OBVIOUSLY WRONG ! CONFUSES ONE. </li></ul>",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 771713,
          "date": "Tue 10 Jan 2023 20:04",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "I SUSPECT IT'S A LEGAL ISSUE, SOME SO OBVIOUSLY WRONG ! CONFUSES ONE. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 758978,
          "date": "Tue 27 Dec 2022 20:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, creating an Amazon S3 File Gateway and an S3 Lifecycle policy to transition data to S3 Glacier Deep Archive, would meet the requirements specified in the prompt.<br><br>The S3 File Gateway allows you to store and retrieve objects in Amazon S3 using standard file system protocols, such as SMB and NFS. This would provide additional storage space for the company's data and allow for low-latency access to the most recently accessed files, as the data would still be stored on the SMB file server.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS, would not provide additional storage space for the company's data and would not allow for low-latency access to the most recently accessed files. <br><br>Option C, creating an FSx for Windows File Server file system, would provide additional storage space but would not include file lifecycle management. <br><br>Option D, installing a utility on each user's computer to access Amazon S3 and creating an S3 Lifecycle policy, would not provide additional storage space on the company's file server.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 758979,
          "date": "Tue 27 Dec 2022 20:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS, would not provide additional storage space for the company's data and would not allow for low-latency access to the most recently accessed files. <br><br>Option C, creating an FSx for Windows File Server file system, would provide additional storage space but would not include file lifecycle management. <br><br>Option D, installing a utility on each user's computer to access Amazon S3 and creating an S3 Lifecycle policy, would not provide additional storage space on the company's file server.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 758151,
          "date": "Tue 27 Dec 2022 06:16",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "The same question and answer explanation exists in a Udemy course.<br>Correct answer is B. <br>Amazon S3 File Gateway provides a seamless way to connect to the cloud to store application data files and backup images as durable objects in Amazon S3 cloud storage. Amazon S3 File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.<br>It can be used for on-premises data-intensive Amazon EC2-based applications that need file protocol access to S3 object storage. Lifecycle policies can then transition the data to S3 Glacier Deep Archive after 7 days.<br><br>D is wrong because is involves too much extra configuration which is unnecessary.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#10",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#10",
          "answers": [
            {
              "choice": "<p>A. Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an API Gateway authorizer to block any requests while the application processes an order.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689128,
          "date": "Sat 08 Oct 2022 10:11",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "B because FIFO is made for that specific purpose",
          "upvote_count": "37",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 689332,
          "date": "Sat 08 Oct 2022 15:11",
          "username": "\t\t\t\trein_chau\t\t\t",
          "content": "Should be B because SQS FIFO queue guarantees message order.",
          "upvote_count": "20",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 837609,
          "date": "Mon 13 Mar 2023 04:51",
          "username": "\t\t\t\tjaswantn\t\t\t",
          "content": "Company is building application , so this is not overloaded with the order(nowhere mentioned in the question). If orders are generated in sequence and notification is sent to SNS, then order would remain intact. Using SQS(FIFO) would bring more cost( which we need to avoid always).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 836720,
          "date": "Sun 12 Mar 2023 05:41",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "SQS is distributed queueing system. Messages are sent into a queue and receivers have to poll messages from SQS. How let Restful API poll? <br>SNS messages are pushed to subscribers. So I select A - SNS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835055,
          "date": "Fri 10 Mar 2023 14:44",
          "username": "\t\t\t\tmark543\t\t\t",
          "content": "Only FIFO can meet the demand",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830163,
          "date": "Sun 05 Mar 2023 19:13",
          "username": "\t\t\t\tTo_mi\t\t\t",
          "content": "Who decides what option is marked as a correct answer? Most of them are incorrect. The correct answer to this question is B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821056,
          "date": "Sat 25 Feb 2023 01:33",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "It's B FIFO",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819293,
          "date": "Thu 23 Feb 2023 15:35",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. Occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order.<br>FIFO queues offer first-in-first-out delivery and exactly-once processing: the order in which messages are sent and received is strictly preserved.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 817532,
          "date": "Wed 22 Feb 2023 07:31",
          "username": "\t\t\t\trhm\t\t\t",
          "content": "Should be B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 811970,
          "date": "Fri 17 Feb 2023 15:24",
          "username": "\t\t\t\tkhasport\t\t\t",
          "content": "almost people choice B and me too. But I don't know why correct answer is A and who is make correct answer ? is it admin of examtopic page ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807138,
          "date": "Mon 13 Feb 2023 08:22",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "Amazon Simple Queue Service (Amazon SQS) FIFO",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 773958,
          "date": "Fri 13 Jan 2023 00:59",
          "username": "\t\t\t\tMoMoCh4n\t\t\t",
          "content": "I think the wrong answers are here intentionally as a way for us to deep dive into the topic and gain more knowledge or maybe I'm givingthem too much credit.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Highly doubt it. There is app with identical questions and wrong answers named TestKing that gives you the wrong answers too with no feedback from people or explanations of answers.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 820460,
          "date": "Fri 24 Feb 2023 13:24",
          "username": "\t\t\t\tAndrew123123\t\t\t",
          "content": "Highly doubt it. There is app with identical questions and wrong answers named TestKing that gives you the wrong answers too with no feedback from people or explanations of answers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768608,
          "date": "Sat 07 Jan 2023 14:53",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "Answer is B<br>SNS has FIFO topics for ordering and deduplication, but the answer does not mention that. So answer A can be assumed to be standard one without ordering feature.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 765463,
          "date": "Wed 04 Jan 2023 11:02",
          "username": "\t\t\t\tdexpos\t\t\t",
          "content": "Hi folks, I just asked to the support about having different suggested answer vs the one selected from the community and the reply is: \\\"If the provided answer is not correct or against the discussion then you can go with the user voted ones.\\\" . Community wins if we agree :). Hope this helps.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 761347,
          "date": "Thu 29 Dec 2022 19:15",
          "username": "\t\t\t\tMars2k\t\t\t",
          "content": "Suggested answers often seem to be different than the answer with the most votes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759713,
          "date": "Wed 28 Dec 2022 11:30",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Answer B.  API Gateway endpoint sits in between applications and integration. It works like a tunnel which connects to SQS service. Once the connection is established you have two services Standard SQS which transmits without order and is scaleable and FIFO Queue where everything is sent in the same order.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 758966,
          "date": "Tue 27 Dec 2022 20:32",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT ANSWER***<br><br>Option B, using an API Gateway integration to send a message to an Amazon Simple Queue Service (SQS) FIFO queue when the application receives an order and configuring the SQS FIFO queue to invoke an AWS Lambda function for processing, would meet the requirements specified in the prompt.<br><br>SQS FIFO queues ensure that messages are processed in the order that they are received, and API Gateway integrations allow you to send messages to an SQS queue when certain events occur, such as when the application receives an order. By configuring the SQS FIFO queue to invoke an AWS Lambda function for processing, you can ensure that orders are processed in the order that they are received.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***INCORRECT ANSWERS***<br><br>Option A, using an API Gateway integration to publish a message to an Amazon Simple Notification Service (SNS) topic and subscribing an AWS Lambda function to the topic, would not guarantee that orders are processed in the order that they are received. <br><br>Option C, using an API Gateway authorizer to block requests while the application processes an order, would not allow for parallel processing of orders and could lead to delays in processing. <br><br>Option D, using an API Gateway integration to send a message to an SQS standard queue, would not guarantee that orders are processed in the order that they are received.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 758968,
          "date": "Tue 27 Dec 2022 20:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***INCORRECT ANSWERS***<br><br>Option A, using an API Gateway integration to publish a message to an Amazon Simple Notification Service (SNS) topic and subscribing an AWS Lambda function to the topic, would not guarantee that orders are processed in the order that they are received. <br><br>Option C, using an API Gateway authorizer to block requests while the application processes an order, would not allow for parallel processing of orders and could lead to delays in processing. <br><br>Option D, using an API Gateway integration to send a message to an SQS standard queue, would not guarantee that orders are processed in the order that they are received.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#11",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management.<br>What should a solutions architect do to accomplish this goal?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#11",
          "answers": [
            {
              "choice": "<p>A. Use AWS Secrets Manager. Turn on automatic rotation.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Systems Manager Parameter Store. Turn on automatic rotation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689131,
          "date": "Sat 08 Oct 2022 10:15",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "B is wrong because parameter store does not support auto rotation, unless the customer writes it themselves, A is the answer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>correct. see link https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/ for differences between SSM Parameter Store and AWS Secrets Manager</li><li>That was a fantastic link. This part of their site \\\"comparison of AWS services\\\" is superb. Thanks.</li><li>READ!!! AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. <br>https://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-within-a-virtual-private-cloud/yhttps://aws.amazon.com/secrets-manager/?nc1=h_ls</li><li>ty bro, I was confused about that and you just mentioned the \\\"key\\\" phrase, B doesn't support autorotation</li></ul>",
          "upvote_count": "41",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 734229,
          "date": "Sat 03 Dec 2022 06:48",
          "username": "\t\t\t\tkewl\t\t\t",
          "content": "correct. see link https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/ for differences between SSM Parameter Store and AWS Secrets Manager<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That was a fantastic link. This part of their site \\\"comparison of AWS services\\\" is superb. Thanks.</li></ul>",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 740322,
          "date": "Fri 09 Dec 2022 19:01",
          "username": "\t\t\t\tmrbottomwood\t\t\t",
          "content": "That was a fantastic link. This part of their site \\\"comparison of AWS services\\\" is superb. Thanks.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 706796,
          "date": "Sat 29 Oct 2022 01:10",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "READ!!! AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. <br>https://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-within-a-virtual-private-cloud/yhttps://aws.amazon.com/secrets-manager/?nc1=h_ls",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 698037,
          "date": "Tue 18 Oct 2022 10:23",
          "username": "\t\t\t\tiCcma\t\t\t",
          "content": "ty bro, I was confused about that and you just mentioned the \\\"key\\\" phrase, B doesn't support autorotation",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760934,
          "date": "Thu 29 Dec 2022 12:28",
          "username": "\t\t\t\tleeyoung\t\t\t",
          "content": "Admin is trying to fail everybody in the exam.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 836727,
          "date": "Sun 12 Mar 2023 06:20",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "With AWS Secrets Manager, the application code can retrieve credentials securely by calling Secrets Manager APIs, eliminating the need to store secrets in the code or configuration files.<br>So if we select A, there are a lot code change (from read pwd from file to call API). <br><br>AWS Systems Manager Parameter Store can useAWS Secrets Manager to manage password. <br>And \\\"You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter. \\\"<br>It is easy to switch to use “the unique name”.<br><br>So B is better than A.  (B include A)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821537,
          "date": "Sat 25 Feb 2023 14:59",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "AWS Secrets Manager enables you to rotate, manage, and retrieve database credentials, API keys and other secrets throughout their lifecycle. It also makes it really easy for you to follow security best practices such as encrypting secrets and rotating these regularly.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821129,
          "date": "Sat 25 Feb 2023 04:07",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "It's a secret key, it will be A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819014,
          "date": "Thu 23 Feb 2023 10:22",
          "username": "\t\t\t\tahalamri\t\t\t",
          "content": "A is Correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 814051,
          "date": "Sun 19 Feb 2023 13:17",
          "username": "\t\t\t\tcheese929\t\t\t",
          "content": "A is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810886,
          "date": "Thu 16 Feb 2023 17:09",
          "username": "\t\t\t\tBilal_2019\t\t\t",
          "content": "To minimize the operational overhead of credential management, a solutions architect could implement the following:<br><br>Use AWS Secrets Manager to store and manage the database user names and passwords securely.<br><br>Update the application to retrieve the user names and passwords from AWS Secrets Manager instead of from the local file.<br><br>By using AWS Secrets Manager, the company can centrally manage the database user names and passwords and enforce security best practices such as regular rotation of secrets, fine-grained access control, and audit trail of secret usage. This can help simplify the credential management and improve the security of the application.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810884,
          "date": "Thu 16 Feb 2023 17:08",
          "username": "\t\t\t\tBilal_2019\t\t\t",
          "content": "Yes, using AWS Systems Manager Parameter Store is a good option for minimizing the operational overhead of credential management in this scenario. Here's what you can do:<br><br>Store the database credentials securely in the Parameter Store as a SecureString data type.<br>Enable automatic rotation of the credentials to periodically generate a new set of credentials and update the stored value in the Parameter Store.<br>Modify the application to retrieve the credentials from the Parameter Store at runtime, rather than storing them locally in a file.<br>By doing this, the management and rotation of the credentials can be automated, which reduces the operational overhead and ensures the security of the system.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 810878,
          "date": "Thu 16 Feb 2023 17:06",
          "username": "\t\t\t\tBilal_2019\t\t\t",
          "content": "Chat GPT Chooses A with explanation as below<br><br>To minimize the operational overhead of credential management, a solutions architect could implement the following:<br><br>Use AWS Secrets Manager to store and manage the database user names and passwords securely.<br><br>Update the application to retrieve the user names and passwords from AWS Secrets Manager instead of from the local file.<br><br>By using AWS Secrets Manager, the company can centrally manage the database user names and passwords and enforce security best practices such as regular rotation of secrets, fine-grained access control, and audit trail of secret usage. This can help simplify the credential management and improve the security of the application.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Also, chooses B <br><br>Yes, using AWS Systems Manager Parameter Store is a good option for minimizing the operational overhead of credential management in this scenario. Here's what you can do:<br><br>Store the database credentials securely in the Parameter Store as a SecureString data type.<br>Enable automatic rotation of the credentials to periodically generate a new set of credentials and update the stored value in the Parameter Store.<br>Modify the application to retrieve the credentials from the Parameter Store at runtime, rather than storing them locally in a file.<br>By doing this, the management and rotation of the credentials can be automated, which reduces the operational overhead and ensures the security of the system.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810882,
          "date": "Thu 16 Feb 2023 17:07",
          "username": "\t\t\t\tBilal_2019\t\t\t",
          "content": "Also, chooses B <br><br>Yes, using AWS Systems Manager Parameter Store is a good option for minimizing the operational overhead of credential management in this scenario. Here's what you can do:<br><br>Store the database credentials securely in the Parameter Store as a SecureString data type.<br>Enable automatic rotation of the credentials to periodically generate a new set of credentials and update the stored value in the Parameter Store.<br>Modify the application to retrieve the credentials from the Parameter Store at runtime, rather than storing them locally in a file.<br>By doing this, the management and rotation of the credentials can be automated, which reduces the operational overhead and ensures the security of the system.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807139,
          "date": "Mon 13 Feb 2023 08:25",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "Selected Answer: A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806387,
          "date": "Sun 12 Feb 2023 14:52",
          "username": "\t\t\t\tMichaelCarrasco\t\t\t",
          "content": "Letter B is correctly.<br>I´m AWS Certified Security Specialty and this question was in my exam and we recommend to use AWS Systems Manager Parameter Store and turn on automatic rotation.<br><br>www.linkedin.com/in/michaelwcarrasco",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 800589,
          "date": "Tue 07 Feb 2023 06:49",
          "username": "\t\t\t\tBhrino\t\t\t",
          "content": "is every question past the first page wrong lmao<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How do we prepare the exam with so many wrong answers?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801664,
          "date": "Wed 08 Feb 2023 05:43",
          "username": "\t\t\t\tBurma\t\t\t",
          "content": "How do we prepare the exam with so many wrong answers?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 778884,
          "date": "Tue 17 Jan 2023 13:11",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "There's no automatic rotation in Parameter Store.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778056,
          "date": "Mon 16 Jan 2023 18:54",
          "username": "\t\t\t\tnalindm\t\t\t",
          "content": "parameter store does not support auto rotation",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774049,
          "date": "Fri 13 Jan 2023 03:54",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "A.  use AWS secrets Manager...<br>No debate",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773232,
          "date": "Thu 12 Jan 2023 08:43",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "B is wrong because it does not support automatic rotation",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#12",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#12",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route traffic to the CloudFront distribution.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route traffic to the CloudFront distribution.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718554,
          "date": "Tue 15 Nov 2022 08:11",
          "username": "\t\t\t\tKartikey140\t\t\t",
          "content": "Answer is A<br>Explanation - AWS Global Accelerator vs CloudFront<br>• They both use the AWS global network and its edge locations around the world<br>• Both services integrate with AWS Shield for DDoS protection.<br>• CloudFront <br>• Improves performance for both cacheable content (such as images and videos) <br>• Dynamic content (such as API acceleration and dynamic site delivery)<br>• Content is served at the edge<br>• Global Accelerator <br>• Improves performance for a wide range of applications over TCP or UDP <br>• Proxying packets at the edge to applications running in one or more AWS Regions.<br>• Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP<br>• Good for HTTP use cases that require static IP addresses <br>• Good for HTTP use cases that required deterministic, fast regional failover<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>By creating a CloudFront distribution that has both the S3 bucket and the ALB as origins, the company can reduce latency for both the static and dynamic data. The CloudFront distribution acts as a content delivery network (CDN), caching the data closer to the users and reducing the latency. The company can then configure Route 53 to route traffic to the CloudFront distribution, providing improved performance for the web application.</li></ul>",
          "upvote_count": "42",
          "selected_answers": ""
        },
        {
          "id": 794804,
          "date": "Wed 01 Feb 2023 02:58",
          "username": "\t\t\t\tdaizy\t\t\t",
          "content": "By creating a CloudFront distribution that has both the S3 bucket and the ALB as origins, the company can reduce latency for both the static and dynamic data. The CloudFront distribution acts as a content delivery network (CDN), caching the data closer to the users and reducing the latency. The company can then configure Route 53 to route traffic to the CloudFront distribution, providing improved performance for the web application.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 717912,
          "date": "Mon 14 Nov 2022 12:16",
          "username": "\t\t\t\tkanweng\t\t\t",
          "content": "Q: How is AWS Global Accelerator different from Amazon CloudFront?<br><br>A: AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 836736,
          "date": "Sun 12 Mar 2023 06:44",
          "username": "\t\t\t\tsanking\t\t\t",
          "content": "ChatGPT's answer is also C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 831493,
          "date": "Tue 07 Mar 2023 03:33",
          "username": "\t\t\t\tcegama543\t\t\t",
          "content": "Option C is the correct solution to improve performance and reduce latency for the static and dynamic data of the web application.<br>Option A is incorrect because it only includes the S3 bucket and ALB as origins for the CloudFront distribution, missing the opportunity to cache the static data in edge locations closer to the users.<br><br>Option B is incorrect because it includes the S3 bucket as an endpoint for the AWS Global Accelerator, which is not necessary for the static data since it will already be cached in the CloudFront distribution.<br><br>Option D is incorrect because it creates two domain names for the web application, which can add complexity to the configuration and increase the risk of errors. Using a single domain name and routing traffic to the closest endpoint using the AWS Global Accelerator is a simpler and more effective solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 824343,
          "date": "Tue 28 Feb 2023 04:13",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "How about C, to improve performance and reduce latency for both static data and dynamic data?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 814965,
          "date": "Mon 20 Feb 2023 08:45",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "I think dynamic data should not be cached. Therefore cloudfront is not for dynamic cached. A cannot be the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 797870,
          "date": "Sat 04 Feb 2023 12:24",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Quoted from Amazon \\\"Fortunately, Amazon CloudFront can serve both types of content, to reduce latency, protect your architecture, and optimize costs. In this post, we demonstrate how to use CloudFront to deliver both static and dynamic content using a single distribution, for dynamic and static websites and web applications.\\\"",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 796105,
          "date": "Thu 02 Feb 2023 15:15",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "So for a while i was tempted by C Answer and i would ready to defy the whole community who vouched for A.  Then i understood that using AWS Global Accelerator on two many endpoints doesn't make sense at all because the AWS Global improve Geo Routing coming from users for only one endpoint which should be the ALB in that case.<br><br>So C is totally false. Using CloudFront in front of the ALB with 2 origins : S3 and ALB makes totally sense and is a good practice to improve Content Delivery for STATIC/DYNAMIC content at the same time : https://docs.aws.amazon.com/global-accelerator/latest/dg/about-endpoints.html<br><br>Once i digested the fact that AWS Cloud Front can afford having multiple origins (S3 and ALB i was sure A is a hell yeah",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 775870,
          "date": "Sat 14 Jan 2023 20:40",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Out of A vs C i choose A,CloudFront uses multiple sets of dynamically changing IP addresses while Global Accelerator will provide you a set of static IP addresses as a fixed entry point to your applications.<br>CloudFront pricing is mainly based on data transfer out and HTTP requests while Global Accelerator charges a fixed hourly fee and an incremental charge over your standard Data Transfer rates, also called a Data Transfer-Premium fee (DT-Premium).<br>CloudFront uses Edge Locations to cache content while Global Accelerator uses Edge Locations to find an optimal pathway to the nearest regional endpoint.<br>CloudFront is designed to handle HTTP protocol meanwhile Global Accelerator is best used for both HTTP and non-HTTP protocols such as TCP and UDP.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 767921,
          "date": "Fri 06 Jan 2023 18:24",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To improve performance and reduce latency for static data and dynamic data, you can use Amazon CloudFront, a content delivery network (CDN) service. CloudFront delivers content from origins, such as an S3 bucket or an Application Load Balancer (ALB), to users over the internet with low latency and high data transfer speeds. To set up CloudFront for your web application, you can create a distribution and specify the S3 bucket and the ALB as origins. CloudFront will cache static data from the S3 bucket and dynamic data from the ALB.  You can then configure Amazon Route 53, the DNS service, to route traffic to the CloudFront distribution. This will allow users to access the web application through CloudFront, which can improve performance and reduce latency.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763475,
          "date": "Mon 02 Jan 2023 04:01",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "its A https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 758989,
          "date": "Tue 27 Dec 2022 21:01",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, creating an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins and configuring Route 53 to route traffic to the CloudFront distribution, would be the best solution to improve performance and reduce latency for the static data and dynamic data.<br><br>CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content by caching it at edge locations around the world. By creating a CloudFront distribution with the S3 bucket and the ALB as origins, you can improve performance and reduce latency for both static data (stored in the S3 bucket) and dynamic data (generated by the web application running on the EC2 instances behind the ALB). You can then configure Route 53 to route traffic to the CloudFront distribution, which will automatically route traffic to the nearest edge location to minimize latency.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, creating a CloudFront distribution with the ALB as an origin and an AWS Global Accelerator standard accelerator with the S3 bucket as an endpoint, would not improve performance and reduce latency for static data stored in the S3 bucket. <br><br>Option C, creating a CloudFront distribution with the S3 bucket as an origin and an AWS Global Accelerator standard accelerator with the ALB and the CloudFront distribution as endpoints, would not allow the web application to access dynamic data generated by the EC2 instances. <br><br>Option D, creating two domain names and pointing one domain name to the CloudFront DNS name for dynamic content and the other domain name to the accelerator DNS name for static content, would not improve performance and reduce latency for both static and dynamic data.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 758991,
          "date": "Tue 27 Dec 2022 21:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, creating a CloudFront distribution with the ALB as an origin and an AWS Global Accelerator standard accelerator with the S3 bucket as an endpoint, would not improve performance and reduce latency for static data stored in the S3 bucket. <br><br>Option C, creating a CloudFront distribution with the S3 bucket as an origin and an AWS Global Accelerator standard accelerator with the ALB and the CloudFront distribution as endpoints, would not allow the web application to access dynamic data generated by the EC2 instances. <br><br>Option D, creating two domain names and pointing one domain name to the CloudFront DNS name for dynamic content and the other domain name to the accelerator DNS name for static content, would not improve performance and reduce latency for both static and dynamic data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744755,
          "date": "Wed 14 Dec 2022 06:39",
          "username": "\t\t\t\tMaxMa\t\t\t",
          "content": "C is incorrect maybe because CloudFront can not be endpoint to Accelerator.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 742451,
          "date": "Mon 12 Dec 2022 07:16",
          "username": "\t\t\t\trezba1987\t\t\t",
          "content": "When you want to use CloudFront to distribute your content, you create a distribution and choose the configuration settings you want. Also you can use distributions to serve the static and dynamic content, for example, .html, .css, .js, and image files, using HTTP or HTTPS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 740009,
          "date": "Fri 09 Dec 2022 11:54",
          "username": "\t\t\t\tShailendradhaniya\t\t\t",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 728509,
          "date": "Sun 27 Nov 2022 19:28",
          "username": "\t\t\t\thakant\t\t\t",
          "content": "Answer A <br>CF can have multiple origins just like ALB.  Need to set the routes manually using Berhavior tab in the console<br>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-distribution-serve-content/#:~:text=%20Yes%2C%20you%20can%20configure%20a%20single%20CloudFront,serve%20different%20types%20of%20requests%20from%20multiple%20origins.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727351,
          "date": "Sat 26 Nov 2022 08:12",
          "username": "\t\t\t\togwu2000\t\t\t",
          "content": "C.  Not A. Why should CloudFront distribution have two origins - S3 bucket and the ALB ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CloudFront improves performance for both cacheable content (such as images and videos), whereas AWS Global Accelerator does not.</li><li>CloudFront improves performance for both cacheable content (such as images and videos), and Dynamic content, whereas AWS Global Accelerator does not support Dynamic content.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 730170,
          "date": "Tue 29 Nov 2022 10:39",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "CloudFront improves performance for both cacheable content (such as images and videos), whereas AWS Global Accelerator does not.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CloudFront improves performance for both cacheable content (such as images and videos), and Dynamic content, whereas AWS Global Accelerator does not support Dynamic content.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 730171,
          "date": "Tue 29 Nov 2022 10:40",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "CloudFront improves performance for both cacheable content (such as images and videos), and Dynamic content, whereas AWS Global Accelerator does not support Dynamic content.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#13",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#13",
          "answers": [
            {
              "choice": "<p>A. Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB.  Use the RDS API to rotate the secrets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689350,
          "date": "Sat 08 Oct 2022 15:28",
          "username": "\t\t\t\trein_chau\t\t\t",
          "content": "A is correct.<br>https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/",
          "upvote_count": "17",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 814080,
          "date": "Sun 19 Feb 2023 13:56",
          "username": "\t\t\t\tcheese929\t\t\t",
          "content": "A is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778888,
          "date": "Tue 17 Jan 2023 13:14",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "It's A, as Secrets Manager does support replicating secrets into multiple AWS Regions: https://docs.aws.amazon.com/secretsmanager/latest/userguide/create-manage-multi-region-secrets.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 773240,
          "date": "Thu 12 Jan 2023 08:58",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "it's A, here the question specify that we want the LEAST overhead<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 806418,
          "date": "Sun 12 Feb 2023 15:22",
          "username": "\t\t\t\tMichaelCarrasco\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-multiple-regions/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767924,
          "date": "Fri 06 Jan 2023 18:28",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "AWS Secrets Manager is a secrets management service that enables you to store, manage, and rotate secrets such as database credentials, API keys, and SSH keys. Secrets Manager can help you minimize the operational overhead of rotating credentials for your Amazon RDS for MySQL databases across multiple Regions. With Secrets Manager, you can store the credentials as secrets and use multi-Region secret replication to replicate the secrets to the required Regions. You can then configure Secrets Manager to rotate the secrets on a schedule so that the credentials are rotated automatically without the need for manual intervention. This can help reduce the risk of secrets being compromised and minimize the operational overhead of credential management.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 758992,
          "date": "Tue 27 Dec 2022 21:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, storing the credentials as secrets in AWS Secrets Manager and using multi-Region secret replication for the required Regions, and configuring Secrets Manager to rotate the secrets on a schedule, would meet the requirements with the least operational overhead.<br><br>AWS Secrets Manager allows you to store, manage, and rotate secrets, such as database credentials, across multiple AWS Regions. By enabling multi-Region secret replication, you can replicate the secrets across the required Regions to allow for seamless rotation of the credentials during maintenance activities. Additionally, Secrets Manager provides automatic rotation of secrets on a schedule, which would minimize the operational overhead of rotating the credentials on a monthly basis.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, storing the credentials as secrets in AWS Systems Manager and using multi-Region secret replication, would not provide automatic rotation of secrets on a schedule. <br><br>Option C, storing the credentials in an S3 bucket with SSE enabled and using EventBridge to invoke an AWS Lambda function to rotate the credentials, would not provide automatic rotation of secrets on a schedule. <br><br>Option D, encrypting the credentials as secrets using KMS multi-Region customer managed keys and storing the secrets in a DynamoDB global table, would not provide automatic rotation of secrets on a schedule and would require additional operational overhead to retrieve the secrets from DynamoDB and use the RDS API to rotate the secrets.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 758994,
          "date": "Tue 27 Dec 2022 21:04",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, storing the credentials as secrets in AWS Systems Manager and using multi-Region secret replication, would not provide automatic rotation of secrets on a schedule. <br><br>Option C, storing the credentials in an S3 bucket with SSE enabled and using EventBridge to invoke an AWS Lambda function to rotate the credentials, would not provide automatic rotation of secrets on a schedule. <br><br>Option D, encrypting the credentials as secrets using KMS multi-Region customer managed keys and storing the secrets in a DynamoDB global table, would not provide automatic rotation of secrets on a schedule and would require additional operational overhead to retrieve the secrets from DynamoDB and use the RDS API to rotate the secrets.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757117,
          "date": "Mon 26 Dec 2022 07:25",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "vote A !",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747456,
          "date": "Fri 16 Dec 2022 18:35",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "AWS Secret Manager",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 744296,
          "date": "Tue 13 Dec 2022 18:08",
          "username": "\t\t\t\tngochieu276\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 740036,
          "date": "Fri 09 Dec 2022 12:29",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "Most of these questions have secrets manager as the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734232,
          "date": "Sat 03 Dec 2022 06:55",
          "username": "\t\t\t\tkewl\t\t\t",
          "content": "rotate credentials is the keyword and systems manager doesn't support rotation. check link <br>https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>secrets-manager supports rotational but systems-manager-parameter-store doesn't support</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741091,
          "date": "Sat 10 Dec 2022 17:17",
          "username": "\t\t\t\tkrathore911\t\t\t",
          "content": "secrets-manager supports rotational but systems-manager-parameter-store doesn't support",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723477,
          "date": "Mon 21 Nov 2022 13:40",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723155,
          "date": "Mon 21 Nov 2022 04:50",
          "username": "\t\t\t\tMegako\t\t\t",
          "content": "Me Pick A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 722378,
          "date": "Sun 20 Nov 2022 05:17",
          "username": "\t\t\t\tABCMail\t\t\t",
          "content": "AWS secrets manager",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 707144,
          "date": "Sat 29 Oct 2022 13:16",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Ans is A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 697648,
          "date": "Mon 17 Oct 2022 21:12",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 693964,
          "date": "Thu 13 Oct 2022 16:17",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "AWS Secrets Manager is meant for storing secrets like credentials to RDS database etc. <br>Capability to force rotation of secrets every X days. Multi Region Secret Keys <br>https://youtu.be/GPab-mc-8nU",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#14",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.<br>The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#14",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Redshift with a single node for leader and compute functionality.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon ElastiCache for Memcached with EC2 Spot Instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 691182,
          "date": "Mon 10 Oct 2022 15:28",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "C, AURORA is 5x performance improvement over MySQL on RDS and handles more read requests than write,; maintaining high availability =Multi-AZ deployment",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 758996,
          "date": "Tue 27 Dec 2022 21:05",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C, using Amazon Aurora with a Multi-AZ deployment and configuring Aurora Auto Scaling with Aurora Replicas, would be the best solution to meet the requirements.<br><br>Aurora is a fully managed, MySQL-compatible relational database that is designed for high performance and high availability. Aurora Multi-AZ deployments automatically maintain a synchronous standby replica in a different Availability Zone to provide high availability. Additionally, Aurora Auto Scaling allows you to automatically scale the number of Aurora Replicas in response to read workloads, allowing you to meet the demand of unpredictable read workloads while maintaining high availability. This would provide an automated solution for scaling the database to meet the demand of the application while maintaining high availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using Amazon Redshift with a single node for leader and compute functionality, would not provide high availability. <br><br>Option B, using Amazon RDS with a Single-AZ deployment and configuring RDS to add reader instances in a different Availability Zone, would not provide high availability and would not automatically scale the number of reader instances in response to read workloads. <br><br>Option D, using Amazon ElastiCache for Memcached with EC2 Spot Instances, would not provide a database solution and would not meet the requirements.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 758997,
          "date": "Tue 27 Dec 2022 21:05",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using Amazon Redshift with a single node for leader and compute functionality, would not provide high availability. <br><br>Option B, using Amazon RDS with a Single-AZ deployment and configuring RDS to add reader instances in a different Availability Zone, would not provide high availability and would not automatically scale the number of reader instances in response to read workloads. <br><br>Option D, using Amazon ElastiCache for Memcached with EC2 Spot Instances, would not provide a database solution and would not meet the requirements.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 823069,
          "date": "Mon 27 Feb 2023 01:02",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Right Answer C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807143,
          "date": "Mon 13 Feb 2023 08:28",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "Amazon Aurora",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 773246,
          "date": "Thu 12 Jan 2023 09:01",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "C because other answers are not a good-fit for the question",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 767927,
          "date": "Fri 06 Jan 2023 18:33",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability, you can use Amazon Aurora with a Multi-AZ deployment. Aurora is a fully managed, MySQL-compatible database service that can automatically scale up or down based on workload demands. With a Multi-AZ deployment, Aurora maintains a synchronous standby replica in a different Availability Zone (AZ) to provide high availability in the event of an outage.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753777,
          "date": "Fri 23 Dec 2022 01:53",
          "username": "\t\t\t\tSmartDude\t\t\t",
          "content": "Why is B incorrect??<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones so you need you database work with Multi AZ too.</li><li>B can't scale well</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806430,
          "date": "Sun 12 Feb 2023 15:33",
          "username": "\t\t\t\tMichaelCarrasco\t\t\t",
          "content": "The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones so you need you database work with Multi AZ too.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771946,
          "date": "Wed 11 Jan 2023 02:36",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "B can't scale well",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 746442,
          "date": "Thu 15 Dec 2022 20:07",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Aurora 5x faster and 3x improves performance",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 740721,
          "date": "Sat 10 Dec 2022 07:21",
          "username": "\t\t\t\tsanjay3x1\t\t\t",
          "content": "no drought Ans is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723480,
          "date": "Mon 21 Nov 2022 13:41",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722423,
          "date": "Sun 20 Nov 2022 07:08",
          "username": "\t\t\t\tABCMail\t\t\t",
          "content": "Aurora offers multi AZ for HA",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 707147,
          "date": "Sat 29 Oct 2022 13:19",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Ans is Aurora",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 707026,
          "date": "Sat 29 Oct 2022 08:30",
          "username": "\t\t\t\tkeezbadger\t\t\t",
          "content": "C is the answer. Aurora is fast, and for this case will support unpredictable workloads through its read replicas. Simple!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 705066,
          "date": "Thu 27 Oct 2022 01:34",
          "username": "\t\t\t\tukwafabian\t\t\t",
          "content": "\\\"Read workloads\\\" \\\"Maintaining high availability\\\" = Read replica's",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 697652,
          "date": "Mon 17 Oct 2022 21:26",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "C is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 693972,
          "date": "Thu 13 Oct 2022 16:21",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "1. Migration from My SQL, Postgres SQL to Aurora is 5x and 3x times improves performance . Also provision for Read replicas",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 693741,
          "date": "Thu 13 Oct 2022 10:22",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "High availability + SQL -> C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#15",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently migrated to AWS and wants to implement a solution to protect the traffic that flows in and out of the production VPC.  The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as traffic flow inspection and traffic filtering. The company wants to have the same functionalities in the AWS Cloud.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#15",
          "answers": [
            {
              "choice": "<p>A. Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for the production VPC. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692608,
          "date": "Wed 12 Oct 2022 05:41",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I agree with C. <br>**AWS Network Firewall** is a stateful, managed network firewall and intrusion detection and prevention service for your virtual private cloud (VPC) that you created in Amazon Virtual Private Cloud (Amazon VPC). With Network Firewall, you can filter traffic at the perimeter of your VPC.  This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>And I'm not sure Traffic Mirroring can be for filtering</li></ul>",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692610,
          "date": "Wed 12 Oct 2022 05:47",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "And I'm not sure Traffic Mirroring can be for filtering",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 689374,
          "date": "Sat 08 Oct 2022 15:54",
          "username": "\t\t\t\trein_chau\t\t\t",
          "content": "C is correct. AWS Network Firewall supports both inspection and filtering as required.<br>B is incorrect because Traffic Mirroring only for inspection.",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 823073,
          "date": "Mon 27 Feb 2023 01:04",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "C is correct. AWS Network Firewall supports both inspection and filtering as required.<br>B is incorrect because Traffic Mirroring only for inspection.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822556,
          "date": "Sun 26 Feb 2023 15:52",
          "username": "\t\t\t\tsky09\t\t\t",
          "content": "Option B, using Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering, is the most appropriate solution for the company's requirements. Traffic Mirroring allows the company to replicate network traffic to an Amazon Elastic Compute Cloud (Amazon EC2) instance or an Amazon Partner Network (APN) partner for inspection and filtering. The inspection server can be set up in an EC2 instance, and traffic from the production VPC can be mirrored to this instance for inspection and filtering, similar to how the on-premises inspection server operated. This solution allows the company to maintain the same functionalities they had on-premises and also provides them with greater flexibility and scalability in the AWS Cloud.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807144,
          "date": "Mon 13 Feb 2023 08:29",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "C \\\"performed specific operations such as traffic flow inspection and traffic filtering\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797900,
          "date": "Sat 04 Feb 2023 12:48",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "C.  it works like a gatekeeper for connection coming in and out of the VPC. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 767929,
          "date": "Fri 06 Jan 2023 18:37",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "I would recommend option C: Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC. <br><br>AWS Network Firewall is a managed firewall service that provides filtering for both inbound and outbound network traffic. It allows you to create rules for traffic inspection and filtering, which can help protect your production VPC. <br><br>Option A: Amazon GuardDuty is a threat detection service, not a traffic inspection or filtering service.<br><br>Option B: Traffic Mirroring is a feature that allows you to replicate and send a copy of network traffic from a VPC to another VPC or on-premises location. It is not a service that performs traffic inspection or filtering.<br><br>Option D: AWS Firewall Manager is a security management service that helps you to centrally configure and manage firewalls across your accounts. It is not a service that performs traffic inspection or filtering.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759139,
          "date": "Tue 27 Dec 2022 23:59",
          "username": "\t\t\t\tSoheila\t\t\t",
          "content": "c is correctaws firewal manager makes it easy to centrally manage waf rulles .so c is correct with network fire wall you can filter traffic at the permeter of your vpc",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759000,
          "date": "Tue 27 Dec 2022 21:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C, using AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for the production VPC, would meet the requirements.<br><br>AWS Network Firewall is a stateful firewall that provides traffic inspection and traffic filtering for Amazon Virtual Private Clouds (VPCs). You can create rules to control traffic flow in and out of your VPC, allowing you to implement the same functionalities as the inspection server in the company's on-premises data center. This would provide a solution for protecting the traffic that flows in and out of the production VPC. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using Amazon GuardDuty for traffic inspection and traffic filtering, would not provide the ability to create specific rules for traffic inspection and traffic filtering. <br><br>Option B, using Traffic Mirroring to mirror traffic for inspection and filtering, would not provide the ability to create specific rules for traffic inspection and traffic filtering. <br><br>Option D, using AWS Firewall Manager, would not provide the ability to create specific rules for traffic inspection and traffic filtering.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759001,
          "date": "Tue 27 Dec 2022 21:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using Amazon GuardDuty for traffic inspection and traffic filtering, would not provide the ability to create specific rules for traffic inspection and traffic filtering. <br><br>Option B, using Traffic Mirroring to mirror traffic for inspection and filtering, would not provide the ability to create specific rules for traffic inspection and traffic filtering. <br><br>Option D, using AWS Firewall Manager, would not provide the ability to create specific rules for traffic inspection and traffic filtering.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749647,
          "date": "Mon 19 Dec 2022 09:57",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html<br>Network Firewall to monitor and protect Amazon VPC traffic in a number of ways,.<br> With Network Firewall, can filter traffic at the perimeter of your VPC.  This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747461,
          "date": "Fri 16 Dec 2022 18:37",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "AWS Network Firewall supports both inspection and filtering as required.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 746443,
          "date": "Thu 15 Dec 2022 20:09",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "I agree with C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743166,
          "date": "Mon 12 Dec 2022 19:34",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "C<br>AWS Network Firewall is a managed service that provides protection for the traffic flowing in and out of a VPC.  It allows you to create rules for traffic inspection and traffic filtering, which would meet the requirements of the company in this scenario. Amazon GuardDuty is a threat detection service, not a traffic inspection and filtering service. Traffic Mirroring is a network monitoring tool that allows you to copy traffic from one interface to another for analysis, but it does not provide the required traffic inspection and filtering functionality. AWS Firewall Manager is a service that helps you manage firewall rules across multiple AWS accounts and VPCs, but it does not provide the ability to inspect and filter traffic.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727287,
          "date": "Sat 26 Nov 2022 06:51",
          "username": "\t\t\t\twisoxe8356\t\t\t",
          "content": "Not A - Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. like someone strange continuely download data from your s3<br><br>Not B - As it is moniroring not filtering<br><br>C - good to do both<br><br>D - configure and manage firewall rules, not monitoring",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723481,
          "date": "Mon 21 Nov 2022 13:42",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717509,
          "date": "Sun 13 Nov 2022 21:24",
          "username": "\t\t\t\tsandra42\t\t\t",
          "content": "Network Firewall",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707156,
          "date": "Sat 29 Oct 2022 13:35",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Ans is C (AWS Firewall Manager).<br>https://aws.amazon.com/network-firewall/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc.<br>Traffic Mirroring = \\\"copy\\\". https://docs.aws.amazon.com/es_es/vpc/latest/mirroring/traffic-mirroring-how-it-works.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer is Network Firewall, not Firewall Manager.</li><li>ups, its Network Firewall</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 709047,
          "date": "Tue 01 Nov 2022 11:41",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Answer is Network Firewall, not Firewall Manager.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ups, its Network Firewall</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 719089,
          "date": "Tue 15 Nov 2022 21:06",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ups, its Network Firewall",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#16",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#16",
          "answers": [
            {
              "choice": "<p>A. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 691510,
          "date": "Mon 10 Oct 2022 23:07",
          "username": "\t\t\t\trodriiviru\t\t\t",
          "content": "https://docs.aws.amazon.com/quicksight/latest/user/sharing-a-dashboard.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html<br>^ more percise link</li><li>Agree with you</li></ul>",
          "upvote_count": "42",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 692635,
          "date": "Wed 12 Oct 2022 06:44",
          "username": "\t\t\t\tmattlai\t\t\t",
          "content": "https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html<br>^ more percise link",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 692617,
          "date": "Wed 12 Oct 2022 06:02",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "Agree with you",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 691207,
          "date": "Mon 10 Oct 2022 15:42",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "A, The rest of the company should have only limited access you have to create IAM role<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer is B.  Permissions are handled directly.<br>https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html</li><li>“Permissions are handled directly” is a broad response that doesn't say anything or make a point. So you're saying quicksight will automatically know which person is on the management team and which person isn't. No it won't without instructions! So you need to set up IAM groups and limit their access that way. IAM (identity and “ACCESS” management) That's the other part of the question that needs to be addressed.</li><li>ajá...read...https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html - <br>In the Share dashboard page that opens, do the following:<br>For Invite users and groups to dashboard at left, enter a user email or group name in the search box.<br>Any users or groups that match your query appear in a list below the search box. Only active users and groups appear in the list.<br>For the user or group that you want to grant access to the dashboard, choose Add. Then choose the level of permissions that you want them to have. ********\\\"it says NO here go to the IAM and assign the permissions.\\\" So you don't manage by IAM. Ok, correct answer is B</li></ul>",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 707173,
          "date": "Sat 29 Oct 2022 14:00",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Answer is B.  Permissions are handled directly.<br>https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>“Permissions are handled directly” is a broad response that doesn't say anything or make a point. So you're saying quicksight will automatically know which person is on the management team and which person isn't. No it won't without instructions! So you need to set up IAM groups and limit their access that way. IAM (identity and “ACCESS” management) That's the other part of the question that needs to be addressed.</li><li>ajá...read...https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html - <br>In the Share dashboard page that opens, do the following:<br>For Invite users and groups to dashboard at left, enter a user email or group name in the search box.<br>Any users or groups that match your query appear in a list below the search box. Only active users and groups appear in the list.<br>For the user or group that you want to grant access to the dashboard, choose Add. Then choose the level of permissions that you want them to have. ********\\\"it says NO here go to the IAM and assign the permissions.\\\" So you don't manage by IAM. Ok, correct answer is B</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712801,
          "date": "Mon 07 Nov 2022 05:08",
          "username": "\t\t\t\tJayyRock\t\t\t",
          "content": "“Permissions are handled directly” is a broad response that doesn't say anything or make a point. So you're saying quicksight will automatically know which person is on the management team and which person isn't. No it won't without instructions! So you need to set up IAM groups and limit their access that way. IAM (identity and “ACCESS” management) That's the other part of the question that needs to be addressed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ajá...read...https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html - <br>In the Share dashboard page that opens, do the following:<br>For Invite users and groups to dashboard at left, enter a user email or group name in the search box.<br>Any users or groups that match your query appear in a list below the search box. Only active users and groups appear in the list.<br>For the user or group that you want to grant access to the dashboard, choose Add. Then choose the level of permissions that you want them to have. ********\\\"it says NO here go to the IAM and assign the permissions.\\\" So you don't manage by IAM. Ok, correct answer is B</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 719098,
          "date": "Tue 15 Nov 2022 21:23",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ajá...read...https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html - <br>In the Share dashboard page that opens, do the following:<br>For Invite users and groups to dashboard at left, enter a user email or group name in the search box.<br>Any users or groups that match your query appear in a list below the search box. Only active users and groups appear in the list.<br>For the user or group that you want to grant access to the dashboard, choose Add. Then choose the level of permissions that you want them to have. ********\\\"it says NO here go to the IAM and assign the permissions.\\\" So you don't manage by IAM. Ok, correct answer is B",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 836632,
          "date": "Sun 12 Mar 2023 01:49",
          "username": "\t\t\t\tVish216\t\t\t",
          "content": "Option A is not correct because it suggests sharing dashboards with IAM roles, which are meant for managing access to AWS services, not QuickSight.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834060,
          "date": "Thu 09 Mar 2023 15:48",
          "username": "\t\t\t\tudo2020\t\t\t",
          "content": "The question states:<br>\\\"...others should have limited access\\\"<br>This is only possible with IAM rules.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823077,
          "date": "Mon 27 Feb 2023 01:14",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Answer is B.  Permissions are handled directly.<br>https://docs.aws.amazon.com/quicksight/latest/user/share-a-dashboard-grant-access-users.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821139,
          "date": "Sat 25 Feb 2023 04:15",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "IAM roles restrict permissions and access",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 803559,
          "date": "Thu 09 Feb 2023 19:00",
          "username": "\t\t\t\tcyrillo\t\t\t",
          "content": "Quicksight shares information with users and groups not iam roles answer B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 803225,
          "date": "Thu 09 Feb 2023 13:48",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "IAM Role is broad involving many principles, whereas IAM users and groups can be separated for certain permissions for e.g. IAM Group for managers and if you want to add anyone in specific you can ad IAM Users which might not be managers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795297,
          "date": "Wed 01 Feb 2023 17:31",
          "username": "\t\t\t\tvj_csc\t\t\t",
          "content": "Which answer is accepted in exam A or B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 794874,
          "date": "Wed 01 Feb 2023 07:19",
          "username": "\t\t\t\thanen\t\t\t",
          "content": "If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.<br><br>Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license<br>https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 781013,
          "date": "Thu 19 Jan 2023 11:26",
          "username": "\t\t\t\toguz11\t\t\t",
          "content": "ChatGPT's answer for this question;<br>A.  Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.<br><br>This solution meets the requirements as it allows for data visualization and includes all data sources within the data lake. Additionally, by sharing the dashboards with the appropriate IAM roles, it ensures that only the company's management team has full access to all visualizations and the rest of the company has only limited access.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I selected A . Definitely agree with ChatGPT'S explanation !!'</li><li>Interesting... B.  Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.<br>This solution meets the requirements because it allows for the creation of visualizations using all data sources within the data lake, and it allows for the management team to have full access to all visualizations while providing limited access for the rest of the company by sharing the dashboards with the appropriate users and groups using IAM roles.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791489,
          "date": "Sun 29 Jan 2023 10:15",
          "username": "\t\t\t\tmelanincloudgal\t\t\t",
          "content": "I selected A . Definitely agree with ChatGPT'S explanation !!'",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791305,
          "date": "Sun 29 Jan 2023 02:51",
          "username": "\t\t\t\tgogod2\t\t\t",
          "content": "Interesting... B.  Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.<br>This solution meets the requirements because it allows for the creation of visualizations using all data sources within the data lake, and it allows for the management team to have full access to all visualizations while providing limited access for the rest of the company by sharing the dashboards with the appropriate users and groups using IAM roles.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778925,
          "date": "Tue 17 Jan 2023 13:59",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "QuickSight is a BI dashboard that can combine data from multiple sources, including S3 and PostgreSQL.<br><br>https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html<br>https://docs.aws.amazon.com/quicksight/latest/user/welcome.html<br><br>You can share QuickSight dashboards with:<br>- specific users & groups<br>- everyone in your AWS account<br>- anyone on the internet<br><br>https://docs.aws.amazon.com/quicksight/latest/user/sharing-a-dashboard.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778509,
          "date": "Tue 17 Jan 2023 04:36",
          "username": "\t\t\t\tnalindm\t\t\t",
          "content": "If you have data in sources other than Amazon S3, you can use Athena Federated Query to query the data in place or build pipelines that extract data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data stored in relational, non-relational, object, and custom data sources.<br>Athena uses data source connectors that run on AWS Lambda to run federated queries. A data source connector is a piece of code that can translate between your target data source and Athena. You can think of a connector as an extension of Athena's query engine. Prebuilt Athena data source connectors exist for data sources like Amazon CloudWatch Logs, Amazon DynamoDB, Amazon DocumentDB, and Amazon RDS, and JDBC-compliant relational data sources such MySQL, and PostgreSQL under the Apache 2.0 license.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is incorrect.<br>this option solves the problem of access sharing with resources but does not take care of delta in data. Also, you connect user and groups in your QuickSight account but not IAM Roles.<br>Reference :- https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778511,
          "date": "Tue 17 Jan 2023 04:40",
          "username": "\t\t\t\tnalindm\t\t\t",
          "content": "B is incorrect.<br>this option solves the problem of access sharing with resources but does not take care of delta in data. Also, you connect user and groups in your QuickSight account but not IAM Roles.<br>Reference :- https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776306,
          "date": "Sun 15 Jan 2023 10:04",
          "username": "\t\t\t\tAsamsk\t\t\t",
          "content": "bis correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 767934,
          "date": "Fri 06 Jan 2023 18:43",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Iwould recommend option B: Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.<br><br>Amazon QuickSight is a fast, cloud-powered business intelligence service that makes it easy to build and share interactive data visualizations. You can use it to connect to multiple data sources, including Amazon S3 and Amazon RDS for PostgreSQL, and create new datasets. You can then publish dashboards to visualize the data and share them with the appropriate users and groups, using IAM roles to grant different levels of access.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 767435,
          "date": "Fri 06 Jan 2023 09:53",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "You can share dashboards and visuals with specific users or groups in your account or with everyone in your Amazon QuickSight account.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762156,
          "date": "Fri 30 Dec 2022 17:46",
          "username": "\t\t\t\tRupesh16\t\t\t",
          "content": "Below explanation lot of confusing. I am actually looking for confirmation on correct answer - but I think Option A Sounds reasonable.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#17",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket.<br>What should the solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#17",
          "answers": [
            {
              "choice": "<p>A. Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692001,
          "date": "Tue 11 Oct 2022 12:29",
          "username": "\t\t\t\tsba21\t\t\t",
          "content": "Always remember that you should associate IAM roles to EC2 instances",
          "upvote_count": "38",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759007,
          "date": "Tue 27 Dec 2022 21:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct option to meet this requirement is A: Create an IAM role that grants access to the S3 bucket and attach the role to the EC2 instances.<br><br>An IAM role is an AWS resource that allows you to delegate access to AWS resources and services. You can create an IAM role that grants access to the S3 bucket and then attach the role to the EC2 instances. This will allow the EC2 instances to access the S3 bucket and the documents stored within it.<br><br>Option B is incorrect because an IAM policy is used to define permissions for an IAM user or group, not for an EC2 instance.<br><br>Option C is incorrect because an IAM group is used to group together IAM users and policies, not to grant access to resources.<br><br>Option D is incorrect because an IAM user is used to represent a person or service that interacts with AWS resources, not to grant access to resources.",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823078,
          "date": "Mon 27 Feb 2023 01:15",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "IAM Role is the correct anwser.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 807149,
          "date": "Mon 13 Feb 2023 08:39",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "IAM Role",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790979,
          "date": "Sat 28 Jan 2023 20:02",
          "username": "\t\t\t\tPankul\t\t\t",
          "content": "Associate IAM roles to EC2 instances",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 767938,
          "date": "Fri 06 Jan 2023 18:53",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "An IAM role is an AWS identity that you can create and use to delegate permissions to AWS resources. To give the EC2 instances access to the S3 bucket, you can create an IAM role that grants the necessary permissions and then attach the role to the instances. This will allow the instances to access the S3 bucket using the permissions granted by the role.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757141,
          "date": "Mon 26 Dec 2022 07:51",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "it's A: Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750464,
          "date": "Tue 20 Dec 2022 04:52",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749683,
          "date": "Mon 19 Dec 2022 10:41",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "How can I grant my Amazon EC2 instance access to an Amazon S3 bucket?<br>https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 746448,
          "date": "Thu 15 Dec 2022 20:14",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "IAM role can be assigned to Amazon EC2 instance<br>1. create one IAM role 2. give S3 access to it.3.Attach to EC2",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 740793,
          "date": "Sat 10 Dec 2022 09:58",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "A since EC2 instance is tied to role not policy",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737162,
          "date": "Tue 06 Dec 2022 20:29",
          "username": "\t\t\t\tbeto07\t\t\t",
          "content": "IAM Role = services",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 736265,
          "date": "Mon 05 Dec 2022 21:18",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Option A is the correct one",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 735070,
          "date": "Sun 04 Dec 2022 13:22",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "AAAAAAAAAAAA",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 731185,
          "date": "Wed 30 Nov 2022 08:10",
          "username": "\t\t\t\tmiki111\t\t\t",
          "content": "AAAAAAAAAA",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727903,
          "date": "Sun 27 Nov 2022 04:01",
          "username": "\t\t\t\tDerekMinstP\t\t\t",
          "content": "A for sure",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726156,
          "date": "Thu 24 Nov 2022 21:07",
          "username": "\t\t\t\tsherbo\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#18",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.<br>A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.<br>Which combination of actions will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#18",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692072,
          "date": "Tue 11 Oct 2022 14:10",
          "username": "\t\t\t\tsba21\t\t\t",
          "content": "It looks like A-B",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 709943,
          "date": "Wed 02 Nov 2022 18:09",
          "username": "\t\t\t\tiis\t\t\t",
          "content": "AB is OK. It can be done more straightforwardly. Just connect the S3 event to Lambda, and it is done. I don't think we need SQS or anything.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Use SQS can make it more durable.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 771951,
          "date": "Wed 11 Jan 2023 02:48",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "Use SQS can make it more durable.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 835738,
          "date": "Sat 11 Mar 2023 09:27",
          "username": "\t\t\t\tcheese929\t\t\t",
          "content": "Agree with the general answer. its A+B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 827240,
          "date": "Thu 02 Mar 2023 19:34",
          "username": "\t\t\t\tNikhilcy\t\t\t",
          "content": "Why B?<br>Message gets automatically deleted from queue once it goes out of it. FIFO<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Not deleted but hidden while being processed</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 833703,
          "date": "Thu 09 Mar 2023 09:14",
          "username": "\t\t\t\tcamelstrike\t\t\t",
          "content": "Not deleted but hidden while being processed",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823365,
          "date": "Mon 27 Feb 2023 09:00",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "AB definitely Okay",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 807155,
          "date": "Mon 13 Feb 2023 08:54",
          "username": "\t\t\t\tbuiducvu\t\t\t",
          "content": "AB definitely Okay",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 767943,
          "date": "Fri 06 Jan 2023 18:58",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "AB definitely Okay",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 760276,
          "date": "Wed 28 Dec 2022 20:52",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Obviously A & B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 759021,
          "date": "Tue 27 Dec 2022 21:20",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To design a solution that uses durable, stateless components to process images automatically, a solutions architect could consider the following actions:<br><br>Option A involves creating an SQS queue and configuring the S3 bucket to send a notification to the queue when an image is uploaded. This allows the application to decouple the image upload process from the image processing process and ensures that the image processing process is triggered automatically when a new image is uploaded. <br><br>Option B involves configuring the Lambda function to use the SQS queue as the invocation source. When the SQS message is successfully processed, the message is deleted from the queue. This ensures that the Lambda function is invoked only once per image and that the image is not processed multiple times.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C is incorrect because it involves storing state (the file name) in memory, which is not a durable or scalable solution.<br><br>Option D is incorrect because it involves launching an EC2 instance to monitor the SQS queue, which is not a stateless solution.<br><br>Option E is incorrect because it involves using Amazon EventBridge (formerly Amazon CloudWatch Events) to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic, which is not related to the image processing process.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 759022,
          "date": "Tue 27 Dec 2022 21:20",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C is incorrect because it involves storing state (the file name) in memory, which is not a durable or scalable solution.<br><br>Option D is incorrect because it involves launching an EC2 instance to monitor the SQS queue, which is not a stateless solution.<br><br>Option E is incorrect because it involves using Amazon EventBridge (formerly Amazon CloudWatch Events) to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic, which is not related to the image processing process.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 750545,
          "date": "Tue 20 Dec 2022 06:54",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "1)SQS + Lambda 2) SQS FIFO + Lambda 3 ) SNS + Lambda",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743366,
          "date": "Mon 12 Dec 2022 23:02",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "A and B looks reasonable",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 741010,
          "date": "Sat 10 Dec 2022 14:39",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "ok, A and B are the \\\"correct\\\" options given the set that we were provided, but you can simply configure a trigger in the S3 to invoke the lambda that will process and upload the image... As an architect I would never go the way the solution is presented in this scenario.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 731187,
          "date": "Wed 30 Nov 2022 08:12",
          "username": "\t\t\t\tmiki111\t\t\t",
          "content": "AAAAAAAAAABBBBBBBBBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726158,
          "date": "Thu 24 Nov 2022 21:10",
          "username": "\t\t\t\tsherbo\t\t\t",
          "content": "A and B are most correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 723488,
          "date": "Mon 21 Nov 2022 13:45",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717596,
          "date": "Mon 14 Nov 2022 01:58",
          "username": "\t\t\t\tcrystally77\t\t\t",
          "content": "How about \\\"E\\\". Amazon EventBridge can monitor S3 bucket and send an alert to an SNS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>it required the owner's app process image which is not realistic in usage. It's like automation all process and manual the last steps using human effort.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727388,
          "date": "Sat 26 Nov 2022 09:18",
          "username": "\t\t\t\tTuLe\t\t\t",
          "content": "it required the owner's app process image which is not realistic in usage. It's like automation all process and manual the last steps using human effort.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 697659,
          "date": "Mon 17 Oct 2022 21:48",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "A+B seems to be correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AB"
        }
      ]
    },
    {
      "question_id": "#19",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC.  The application servers and database servers are deployed in private subnets in the same VPC.  The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC.  The appliance is configured with an IP interface that can accept IP packets.<br>A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#19",
          "answers": [
            {
              "choice": "<p>A. Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy a Gateway Load Balancer in the inspection VPC.  Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 689343,
          "date": "Sat 08 Oct 2022 15:23",
          "username": "\t\t\t\tCloudGuru99\t\t\t",
          "content": "Answer is D . Use Gateway Load balancer <br>REF: https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-inspection-using-aws-gateway-load-balancer/",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 712194,
          "date": "Sun 06 Nov 2022 09:15",
          "username": "\t\t\t\tpm2229\t\t\t",
          "content": "It's D, Coz.. Gateway Load Balancer is a new type of load balancer that operates at layer 3 of the OSI model and is built on Hyperplane, which is capable of handling several thousands of connections per second. Gateway Load Balancer endpoints are configured in spoke VPCs originating or receiving traffic from the Internet. This architecture allows you to perform inline inspection of traffic from multiple spoke VPCs in a simplified and scalable fashion while still centralizing your virtual appliances.",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 837481,
          "date": "Mon 13 Mar 2023 01:12",
          "username": "\t\t\t\tBang3R\t\t\t",
          "content": "D.  Deploy a Gateway Load Balancer in the inspection VPC.  Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.<br><br>A Gateway Load Balancer can inspect traffic before forwarding it to a virtual appliance for additional processing. The solution will not require changing the existing architecture and will have the least amount of operational overhead. The appliance can be configured with a specific IP interface to accept IP packets. The Gateway Load Balancer can be configured with an endpoint to route incoming packets to the appliance. The solution ensures all traffic to the web application is inspected before it reaches the web server.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823739,
          "date": "Mon 27 Feb 2023 14:35",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. This decreases potential points of failure in your network and increases availability.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 778416,
          "date": "Tue 17 Jan 2023 01:46",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774066,
          "date": "Fri 13 Jan 2023 04:23",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "A.  Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768039,
          "date": "Fri 06 Jan 2023 21:01",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The solution with the least operational overhead would be option D: Deploy a Gateway Load Balancer in the inspection VPC.  Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.<br><br>A Gateway Load Balancer is a fully managed, network layer load balancer that routes traffic to targets in VPCs and on-premises networks. It is designed to handle millions of requests per second while maintaining high performance and low latencies. It also integrates with Amazon VPC to allow traffic to flow between your on-premises data centers and your VPCs.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765954,
          "date": "Wed 04 Jan 2023 18:39",
          "username": "\t\t\t\tCloud69\t\t\t",
          "content": "ChatGPT says B is correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For me it shows A</li><li>Dont trust chatgpt,run in 2 3 session it will change answer.Marketing hype.Needs a lot of work .</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 802848,
          "date": "Thu 09 Feb 2023 06:12",
          "username": "\t\t\t\tSONA_M_\t\t\t",
          "content": "For me it shows A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776280,
          "date": "Sun 15 Jan 2023 09:42",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Dont trust chatgpt,run in 2 3 session it will change answer.Marketing hype.Needs a lot of work .",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763485,
          "date": "Mon 02 Jan 2023 05:16",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "As only gateway load balancer helps you work with packets.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759023,
          "date": "Tue 27 Dec 2022 21:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements with the least operational overhead is D: Deploy a Gateway Load Balancer in the inspection VPC and create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.<br><br>A Gateway Load Balancer is a fully managed service that provides a single point of contact for clients and distributes incoming traffic across multiple targets, such as Amazon Elastic Compute Cloud (EC2) instances and containers, in one or more virtual private clouds (VPCs). You can deploy a Gateway Load Balancer in the inspection VPC and create a Gateway Load Balancer endpoint to receive the incoming packets from the web servers in the application's VPC and forward the packets to the appliance for packet inspection. This will allow you to inspect all traffic to the web application with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because a Network Load Balancer is designed to handle traffic at the connection level and is not suitable for packet inspection.<br><br>Option B is incorrect because an Application Load Balancer is designed to handle traffic at the request level and is not suitable for packet inspection.<br><br>Option C is incorrect because a transit gateway is designed to allow multiple VPCs and on-premises networks to connect to each other, but it is not suitable for packet inspection.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759024,
          "date": "Tue 27 Dec 2022 21:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because a Network Load Balancer is designed to handle traffic at the connection level and is not suitable for packet inspection.<br><br>Option B is incorrect because an Application Load Balancer is designed to handle traffic at the request level and is not suitable for packet inspection.<br><br>Option C is incorrect because a transit gateway is designed to allow multiple VPCs and on-premises networks to connect to each other, but it is not suitable for packet inspection.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 749707,
          "date": "Mon 19 Dec 2022 11:09",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/<br>Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. This decreases potential points of failure in your network and increases availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>easy and meaning</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757153,
          "date": "Mon 26 Dec 2022 08:07",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "easy and meaning",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 746713,
          "date": "Fri 16 Dec 2022 03:37",
          "username": "\t\t\t\tyoben84\t\t\t",
          "content": "it says : The appliance is configured with an IP interface that can accept IP packets.<br>Can we understand why it is option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743373,
          "date": "Mon 12 Dec 2022 23:11",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "It operates at layer 3 and it is used for analysing network traffic",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741018,
          "date": "Sat 10 Dec 2022 14:47",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "I voted for D, but isn't this question a little weird? It suggests: FW in the inspection VPC -> web server in the public VPC -> web app in the private<br>Should this be: web server in the public vpc -> FW in the inspection VPC -> web app in the private? Have I miss read and missunderstood the question?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>the question says the SA needs to integrate \\\"Web Application [in private subnets] with the appliance [which is a third-party appliance].\\\" The AWS description of a Gateway Load Balancer is they \\\"help you easily deploy, scale, and manage your third-party virtual appliances''. I say, keep the question easy and show you know that GLB = manage 3rd party appliances. <br><br>https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 742959,
          "date": "Mon 12 Dec 2022 16:06",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "the question says the SA needs to integrate \\\"Web Application [in private subnets] with the appliance [which is a third-party appliance].\\\" The AWS description of a Gateway Load Balancer is they \\\"help you easily deploy, scale, and manage your third-party virtual appliances''. I say, keep the question easy and show you know that GLB = manage 3rd party appliances. <br><br>https://aws.amazon.com/elasticloadbalancing/gateway-load-balancer/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737516,
          "date": "Wed 07 Dec 2022 08:04",
          "username": "\t\t\t\tmuvest\t\t\t",
          "content": "D is the answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 731194,
          "date": "Wed 30 Nov 2022 08:18",
          "username": "\t\t\t\tmiki111\t\t\t",
          "content": "DDDDDDDDDD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 726165,
          "date": "Thu 24 Nov 2022 21:14",
          "username": "\t\t\t\tsherbo\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#20",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance.<br>A solutions architect needs to minimize the time that is required to clone the production data into the test environment.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#20",
          "answers": [
            {
              "choice": "<p>A. Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 704695,
          "date": "Wed 26 Oct 2022 15:03",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html<br><br>Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692623,
          "date": "Wed 12 Oct 2022 06:22",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "https://aws.amazon.com/cn/about-aws/whats-new/2020/11/amazon-ebs-fast-snapshot-restore-now-available-us-govcloud-regions/",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 823774,
          "date": "Mon 27 Feb 2023 15:02",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "The EBS fast snapshot restore feature allows you to restore EBS snapshots to new EBS volumes with minimal downtime. This is particularly useful when you need to restore large volumes or when you need to restore a volume to an EC2 instance in a different Availability Zone. When you enable the fast snapshot restore feature, the EBS volume is restored from the snapshot in the shortest amount of time possible, typically within a few minutes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 805012,
          "date": "Sat 11 Feb 2023 06:40",
          "username": "\t\t\t\tBofi\t\t\t",
          "content": "Option A is correct because the question stated that the software that will access the test environment needs High I/O performance which is the core feature of instance store. The only risk for instance store its lost when the EC2 that it is attached to is terminated, however, this is a test environment, long term durability may not be required. Option C is not correct because it mentioned creating a new EBS and restoring the snapshot. The snap shot can be restored without creating a new EBS. It did not satisfy the minimum overhead requirement",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 803530,
          "date": "Thu 09 Feb 2023 18:28",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "D.  They are all viable solutions, however EBS fast snapshot will increase the speed as the question does ask for minimal time and not about cost, automation, minimum overheads etc.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 774085,
          "date": "Fri 13 Jan 2023 05:07",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "C is correct<br>Option A, restoring EBS snapshots onto EC2 instance store volumes is not correct, because EC2 Instance store volumes are not as durable as EBS volumes, it may not guarantee the data durability and availability.<br>Option B, using the EBS Multi-Attach feature is not correct, because it would still need to detach and reattach the volumes, and it will cause the data unavailability.<br>Option D, using the EBS fast snapshot restore feature is not correct, it would still require to create new volumes and attach them to the instances, and it does not guarantee the data ready for use as soon as the restore process completes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is wrong because Multi-Attach (which isn't available for all instance types) allows attaching the SAME EBS volume to multiple EC2 instances, which would mean that modifications in the test environment would also modify production data.<br><br>Option D is correct, the data IS ready for use as soon as the restore process completes. It ensures that the I/O performance remains consistent even when reading blocks for the first time.<br><br>Option C is incorrect as it's saying you're creating new instances with completely new volumes and THEN restoring the EBS snapshots. Creating new, empty volumes is unnecessary. Just restore them from the EBS snapshot.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778949,
          "date": "Tue 17 Jan 2023 14:21",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Option B is wrong because Multi-Attach (which isn't available for all instance types) allows attaching the SAME EBS volume to multiple EC2 instances, which would mean that modifications in the test environment would also modify production data.<br><br>Option D is correct, the data IS ready for use as soon as the restore process completes. It ensures that the I/O performance remains consistent even when reading blocks for the first time.<br><br>Option C is incorrect as it's saying you're creating new instances with completely new volumes and THEN restoring the EBS snapshots. Creating new, empty volumes is unnecessary. Just restore them from the EBS snapshot.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774081,
          "date": "Fri 13 Jan 2023 05:02",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "C.  Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.<br>Take EBS snapshots of the production EBS volumes, which are point-in-time copies of the data.<br>Create and initialize new EBS volumes in the test environment.<br>Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots. This will allow the data to be ready for use as soon as the restore process completes, and it ensures that the software that accesses the data will have consistently high I/O performance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The EBS fast snapshot restore feature is the one that gives you consistently high I/O performance.<br><br>From the AWS docs:<br>\\\"Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.\\\"<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778953,
          "date": "Tue 17 Jan 2023 14:24",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "The EBS fast snapshot restore feature is the one that gives you consistently high I/O performance.<br><br>From the AWS docs:<br>\\\"Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.\\\"<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768049,
          "date": "Fri 06 Jan 2023 21:09",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The EBS fast snapshot restore feature allows you to restore EBS snapshots to new EBS volumes with minimal downtime. This is particularly useful when you need to restore large volumes or when you need to restore a volume to an EC2 instance in a different Availability Zone. When you enable the fast snapshot restore feature, the EBS volume is restored from the snapshot in the shortest amount of time possible, typically within a few minutes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763488,
          "date": "Mon 02 Jan 2023 05:24",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759027,
          "date": "Tue 27 Dec 2022 21:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements is D: Take EBS snapshots of the production EBS volumes, turn on the EBS fast snapshot restore feature on the EBS snapshots, and restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.<br><br>EBS fast snapshot restore is a feature that enables you to restore an EBS snapshot to a new EBS volume within seconds, providing consistently high I/O performance. By taking EBS snapshots of the production EBS volumes, turning on the EBS fast snapshot restore feature, and restoring the snapshots into new EBS volumes, you can quickly clone the production data into the test environment and minimize the time required to do so. The new EBS volumes can be attached to EC2 instances in the test environment to provide access to the cloned data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because restoring EBS snapshots onto EC2 instance store volumes will not provide consistently high I/O performance.<br><br>Option B is incorrect because using the EBS Multi-Attach feature to attach the production EBS volumes to the EC2 instances in the test environment could potentially affect the production environment and is not a recommended practice.<br><br>Option C is incorrect because creating and initializing new EBS volumes and restoring the production data onto them can take longer than restoring the data from an EBS snapshot with the EBS fast snapshot restore feature.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759029,
          "date": "Tue 27 Dec 2022 21:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because restoring EBS snapshots onto EC2 instance store volumes will not provide consistently high I/O performance.<br><br>Option B is incorrect because using the EBS Multi-Attach feature to attach the production EBS volumes to the EC2 instances in the test environment could potentially affect the production environment and is not a recommended practice.<br><br>Option C is incorrect because creating and initializing new EBS volumes and restoring the production data onto them can take longer than restoring the data from an EBS snapshot with the EBS fast snapshot restore feature.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 755842,
          "date": "Sun 25 Dec 2022 18:01",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "Selected Answer: D<br><br>Amazon EBS fast snapshot restore (FSR) enables you to create a volume from a snapshot that is fully initialized at creation. This eliminates the latency of I/O operations on a block when it is accessed for the first time. Volumes that are created using fast snapshot restore instantly deliver all of their provisioned performance.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-fast-snapshot-restore.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754921,
          "date": "Sat 24 Dec 2022 15:06",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/new-amazon-ebs-fast-snapshot-restore-fsr/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747470,
          "date": "Fri 16 Dec 2022 18:52",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "D.  Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743229,
          "date": "Mon 12 Dec 2022 20:10",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer :C <br>take EBS snapshots of the production EBS volumes and create new EBS volumes in the test environment. The new EBS volumes should be initialized and attached to the EC2 instances in the test environment before restoring the production data from the EBS snapshots. This will minimize the time that is required to clone the production data, as the new EBS volumes will be ready to accept the data from the EBS snapshots as soon as the snapshots are restored. Option D, using the EBS fast snapshot restore feature, would not provide a solution for minimizing the time that is required to clone the data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723491,
          "date": "Mon 21 Nov 2022 13:47",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716625,
          "date": "Sat 12 Nov 2022 10:55",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Minimize the time is a key requirement. So D. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 697669,
          "date": "Mon 17 Oct 2022 22:06",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "D seems to be the correct one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#21",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#21",
          "answers": [
            {
              "choice": "<p>A. Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB. <br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 693746,
          "date": "Thu 13 Oct 2022 10:38",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "D because all of the components are infinitely scalable<br>dynamoDB, API Gateway, Lambda, and of course s3+cloudfront",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759032,
          "date": "Tue 27 Dec 2022 21:24",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements with the least operational overhead is D: Use an Amazon S3 bucket to host the website's static content, deploy an Amazon CloudFront distribution, set the S3 bucket as the origin, and use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB. <br><br>Using Amazon S3 to host static content and Amazon CloudFront to distribute the content can provide high performance and scale for websites with millions of requests each hour. Amazon API Gateway and AWS Lambda can be used to build scalable and highly available backend APIs to support the website, and Amazon DynamoDB can be used to store the data. This solution requires minimal operational overhead as it leverages fully managed services that automatically scale to meet demand.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because using multiple S3 buckets to host the full website would not provide the required performance and scale for millions of requests each hour with millisecond latency.<br><br>Option B is incorrect because deploying the full website on EC2 instances and using an Application Load Balancer (ALB) and an RDS database would require more operational overhead to maintain and scale the infrastructure.<br><br>Option C is incorrect because while deploying the application in containers and hosting them on Amazon Elastic Kubernetes Service (EKS) can provide high performance and scale, it would require more operational overhead to maintain and scale the infrastructure compared to using fully managed services like S3 and CloudFront.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759034,
          "date": "Tue 27 Dec 2022 21:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because using multiple S3 buckets to host the full website would not provide the required performance and scale for millions of requests each hour with millisecond latency.<br><br>Option B is incorrect because deploying the full website on EC2 instances and using an Application Load Balancer (ALB) and an RDS database would require more operational overhead to maintain and scale the infrastructure.<br><br>Option C is incorrect because while deploying the application in containers and hosting them on Amazon Elastic Kubernetes Service (EKS) can provide high performance and scale, it would require more operational overhead to maintain and scale the infrastructure compared to using fully managed services like S3 and CloudFront.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 750299,
          "date": "Mon 19 Dec 2022 23:41",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "high I/O = DynamoDB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749716,
          "date": "Mon 19 Dec 2022 11:23",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "millisecond latency --> DynamoDB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 740802,
          "date": "Sat 10 Dec 2022 10:19",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "only all services in D are auto-scaling",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723494,
          "date": "Mon 21 Nov 2022 13:52",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723174,
          "date": "Mon 21 Nov 2022 06:17",
          "username": "\t\t\t\tABCMail\t\t\t",
          "content": "Serverless technologies are better options",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 716629,
          "date": "Sat 12 Nov 2022 11:01",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Why not B? Application load balancer can accept millions of request/hr?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For me, the keyword was millisecond latency. Option B suggests RDS as the database, but Option D is DynamoDB. <br><br>DynamoDB - Fast, flexible NoSQL database service for single-digit millisecond performance at any scale</li><li>Yes, and also LEAST operational overhead. Scaling the application on EC2 instance is hard work require the very good architect.</li><li>And scaling takes time, so Auto Scaling groups cannot react instantly to a massive surge in demand</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 717604,
          "date": "Mon 14 Nov 2022 02:16",
          "username": "\t\t\t\tkeithkifo\t\t\t",
          "content": "For me, the keyword was millisecond latency. Option B suggests RDS as the database, but Option D is DynamoDB. <br><br>DynamoDB - Fast, flexible NoSQL database service for single-digit millisecond performance at any scale<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes, and also LEAST operational overhead. Scaling the application on EC2 instance is hard work require the very good architect.</li><li>And scaling takes time, so Auto Scaling groups cannot react instantly to a massive surge in demand</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727393,
          "date": "Sat 26 Nov 2022 09:37",
          "username": "\t\t\t\tTuLe\t\t\t",
          "content": "Yes, and also LEAST operational overhead. Scaling the application on EC2 instance is hard work require the very good architect.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>And scaling takes time, so Auto Scaling groups cannot react instantly to a massive surge in demand</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732637,
          "date": "Thu 01 Dec 2022 14:12",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "And scaling takes time, so Auto Scaling groups cannot react instantly to a massive surge in demand",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 716053,
          "date": "Fri 11 Nov 2022 13:47",
          "username": "\t\t\t\tsodyam\t\t\t",
          "content": "D is the correct answer due to milliseconds latency which will involve cloud front.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 713928,
          "date": "Tue 08 Nov 2022 16:36",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "D is the correct answer due to milliseconds latency which will involve cloud front.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708063,
          "date": "Sun 30 Oct 2022 23:14",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Ans is correct D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 697670,
          "date": "Mon 17 Oct 2022 22:08",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "D is the correct one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 696466,
          "date": "Sun 16 Oct 2022 21:26",
          "username": "\t\t\t\tqueen101\t\t\t",
          "content": "DDDDDDDDDDDDDDDD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695065,
          "date": "Sat 15 Oct 2022 02:21",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "D: because of least operational overhead",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 692629,
          "date": "Wed 12 Oct 2022 06:33",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I feel like the scenario is not only static resource but also dynamic resources.<br>API Gateway + Lambda has a good scalibility",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692444,
          "date": "Wed 12 Oct 2022 00:22",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "the answer is D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#22",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files.<br>Which storage option meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#22",
          "answers": [
            {
              "choice": "<p>A. S3 Standard<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. S3 Intelligent-Tiering<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. S3 Standard-Infrequent Access (S3 Standard-IA)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. S3 One Zone-Infrequent Access (S3 One Zone-IA)<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696276,
          "date": "Sun 16 Oct 2022 16:10",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "\\\"unpredictable pattern\\\" - always go for Intelligent Tiering of S3<br>It also meets the resiliency requirement: \\\"S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive redundantly store objects on multiple devices across a minimum of three Availability Zones in an AWS Region\\\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 826100,
          "date": "Wed 01 Mar 2023 18:07",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 789345,
          "date": "Fri 27 Jan 2023 06:36",
          "username": "\t\t\t\tRishi1\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774090,
          "date": "Fri 13 Jan 2023 05:14",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "C.  S3 Standard-Infrequent Access (S3 Standard-IA)<br><br>S3 Standard-IA is designed for infrequently accessed data, which is a good fit for the media files that are rarely accessed in an unpredictable pattern. S3 Standard-IA is also cross-Region replicated, providing resilience to the loss of an Availability Zone. Additionally, S3 Standard-IA has a lower storage and retrieval cost compared to S3 Standard and S3 Intelligent-Tiering, which makes it a cost-effective option for storing infrequently accessed data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769076,
          "date": "Sun 08 Jan 2023 05:06",
          "username": "\t\t\t\tvinhle\t\t\t",
          "content": "B is clearly",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759035,
          "date": "Tue 27 Dec 2022 21:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The storage option that meets these requirements is B: S3 Intelligent-Tiering.<br><br>Amazon S3 Intelligent Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. It can store objects in two access tiers: the frequent access tier and the infrequent access tier. The frequent access tier is optimized for frequently accessed objects and is charged at the same rate as S3 Standard. The infrequent access tier is optimized for objects that are not accessed frequently and are charged at a lower rate than S3 Standard.<br><br>S3 Intelligent Tiering is a good choice for storing media files that are accessed frequently and infrequently in an unpredictable pattern because it automatically moves data to the most cost-effective storage tier based on access patterns, minimizing storage and retrieval costs. It is also resilient to the loss of an Availability Zone because it stores objects in multiple Availability Zones within a region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, S3 Standard, is not a good choice because it does not offer the cost optimization of S3 Intelligent-Tiering.<br><br>Option C, S3 Standard-Infrequent Access (S3 Standard-IA), is not a good choice because it is optimized for infrequently accessed objects and does not offer the cost optimization of S3 Intelligent-Tiering.<br><br>Option D, S3 One Zone-Infrequent Access (S3 One Zone-IA), is not a good choice because it is not resilient to the loss of an Availability Zone. It stores objects in a single Availability Zone, making it less durable than other storage classes.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759036,
          "date": "Tue 27 Dec 2022 21:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, S3 Standard, is not a good choice because it does not offer the cost optimization of S3 Intelligent-Tiering.<br><br>Option C, S3 Standard-Infrequent Access (S3 Standard-IA), is not a good choice because it is optimized for infrequently accessed objects and does not offer the cost optimization of S3 Intelligent-Tiering.<br><br>Option D, S3 One Zone-Infrequent Access (S3 One Zone-IA), is not a good choice because it is not resilient to the loss of an Availability Zone. It stores objects in a single Availability Zone, making it less durable than other storage classes.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 750315,
          "date": "Tue 20 Dec 2022 00:22",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "unpredictable pattern = Intelligent Tiering",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 739313,
          "date": "Thu 08 Dec 2022 18:44",
          "username": "\t\t\t\t333666999\t\t\t",
          "content": "S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 736575,
          "date": "Tue 06 Dec 2022 07:41",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Since there are files which will be accessed frequently and others infrequently",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 735079,
          "date": "Sun 04 Dec 2022 13:39",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "\\\"unpredictable pattern\\\" - remember the keyword and always go for Intelligent Tiering of S3",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723495,
          "date": "Mon 21 Nov 2022 13:53",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715384,
          "date": "Thu 10 Nov 2022 17:29",
          "username": "\t\t\t\tAbhiJo\t\t\t",
          "content": "B is correct, C is incorrect because of requirement for frequent access as well",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 713929,
          "date": "Tue 08 Nov 2022 16:39",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "Since it said some data a access frequently and some are unpredictable, i will go for B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708066,
          "date": "Sun 30 Oct 2022 23:16",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ans is correct B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 697672,
          "date": "Mon 17 Oct 2022 22:10",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "B is the correct one.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 696467,
          "date": "Sun 16 Oct 2022 21:31",
          "username": "\t\t\t\tqueen101\t\t\t",
          "content": "BBBBBBBBBBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695066,
          "date": "Sat 15 Oct 2022 02:22",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "B.  S3 Intelligent-Tiering for unpredictable or vague usecase",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#23",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely.<br>Which storage solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#23",
          "answers": [
            {
              "choice": "<p>A. Configure S3 Intelligent-Tiering to automatically migrate objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 695067,
          "date": "Sat 15 Oct 2022 02:23",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "B: Transition to Glacier deep archive for cost efficiency",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 759037,
          "date": "Tue 27 Dec 2022 21:27",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The storage solution that will meet these requirements most cost-effectively is B: Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.<br><br>Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost Amazon S3 storage class for long-term retention of data that is rarely accessed and for which retrieval times of several hours are acceptable. It is the lowest-cost storage option in Amazon S3, making it a cost-effective choice for storing backup files that are not accessed after 1 month.<br><br>You can use an S3 Lifecycle configuration to automatically transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month. This will minimize the storage costs for the backup files that are not accessed frequently.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, configuring S3 Intelligent-Tiering to automatically migrate objects, is not a good choice because it is not designed for long-term storage and does not offer the cost benefits of S3 Glacier Deep Archive.<br><br>Option C, transitioning objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.<br><br>Option D, transitioning objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.</li><li>Also S3 Standard-IA &amp; One Zone-IA stores the data for max of 30 days and not indefinitely.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759038,
          "date": "Tue 27 Dec 2022 21:27",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, configuring S3 Intelligent-Tiering to automatically migrate objects, is not a good choice because it is not designed for long-term storage and does not offer the cost benefits of S3 Glacier Deep Archive.<br><br>Option C, transitioning objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.<br><br>Option D, transitioning objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month, is not a good choice because it is not the lowest-cost storage option and would not provide the cost benefits of S3 Glacier Deep Archive.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Also S3 Standard-IA &amp; One Zone-IA stores the data for max of 30 days and not indefinitely.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 773324,
          "date": "Thu 12 Jan 2023 10:48",
          "username": "\t\t\t\tvgchan\t\t\t",
          "content": "Also S3 Standard-IA & One Zone-IA stores the data for max of 30 days and not indefinitely.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 826228,
          "date": "Wed 01 Mar 2023 20:52",
          "username": "\t\t\t\tenc_0343\t\t\t",
          "content": "The answer is B.  \\\"S3 Glacier Deep Archive is Amazon S3's lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year.\\\" See here: https://aws.amazon.com/s3/storage-classes/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 821156,
          "date": "Sat 25 Feb 2023 04:39",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "Files are only required to be kept up to 7 years for businesses to Deep archive is the most cost optimal as well as useful in this scenario.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750320,
          "date": "Tue 20 Dec 2022 00:26",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "Glacier deep archive = lowest cost (accessed once or twice a year)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749673,
          "date": "Mon 19 Dec 2022 10:31",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "Correct answer: B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747498,
          "date": "Fri 16 Dec 2022 19:33",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Transition to Glacier is cost effective.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743798,
          "date": "Tue 13 Dec 2022 10:11",
          "username": "\t\t\t\tHacar\t\t\t",
          "content": "B is the answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743263,
          "date": "Mon 12 Dec 2022 20:34",
          "username": "\t\t\t\tCertified101\t\t\t",
          "content": "Amazon S3 Glacier Deep Archive – for long term storage: Minimum storage duration of 180 days",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736576,
          "date": "Tue 06 Dec 2022 07:43",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Since deep archive is the cheapest storage option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 735266,
          "date": "Sun 04 Dec 2022 18:01",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "Deep archive is cheaper",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732223,
          "date": "Thu 01 Dec 2022 04:57",
          "username": "\t\t\t\tENNYBOLA\t\t\t",
          "content": "i thought it can only go to deep archive after 90 days?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Nah pretty sure its minimum storage time 180 days. Meaning you can't remove it from glacier storage for half a year, but you can put it into glacier whenever you want.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 805756,
          "date": "Sun 12 Feb 2023 00:44",
          "username": "\t\t\t\tlofzee\t\t\t",
          "content": "Nah pretty sure its minimum storage time 180 days. Meaning you can't remove it from glacier storage for half a year, but you can put it into glacier whenever you want.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723496,
          "date": "Mon 21 Nov 2022 13:54",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723196,
          "date": "Mon 21 Nov 2022 06:58",
          "username": "\t\t\t\tTsho\t\t\t",
          "content": "BBBBBBBBB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721070,
          "date": "Fri 18 Nov 2022 06:07",
          "username": "\t\t\t\trenekton\t\t\t",
          "content": "B is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 713932,
          "date": "Tue 08 Nov 2022 16:41",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 697673,
          "date": "Mon 17 Oct 2022 22:11",
          "username": "\t\t\t\tGameDad09\t\t\t",
          "content": "B is the correct one.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#24",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling.<br>How should the solutions architect generate the information with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#24",
          "answers": [
            {
              "choice": "<p>A. Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692095,
          "date": "Tue 11 Oct 2022 14:29",
          "username": "\t\t\t\tsba21\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/68306-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "21",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 696285,
          "date": "Sun 16 Oct 2022 16:30",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "The requested result is a graph, so...<br>A - can't be as the result is a report<br>B - can't be as it is limited to 14 days visibility and the graph has to cover 2 months<br>C - seems to provide graphs and the best option available, as...<br>D - could provide graphs, BUT involves operational overhead, which has been requested to be minimised.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Cost Explorer, AWS prepares the data about your costs for the current month and the last 12 months: https://aws.amazon.com/aws-cost-management/aws-cost-explorer/</li><li>14 days? Fam, you ever logged into the console?</li><li>B.  This is correct because there is no limit of 14 days. Quoted from Amazon \\\"AWS prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months.\\\" (https://aws.amazon.com/aws-cost-management/aws-cost-explorer/).</li><li>12 months data visible on Cost Explorer.</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 731271,
          "date": "Wed 30 Nov 2022 10:30",
          "username": "\t\t\t\tUdoyen\t\t\t",
          "content": "Cost Explorer, AWS prepares the data about your costs for the current month and the last 12 months: https://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 805758,
          "date": "Sun 12 Feb 2023 00:47",
          "username": "\t\t\t\tlofzee\t\t\t",
          "content": "14 days? Fam, you ever logged into the console?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 798101,
          "date": "Sat 04 Feb 2023 16:23",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "B.  This is correct because there is no limit of 14 days. Quoted from Amazon \\\"AWS prepares the data about your costs for the current month and the last 12 months, and then calculates the forecast for the next 12 months.\\\" (https://aws.amazon.com/aws-cost-management/aws-cost-explorer/).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700643,
          "date": "Fri 21 Oct 2022 10:02",
          "username": "\t\t\t\tgoku58\t\t\t",
          "content": "12 months data visible on Cost Explorer.",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 816183,
          "date": "Tue 21 Feb 2023 05:16",
          "username": "\t\t\t\ttoohuynh\t\t\t",
          "content": "Feel like all the answers have a little bit ambiguous, so here is my breaking them down:<br>AWS Billing and Cost Management provides a summarised view of spending i.e. what you spent so far this month, and the predicted end of month bill, this is quite static and gives you a high level overview of spending. In addition you can configure your billing details from here. All of these features are free to use with no charge for accessing the interface.<br><br>AWS Cost explorer on the other hand is a paid service ($0.01 per query). By using cost explorer you can dig down into the finer details of expenditure, such as on a region, service, usage type or even tag based level. Using this you can identify costs by targeting your query to be specific enough to identify these charges. Additionally you can make use of hourly billing to get the most accurate upto date billing",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 813160,
          "date": "Sat 18 Feb 2023 16:06",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "B.  is correct.C.  there is not such thing as \\\"the AWS Billing and Cost Management dashboard\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 794880,
          "date": "Wed 01 Feb 2023 07:32",
          "username": "\t\t\t\thanen\t\t\t",
          "content": "AWS Cost Explorer would be the easiest way to graph this data. Cost Explorer can be accessed easily and has features for filtering billing data and graphing across relevant time periods.<br>https://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 789346,
          "date": "Fri 27 Jan 2023 06:44",
          "username": "\t\t\t\tRishi1\t\t\t",
          "content": "most comprehensive cost tool --B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 785218,
          "date": "Mon 23 Jan 2023 11:26",
          "username": "\t\t\t\tTony1980\t\t\t",
          "content": "Correct Answer is B:<br>The solutions architect can use the AWS Cost Explorer to generate a graph comparing the last 2 months of EC2 costs. This tool allows the user to view and analyze cost and usage data, and can be used to identify the root cause of the vertical scaling. Additionally, the solutions architect can use CloudWatch metrics to monitor the resource usage of the specific instances in question and identify any abnormal behavior. This solution would have minimal operational overhead as it utilizes built-in AWS services that do not require additional setup or maintenance.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 785217,
          "date": "Mon 23 Jan 2023 11:25",
          "username": "\t\t\t\tTony1980\t\t\t",
          "content": "The solutions architect can use the AWS Cost Explorer to generate a graph comparing the last 2 months of EC2 costs. This tool allows the user to view and analyze cost and usage data, and can be used to identify the root cause of the vertical scaling. Additionally, the solutions architect can use CloudWatch metrics to monitor the resource usage of the specific instances in question and identify any abnormal behavior. This solution would have minimal operational overhead as it utilizes built-in AWS services that do not require additional setup or maintenance.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782414,
          "date": "Fri 20 Jan 2023 16:32",
          "username": "\t\t\t\tjainparag1\t\t\t",
          "content": "B seems correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778593,
          "date": "Tue 17 Jan 2023 06:28",
          "username": "\t\t\t\tnalindm\t\t\t",
          "content": "A is incorrect as AWS Budgets does not support for this use case as it's suitable for users to better manage their AWS costs and avoid unexpected charges. D is wrong as it not a LEAST operational overhead solution with using QuickSight. C is incorrect as AWS Billing and Cost Management dashboard does not give you in-depth analysis of this use case with a graphical interface.<br>There for B is the correct answer with Cost Explorer's granular filtering feature will give you in-depth analysis with graphical view.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 771583,
          "date": "Tue 10 Jan 2023 16:51",
          "username": "\t\t\t\tmackeda\t\t\t",
          "content": "C, Please refer the following link https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/view-billing-dashboard.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As far as I can see, it says that the Billing Dashboard only provides a general view of your spending, whereas the question is asking for in-depth analysis. <br><br>From your link:<br>\\\"Viewing your AWS costs in the AWS Billing console dashboard doesn't require turning on Cost Explorer. To turn on Cost Explorer to access additional views of your cost and usage data, see Enabling AWS Cost Explorer.\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778984,
          "date": "Tue 17 Jan 2023 14:53",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "As far as I can see, it says that the Billing Dashboard only provides a general view of your spending, whereas the question is asking for in-depth analysis. <br><br>From your link:<br>\\\"Viewing your AWS costs in the AWS Billing console dashboard doesn't require turning on Cost Explorer. To turn on Cost Explorer to access additional views of your cost and usage data, see Enabling AWS Cost Explorer.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769223,
          "date": "Sun 08 Jan 2023 09:58",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "I prefer B. <br>I think this is not a good question. Cost Explorer is under the AWS billing & cost management service, i.e. Cost Explorer is a kind of dashboards from the latter one. But answer B states \\\"in-depth analysis\\\" which matches the question's need.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 768063,
          "date": "Fri 06 Jan 2023 21:31",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The AWS Billing and Cost Management dashboard provides a set of default graphs that allow you to view your costs and usage by service, by linked account, by date, and more. You can use the dashboard to view the cost and usage trends for your resources and identify the root cause of cost increases. You can also customize the graphs by adding or removing data points, adjusting the time period, or changing the graph type.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Do you have a link to AWS Docs where it says you can identify root causes of cost increases using the Billing and Cost Management dashbaord?<br><br>\\\"Cost Anomaly Detection\\\" identifies root causes of cost increases, and it's a feature of Cost Explorer (which in turn is a feature of Cost Management). So I would have thought the correct answer is B. <br>https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778989,
          "date": "Tue 17 Jan 2023 14:56",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Do you have a link to AWS Docs where it says you can identify root causes of cost increases using the Billing and Cost Management dashbaord?<br><br>\\\"Cost Anomaly Detection\\\" identifies root causes of cost increases, and it's a feature of Cost Explorer (which in turn is a feature of Cost Management). So I would have thought the correct answer is B. <br>https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755846,
          "date": "Sun 25 Dec 2022 18:05",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "Selected Answer: B<br>AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Get started quickly by creating custom reports that analyze cost and usage data. Analyze your data at a high level (for example, total costs and usage across all accounts) or dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies.<br>https://aws.amazon.com/aws-cost-management/aws-cost-explorer/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750393,
          "date": "Tue 20 Dec 2022 03:01",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will generate the information with the least operational overhead is C: Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.<br><br>The AWS Billing and Cost Management dashboard provides a range of tools and features to help you monitor and optimize your AWS costs. It includes customizable graphs that allow you to view and compare your costs across various dimensions, such as instance types. You can use the graphs from the dashboard to compare EC2 costs based on instance types for the last 2 months to identify the root cause of the vertical scaling. This will allow you to quickly and easily perform an in-depth analysis of your EC2 costs with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using AWS Budgets to create a budget report, is not a good choice because it does not provide the granular level of detail needed to identify the root cause of the vertical scaling.<br><br>Option B, using Cost Explorer's granular filtering feature, is not a good choice because it requires additional operational overhead to set up and use.<br><br>Option D, using AWS Cost and Usage Reports and Amazon QuickSight, is not a good choice because it requires additional operational overhead to set up and use, and is more complex than using the graphs from the AWS Billing and Cost Management dashboard.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750394,
          "date": "Tue 20 Dec 2022 03:01",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using AWS Budgets to create a budget report, is not a good choice because it does not provide the granular level of detail needed to identify the root cause of the vertical scaling.<br><br>Option B, using Cost Explorer's granular filtering feature, is not a good choice because it requires additional operational overhead to set up and use.<br><br>Option D, using AWS Cost and Usage Reports and Amazon QuickSight, is not a good choice because it requires additional operational overhead to set up and use, and is more complex than using the graphs from the AWS Billing and Cost Management dashboard.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750337,
          "date": "Tue 20 Dec 2022 00:55",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "needs to create a graph comparing the last 2 months of EC2 costs= cost explorer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743276,
          "date": "Mon 12 Dec 2022 20:48",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B <br>AWS Cost Explorer to perform an in-depth analysis of EC2 costs based on instance types. Cost Explorer allows the user to filter the cost and usage data by different dimensions, including instance type, to identify trends and anomalies in the usage and costs. Option C, using graphs from the AWS Billing and Cost Management dashboard, would not provide the necessary level of granularity to perform an in-depth analysis of EC2 costs based on instance types.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#25",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database.<br>During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#25",
          "answers": [
            {
              "choice": "<p>A. Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696302,
          "date": "Sun 16 Oct 2022 16:48",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A - refactoring can be a solution, BUT requires a LOT of effort - not the answer<br>B - DynamoDB is NoSQL and Aurora is SQL, so it requires a DB migration... again a LOT of effort, so no the answer<br>C and D are similar in structure, but...<br>C uses SNS, which would notify the 2nd Lambda function... provoking the same bottleneck... not the solution<br>D uses SQS, so the 2nd lambda function can go to the queue when responsive to keep with the DB load process. <br>Usually the app decoupling helps with the performance improvement by distributing load. In this case, the bottleneck is solved by uses queues... so D is the answer.",
          "upvote_count": "40",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768067,
          "date": "Fri 06 Jan 2023 21:35",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "By using two Lambda functions, you can separate the tasks of receiving the information and loading the information into the database. This will allow you to scale each function independently, improving scalability.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759044,
          "date": "Tue 27 Dec 2022 21:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements is D: Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.<br><br>Using separate Lambda functions for receiving and loading the information can help improve scalability and minimize the configuration effort. By using an Amazon SQS queue to integrate the Lambda functions, you can decouple the functions and allow them to scale independently. This can help reduce the burden on the receiving function, improving its performance and scalability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, refactoring the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances and connecting the database using native JDBC drivers, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option B, changing the platform from Aurora to Amazon DynamoDB and provisioning a DynamoDB Accelerator (DAX) cluster, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option C, integrating the Lambda functions using Amazon SNS, is not a good choice because it does not provide the decoupling and scaling benefits of using an Amazon SQS queue.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759045,
          "date": "Tue 27 Dec 2022 21:32",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, refactoring the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances and connecting the database using native JDBC drivers, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option B, changing the platform from Aurora to Amazon DynamoDB and provisioning a DynamoDB Accelerator (DAX) cluster, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option C, integrating the Lambda functions using Amazon SNS, is not a good choice because it does not provide the decoupling and scaling benefits of using an Amazon SQS queue.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 757386,
          "date": "Mon 26 Dec 2022 13:16",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "It's D (100%)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750815,
          "date": "Tue 20 Dec 2022 13:00",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "improve scalability = SQS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750396,
          "date": "Tue 20 Dec 2022 03:04",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements is D: Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.<br><br>Using separate Lambda functions for receiving and loading the information can help improve scalability and minimize the configuration effort. By using an Amazon SQS queue to integrate the Lambda functions, you can decouple the functions and allow them to scale independently. This can help reduce the burden on the receiving function, improving its performance and scalability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, refactoring the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances and connecting the database using native JDBC drivers, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option B, changing the platform from Aurora to Amazon DynamoDB and provisioning a DynamoDB Accelerator (DAX) cluster, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option C, integrating the Lambda functions using Amazon SNS, is not a good choice because it does not provide the decoupling and scaling benefits of using an Amazon SQS queue.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750398,
          "date": "Tue 20 Dec 2022 03:04",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, refactoring the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances and connecting the database using native JDBC drivers, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option B, changing the platform from Aurora to Amazon DynamoDB and provisioning a DynamoDB Accelerator (DAX) cluster, is not a good choice because it would require significant effort to redesign and refactor the code and would not improve scalability.<br><br>Option C, integrating the Lambda functions using Amazon SNS, is not a good choice because it does not provide the decoupling and scaling benefits of using an Amazon SQS queue.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723498,
          "date": "Mon 21 Nov 2022 13:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723204,
          "date": "Mon 21 Nov 2022 07:16",
          "username": "\t\t\t\tABCMail\t\t\t",
          "content": "Two single responsibility functions offer a better solution.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 721796,
          "date": "Sat 19 Nov 2022 07:11",
          "username": "\t\t\t\takosigengen\t\t\t",
          "content": "D.  Keyword is to handle load which will be taking care of bySQS.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 692707,
          "date": "Wed 12 Oct 2022 07:55",
          "username": "\t\t\t\tAjai23\t\t\t",
          "content": "Process of elimination, D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692659,
          "date": "Wed 12 Oct 2022 07:10",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "Atually I'm really confused by those options.<br>A is not right obiously, but the remaining options don't make sense, either...<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The idea is to avoid bottleneck on processing data by splitting the processes in two stages using two different Lambda and insert an SQS as intermediary so to crate an asynchronous process</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743382,
          "date": "Mon 12 Dec 2022 23:28",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "The idea is to avoid bottleneck on processing data by splitting the processes in two stages using two different Lambda and insert an SQS as intermediary so to crate an asynchronous process",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 692448,
          "date": "Wed 12 Oct 2022 00:31",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "the answer is D",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#26",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes.<br>What should a solutions architect do to accomplish this goal?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#26",
          "answers": [
            {
              "choice": "<p>A. Turn on AWS Config with the appropriate rules.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Turn on AWS Trusted Advisor with the appropriate checks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Turn on Amazon Inspector with the appropriate assessment template.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 711498,
          "date": "Sat 05 Nov 2022 03:08",
          "username": "\t\t\t\tgokalpkocer3\t\t\t",
          "content": "Configuration changes= AWS Config",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 759048,
          "date": "Tue 27 Dec 2022 21:32",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will accomplish this goal is A: Turn on AWS Config with the appropriate rules.<br><br>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor and record changes to the configuration of your Amazon S3 buckets. By turning on AWS Config and enabling the appropriate rules, you can ensure that your S3 buckets do not have unauthorized configuration changes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759049,
          "date": "Tue 27 Dec 2022 21:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 804911,
          "date": "Sat 11 Feb 2023 02:47",
          "username": "\t\t\t\tal64\t\t\t",
          "content": "aws: A - aws config",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 792601,
          "date": "Mon 30 Jan 2023 10:28",
          "username": "\t\t\t\tKhushna\t\t\t",
          "content": "AAAAaaaaaaaaaaaaaaaaa",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768083,
          "date": "Fri 06 Jan 2023 22:05",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "o ensure that Amazon S3 buckets do not have unauthorized configuration changes, a solutions architect should turn on AWS Config with the appropriate rules.<br><br>AWS Config is a service that provides you with a detailed view of the configuration of your AWS resources. It continuously records configuration changes to your resources and allows you to review, audit, and compare these changes over time. By turning on AWS Config and enabling the appropriate rules, you can monitor the configuration changes to your Amazon S3 buckets and receive notifications when unauthorized changes are made.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750838,
          "date": "Tue 20 Dec 2022 13:16",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "unauthorized config changes = aws config",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750402,
          "date": "Tue 20 Dec 2022 03:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will accomplish this goal is A: Turn on AWS Config with the appropriate rules.<br><br>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor and record changes to the configuration of your Amazon S3 buckets. By turning on AWS Config and enabling the appropriate rules, you can ensure that your S3 buckets do not have unauthorized configuration changes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750403,
          "date": "Tue 20 Dec 2022 03:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "AWS Trusted Advisor (Option B) is a service that provides best practice recommendations for your AWS resources, but it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon Inspector (Option C) is a service that helps you assess the security and compliance of your applications. While it can be used to assess the security of your S3 buckets, it does not monitor or record changes to the configuration of your S3 buckets.<br><br>Amazon S3 server access logging (Option D) enables you to log requests made to your S3 bucket. While it can help you identify changes to your S3 bucket, it does not monitor or record changes to the configuration of your S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736672,
          "date": "Tue 06 Dec 2022 10:19",
          "username": "\t\t\t\tmemiy12\t\t\t",
          "content": "AWS Config",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 736584,
          "date": "Tue 06 Dec 2022 07:54",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "AWS config will monitor config changes",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723499,
          "date": "Mon 21 Nov 2022 13:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723206,
          "date": "Mon 21 Nov 2022 07:23",
          "username": "\t\t\t\tABCMail\t\t\t",
          "content": "AWS config allows scrutiny of past chnages",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 717163,
          "date": "Sun 13 Nov 2022 08:59",
          "username": "\t\t\t\tgrzeev\t\t\t",
          "content": "AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 712961,
          "date": "Mon 07 Nov 2022 10:16",
          "username": "\t\t\t\tpspinelli19\t\t\t",
          "content": "With Config you can limit changes to your entire account/s.<br>https://www.examtopics.com/discussions/amazon/view/27941-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 709578,
          "date": "Wed 02 Nov 2022 03:43",
          "username": "\t\t\t\tSolarch\t\t\t",
          "content": "Answer is A.  Trusted Advisor gives you a general check of your system and identifies ways to optimize your infrastructue and improve it. <br>While AWS config is more about specific resource. Like stated ( S3 bucket).Config lets you select particular resource you want to evaluate .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708051,
          "date": "Sun 30 Oct 2022 22:24",
          "username": "\t\t\t\tkeezbadger\t\t\t",
          "content": "A is the right answer. The key word in the question is \\\"Review\\\" Hence. AWS config use case here, \\\"Evaluate resource configurations for potential vulnerabilities, and review your configuration history after potential incidents to examine your security posture.\\\"<br><br>Though Trusted advisor is similar but what it does is that, it provides important \\\"recommendations\\\" to optimize your cloud deployments, improve resilience, and address security gaps.<br><br>The keyword for Trusted advisor is Recommendation.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 696319,
          "date": "Sun 16 Oct 2022 17:08",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A - according to the picture in the documentation... \\\"AWS Config automatically evaluates the recorded configuration against the configuration that you specify.\\\"<br>https://d1.awsstatic.com/config-diagram-092122.974fe2a4cb6aae1fe564fdbbe30ab55841a9858e.png",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 695147,
          "date": "Sat 15 Oct 2022 05:17",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Config - With AWS Config, you can dive deep into how your bucket was configured at any point in time. Additionally,Config rules enable you to check whether your S3 buckets have logging and versioning enabled<br>https://aws.amazon.com/about-aws/whats-new/2016/10/record-and-govern-s3-bucket-configurations-with-aws-config/<br><br>S3 only permissions check is done by Trust advisor - apart from other checksroot MFA, SG open ports, RDS Public Snapshots , EBS Public Snapshots , IAM User - one min, Service limits",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#27",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#27",
          "answers": [
            {
              "choice": "<p>A. Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692736,
          "date": "Wed 12 Oct 2022 08:08",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Answere A : https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html<br><br>Share a single dashboard and designate specific email addresses of the people who can view the dashboard. Each of these users creates their own password that they must enter to view the dashboard.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks for the link! No doubt A is the answer.</li></ul>",
          "upvote_count": "35",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 696327,
          "date": "Sun 16 Oct 2022 17:17",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Thanks for the link! No doubt A is the answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 823123,
          "date": "Mon 27 Feb 2023 03:43",
          "username": "\t\t\t\tsky09\t\t\t",
          "content": "I will go with B, as ask is for a user ( manager) , not for everyone who gets the link. <br>The most secure and least privileged solution for providing access to an Amazon CloudWatch dashboard for a user without an AWS account is to create an IAM user for the product manager with the appropriate permissions. By attaching the CloudWatchReadOnlyAccess policy to the user, the product manager can access only the read-only activities of Amazon CloudWatch, as per the principle of least privilege. The solutions architect should then share the login credentials and browser URL of the correct dashboard with the product manager.<br><br>Option A is incorrect because it is not secure as it requires sharing the dashboard link, which could lead to unauthorized access.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But how can the manager use an IAM role when the question says they do not have an AWS account?</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 826230,
          "date": "Wed 01 Mar 2023 20:55",
          "username": "\t\t\t\tenc_0343\t\t\t",
          "content": "But how can the manager use an IAM role when the question says they do not have an AWS account?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823122,
          "date": "Mon 27 Feb 2023 03:39",
          "username": "\t\t\t\tsky09\t\t\t",
          "content": "i will go with B, because its asking for a user and for everyone who gets the link. <br>The most secure and least privileged solution for providing access to an Amazon CloudWatch dashboard for a user without an AWS account is to create an IAM user for the product manager with the appropriate permissions. By attaching the CloudWatchReadOnlyAccess policy to the user, the product manager can access only the read-only activities of Amazon CloudWatch, as per the principle of least privilege. The solutions architect should then share the login credentials and browser URL of the correct dashboard with the product manager.<br><br>Option A is incorrect because it is not secure as it requires sharing the dashboard link, which could lead to unauthorized access.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779021,
          "date": "Tue 17 Jan 2023 15:29",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "The answer is A, because the question says to follow the principle of least privileges.<br><br>When sharing a dashboard by providing an e-mail address, AWS creates an IAM role behind the scenes with only 4 permissions:<br>- cloudwatch:GetInsightRuleReport<br>- cloudwatch:GetMetricData<br>- cloudwatch:DescribeAlarms<br>- ec2:DescribeTags<br><br>The person you share the dashboard with has to enter a username + password every time they want to see the dashboard (even without having an IAM user!) and they will then get the permissions assigned to the previously created IAM role (happening behind the scenes).<br><br>Option B suggests creating an IAM user with the CloudWatchReadOnlyAccess policy, which provides far more access than the 4 permissions listed above.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779024,
          "date": "Tue 17 Jan 2023 15:30",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778424,
          "date": "Tue 17 Jan 2023 02:04",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "Answer: A<br>https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=testing<br>To share a dashboard publicly<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html</li><li>To share a dashboard with specific users</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778425,
          "date": "Tue 17 Jan 2023 02:05",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To share a dashboard with specific users</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778427,
          "date": "Tue 17 Jan 2023 02:06",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "To share a dashboard with specific users",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778321,
          "date": "Mon 16 Jan 2023 22:53",
          "username": "\t\t\t\tsimplimarvelous\t\t\t",
          "content": "Answer is A, cloudwatchreadonly access allows to much permission to cloudwatch<br>CloudWatchReadOnlyAccess<br>The CloudWatchReadOnlyAccess policy grants read-only access to CloudWatch.<br><br>The following is the content of the CloudWatchReadOnlyAccess policy.{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Effect\\\": \\\"Allow\\\",<br>\\\"Action\\\": [<br>\\\"autoscaling:Describe*\\\",<br>\\\"cloudwatch:Describe*\\\",<br>\\\"cloudwatch:Get*\\\",<br>\\\"cloudwatch:List*\\\",<br>\\\"logs:Get*\\\",<br>\\\"logs:List*\\\",<br>\\\"logs:StartQuery\\\",<br>\\\"logs:StopQuery\\\",<br>\\\"logs:Describe*\\\",<br>\\\"logs:TestMetricFilter\\\",<br>\\\"logs:FilterLogEvents\\\",<br>\\\"sns:Get*\\\",<br>\\\"sns:List*\\\"<br>],<br>\\\"Resource\\\": \\\"*\\\"<br>}<br> ]<br>}<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks for looking up what's inside the CloudWatchReadOnlyAccess policy!<br><br>These are the permissions that are granted if you were to share a dashboard with the Share feature:<br><br>- cloudwatch:GetInsightRuleReport<br>- cloudwatch:GetMetricData<br>- cloudwatch:DescribeAlarms<br>- ec2:DescribeTags<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779013,
          "date": "Tue 17 Jan 2023 15:22",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Thanks for looking up what's inside the CloudWatchReadOnlyAccess policy!<br><br>These are the permissions that are granted if you were to share a dashboard with the Share feature:<br><br>- cloudwatch:GetInsightRuleReport<br>- cloudwatch:GetMetricData<br>- cloudwatch:DescribeAlarms<br>- ec2:DescribeTags<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773687,
          "date": "Thu 12 Jan 2023 17:08",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Option B: is the one that complies with the principle of least privilege. <br>It is the safest and easiest option to track the IAM user.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's not B, as the CloudWatchReadOnlyAccess policy contains more permissions than the IAM role that is created behind the scenes when using the Share feature.<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 779016,
          "date": "Tue 17 Jan 2023 15:23",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "It's not B, as the CloudWatchReadOnlyAccess policy contains more permissions than the IAM role that is created behind the scenes when using the Share feature.<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-iamrole",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768477,
          "date": "Sat 07 Jan 2023 12:31",
          "username": "\t\t\t\tKayamables\t\t\t",
          "content": "Option B, seem more accurate. Key word here is \\\"periodically\\\". If A, then you will need to share dashboard every single time the manager needs access. That to me doesn't seem efficient. Option A would be correct in a one time scenario. Think about it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You do not have to re-share the dashboard every time the manager needs access. You provide their e-mail address when sharing it, and they will then receive an e-mail with a username + password. They then use their credentials to see the dashboard whenever they need to see it.<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.htmlScroll to \\\"Share a single dashboard with specific users\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779018,
          "date": "Tue 17 Jan 2023 15:25",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "You do not have to re-share the dashboard every time the manager needs access. You provide their e-mail address when sharing it, and they will then receive an e-mail with a username + password. They then use their credentials to see the dashboard whenever they need to see it.<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.htmlScroll to \\\"Share a single dashboard with specific users\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768084,
          "date": "Fri 06 Jan 2023 22:14",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The solution that will meet these requirements and follow the principle of least privilege is option A: Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.<br>AWS CloudWatch allows you to share a dashboard with other AWS accounts or with individuals who do not have an AWS account. By sharing the dashboard from the CloudWatch console, you can enter the product manager's email address and complete the sharing steps. This will create a shareable link for the dashboard that the product manager can use to access the dashboard. This solution follows the principle of least privilege because it grants the product manager access to the dashboard only, and not to any other AWS resources.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763502,
          "date": "Mon 02 Jan 2023 06:17",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Validated A is correct choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750404,
          "date": "Tue 20 Dec 2022 03:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements is A: Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.<br><br>To provide the product manager with access to the CloudWatch dashboard while following the principle of least privilege, the solutions architect can use the sharing feature in the CloudWatch console. The solutions architect can enter the product manager's email address and complete the sharing steps, which will generate a shareable link for the dashboard. The solutions architect can then provide this link to the product manager, who can use it to access the dashboard without needing an AWS account or login credentials.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, creating an IAM user specifically for the product manager and attaching the CloudWatchReadOnlyAccess AWS managed policy, is not a good choice because it would give the product manager more permissions than are necessary to access the dashboard.<br><br>Option C, creating an IAM user for the company's employees and attaching the ViewOnlyAccess AWS managed policy, is not a good choice because it would not provide access to the product manager, who is not an employee of the company.<br><br>Option D, deploying a bastion server, is not a good choice because it is unnecessarily complex and would not follow the principle of least privilege.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750405,
          "date": "Tue 20 Dec 2022 03:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, creating an IAM user specifically for the product manager and attaching the CloudWatchReadOnlyAccess AWS managed policy, is not a good choice because it would give the product manager more permissions than are necessary to access the dashboard.<br><br>Option C, creating an IAM user for the company's employees and attaching the ViewOnlyAccess AWS managed policy, is not a good choice because it would not provide access to the product manager, who is not an employee of the company.<br><br>Option D, deploying a bastion server, is not a good choice because it is unnecessarily complex and would not follow the principle of least privilege.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749773,
          "date": "Mon 19 Dec 2022 12:41",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html#share-cloudwatch-dashboard-email-addresses",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747083,
          "date": "Fri 16 Dec 2022 12:06",
          "username": "\t\t\t\tFlouz\t\t\t",
          "content": "i agree, sharing the link makes it accessible to anyone who has the link. Shouldnt it be D, through the Bastion Host? Because he doesnt have an AWS account, so how can he even use the IAM role if we create one?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 746406,
          "date": "Thu 15 Dec 2022 19:10",
          "username": "\t\t\t\tNiceGuy1169\t\t\t",
          "content": "I think the ask here is \\\"The company's product manager needs to access this dashboard periodically.\\\"I think \\\"periodically\\\" is the key word... if you were looking for a 1-time sharing, then, yes A is the choice. Since this is an ongoing thing... best to set him up with his own account with the least privilege",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743289,
          "date": "Mon 12 Dec 2022 21:09",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer B <br>This solution will allow the product manager to access the dashboard without providing unnecessary permissions or requiring the product manager to have an AWS account.Option A is not correct because sharing the dashboard from the CloudWatch console and providing a shareable link to the product manager would not provide the product manager with sufficient access to the dashboard. The product manager would only be able to view the dashboard, but would not be able to interact with it or make any changes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741668,
          "date": "Sun 11 Dec 2022 13:16",
          "username": "\t\t\t\tprethesh\t\t\t",
          "content": "for non aws account holders https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-dashboard-sharing.html<br><br>Share a single dashboard and designate specific email addresses of the people who can view the dashboard. Each of these users creates their own password that they must enter to view the dashboard.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 738224,
          "date": "Wed 07 Dec 2022 19:32",
          "username": "\t\t\t\tVJ_For_Azure_AWS\t\t\t",
          "content": "I believe correct answer is B, because as per as question he does not have AWS account that is why he is creating IM account. Also first option says share dashboard which I believe by default is public dashboard where anyone can have access if they have URL.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can share your CloudWatch dashboards with people who do not have direct access to your AWS account. This enables you to share dashboards across teams, with stakeholders, and with people external to your organization. You can even display dashboards on big screens in team areas, or embed them in Wikis and other webpages.<br><br>Following permission will be allowed for the public view,<br>cloudwatch:GetInsightRuleReport<br><br>cloudwatch:GetMetricData<br><br>cloudwatch:DescribeAlarms<br><br>ec2:DescribeTags</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 741656,
          "date": "Sun 11 Dec 2022 13:03",
          "username": "\t\t\t\tkrathore911\t\t\t",
          "content": "You can share your CloudWatch dashboards with people who do not have direct access to your AWS account. This enables you to share dashboards across teams, with stakeholders, and with people external to your organization. You can even display dashboards on big screens in team areas, or embed them in Wikis and other webpages.<br><br>Following permission will be allowed for the public view,<br>cloudwatch:GetInsightRuleReport<br><br>cloudwatch:GetMetricData<br><br>cloudwatch:DescribeAlarms<br><br>ec2:DescribeTags",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#28",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#28",
          "answers": [
            {
              "choice": "<p>A. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 708092,
          "date": "Mon 31 Oct 2022 01:20",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Tricky question!!! forget one-way or two-way. In this scenario, AWS applications (Amazon Chime, Amazon Connect, Amazon QuickSight, AWS Single Sign-On, Amazon WorkDocs, Amazon WorkMail, Amazon WorkSpaces, AWS Client VPN, AWS Management Console, and AWS Transfer Family) need to be able to look up objects from the on-premises domain in order for them to function. This tells you that authentication needs to flow both ways. This scenario requires a two-way trust between the on-premises and AWS Managed Microsoft AD domains.<br>It is a requirement of the application<br>Scenario 2: https://aws.amazon.com/es/blogs/security/everything-you-wanted-to-know-about-trusts-with-aws-managed-microsoft-ad/",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 712870,
          "date": "Mon 07 Nov 2022 08:15",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "Answer Bas we have AWS SSO which requires two way trust. As per documentation - A two-way trust is required for AWS Enterprise Apps such as Amazon Chime, Amazon Connect, Amazon QuickSight, AWS IAM Identity Center (successor to AWS Single Sign-On), Amazon WorkDocs, Amazon WorkMail, Amazon WorkSpaces, and the AWS Management Console. AWS Managed Microsoft AD must be able to query the users and groups in your self-managed AD. <br><br>Amazon EC2, Amazon RDS, and Amazon FSx will work with either a one-way or two-way trust.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 830938,
          "date": "Mon 06 Mar 2023 16:05",
          "username": "\t\t\t\tStuMoz\t\t\t",
          "content": "D.  I'm going for this because adding the AWS directory service means that you can manage adding users within AWS as well as on prem. Installing an identity provider on premises (like AD Federation Service) means you can continue to manage everything on premises and use SAML with SSO",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 825599,
          "date": "Wed 01 Mar 2023 08:15",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "B<br><br>Create a two-way trust relationship – When two-way trust relationships are created between AWS Managed Microsoft AD and a self-managed directory in AD, users in your self-managed directory in AD can sign in with their corporate credentials to various AWS services and business applications. One-way trusts do not work with IAM Identity Center.<br><br>AWS IAM Identity Center (successor to AWS Single Sign-On) requires a two-way trust so that it has permissions to read user and group information from your domain to synchronize user and group metadata. IAM Identity Center uses this metadata when assigning access to permission sets or applications. User and group metadata is also used by applications for collaboration, like when you share a dashboard with another user or group. The trust from AWS Directory Service for Microsoft Active Directory to your domain permits IAM Identity Center to trust your domain for authentication. The trust in the opposite direction grants AWS permissions to read user and group metadata.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814736,
          "date": "Mon 20 Feb 2023 03:14",
          "username": "\t\t\t\tsofiella\t\t\t",
          "content": "The solution that will meet these requirements is option A, which is to enable AWS Single Sign-On (AWS SSO) from the AWS SSO console and create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.<br><br>This option provides a secure and efficient way to integrate the company's on-premises Microsoft Active Directory with AWS SSO, allowing users to log in to AWS accounts and applications using their existing Active Directory credentials. A one-way trust enables authentication from the Active Directory to AWS SSO, but not the other way around, ensuring that the Active Directory is not exposed to security risks from AWS SSO.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780501,
          "date": "Wed 18 Jan 2023 23:55",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "D.  Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.<br><br>The company can use AWS SSO to enable SSO across all the company's accounts that are managed by AWS Organizations. To achieve this, the company will need to deploy an identity provider (IdP) on-premises, such as Microsoft Active Directory, and configure it to work with AWS SSO. This will allow the company to continue managing its users and groups in the on-premises self-managed Microsoft Active Directory, while also providing SSO across all the company's AWS accounts.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779137,
          "date": "Tue 17 Jan 2023 18:30",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "It's B.  In order to connect an on-premise MS AD to AWS SSO (now AWS Identity Centre), you can either use an AD Connector (not one of the options) or a 2-way trust relationship between an AWS Managed MS AD and an on-premise MS AD.  <br><br>The AWS docs specifically say that a 1-way trust relationship does NOT work with SSO.<br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774379,
          "date": "Fri 13 Jan 2023 12:27",
          "username": "\t\t\t\tdev1978\t\t\t",
          "content": "I really don't get the two way trust. The question only mentions SSO (federation) to access the AWS console. It doesn't mention what happens in terms of the authentication in each service/app. So I would go for A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 768092,
          "date": "Fri 06 Jan 2023 22:30",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "AWS Single Sign-On (AWS SSO) does not require a two-way trust. It allows you to manage user identities and group membership in your self-managed Microsoft Active Directory and to use those identities to grant users access to your AWS accounts.<br>A two-way trust relationship is not required for AWS SSO to function. A one-way forest trust or a one-way domain trust is sufficient.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS docs specifically say this: \\\"One-way trusts do not work with IAM Identity Center.\\\" (Identity Centre is the successor to AWS SSO)<br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779142,
          "date": "Tue 17 Jan 2023 18:32",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "AWS docs specifically say this: \\\"One-way trusts do not work with IAM Identity Center.\\\" (Identity Centre is the successor to AWS SSO)<br><br>https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 763510,
          "date": "Mon 02 Jan 2023 06:37",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Udp is possible in aws global accelrator,so its a better choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763505,
          "date": "Mon 02 Jan 2023 06:23",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Choose B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754690,
          "date": "Sat 24 Dec 2022 05:02",
          "username": "\t\t\t\twaiyiu9981\t\t\t",
          "content": "Should I believe the most voted or the solution shown in this website? As it always differ...",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750928,
          "date": "Tue 20 Dec 2022 14:31",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory = two-way forest trust",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750408,
          "date": "Tue 20 Dec 2022 03:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements is A: Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.<br><br>AWS Single Sign-On (AWS SSO) is a service that enables users to sign in to all of their accounts in AWS Organizations centrally by using their corporate credentials. To use AWS SSO with a self-managed Microsoft Active Directory, the company can enable AWS SSO from the AWS SSO console and create a one-way forest trust or a one-way domain trust by using AWS Directory Service for Microsoft Active Directory. This will allow the company to continue managing the users and groups in its on-premises self-managed Microsoft Active Directory while providing a single sign-on solution across all the company's accounts.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, creating a two-way forest trust, is not a good choice because it would allow the self-managed Microsoft Active Directory to manage the AWS SSO directory and potentially make unauthorized changes.<br><br>Option C, using AWS Directory Service, is not a good choice because it would require the company to manage the directory in the cloud and would not allow the company to continue using its self-managed Microsoft Active Directory.<br><br>Option D, deploying an identity provider (IdP) on premises, is not a good choice because it would not provide a single sign-on solution across all the company's accounts in AWS Organizations.</li><li>Per the URL provided by 17Master, Amazon Single Sign On requires a two way trust relationship as shown in scenario 2https://aws.amazon.com/es/blogs/security/everything-you-wanted-to-know-about-trusts-with-aws-managed-microsoft-ad/</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750409,
          "date": "Tue 20 Dec 2022 03:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, creating a two-way forest trust, is not a good choice because it would allow the self-managed Microsoft Active Directory to manage the AWS SSO directory and potentially make unauthorized changes.<br><br>Option C, using AWS Directory Service, is not a good choice because it would require the company to manage the directory in the cloud and would not allow the company to continue using its self-managed Microsoft Active Directory.<br><br>Option D, deploying an identity provider (IdP) on premises, is not a good choice because it would not provide a single sign-on solution across all the company's accounts in AWS Organizations.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 761687,
          "date": "Fri 30 Dec 2022 06:14",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "Per the URL provided by 17Master, Amazon Single Sign On requires a two way trust relationship as shown in scenario 2https://aws.amazon.com/es/blogs/security/everything-you-wanted-to-know-about-trusts-with-aws-managed-microsoft-ad/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749408,
          "date": "Mon 19 Dec 2022 04:20",
          "username": "\t\t\t\tRichaqua\t\t\t",
          "content": "You need one way trust if it is an AD connector. With MS AD, two way trust will be required",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 743296,
          "date": "Mon 12 Dec 2022 21:20",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer D <br>deploy an identity provider (IdP) on premises and enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. This solution will allow the company to use its on-premises Active Directory for user authentication and access control, and will enable the company to use AWS SSO to provide SSO across its AWS accounts. Option B is not correct for me, it is because, creating a two-way forest trust using AWS Directory Service for Microsoft Active Directory is not necessary in this scenario. A two-way trust relationship can be established by deploying an on-premises IdP and enabling AWS SSO",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736947,
          "date": "Tue 06 Dec 2022 16:28",
          "username": "\t\t\t\treeba_908\t\t\t",
          "content": "i choose A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>why did you choose A? is it because it is one way from on premise to the cloud?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747429,
          "date": "Fri 16 Dec 2022 17:49",
          "username": "\t\t\t\tyoben84\t\t\t",
          "content": "why did you choose A? is it because it is one way from on premise to the cloud?",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#29",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions.<br>The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#29",
          "answers": [
            {
              "choice": "<p>A. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB.  Create an Amazon CloudFront distribution that uses the latency record as an origin.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB.  Deploy an Amazon CloudFront distribution that uses the weighted record as an origin.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703159,
          "date": "Mon 24 Oct 2022 17:22",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with A,<br>Global Accelerator has automatic failover and is perfect for this scenario with VoIP<br>https://aws.amazon.com/global-accelerator/faqs/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thank you for your link, it make me consolidate A. </li><li>This option does not meet the requirements because AWS Global Accelerator is only used to route traffic to the optimal AWS Region, it does not provide automatic failover between regions.</li><li>Instant regional failover: AWS Global Accelerator automatically checks the health of your applications and routes user traffic only to healthy application endpoints. If the health status changes or you make configuration updates, AWS Global Accelerator reacts instantaneously to route your users to the next available endpoint.</li></ul>",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 706158,
          "date": "Fri 28 Oct 2022 07:30",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "Thank you for your link, it make me consolidate A. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This option does not meet the requirements because AWS Global Accelerator is only used to route traffic to the optimal AWS Region, it does not provide automatic failover between regions.</li><li>Instant regional failover: AWS Global Accelerator automatically checks the health of your applications and routes user traffic only to healthy application endpoints. If the health status changes or you make configuration updates, AWS Global Accelerator reacts instantaneously to route your users to the next available endpoint.</li></ul>",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 780515,
          "date": "Thu 19 Jan 2023 00:08",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "This option does not meet the requirements because AWS Global Accelerator is only used to route traffic to the optimal AWS Region, it does not provide automatic failover between regions.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Instant regional failover: AWS Global Accelerator automatically checks the health of your applications and routes user traffic only to healthy application endpoints. If the health status changes or you make configuration updates, AWS Global Accelerator reacts instantaneously to route your users to the next available endpoint.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 825603,
          "date": "Wed 01 Mar 2023 08:25",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "Instant regional failover: AWS Global Accelerator automatically checks the health of your applications and routes user traffic only to healthy application endpoints. If the health status changes or you make configuration updates, AWS Global Accelerator reacts instantaneously to route your users to the next available endpoint.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 711284,
          "date": "Fri 04 Nov 2022 18:34",
          "username": "\t\t\t\tmouhannadhaj\t\t\t",
          "content": "CloudFront uses Edge Locations to cache content while Global Accelerator uses Edge Locations to find an optimal pathway to the nearest regional endpoint. CloudFront is designed to handle HTTP protocol meanwhile Global Accelerator is best used for both HTTP and non-HTTP protocols such as TCP and UDP. so i think A is a better answer",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823917,
          "date": "Mon 27 Feb 2023 17:01",
          "username": "\t\t\t\tthepix17\t\t\t",
          "content": "Answer is A, Cloudfront can be discounted as it is not for UDP traffic",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814892,
          "date": "Mon 20 Feb 2023 07:41",
          "username": "\t\t\t\tTECHNOWARRIOR\t\t\t",
          "content": "Amazon Route 53 Latency Record: Supports failover across Regions, enabling traffic to be routed to another Region if the primary Region becomes unavailable. NLB as an AWS Global Accelerator Endpoint: Supports failover within a Region, enabling traffic to be distributed to other targets if one or more targets become unavailable.The first approach can provide better end-user latency and high availability, but at the cost of additional complexity and cost. The second approach provides a simpler and more streamlined solution, but may not be as effective in reducing end-user latency or providing failover support.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814880,
          "date": "Mon 20 Feb 2023 07:22",
          "username": "\t\t\t\tTECHNOWARRIOR\t\t\t",
          "content": "Amazon Route 53 Latency Record: Supports failover across Regions, enabling traffic to be routed to another Region if the primary Region becomes unavailable. NLB as an AWS Global Accelerator Endpoint: Supports failover within a Region, enabling traffic to be distributed to other targets if one or more targets become unavailable.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804854,
          "date": "Sat 11 Feb 2023 00:12",
          "username": "\t\t\t\tharirkmusa\t\t\t",
          "content": "Answer is C.  Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB.  Create an Amazon CloudFront distribution that uses the latency record as an origin.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804204,
          "date": "Fri 10 Feb 2023 11:06",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "A.  Global accelerator will connect all regions, it has low latency and failover.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 803769,
          "date": "Fri 10 Feb 2023 00:18",
          "username": "\t\t\t\tDeepak_k\t\t\t",
          "content": "Answer A: Clearly explained AWS Global Accelerator used for RTC decreases latency and delivers greater Performance<br><br>https://aws.amazon.com/blogs/networking-and-content-delivery/improving-real-time-communication-rtc-client-experience-with-aws-global-accelerator/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793601,
          "date": "Tue 31 Jan 2023 04:45",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "Option A, Deploying a Network Load Balancer (NLB) with an associated target group and using it as an AWS Global Accelerator endpoint in each Region, would not meet the requirements for routing users to the Region with the lowest latency. AWS Global Accelerator routes users to the Region with the closest AWS Region, but it does not route users to the Region with the lowest latency. To route users to the Region with the lowest latency, the solution needs to use Amazon Route 53 latency records, which direct users to the Region with the lowest latency based on latency measurements.<br><br>Therefore, Option C, Deploying a Network Load Balancer (NLB) with an associated target group, and creating an Amazon Route 53 latency record that points to aliases for each NLB, would be the best solution to meet the requirements of routing users to the Region with the lowest latency and automating failover between Regions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 785297,
          "date": "Mon 23 Jan 2023 12:37",
          "username": "\t\t\t\toguz11\t\t\t",
          "content": "Answer is A for chatGPT;<br><br>Network Load Balancer (NLB) can handle UDP traffic and also it can route traffic to the region with the lowest latency by using the Global Accelerator feature, it uses the Global Accelerator to route traffic to the best performing endpoint based on health and geographic location.<br>Using an Auto Scaling group ensures that the service can scale as necessary, and also NLB provides automatic failover between regions.<br>AWS Global Accelerator directs traffic to the optimal AWS region for a given client, improving the performance and availability of applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 761711,
          "date": "Fri 30 Dec 2022 07:15",
          "username": "\t\t\t\tgangadharprakash\t\t\t",
          "content": "agree with A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759375,
          "date": "Wed 28 Dec 2022 06:07",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "2 Distinct things that should pivot you towards choosing A rather than C. <br>1. VoIP UDP traffic- CLoudFront is not meant for this, it is for delivering static/dynamic content. GA is more suited for this.<br>2. Automatic regional failover- it is one of the key features of GA(GLobal Accelerator).",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757402,
          "date": "Mon 26 Dec 2022 13:42",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "The company needs to route users to the Region with the lowest latency =>I think C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750970,
          "date": "Tue 20 Dec 2022 15:07",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "UDP protocol = NLB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750410,
          "date": "Tue 20 Dec 2022 03:29",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "A.  Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.<br><br>This solution will meet the requirements because the Network Load Balancer is able to route traffic to the lowest latency Region using the Global Accelerator. The Auto Scaling group will provide automated failover between Regions if an instance goes down. The NLB is also able to handle UDP connections, which is necessary for the VoIP service.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749602,
          "date": "Mon 19 Dec 2022 08:57",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "We can drop B and D as they refer to using ALB and it should be clear that you cannot use ALB with anything other than HTTP/HTTPs.<br>Coming to using CF or Global Accelerator. CF moves your content closer to your location i.e,. CACHED whereas GA moves your AWS Network closer to your location. So you can eliminate the option that uses CF and go for GA. ",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748783,
          "date": "Sun 18 Dec 2022 12:24",
          "username": "\t\t\t\tstudis\t\t\t",
          "content": "I am more with C since route 53 has a routing policy based on lactency.<br>Why I don't think it is A is because even though global accelerator can reduce latency, from my understanding we need to have an NLB in each region whereas as written in A, it is like we are going to use only one NLB",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#30",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance.<br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#30",
          "answers": [
            {
              "choice": "<p>A. Stop the DB instance when tests are completed. Restart the DB instance when required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 691245,
          "date": "Mon 10 Oct 2022 16:25",
          "username": "\t\t\t\thanhdroid\t\t\t",
          "content": "Answer C, you still pay for storage when an RDS database is stopped",
          "upvote_count": "20",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695208,
          "date": "Sat 15 Oct 2022 07:28",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "C - Create a manual Snapshot of DB and shift to S3- Standard and Restore form Manual Snapshot when required.<br><br>Not A - By stopping the DB although you are not paying for DB hours you are still paying for Provisioned IOPs , the storage for Stopped DB is more than Snapshot of underlying EBS vol.and Automated Back ups .<br>Not D - Is possible but not MOST cost effective, no need to run the RDS when not needed.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 770766,
          "date": "Mon 09 Jan 2023 19:36",
          "username": "\t\t\t\tTiba\t\t\t",
          "content": "You can't stop an Amazon RDS for SQL Server DB instance in a Multi-AZ configuration.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768100,
          "date": "Fri 06 Jan 2023 22:53",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Amazon RDS for MySQL allows you to create a snapshot of your DB instance and store it in Amazon S3. You can then terminate the DB instance and restore it from the snapshot when required. This will allow you to reduce the cost of running the resource-intensive tests without reducing the compute and memory attributes of the DB instance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763514,
          "date": "Mon 02 Jan 2023 06:43",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C is right choice here",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 758953,
          "date": "Tue 27 Dec 2022 20:18",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "Explanation from the same question on UDEMY!<br>Taking a snapshot of the instance and storing the snapshot is the most cost-effective solution. When needed, a new database can be created from the snapshot. Performance Insights can be enabled on the new instance if needed. Note that the previous data from Performance Insights will not be associated with the new instance, however this was not a requirement.<br>CORRECT: \\\"Create a snapshot of the database when the tests are completed. Terminate the DB instance. Create a new DB instance from the snapshot when required” is the correct answer (as explained above.)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>INCORRECT: \\\"Stop the DB instance once all tests are completed. Start the DB instance again when required” is incorrect. You will be charged when your instance is stopped. When an instance is stopped you are charged for provisioned storage, manual snapshots, and automated backup storage within your specified retention window, but not for database instance hours. This is more costly compared to using snapshots.<br>INCORRECT: \\\"Create an Auto Scaling group for the DB instance and reduce the desired capacity to 0 once the tests are completed” is incorrect. You cannot use Auto Scaling groups with Amazon RDS instances.<br>INCORRECT: \\\"Modify the DB instance size to a smaller capacity instance when all the tests have been completed. Scale up again when required” is incorrect. This will reduce compute and memory capacity and will be more costly than taking a snapshot and terminating the DB. </li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 758956,
          "date": "Tue 27 Dec 2022 20:19",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "INCORRECT: \\\"Stop the DB instance once all tests are completed. Start the DB instance again when required” is incorrect. You will be charged when your instance is stopped. When an instance is stopped you are charged for provisioned storage, manual snapshots, and automated backup storage within your specified retention window, but not for database instance hours. This is more costly compared to using snapshots.<br>INCORRECT: \\\"Create an Auto Scaling group for the DB instance and reduce the desired capacity to 0 once the tests are completed” is incorrect. You cannot use Auto Scaling groups with Amazon RDS instances.<br>INCORRECT: \\\"Modify the DB instance size to a smaller capacity instance when all the tests have been completed. Scale up again when required” is incorrect. This will reduce compute and memory capacity and will be more costly than taking a snapshot and terminating the DB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753845,
          "date": "Fri 23 Dec 2022 05:29",
          "username": "\t\t\t\tbenjl\t\t\t",
          "content": "Answer is C,<br>Because the question say monthly test, and you can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html<br>So, in this case, if it run a test once a month, creating a snapshot is more appropriate and cost-effective way.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750411,
          "date": "Tue 20 Dec 2022 03:35",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, stopping the DB instance when tests are completed and restarting it when required, would be the most cost-effective solution for reducing the cost of running resource-intensive tests on an Amazon RDS for MySQL DB instance.<br><br>By stopping the DB instance, you will no longer be charged for any compute or memory resources used by the instance. When the tests are completed, you can restart the DB instance to resume using it. This will allow you to avoid paying for resources that are not being used, while still maintaining the same compute and memory attributes of the DB instance for the tests.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, using an Auto Scaling policy with the DB instance to automatically scale when tests are completed, would not be a cost-effective solution as it would not reduce the cost of running the tests. Auto Scaling allows you to automatically increase or decrease the capacity of your DB instance based on predefined rules, but it does not provide a way to reduce the cost of running the tests.<br><br>Option C, creating a snapshot when tests are completed and then terminating the DB instance and restoring the snapshot when required, would also not be a cost-effective solution. While creating a snapshot can be a useful way to save a copy of your database, it does not reduce the cost of running the tests. Additionally, restoring a snapshot to a new DB instance would require you to pay for the resources used by the new instance.</li><li>https://docs.aws.amazon.com/pt_br/AmazonRDS/latest/UserGuide/USER_StopInstance.html<br>Important<br>\\\"You can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. This way, it doesn't fall behind any required maintenance updates.\\\"</li><li>Option D, modifying the DB instance to a low-capacity instance when tests are completed and then modifying it back again when required, would not meet the requirement to maintain the same compute and memory attributes of the DB instance for the tests. Modifying the DB instance to a low-capacity instance would result in a reduction in the resources available to the DB instance, which would not be sufficient for the resource-intensive tests.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750412,
          "date": "Tue 20 Dec 2022 03:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, using an Auto Scaling policy with the DB instance to automatically scale when tests are completed, would not be a cost-effective solution as it would not reduce the cost of running the tests. Auto Scaling allows you to automatically increase or decrease the capacity of your DB instance based on predefined rules, but it does not provide a way to reduce the cost of running the tests.<br><br>Option C, creating a snapshot when tests are completed and then terminating the DB instance and restoring the snapshot when required, would also not be a cost-effective solution. While creating a snapshot can be a useful way to save a copy of your database, it does not reduce the cost of running the tests. Additionally, restoring a snapshot to a new DB instance would require you to pay for the resources used by the new instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/pt_br/AmazonRDS/latest/UserGuide/USER_StopInstance.html<br>Important<br>\\\"You can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. This way, it doesn't fall behind any required maintenance updates.\\\"</li><li>Option D, modifying the DB instance to a low-capacity instance when tests are completed and then modifying it back again when required, would not meet the requirement to maintain the same compute and memory attributes of the DB instance for the tests. Modifying the DB instance to a low-capacity instance would result in a reduction in the resources available to the DB instance, which would not be sufficient for the resource-intensive tests.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770281,
          "date": "Mon 09 Jan 2023 12:20",
          "username": "\t\t\t\tbombtux\t\t\t",
          "content": "https://docs.aws.amazon.com/pt_br/AmazonRDS/latest/UserGuide/USER_StopInstance.html<br>Important<br>\\\"You can stop a DB instance for up to seven days. If you don't manually start your DB instance after seven days, your DB instance is automatically started. This way, it doesn't fall behind any required maintenance updates.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750413,
          "date": "Tue 20 Dec 2022 03:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D, modifying the DB instance to a low-capacity instance when tests are completed and then modifying it back again when required, would not meet the requirement to maintain the same compute and memory attributes of the DB instance for the tests. Modifying the DB instance to a low-capacity instance would result in a reduction in the resources available to the DB instance, which would not be sufficient for the resource-intensive tests.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749526,
          "date": "Mon 19 Dec 2022 07:21",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C is the best and most cost effective option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743790,
          "date": "Tue 13 Dec 2022 10:06",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "A<br>Stopping the DB instance when tests are completed and restarting it when required will be the most cost-effective solution for reducing the cost of running the resource-intensive tests. When an Amazon RDS for MySQL DB instance is stopped, the instance will no longer be charged for compute and memory usage, which will significantly reduce the cost of running the tests. Option C is not correct for me, it is because, Snapshots are used to create backups of data, but do not reduce the cost of running a DB instance.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723503,
          "date": "Mon 21 Nov 2022 13:59",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708098,
          "date": "Mon 31 Oct 2022 01:37",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692724,
          "date": "Wed 12 Oct 2022 08:01",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "If instance state is stopped, it's not billed.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The underlying EBS volumes or provisioned IOPS are. Those charges are higher than storing a snapshot in S3 and restoring once a month from that.</li><li>It's a DB instance, not an EC2 instance. If the DB instance is stopped, you are still paying for the storage.</li><li>Thank you for your explanation</li><li>Thanks for your reply.</li><li>While your DB instance is stopped, you are charged for provisioned storage (including Provisioned IOPS)</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779490,
          "date": "Wed 18 Jan 2023 02:35",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "The underlying EBS volumes or provisioned IOPS are. Those charges are higher than storing a snapshot in S3 and restoring once a month from that.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 697432,
          "date": "Mon 17 Oct 2022 15:35",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "It's a DB instance, not an EC2 instance. If the DB instance is stopped, you are still paying for the storage.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thank you for your explanation</li><li>Thanks for your reply.</li></ul>",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 706164,
          "date": "Fri 28 Oct 2022 07:35",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "Thank you for your explanation",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 716551,
          "date": "Sat 12 Nov 2022 09:21",
          "username": "\t\t\t\tJerry84\t\t\t",
          "content": "Thanks for your reply.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695131,
          "date": "Sat 15 Oct 2022 04:27",
          "username": "\t\t\t\tRachness\t\t\t",
          "content": "While your DB instance is stopped, you are charged for provisioned storage (including Provisioned IOPS)",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#31",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check.<br>What should a solutions architect do to accomplish this?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#31",
          "answers": [
            {
              "choice": "<p>A. Use AWS Config rules to define and detect resources that are not properly tagged.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 754382,
          "date": "Fri 23 Dec 2022 18:21",
          "username": "\t\t\t\tkurinei021\t\t\t",
          "content": "Answer from ChatGPT:<br><br>Yes, you can use AWS Config to create tags for your resources. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to create rules that automatically tag resources when they are created or when their configurations change.<br><br>To create tags for your resources using AWS Config, you will need to create an AWS Config rule that specifies the tag key and value you want to use and the resources you want to apply the tag to. You can then enable the rule and AWS Config will automatically apply the tag to the specified resources when they are created or when their configurations change.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 830866,
          "date": "Mon 06 Mar 2023 15:03",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 798715,
          "date": "Sun 05 Feb 2023 10:40",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "I found this question very vague.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774136,
          "date": "Fri 13 Jan 2023 06:11",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "D.  Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.<br><br>A solution architect can accomplish this by writing API calls to check all resources (EC2 instances, RDS DB instances, and Redshift clusters) for proper tag allocation. Then, schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code. This way, the check will be automated and it eliminates the need to manually check and configure the resources. The Lambda function can be triggered periodically and will check all resources, this way it will minimize the effort of configuring and operating the check.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How about the key sentence \\\"The company wants to minimize the effort of configuring and operating this check\\\". Either A or B and i vouch for A</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796930,
          "date": "Fri 03 Feb 2023 10:34",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "How about the key sentence \\\"The company wants to minimize the effort of configuring and operating this check\\\". Either A or B and i vouch for A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751076,
          "date": "Tue 20 Dec 2022 16:09",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "are configured with tags = AWS config",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750417,
          "date": "Tue 20 Dec 2022 03:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To minimize the effort of ensuring that all Amazon EC2 instances, Amazon RDS DB instances, and Amazon Redshift clusters are properly tagged, a solutions architect should use AWS Config rules to define and detect resources that are not properly tagged.<br><br>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use Config rules to define conditions for resources in your AWS environment and then automatically check whether those conditions are met. If a resource does not meet the conditions specified by a Config rule, the rule can trigger an AWS Config event that can be used to take corrective action.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Using AWS Config rules to define and detect resources that are not properly tagged allows you to automate the process of checking for and correcting improperly tagged resources. This will minimize the effort required to configure and operate this check, as you will not need to manually check for or tag improperly tagged resources.<br><br>Option B, using Cost Explorer to display resources that are not properly tagged and then tagging those resources manually, would not be an effective solution as it would require manual effort to identify and tag improperly tagged resources.</li><li>Option C, writing API calls to check all resources for proper tag allocation and then running the code periodically on an EC2 instance, would also not be an effective solution as it would require manual effort to run the code and check for improperly tagged resources.<br><br>Option D, writing API calls to check all resources for proper tag allocation and scheduling an AWS Lambda function through Amazon CloudWatch to periodically run the code, would be a more automated solution than option C, but it would still require manual effort to write and maintain the code and schedule the Lambda function. Using AWS Config rules would be a more efficient and effective way to automate the process of checking for and correcting improperly tagged resources.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750418,
          "date": "Tue 20 Dec 2022 03:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Using AWS Config rules to define and detect resources that are not properly tagged allows you to automate the process of checking for and correcting improperly tagged resources. This will minimize the effort required to configure and operate this check, as you will not need to manually check for or tag improperly tagged resources.<br><br>Option B, using Cost Explorer to display resources that are not properly tagged and then tagging those resources manually, would not be an effective solution as it would require manual effort to identify and tag improperly tagged resources.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, writing API calls to check all resources for proper tag allocation and then running the code periodically on an EC2 instance, would also not be an effective solution as it would require manual effort to run the code and check for improperly tagged resources.<br><br>Option D, writing API calls to check all resources for proper tag allocation and scheduling an AWS Lambda function through Amazon CloudWatch to periodically run the code, would be a more automated solution than option C, but it would still require manual effort to write and maintain the code and schedule the Lambda function. Using AWS Config rules would be a more efficient and effective way to automate the process of checking for and correcting improperly tagged resources.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750419,
          "date": "Tue 20 Dec 2022 03:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C, writing API calls to check all resources for proper tag allocation and then running the code periodically on an EC2 instance, would also not be an effective solution as it would require manual effort to run the code and check for improperly tagged resources.<br><br>Option D, writing API calls to check all resources for proper tag allocation and scheduling an AWS Lambda function through Amazon CloudWatch to periodically run the code, would be a more automated solution than option C, but it would still require manual effort to write and maintain the code and schedule the Lambda function. Using AWS Config rules would be a more efficient and effective way to automate the process of checking for and correcting improperly tagged resources.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 743796,
          "date": "Tue 13 Dec 2022 10:10",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "D is correct <br>AWS Lambda function through Amazon CloudWatch to periodically run the code. This will enable the company to automatically check its resources for proper tag allocation without the need for manual intervention. Option A is not correct for me, it is because, AWS Config rules cannot be used to detect resources that are not properly tagged. AWS Config rules can be used to evaluate the configuration of resources, but not to check for proper tag allocation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 740994,
          "date": "Sat 10 Dec 2022 14:17",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "https://docs.aws.amazon.com/config/latest/developerguide/tagging.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723506,
          "date": "Mon 21 Nov 2022 14:02",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719422,
          "date": "Wed 16 Nov 2022 08:19",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is A. <br>https://docs.aws.amazon.com/config/latest/developerguide/tagging.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 710777,
          "date": "Thu 03 Nov 2022 21:21",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "Easiest option is A",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 708100,
          "date": "Mon 31 Oct 2022 01:40",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: R"
        },
        {
          "id": 696470,
          "date": "Sun 16 Oct 2022 21:33",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A can do the task and is the one involving less effort.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 692732,
          "date": "Wed 12 Oct 2022 08:07",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I think Config works",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 692464,
          "date": "Wed 12 Oct 2022 00:58",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is A",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#32",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images.<br>Which method is the MOST cost-effective for hosting the website?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#32",
          "answers": [
            {
              "choice": "<p>A. Containerize the website and host it in AWS Fargate.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon S3 bucket and host the website there.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a web server on an Amazon EC2 instance to host the website.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692782,
          "date": "Wed 12 Oct 2022 08:47",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Good answer is B: client-side JavaScript. the website is static, so it must be S3.",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 692737,
          "date": "Wed 12 Oct 2022 08:08",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "HTML, CSS, client-side JavaScript, and images are all static resources.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 830870,
          "date": "Mon 06 Mar 2023 15:06",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "The most cost-effective method for hosting the website is option B: Create an Amazon S3 bucket and host the website there.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 768101,
          "date": "Fri 06 Jan 2023 22:57",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The most cost-effective method for hosting the website is option B: Create an Amazon S3 bucket and host the website there.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754016,
          "date": "Fri 23 Dec 2022 09:10",
          "username": "\t\t\t\torionizzie\t\t\t",
          "content": "static content thru S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751145,
          "date": "Tue 20 Dec 2022 16:44",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "In general, it is more cost-effective to use S3 for hosting static website content because it is a lower-cost storage service compared to Fargate, which is a compute service",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750422,
          "date": "Tue 20 Dec 2022 03:49",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most cost-effective method for hosting a website that consists of HTML, CSS, client-side JavaScript, and images would be to create an Amazon S3 bucket and host the website there.<br><br>Amazon S3 (Simple Storage Service) is an object storage service that enables you to store and retrieve data over the internet. It is a highly scalable, reliable, and low-cost storage service that is well-suited for hosting static websites. You can use Amazon S3 to host a website by creating a bucket, uploading your website content to the bucket, and then configuring the bucket as a static website hosting location.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Hosting a website in an Amazon S3 bucket is generally more cost-effective than hosting it on an Amazon EC2 instance or using a containerized solution like AWS Fargate, as it does not require you to pay for compute resources. It is also more cost-effective than configuring an Application Load Balancer with an AWS Lambda target that uses the Express.js framework, as this approach would require you to pay for both compute resources and the use of the Application Load Balancer and AWS Lambda.<br><br>In summary, hosting a website in an Amazon S3 bucket is the most cost-effective method for hosting a website that consists of HTML, CSS, client-side JavaScript, and images.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750423,
          "date": "Tue 20 Dec 2022 03:49",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Hosting a website in an Amazon S3 bucket is generally more cost-effective than hosting it on an Amazon EC2 instance or using a containerized solution like AWS Fargate, as it does not require you to pay for compute resources. It is also more cost-effective than configuring an Application Load Balancer with an AWS Lambda target that uses the Express.js framework, as this approach would require you to pay for both compute resources and the use of the Application Load Balancer and AWS Lambda.<br><br>In summary, hosting a website in an Amazon S3 bucket is the most cost-effective method for hosting a website that consists of HTML, CSS, client-side JavaScript, and images.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749539,
          "date": "Mon 19 Dec 2022 07:34",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747521,
          "date": "Fri 16 Dec 2022 20:18",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Static website = S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744920,
          "date": "Wed 14 Dec 2022 10:17",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "B looks correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723507,
          "date": "Mon 21 Nov 2022 14:03",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708101,
          "date": "Mon 31 Oct 2022 01:41",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 695116,
          "date": "Sat 15 Oct 2022 04:00",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "B: Host static website in S3",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 692467,
          "date": "Wed 12 Oct 2022 00:59",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#33",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.<br>What should a solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#33",
          "answers": [
            {
              "choice": "<p>A. Store the transactions data into Amazon DynamoDB.  Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB.  Other applications can consume the transactions data off the Kinesis data stream.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB.  Other applications can consume transaction files stored in Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697437,
          "date": "Mon 17 Oct 2022 15:44",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "I would go for C.  The tricky phrase is \\\"near-real-time solution\\\", pointing to Firehouse, but it can't send data to DynamoDB, so it leaves us with C as best option. <br><br>Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumologic, LogicMonitor, MongoDB, and HTTP End Point as destinations.<br><br>https://aws.amazon.com/kinesis/data-firehose/faqs/#:~:text=Kinesis%20Data%20Firehose%20currently%20supports,HTTP%20End%20Point%20as%20destinations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry but I still can't see how Kinesis Data Stream is 'scalable', since you have to provision the quantity of shards in advance?</li><li>\\\"easily stream data at any scale\\\" <br>This is a description of Kinesis Data Stream. I think you can configure its quantity but still not provision and manage scalability by yourself.</li><li>This was a really tough one. But you have the best explanation on here with reference point. Thanks. I'm going with answer C!</li></ul>",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797320,
          "date": "Fri 03 Feb 2023 20:56",
          "username": "\t\t\t\tlizzard812\t\t\t",
          "content": "Sorry but I still can't see how Kinesis Data Stream is 'scalable', since you have to provision the quantity of shards in advance?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"easily stream data at any scale\\\" <br>This is a description of Kinesis Data Stream. I think you can configure its quantity but still not provision and manage scalability by yourself.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817683,
          "date": "Wed 22 Feb 2023 10:39",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "\\\"easily stream data at any scale\\\" <br>This is a description of Kinesis Data Stream. I think you can configure its quantity but still not provision and manage scalability by yourself.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788074,
          "date": "Wed 25 Jan 2023 20:40",
          "username": "\t\t\t\tLonojack\t\t\t",
          "content": "This was a really tough one. But you have the best explanation on here with reference point. Thanks. I'm going with answer C!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698574,
          "date": "Wed 19 Oct 2022 02:22",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "The answer is C, becauseFirehose does not suppport DynamoDB and another key word is \\\"data\\\" Kinesis Data Streams is the correct choice. Pay attention to key words. AWS likes to trick you up to make sure you know the services.",
          "upvote_count": "17",
          "selected_answers": ""
        },
        {
          "id": 830925,
          "date": "Mon 06 Mar 2023 15:56",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Kinesis Data Streams focuses on ingesting and storing data streams. Kinesis Data Firehose focuses on delivering data streams to select destinations. Both can ingest data streams but the deciding factor in which to use depends on where your streamed data should go to.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 826563,
          "date": "Thu 02 Mar 2023 08:02",
          "username": "\t\t\t\tDaiking\t\t\t",
          "content": "I was confused B because it's the phrase \\\"near-real-time\\\", but the destination of Firehose can not be DynamoDB. <br><br>https://docs.aws.amazon.com/firehose/latest/dev/create-destination.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 820626,
          "date": "Fri 24 Feb 2023 15:49",
          "username": "\t\t\t\tBhawesh\t\t\t",
          "content": "Answer B.  Question says: \\\"The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database\\\". So, only the data stored in database needs to be sensitized NOT the ones which is to be stored in S3. Option C is wrong because option C says:\\\"Use AWS Lambda integration to remove sensitive data from every transaction\\\" which is NOT what the question asks for.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818111,
          "date": "Wed 22 Feb 2023 18:34",
          "username": "\t\t\t\tBhawesh\t\t\t",
          "content": "My vote is: option B.  Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.<br>This question has 2 requirements:<br>1. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. <br>2. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 794508,
          "date": "Tue 31 Jan 2023 19:49",
          "username": "\t\t\t\tpgomess\t\t\t",
          "content": "\\\"Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.\\\" <br>You can't do it with Firehose.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 788435,
          "date": "Thu 26 Jan 2023 07:27",
          "username": "\t\t\t\tsassy2023\t\t\t",
          "content": "KDS doesnt remove sensitive information as required.<br>B is correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Lambda does.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 789952,
          "date": "Fri 27 Jan 2023 20:41",
          "username": "\t\t\t\tdark_firzen\t\t\t",
          "content": "Lambda does.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 787125,
          "date": "Wed 25 Jan 2023 01:46",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Other applications can consume from Kinesis Data Streams with the sensitive information still unremoved ? The question requires that sensitive information be purged from the Data Stream.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 784406,
          "date": "Sun 22 Jan 2023 15:53",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Kinesis Data Streams is a service that allows you to collect, process, and analyze streaming data in real-time. It can handle a large number of transactions and it can scale to match the rate of incoming data. However, it comes with additional costs for data retention, data throughput, and number of shards. Additionally, it requires additional management and maintenance to set up, configure, and monitor the Kinesis data streams.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, using Amazon Kinesis Data Firehose, is a more cost-effective solution for storing and processing large amounts of data in near real-time. This service automatically scales based on the incoming data rate and it can automatically store the data in Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, and it can also invoke a Lambda function to process the data before storing it. This option eliminates the need for additional management, monitoring and maintenance of Kinesis data streams.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 784407,
          "date": "Sun 22 Jan 2023 15:53",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option B, using Amazon Kinesis Data Firehose, is a more cost-effective solution for storing and processing large amounts of data in near real-time. This service automatically scales based on the incoming data rate and it can automatically store the data in Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service, and it can also invoke a Lambda function to process the data before storing it. This option eliminates the need for additional management, monitoring and maintenance of Kinesis data streams.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 783943,
          "date": "Sun 22 Jan 2023 06:54",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "Kinesis data analytics : Option cthe question has the in the first line.<br>• Analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. You can quickly build SQL queries and Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale.<br><br>Kinesis Data Firehose<br>• It can capture, transform, and load streaming data into S3, Redshift, Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards being used today.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Kinesis data stream<br><br>• A massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 783945,
          "date": "Sun 22 Jan 2023 06:55",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "Kinesis data stream<br><br>• A massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768115,
          "date": "Fri 06 Jan 2023 23:23",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB.  Other applications can consume the transactions data off the Kinesis data stream.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Other applications can consume from Kinesis Data Streams with the sensitive information still unremoved ? The question requires that sensitive information be purged from the Data Stream.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 787123,
          "date": "Wed 25 Jan 2023 01:44",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Other applications can consume from Kinesis Data Streams with the sensitive information still unremoved ? The question requires that sensitive information be purged from the Data Stream.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764113,
          "date": "Tue 03 Jan 2023 00:18",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C seems right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 761065,
          "date": "Thu 29 Dec 2022 14:37",
          "username": "\t\t\t\tIdriss10\t\t\t",
          "content": "This is what made the difference : to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751848,
          "date": "Wed 21 Dec 2022 04:47",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "C- ans:- Data storage for 1 to 365 days- Data storage for 1 to 365 days and <br>Kinesis Data Firehose >No data storage",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751177,
          "date": "Tue 20 Dec 2022 17:21",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "low-latency retrieval = dynamodb",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750426,
          "date": "Tue 20 Dec 2022 03:54",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications, while also processing the transactions to remove sensitive data before storing them in a document database for low-latency retrieval, a solutions architect should recommend streaming the transactions data into Amazon Kinesis Data Streams and using AWS Lambda integration to remove sensitive data from every transaction before storing the data in Amazon DynamoDB.  Other applications can consume the transactions data off the Kinesis data stream.<br><br>Amazon Kinesis Data Streams is a fully managed service for real-time processing of streaming data at scale. You can use Kinesis Data Streams to continuously capture and store large amounts of data in real-time, such as financial transactions in this case. You can then process the data using a variety of real-time analytics and processing tools, such as AWS Lambda.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Lambda is a serverless computing platform that allows you to run code in response to events and automatically scale to meet demand. By using Lambda integration with Kinesis Data Streams, you can process the transactions data as it is streamed into the data stream, removing sensitive data from each transaction before storing it in Amazon DynamoDB.  This will allow you to meet the requirement to remove sensitive data from the transactions before storing them in a document database.<br><br>Other applications can consume the transactions data off the Kinesis data stream in real-time, allowing you to meet the requirement to share the transactions data with other internal applications.</li><li>Option A, storing the transactions data in Amazon DynamoDB and using DynamoDB Streams to share the data with other applications, would not meet the requirement to remove sensitive data from the transactions before storing them in a document database.<br><br>Option B, streaming the transactions data into Amazon Kinesis Data Firehose to store the data in Amazon DynamoDB and Amazon S3, and using AWS Lambda integration with Kinesis Data Firehose to remove sensitive data, would not allow other applications to consume the data in real-time.<br><br>Option D, storing the batched transactions data in Amazon S3 as files and using AWS Lambda to process and remove sensitive data before updating the files in Amazon S3 and storing the data in Amazon DynamoDB, would not meet the requirement for a near-real-time solution as it involves batch processing of the transactions data.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750427,
          "date": "Tue 20 Dec 2022 03:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "AWS Lambda is a serverless computing platform that allows you to run code in response to events and automatically scale to meet demand. By using Lambda integration with Kinesis Data Streams, you can process the transactions data as it is streamed into the data stream, removing sensitive data from each transaction before storing it in Amazon DynamoDB.  This will allow you to meet the requirement to remove sensitive data from the transactions before storing them in a document database.<br><br>Other applications can consume the transactions data off the Kinesis data stream in real-time, allowing you to meet the requirement to share the transactions data with other internal applications.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, storing the transactions data in Amazon DynamoDB and using DynamoDB Streams to share the data with other applications, would not meet the requirement to remove sensitive data from the transactions before storing them in a document database.<br><br>Option B, streaming the transactions data into Amazon Kinesis Data Firehose to store the data in Amazon DynamoDB and Amazon S3, and using AWS Lambda integration with Kinesis Data Firehose to remove sensitive data, would not allow other applications to consume the data in real-time.<br><br>Option D, storing the batched transactions data in Amazon S3 as files and using AWS Lambda to process and remove sensitive data before updating the files in Amazon S3 and storing the data in Amazon DynamoDB, would not meet the requirement for a near-real-time solution as it involves batch processing of the transactions data.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750428,
          "date": "Tue 20 Dec 2022 03:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, storing the transactions data in Amazon DynamoDB and using DynamoDB Streams to share the data with other applications, would not meet the requirement to remove sensitive data from the transactions before storing them in a document database.<br><br>Option B, streaming the transactions data into Amazon Kinesis Data Firehose to store the data in Amazon DynamoDB and Amazon S3, and using AWS Lambda integration with Kinesis Data Firehose to remove sensitive data, would not allow other applications to consume the data in real-time.<br><br>Option D, storing the batched transactions data in Amazon S3 as files and using AWS Lambda to process and remove sensitive data before updating the files in Amazon S3 and storing the data in Amazon DynamoDB, would not meet the requirement for a near-real-time solution as it involves batch processing of the transactions data.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#34",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#34",
          "answers": [
            {
              "choice": "<p>A. Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 702901,
          "date": "Mon 24 Oct 2022 12:01",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "CloudTrail - Track user activity and API call history.<br>Config - Assess, audits, and evaluates the configuration and relationships of tag resources.<br><br>Therefore, the answer is B",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831008,
          "date": "Mon 06 Mar 2023 17:37",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a history of configuration changes made to your resources and can be used to track changes made to your resources over time.<br><br>AWS CloudTrail is a service that enables you to record API calls made to your AWS resources. It provides a history of API calls made to your resources, including the identity of the caller, the time of the call, the source of the call, and the response element returned by the service.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 831007,
          "date": "Mon 06 Mar 2023 17:35",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a history of configuration changes made to your resources and can be used to track changes made to your resources over time.<br><br>AWS CloudTrail is a service that enables you to record API calls made to your AWS resources. It provides a history of API calls made to your resources, including the identity of the caller, the time of the call, the source of the call, and the response element returned by the service.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 814277,
          "date": "Sun 19 Feb 2023 17:46",
          "username": "\t\t\t\tMcmono\t\t\t",
          "content": "AWS Config is basically used to track config changes, while cloudtrail is to monitor API calls",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 784448,
          "date": "Sun 22 Jan 2023 16:39",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "A.  Use AWS CloudTrail to track configuration changes and AWS Config to record API calls. This option is the best because it utilizes both AWS CloudTrail and AWS Config, which are both designed for tracking and recording different types of information related to AWS resources and API calls. AWS CloudTrail is used to track user activity and API call history, and AWS Config is used to assess, audit, and evaluate the configuration and relationships of tag resources. Together, they provide a comprehensive and robust solution for compliance, governance, auditing, and security.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>why not the B?. <br>AWS Config is primarily used to assess, audit, and evaluate the configuration and relationships of resources in your AWS environment. It does not record the history of API calls made to these resources. On the other hand, AWS CloudTrail is used to track user activity and API call history. Together, AWS Config and CloudTrail provide a complete picture of the configuration and activity on your AWS resources, which is necessary for compliance, governance, auditing, and security. Therefore, option A is the best choice.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 784451,
          "date": "Sun 22 Jan 2023 16:40",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "why not the B?. <br>AWS Config is primarily used to assess, audit, and evaluate the configuration and relationships of resources in your AWS environment. It does not record the history of API calls made to these resources. On the other hand, AWS CloudTrail is used to track user activity and API call history. Together, AWS Config and CloudTrail provide a complete picture of the configuration and activity on your AWS resources, which is necessary for compliance, governance, auditing, and security. Therefore, option A is the best choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777178,
          "date": "Mon 16 Jan 2023 00:41",
          "username": "\t\t\t\tBakedBacon\t\t\t",
          "content": "CloudTrail tracks user activity as well as any API calls (think of bread crumbs leading to an culprit). Config is exactly what it sounds like; configuration. So think audits, config changes ect.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751213,
          "date": "Tue 20 Dec 2022 17:39",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "auditing = cloudtrail",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750431,
          "date": "Tue 20 Dec 2022 04:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B: Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.<br><br>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a history of configuration changes made to your resources and can be used to track changes made to your resources over time.<br><br>AWS CloudTrail is a service that enables you to record API calls made to your AWS resources. It provides a history of API calls made to your resources, including the identity of the caller, the time of the call, the source of the call, and the response element returned by the service.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Together, AWS Config and AWS CloudTrail can be used to meet the requirements for compliance, governance, auditing, and security by tracking configuration changes and recording a history of API calls made to your AWS resources.<br><br>Amazon CloudWatch is a monitoring service for AWS resources and the applications you run on the cloud. It is not specifically designed for tracking configuration changes or recording a history of API calls.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750432,
          "date": "Tue 20 Dec 2022 04:04",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Together, AWS Config and AWS CloudTrail can be used to meet the requirements for compliance, governance, auditing, and security by tracking configuration changes and recording a history of API calls made to your AWS resources.<br><br>Amazon CloudWatch is a monitoring service for AWS resources and the applications you run on the cloud. It is not specifically designed for tracking configuration changes or recording a history of API calls.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750379,
          "date": "Tue 20 Dec 2022 02:23",
          "username": "\t\t\t\tIBANGA007\t\t\t",
          "content": "B.  Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.<br><br>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It can track configuration changes to your AWS resources and record a history of these changes. AWS CloudTrail is a service that records API calls made to AWS resources and logs the API calls in a CloudTrail event.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750049,
          "date": "Mon 19 Dec 2022 17:50",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "B.  ans :https://aws.amazon.com/about-aws/whats-new/2016/07/aws-cloudtrail-now-access-configuration-history-of-resources-referenced-in-your-api-calls/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749546,
          "date": "Mon 19 Dec 2022 07:43",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743817,
          "date": "Tue 13 Dec 2022 10:32",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Correct Answer is A <br>CloudTrail to track configuration changes and AWS Config to record API calls which Records the configuration state for the resource provided in the request. (AWS Config is a service that records the configuration of your AWS resources and maintains a history of changes made to these resources)AWS CloudTrail, on the other hand, is a service that records API calls made on your AWS account and delivers the log files to you. This service can be used to track configuration changes on your AWS resources in real time. Therefore, the correct solution is to use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736663,
          "date": "Tue 06 Dec 2022 10:08",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "The answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723512,
          "date": "Mon 21 Nov 2022 14:04",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700982,
          "date": "Fri 21 Oct 2022 16:09",
          "username": "\t\t\t\tbansalhp\t\t\t",
          "content": "The answer is B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 699192,
          "date": "Wed 19 Oct 2022 18:33",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "bbbbbbbb",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 692749,
          "date": "Wed 12 Oct 2022 08:18",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "bbbbbbbb",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#35",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#35",
          "answers": [
            {
              "choice": "<p>A. Enable Amazon GuardDuty on the account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable Amazon Inspector on the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable AWS Shield and assign Amazon Route 53 to it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable AWS Shield Advanced and assign the ELB to it.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694490,
          "date": "Fri 14 Oct 2022 06:38",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator standard accelerators.",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 695124,
          "date": "Sat 15 Oct 2022 04:11",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "Answer is D<br>C is incorrect because question says Third party DNS and route 53 is AWS proprietary",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 826577,
          "date": "Thu 02 Mar 2023 08:12",
          "username": "\t\t\t\tDaiking\t\t\t",
          "content": "DDoS attack is a feature of AWS Shield, so I confused C or D.  But it usually determines by Health-Check, and Health-Check runs in the level target group of ELB.  Finally, I would go with D. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760166,
          "date": "Wed 28 Dec 2022 18:39",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Details when to use the service,https://medium.com/@tshemku/aws-waf-vs-firewall-manager-vs-shield-vs-shield-advanced-4c86911e94c6",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751226,
          "date": "Tue 20 Dec 2022 17:46",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "A third-party service is used for the DNS. = Not Route 53 (AWS). The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks = Shield",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750433,
          "date": "Tue 20 Dec 2022 04:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is D: Enable AWS Shield Advanced and assign the ELB to it.<br><br>AWS Shield is a service that provides DDoS protection for your AWS resources. There are two tiers of AWS Shield: AWS Shield Standard and AWS Shield Advanced. AWS Shield Standard is included with all AWS accounts at no additional cost and provides protection against most common network and transport layer DDoS attacks. AWS Shield Advanced provides additional protection against more complex and larger scale DDoS attacks, as well as access to a team of DDoS response experts.<br><br>To detect and protect against large-scale DDoS attacks on a public-facing web application hosted on Amazon EC2 instances behind an Elastic Load Balancer (ELB), you should enable AWS Shield Advanced and assign the ELB to it. This will provide advanced protection against DDoS attacks targeting the ELB and the EC2 instances behind it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon GuardDuty is a threat detection service that analyzes network traffic and other data sources to identify potential threats to your AWS resources. It is not specifically designed for detecting and protecting against DDoS attacks.<br><br>Amazon Inspector is a security assessment service that analyzes the runtime behavior of your Amazon EC2 instances to identify security vulnerabilities. It is not specifically designed for detecting and protecting against DDoS attacks.<br><br>Amazon Route 53 is a DNS service that routes traffic to your resources on the internet. It is not specifically designed for detecting and protecting against DDoS attacks.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750434,
          "date": "Tue 20 Dec 2022 04:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Amazon GuardDuty is a threat detection service that analyzes network traffic and other data sources to identify potential threats to your AWS resources. It is not specifically designed for detecting and protecting against DDoS attacks.<br><br>Amazon Inspector is a security assessment service that analyzes the runtime behavior of your Amazon EC2 instances to identify security vulnerabilities. It is not specifically designed for detecting and protecting against DDoS attacks.<br><br>Amazon Route 53 is a DNS service that routes traffic to your resources on the internet. It is not specifically designed for detecting and protecting against DDoS attacks.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749551,
          "date": "Mon 19 Dec 2022 07:46",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747523,
          "date": "Fri 16 Dec 2022 20:22",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load-balancing-bp6.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741697,
          "date": "Sun 11 Dec 2022 13:51",
          "username": "\t\t\t\tprethesh\t\t\t",
          "content": "https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/best-practices-for-ddos-mitigation.html<br>You can use Shield Advanced to configure DDoS protection for Elastic IP addresses.When an Elastic IP address is assigned per Availability Zone to the Network Load Balancer, Shield Advanced will apply the relevant DDoS protections for the Network Load Balancer traffic.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741457,
          "date": "Sun 11 Dec 2022 07:10",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "D<br>https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load-balancing-bp6.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723515,
          "date": "Mon 21 Nov 2022 14:05",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719851,
          "date": "Wed 16 Nov 2022 18:05",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Large-scale DDoS attacks = AWS Shield Advanced<br>The correct answer is D<br>https://aws.amazon.com/shield/faqs/<br>https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/elastic-load-balancing-bp6.html",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 717096,
          "date": "Sun 13 Nov 2022 06:03",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Same reasoning as given by Ninjawarz",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692475,
          "date": "Wed 12 Oct 2022 01:06",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is D",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#36",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#36",
          "answers": [
            {
              "choice": "<p>A. Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 691176,
          "date": "Mon 10 Oct 2022 15:24",
          "username": "\t\t\t\tpooppants\t\t\t",
          "content": "KMS Multi-region keys are required https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon S3 cross-region replication decrypts and re-encrypts data under a KMS key in the destination Region, even when replicating objects protected by a multi-Region key. So stating that Amazon S3 cross-region replication decrypts and re-encrypts data under a KMS key in the destination Region, even when replicating objects protected by a multi-Region key is required is incorrect</li><li>@magazz: it's not true then. Based on the document from AWS https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html , we will need to setup the replication rule with destination KMS. In order to have the key available in more than 2, then multi-region key should be required. But I'm still not favor option B - we can use server-side when why wasting effort to do client side encryption.</li><li>I would say it's true... Not sure the previous one say \\\"not true\\\" :D. </li><li>It's not clear what you are saying. Are you saying that B is correct or D is correct?</li><li>:D =&gt; is smile i thought</li></ul>",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 722996,
          "date": "Sun 20 Nov 2022 21:47",
          "username": "\t\t\t\tmagazz\t\t\t",
          "content": "Amazon S3 cross-region replication decrypts and re-encrypts data under a KMS key in the destination Region, even when replicating objects protected by a multi-Region key. So stating that Amazon S3 cross-region replication decrypts and re-encrypts data under a KMS key in the destination Region, even when replicating objects protected by a multi-Region key is required is incorrect<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>@magazz: it's not true then. Based on the document from AWS https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html , we will need to setup the replication rule with destination KMS. In order to have the key available in more than 2, then multi-region key should be required. But I'm still not favor option B - we can use server-side when why wasting effort to do client side encryption.</li><li>I would say it's true... Not sure the previous one say \\\"not true\\\" :D. </li><li>It's not clear what you are saying. Are you saying that B is correct or D is correct?</li><li>:D =&gt; is smile i thought</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727892,
          "date": "Sun 27 Nov 2022 03:19",
          "username": "\t\t\t\tTuLe\t\t\t",
          "content": "@magazz: it's not true then. Based on the document from AWS https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html , we will need to setup the replication rule with destination KMS. In order to have the key available in more than 2, then multi-region key should be required. But I'm still not favor option B - we can use server-side when why wasting effort to do client side encryption.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I would say it's true... Not sure the previous one say \\\"not true\\\" :D. </li><li>It's not clear what you are saying. Are you saying that B is correct or D is correct?</li><li>:D =&gt; is smile i thought</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727893,
          "date": "Sun 27 Nov 2022 03:20",
          "username": "\t\t\t\tTuLe\t\t\t",
          "content": "I would say it's true... Not sure the previous one say \\\"not true\\\" :D. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's not clear what you are saying. Are you saying that B is correct or D is correct?</li><li>:D =&gt; is smile i thought</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732787,
          "date": "Thu 01 Dec 2022 16:52",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "It's not clear what you are saying. Are you saying that B is correct or D is correct?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>:D =&gt; is smile i thought</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766007,
          "date": "Wed 04 Jan 2023 19:52",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": ":D => is smile i thought",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 692906,
          "date": "Wed 12 Oct 2022 11:35",
          "username": "\t\t\t\tKJa\t\t\t",
          "content": "Cannot be A - question says customer managed key<br>Cannot B - client side encryption is operational overhead<br>Cannot C -as it says SSE-S3 instead of customer managed<br>so the answer is D though it required one time setup of keys<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How does client side encryption increase OPERATIONAL overhead? Do you think every connected client is sitting there with gpg cli, decrypting/encrypting every packet that comes in/out? No, it's done via SDK -&gt; https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html<br><br>The correct answer is B because that's the only way to actually get the same key across multiple regions with minimal operational overhead</li><li>The data in both S3 buckets must be encrypted and decrypted with the same KMS key. <br>AWS KMS supports multi-Region keys, which are AWS KMS keys in different AWS Regions that can be used interchangeably – as though you had the same key in multiple Regions. <br>\\\"as though\\\" means it's different.<br>SoI agree with B</li><li>key change across regions unless you use multi-Region keys</li><li>fun joke, if u dont do encryption on client side, where else could it be?</li><li>It could be server side. For client side, the application need to finish the encryption and decryption by itself. So S3 object encryption on the server side is less operational overhead. https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html<br><br>But for option B, the major issue is if you create KMS keys in 2 regions, they can not be the same.</li><li>Sorry for the typo, I mean option D. </li></ul>",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 720210,
          "date": "Thu 17 Nov 2022 05:52",
          "username": "\t\t\t\tth3cookie\t\t\t",
          "content": "How does client side encryption increase OPERATIONAL overhead? Do you think every connected client is sitting there with gpg cli, decrypting/encrypting every packet that comes in/out? No, it's done via SDK -> https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html<br><br>The correct answer is B because that's the only way to actually get the same key across multiple regions with minimal operational overhead",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 694498,
          "date": "Fri 14 Oct 2022 06:55",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "The data in both S3 buckets must be encrypted and decrypted with the same KMS key. <br>AWS KMS supports multi-Region keys, which are AWS KMS keys in different AWS Regions that can be used interchangeably – as though you had the same key in multiple Regions. <br>\\\"as though\\\" means it's different.<br>SoI agree with B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>key change across regions unless you use multi-Region keys</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 694503,
          "date": "Fri 14 Oct 2022 07:02",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "key change across regions unless you use multi-Region keys",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 693495,
          "date": "Thu 13 Oct 2022 03:21",
          "username": "\t\t\t\tmattlai\t\t\t",
          "content": "fun joke, if u dont do encryption on client side, where else could it be?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It could be server side. For client side, the application need to finish the encryption and decryption by itself. So S3 object encryption on the server side is less operational overhead. https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html<br><br>But for option B, the major issue is if you create KMS keys in 2 regions, they can not be the same.</li><li>Sorry for the typo, I mean option D. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712756,
          "date": "Mon 07 Nov 2022 03:12",
          "username": "\t\t\t\tNewptone\t\t\t",
          "content": "It could be server side. For client side, the application need to finish the encryption and decryption by itself. So S3 object encryption on the server side is less operational overhead. https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html<br><br>But for option B, the major issue is if you create KMS keys in 2 regions, they can not be the same.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry for the typo, I mean option D. </li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 712757,
          "date": "Mon 07 Nov 2022 03:15",
          "username": "\t\t\t\tNewptone\t\t\t",
          "content": "Sorry for the typo, I mean option D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 831980,
          "date": "Tue 07 Mar 2023 15:11",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Client-side encryption is the act of encrypting your data locally to ensure its security as it passes to the Amazon S3 service. The Amazon S3 service receives your encrypted data; it does not play a role in encrypting or decrypting it.<br><br>To enable client-side encryption, you have the following options:<br><br>Use a key stored in AWS Key Management Service (AWS KMS).<br><br>Use a key that you store within your application.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 829095,
          "date": "Sat 04 Mar 2023 17:00",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "KMS is server side encryption only.So it's not b",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 821221,
          "date": "Sat 25 Feb 2023 06:49",
          "username": "\t\t\t\tKittieHearts\t\t\t",
          "content": "I original thought D as they mentioned customer keys to be managed on the the system side. however, while reviewing the comments, the question does say that the same key should be used in both regions. due to this the answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 820914,
          "date": "Fri 24 Feb 2023 21:30",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "For server side KMS encrypted S3 object, they are not replicated between buckets.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html<br>\\\"By default, Amazon S3 doesn't replicate objects that are stored at rest using server-side encryption with AWS KMS keys stored in AWS KMS. This section explains the additional configuration that you add to direct Amazon S3 to replicate these objects.\\\"<br><br>Client side encryption is handled 100% on the client side so AWS doesn't even know the S3 objects are encrypted.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 818112,
          "date": "Wed 22 Feb 2023 18:34",
          "username": "\t\t\t\tBhawesh\t\t\t",
          "content": "Option B, satisfies both these requirements. Amazon Kinesis Data Firehose calls Lambda function, which removes sensitive info and stores the sanitized data in DynamoDB.  Amazon Kinesis Data Firehose also stores un-sanitized data into S3. Then, other internal application can consume that un-sanitized data from S3.<br><br>Interestingly, Also Option C meets these 2 requirements, but Option B presents less overhead for the other internal applications. example: some internal application may not be able to consume the un-sanitized stream coming from Kinesis (option: C) but all the internal applications should be able to consume the un-sanitized data from S3 (option: B).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812176,
          "date": "Fri 17 Feb 2023 18:46",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "From https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html For<br>most users, the default AWS KMS key store, which is protected by FIPS 140-2 validated cryptographic<br>modules, fulfills their security requirements. There is no need to add an extra layer of maintenance<br>responsibility or a dependency on an additional service. However, you might consider creating a<br>custom key store if your organization has any of the following requirements: Key material cannot be<br>stored in a shared environment. Key material must be subject to a secondary, independent audit<br>path. The HSMs that generate and store key material must be certified at FIPS 140-2 Level 3.<br>https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-overview.html<br>https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 811538,
          "date": "Fri 17 Feb 2023 07:17",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "B is correct. The key part of the question is \\\"customer managed key\\\". Customer managed keys gives more control over its encryption/decryption which no one knows of i.e. AWS except the customer itself and they only store it in KMS. This could be data for banks, governments etc. very confidential and private. Secondly, multi region KMS can be one key so the customer provided key shared in the two regions that the S3's are in.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 809296,
          "date": "Wed 15 Feb 2023 10:02",
          "username": "\t\t\t\tNitiATOS\t\t\t",
          "content": "I am goingwith this option based on Multi regioin key : <br>https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html#:~:text=You%20can%20use%20multi%2DRegion%20keys%20with%20client%2Dside%20encryption%20libraries%2C%20such%20as%20the%20AWS%20Encryption%20SDK%2C%20the%20DynamoDB%20Encryption%20Client%2C%20and%20Amazon%20S3%20client%2Dside%20encryption.%20For",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 805111,
          "date": "Sat 11 Feb 2023 10:37",
          "username": "\t\t\t\tmaciekmaciek\t\t\t",
          "content": "I would choose between B and D, but 'client side' encryption in B is not a good idea - so I choose D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 803933,
          "date": "Fri 10 Feb 2023 04:26",
          "username": "\t\t\t\tWiss7\t\t\t",
          "content": "client side encryption is operational overhead",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 803673,
          "date": "Thu 09 Feb 2023 21:54",
          "username": "\t\t\t\tK0nAn\t\t\t",
          "content": "Since it says LEAST operational ,I think we should go with option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799746,
          "date": "Mon 06 Feb 2023 14:17",
          "username": "\t\t\t\tRONNYC\t\t\t",
          "content": "Multi-region keys can be used for client-side encryption",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 799643,
          "date": "Mon 06 Feb 2023 12:02",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "B because it says \\\"customer managed key\\\". Customer managed is the important part. It's basically a key generated by the customer which is than stored on AWS KMS.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 797312,
          "date": "Fri 03 Feb 2023 20:46",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 794642,
          "date": "Tue 31 Jan 2023 22:49",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#37",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#37",
          "answers": [
            {
              "choice": "<p>A. Use the EC2 serial console to directly access the terminal interface of each instance for administration.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 706211,
          "date": "Fri 28 Oct 2022 08:45",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "How can Session Manager benefit my organization? <br>Ans: No open inbound ports and no need to manage bastion hosts or SSH keys<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Do you know what from the question is it Windows or Linux EC2. I think not so how you want to do SSH session for Windows?<br>Answer is C</li><li>Session Manager provides support for Windows, Linux, and macOS from a single tool</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 718647,
          "date": "Tue 15 Nov 2022 10:41",
          "username": "\t\t\t\tNightducky\t\t\t",
          "content": "Do you know what from the question is it Windows or Linux EC2. I think not so how you want to do SSH session for Windows?<br>Answer is C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Session Manager provides support for Windows, Linux, and macOS from a single tool</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732795,
          "date": "Thu 01 Dec 2022 16:58",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Session Manager provides support for Windows, Linux, and macOS from a single tool",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 830434,
          "date": "Mon 06 Mar 2023 00:47",
          "username": "\t\t\t\tnour\t\t\t",
          "content": "The keyword that makes option B follows the AWS Well-Architected Framework is \\\"IAM role.\\\" IAM roles provide fine-grained access control and are a recommended best practice in the AWS Well-Architected Framework. By attaching the appropriate IAM role to each instance and using AWS Systems Manager Session Manager to establish a remote SSH session, the solution is using IAM roles to control access and follows a recommended best practice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 800808,
          "date": "Tue 07 Feb 2023 12:33",
          "username": "\t\t\t\tShaw605\t\t\t",
          "content": "Answer is B ~ Chat GPT<br>To meet the requirements with the least operational overhead, the company can use the AWS Systems Manager Session Manager. It is a native AWS service that enables secure and auditable access to instances without the need for remote public IP addresses, inbound security group rules, or Bastion hosts. With AWS Systems Manager Session Manager, the company can establish a secure and auditable session to the EC2 instances and perform administrative tasks without the need for additional operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 800806,
          "date": "Tue 07 Feb 2023 12:31",
          "username": "\t\t\t\tShaw605\t\t\t",
          "content": "Answer is B ~ (Chat GPT)<br>A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework.<br>Which solution will meet these requirements with the LEAST operational overhead?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778038,
          "date": "Mon 16 Jan 2023 18:45",
          "username": "\t\t\t\tPranav_523\t\t\t",
          "content": "correct answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 768129,
          "date": "Fri 06 Jan 2023 23:51",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Option B.  Attaching the appropriate IAM role to each existing instance and new instance and using AWS Systems Manager Session Manager to establish a remote SSH session would meet the requirements with the least operational overhead. This approach allows for secure remote access to the instances without the need to manage additional infrastructure or maintain a separate connection to the instances. It also allows for the use of native AWS services and follows the AWS Well-Architected Framework.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 764161,
          "date": "Tue 03 Jan 2023 01:46",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "https://dev.to/aws-builders/aws-systems-manager-session-manager-implementation-f9a#:~:text=Session%20Manager%20is%20a%20fully%20managed%20AWS%20Systems,ports%2C%20maintain%20bastion%20hosts%2C%20or%20manage%20SSH%20keys.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 757460,
          "date": "Mon 26 Dec 2022 14:21",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "EC2 = IAM role",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751287,
          "date": "Tue 20 Dec 2022 18:37",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "administer the instances remotely and securely: <br>EC2 serial console (option A) not intended for regular administration. <br>option B allows administrators to remotely access and administer the instances securely without the need for additional infrastructure or maintenance. <br>option C requires additional infrastructure and maintenance<br>option D can be a complex and time-consuming process.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750441,
          "date": "Tue 20 Dec 2022 04:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B: Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.<br><br>To remotely and securely access and administer the Amazon EC2 instances in the company's AWS account, you should attach the appropriate IAM role to each existing instance and new instance. This will allow the instances to access the required AWS services and resources. Then, you can use AWS Systems Manager Session Manager to establish a remote SSH session to each instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Systems Manager Session Manager is a native AWS service that allows you to remotely and securely access the command line interface of your Amazon EC2 instances, on-premises servers, and virtual machines (VMs) running in other clouds, without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. With Session Manager, you can establish a secure, auditable connection to your instances using the AWS Management Console, the AWS CLI, or the AWS SDKs.<br><br>Using the EC2 serial console to directly access the terminal interface of each instance for administration would not be a repeatable process and would not follow the AWS Well-Architected Framework.</li><li>Creating an administrative SSH key pair and loading the public key into each EC2 instance would require you to manage and rotate the keys, which would increase the operational overhead. Additionally, deploying a bastion host in a public subnet to provide a tunnel for administration of each instance would also increase the operational overhead and potentially introduce security risks.<br><br>Establishing an AWS Site-to-Site VPN connection and instructing administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel would also increase the operational overhead and potentially introduce security risks.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750442,
          "date": "Tue 20 Dec 2022 04:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "AWS Systems Manager Session Manager is a native AWS service that allows you to remotely and securely access the command line interface of your Amazon EC2 instances, on-premises servers, and virtual machines (VMs) running in other clouds, without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. With Session Manager, you can establish a secure, auditable connection to your instances using the AWS Management Console, the AWS CLI, or the AWS SDKs.<br><br>Using the EC2 serial console to directly access the terminal interface of each instance for administration would not be a repeatable process and would not follow the AWS Well-Architected Framework.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Creating an administrative SSH key pair and loading the public key into each EC2 instance would require you to manage and rotate the keys, which would increase the operational overhead. Additionally, deploying a bastion host in a public subnet to provide a tunnel for administration of each instance would also increase the operational overhead and potentially introduce security risks.<br><br>Establishing an AWS Site-to-Site VPN connection and instructing administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel would also increase the operational overhead and potentially introduce security risks.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750443,
          "date": "Tue 20 Dec 2022 04:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Creating an administrative SSH key pair and loading the public key into each EC2 instance would require you to manage and rotate the keys, which would increase the operational overhead. Additionally, deploying a bastion host in a public subnet to provide a tunnel for administration of each instance would also increase the operational overhead and potentially introduce security risks.<br><br>Establishing an AWS Site-to-Site VPN connection and instructing administrators to use their local on-premises machines to connect directly to the instances using SSH keys across the VPN tunnel would also increase the operational overhead and potentially introduce security risks.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749565,
          "date": "Mon 19 Dec 2022 07:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B - AWS best practice for remote SSH access to EC2",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743848,
          "date": "Tue 13 Dec 2022 10:50",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B <br>the question with the least operational overhead, you can attach the appropriate IAM role to each existing instance and new instance. This will allow you to use AWS Systems Manager Session Manager to establish a remote SSH session to each instance without the need to manage SSH keys. Option C is not correct, it is because, it requires you to manage SSH keys, which can be time-consuming and error-prone.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736666,
          "date": "Tue 06 Dec 2022 10:11",
          "username": "\t\t\t\tJohnnyBG\t\t\t",
          "content": "B, No doubt about it",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723521,
          "date": "Mon 21 Nov 2022 14:07",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct for me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719125,
          "date": "Tue 15 Nov 2022 21:59",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "B is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715912,
          "date": "Fri 11 Nov 2022 10:30",
          "username": "\t\t\t\tKeld\t\t\t",
          "content": "The answer is C, there is no indication of which type of EC2 Windows/Linux.<br>SSH only works for Windows<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>SSH Only work in Linux ;)</li><li>Session Manager provides support for Windows, Linux, and macOS from a single tool, so B</li><li>only works for *Linux*</li><li>Is correct C - https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html ------&gt;<br>Cross-platform support for Windows, Linux, and macOS<br><br>Session Manager provides support for Windows, Linux, and macOS from a single tool. For example, you don't need to use an SSH client for Linux and macOS managed nodes or an RDP connection for Windows Server managed nodes.</li><li>B is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743412,
          "date": "Tue 13 Dec 2022 00:23",
          "username": "\t\t\t\tA_New_Guy\t\t\t",
          "content": "SSH Only work in Linux ;)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732793,
          "date": "Thu 01 Dec 2022 16:57",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Session Manager provides support for Windows, Linux, and macOS from a single tool, so B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 715913,
          "date": "Fri 11 Nov 2022 10:31",
          "username": "\t\t\t\tKeld\t\t\t",
          "content": "only works for *Linux*<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Is correct C - https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html ------&gt;<br>Cross-platform support for Windows, Linux, and macOS<br><br>Session Manager provides support for Windows, Linux, and macOS from a single tool. For example, you don't need to use an SSH client for Linux and macOS managed nodes or an RDP connection for Windows Server managed nodes.</li><li>B is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719587,
          "date": "Wed 16 Nov 2022 12:53",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Is correct C - https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html ------><br>Cross-platform support for Windows, Linux, and macOS<br><br>Session Manager provides support for Windows, Linux, and macOS from a single tool. For example, you don't need to use an SSH client for Linux and macOS managed nodes or an RDP connection for Windows Server managed nodes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is correct</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 719588,
          "date": "Wed 16 Nov 2022 12:54",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703313,
          "date": "Mon 24 Oct 2022 21:33",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "The answer is C, they mentioned that it must be native service, option B is not a service, it is one of the option to connect to instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>? Its a service available on AWS, so a native service</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732794,
          "date": "Thu 01 Dec 2022 16:58",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "? Its a service available on AWS, so a native service",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#38",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website.<br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#38",
          "answers": [
            {
              "choice": "<p>A. Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 831986,
          "date": "Tue 07 Mar 2023 15:18",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML, CSS, JavaScript, and images. It does this by placing cache servers in locations around the world, which store copies of the content and serve it to users from the location that is nearest to them.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 818115,
          "date": "Wed 22 Feb 2023 18:35",
          "username": "\t\t\t\tBhawesh\t\t\t",
          "content": "My vote is: option B.  Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.<br>This question has 2 requirements:<br>1. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. <br>2. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799717,
          "date": "Mon 06 Feb 2023 13:36",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "C.  S3 accelerator is best for uploads to S3, whereas Cloudfront is for content delivery. S3 static website can be the origin which is distributed to Cloudfront and routed by Route 53.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797316,
          "date": "Fri 03 Feb 2023 20:51",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "Option C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 768130,
          "date": "Fri 06 Jan 2023 23:53",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Option C.  Adding an Amazon CloudFront distribution in front of the S3 bucket and editing the Route 53 entries to point to the CloudFront distribution would meet the requirements most cost-effectively. CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content by distributing it across a global network of edge locations. When a user accesses the website, CloudFront will automatically route the request to the edge location that provides the lowest latency, reducing the time it takes for the content to be delivered to the user. This solution also allows for easy integration with S3 and Route 53, and provides additional benefits such as DDoS protection and support for custom SSL certificates.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751316,
          "date": "Tue 20 Dec 2022 19:05",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "decrease latency and most cost-effective = cloudfront in front of S3 bucket (content can be served closer to the user, reducing latency). Replicating S3 bucket and Global accelerator would also decrease latency but would be less cost-effective. Transfer accelerator wouldn't decrease latency since it's not for delivering content, but for transfering it",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750444,
          "date": "Tue 20 Dec 2022 04:28",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is C: Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.<br><br>Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML, CSS, JavaScript, and images. It does this by placing cache servers in locations around the world, which store copies of the content and serve it to users from the location that is nearest to them.<br><br>To decrease latency for users who access the static website hosted on Amazon S3, you can add an Amazon CloudFront distribution in front of the S3 bucket and edit the Route 53 entries to point to the CloudFront distribution. This will allow CloudFront to cache the content of the website at locations around the world, which will reduce the time it takes for users to access the website by serving it from the location that is nearest to them.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer A, (WRONG) - Replicating the S3 bucket that contains the website to all AWS Regions and adding Route 53 geolocation routing entries would be more expensive than using CloudFront, as it would require you to pay for the additional storage and data transfer costs associated with replicating the bucket to multiple Regions.<br><br>Answer B, (WRONG) - Provisioning accelerators in AWS Global Accelerator and associating the supplied IP addresses with the S3 bucket would also be more expensive than using CloudFront, as it would require you to pay for the additional cost of the accelerators.<br><br>Answer D, (WRONG) - Enabling S3 Transfer Acceleration on the bucket and editing the Route 53 entries to point to the new endpoint would not reduce latency for users who access the website from around the world, as it only speeds up the transfer of large files over the public internet and does not have cache servers in multiple locations around the world.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750445,
          "date": "Tue 20 Dec 2022 04:29",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Answer A, (WRONG) - Replicating the S3 bucket that contains the website to all AWS Regions and adding Route 53 geolocation routing entries would be more expensive than using CloudFront, as it would require you to pay for the additional storage and data transfer costs associated with replicating the bucket to multiple Regions.<br><br>Answer B, (WRONG) - Provisioning accelerators in AWS Global Accelerator and associating the supplied IP addresses with the S3 bucket would also be more expensive than using CloudFront, as it would require you to pay for the additional cost of the accelerators.<br><br>Answer D, (WRONG) - Enabling S3 Transfer Acceleration on the bucket and editing the Route 53 entries to point to the new endpoint would not reduce latency for users who access the website from around the world, as it only speeds up the transfer of large files over the public internet and does not have cache servers in multiple locations around the world.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 749567,
          "date": "Mon 19 Dec 2022 08:01",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C - Cloudfront is the right answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744933,
          "date": "Wed 14 Dec 2022 10:32",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "CloudFront",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 732146,
          "date": "Thu 01 Dec 2022 02:30",
          "username": "\t\t\t\tDasCert\t\t\t",
          "content": "Isn't Transfer Acceleration the same thing? I mean, what's the difference between C and D?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ok, I got the answer to this:<br>In short, Transfer Acceleration is for Writes and CloudFront is for Reads.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732148,
          "date": "Thu 01 Dec 2022 02:31",
          "username": "\t\t\t\tDasCert\t\t\t",
          "content": "ok, I got the answer to this:<br>In short, Transfer Acceleration is for Writes and CloudFront is for Reads.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 723522,
          "date": "Mon 21 Nov 2022 14:08",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719761,
          "date": "Wed 16 Nov 2022 15:55",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ok CloudFront",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 719127,
          "date": "Tue 15 Nov 2022 22:03",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "C is right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712993,
          "date": "Mon 07 Nov 2022 12:08",
          "username": "\t\t\t\tMordans\t\t\t",
          "content": "ANSWER C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695136,
          "date": "Sat 15 Oct 2022 04:30",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "C: Cloudfront",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692801,
          "date": "Wed 12 Oct 2022 09:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "of course cloudfront it's the answer",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#39",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website.<br>The company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem.<br>Which solution addresses this performance issue?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#39",
          "answers": [
            {
              "choice": "<p>A. Change the storage type to Provisioned IOPS SSD. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Change the DB instance to a memory optimized instance class.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Change the DB instance to a burstable performance instance class.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 751327,
          "date": "Tue 20 Dec 2022 19:18",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "A: Made for high levels of I/O opps for consistent, predictable performance.<br>B: Can improve performance of insert opps, but it's a storage performancerather than processing power problem<br>C: for moderate CPU usage<br>D: for scale read-only replicas and doesn't improve performance of insert opps on the primary DB instance",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832003,
          "date": "Tue 07 Mar 2023 15:34",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "Provisioned IOPS SSD (io1) is a high-performance storage option that is designed for I/O-intensive workloads, such as databases that require a high number of read and write operations per second. It allows you to provide a specific number of input/output operations per second (IOPS) for your Amazon RDS for MySQL database instance, which can improve the performance of insert operations that require high levels of I/O.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 803684,
          "date": "Thu 09 Feb 2023 22:12",
          "username": "\t\t\t\tK0nAn\t\t\t",
          "content": "Change the storage type to Provisioned IOPS SSD would likely address the performance issue described.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 801170,
          "date": "Tue 07 Feb 2023 18:17",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://aws.amazon.com/ebs/features/<br>\\\"Provisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest performance<br>EBS volumes designed for your critical, I/O intensive database applications.<br>These volumes are ideal for both IOPS-intensive and throughput-intensive workloads that require<br>extremely low latency.\\\"<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 784002,
          "date": "Sun 22 Jan 2023 08:03",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "general puRpose SSD oes not fluent with Mysql<br>but provission IOPS SSD are more flexible with the Mysql",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 774757,
          "date": "Fri 13 Jan 2023 18:59",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "A is correct as the Provisioned IOPS is meant for it",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 769113,
          "date": "Sun 08 Jan 2023 07:10",
          "username": "\t\t\t\tvinhle\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 768131,
          "date": "Fri 06 Jan 2023 23:57",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Changing the storage type to Provisioned IOPS SSD would address this performance issue. Provisioned IOPS SSD (io1) is a high-performance storage option designed for I/O-intensive workloads such as databases. It provides a consistent level of IOPS performance, regardless of the size of the data set. By using Provisioned IOPS SSD, the company can ensure that the database has the required level of I/O performance to handle the high volume of updates. This option would provide the best performance improvement for this workload, as it specifically addresses the issue of slow insert operations due to insufficient I/O performance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757469,
          "date": "Mon 26 Dec 2022 14:28",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "A is correct !",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750453,
          "date": "Tue 20 Dec 2022 04:42",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is A: Change the storage type to Provisioned IOPS SSD. <br><br>Provisioned IOPS SSD (io1) is a high-performance storage option that is designed for I/O-intensive workloads, such as databases that require a high number of read and write operations per second. It allows you to provide a specific number of input/output operations per second (IOPS) for your Amazon RDS for MySQL database instance, which can improve the performance of insert operations that require high levels of I/O.<br><br>In this case, the company has noticed that some insert operations are taking 10 seconds or longer, and the database has 2 TB of General Purpose SSD storage, which is not designed for high-performance workloads. Changing the storage type to Provisioned IOPS SSD will address the performance issue by providing a higher number of IOPS, which will improve the performance of the insert operations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer B &amp; C (not correct), Changing the DB instance to a memory-optimized instance class or a burstable performance instance class would not address the performance issue, as these instance classes are not optimized for storage performance.<br><br>Answer D (not correct), Enabling Multi-AZ RDS to read replicas with MySQL native asynchronous replication would not address the performance issue, as read replicas are used for read-heavy workloads and do not improve the performance of write operations on the primary database instance.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750454,
          "date": "Tue 20 Dec 2022 04:42",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Answer B & C (not correct), Changing the DB instance to a memory-optimized instance class or a burstable performance instance class would not address the performance issue, as these instance classes are not optimized for storage performance.<br><br>Answer D (not correct), Enabling Multi-AZ RDS to read replicas with MySQL native asynchronous replication would not address the performance issue, as read replicas are used for read-heavy workloads and do not improve the performance of write operations on the primary database instance.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749981,
          "date": "Mon 19 Dec 2022 16:28",
          "username": "\t\t\t\tMorinator\t\t\t",
          "content": "A with no doubt",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749569,
          "date": "Mon 19 Dec 2022 08:03",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 739351,
          "date": "Thu 08 Dec 2022 19:55",
          "username": "\t\t\t\tparku\t\t\t",
          "content": "fast iops required.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 736841,
          "date": "Tue 06 Dec 2022 14:22",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Answer is A since it is a transaction delay issue",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 732771,
          "date": "Thu 01 Dec 2022 16:30",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "A is the correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723524,
          "date": "Mon 21 Nov 2022 14:09",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720948,
          "date": "Fri 18 Nov 2022 00:02",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "SSD is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#40",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis.<br>The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days.<br>What is the MOST operationally efficient solution that meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#40",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694050,
          "date": "Thu 13 Oct 2022 17:36",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "Definitely A, it's the most operationally efficient compared to D, which requires a lot of code and infrastructure to maintain. A is mostly managed (firehose is fully managed and S3 lifecycles are also managed)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>what about the 30 days minimum requirement to transition to S3 glacier?</li><li>You can directly migrate from S3 standard to glacier without waiting<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</li></ul>",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 737097,
          "date": "Tue 06 Dec 2022 19:02",
          "username": "\t\t\t\tKelvin_ke\t\t\t",
          "content": "what about the 30 days minimum requirement to transition to S3 glacier?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can directly migrate from S3 standard to glacier without waiting<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 746927,
          "date": "Fri 16 Dec 2022 08:52",
          "username": "\t\t\t\tstudis\t\t\t",
          "content": "You can directly migrate from S3 standard to glacier without waiting<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 697021,
          "date": "Mon 17 Oct 2022 09:00",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Only A makes sense operationally.<br>If you think D, just consider what is needed to move the message from SQS to S3... you are polling daily 14 TB to take out 1 TB. .. that's no operationally efficient at all.",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 832014,
          "date": "Tue 07 Mar 2023 15:40",
          "username": "\t\t\t\tbilel500\t\t\t",
          "content": "The correct answer is A: Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 804237,
          "date": "Fri 10 Feb 2023 12:21",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "This question was tricky but after some reading my choice went from D to A.  Which is Operationally efficient.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774148,
          "date": "Fri 13 Jan 2023 06:56",
          "username": "\t\t\t\tjannymacna\t\t\t",
          "content": "A.  Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.<br><br>This solution meets the company's requirements to minimize costs and not manage additional infrastructure while providing high availability. Kinesis Data Firehose is a fully managed service that can automatically ingest streaming data and load it into Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service. By configuring the Firehose to deliver the alerts to an S3 bucket, the company can take advantage of S3's high durability and availability. An S3 Lifecycle configuration can be set up to automatically transition data that is older than 14 days to Amazon S3 Glacier, an extremely low-cost storage class for infrequently accessed data.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 768132,
          "date": "Fri 06 Jan 2023 23:59",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Creating an Amazon Kinesis Data Firehose delivery stream to ingest the alerts and configuring it to deliver the alerts to an Amazon S3 bucket is the most operationally efficient solution that meets the requirements. Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as S3, Redshift, Elasticsearch Service, and Splunk. It can automatically scale to handle the volume and throughput of the alerts, and it can also batch, compress, and encrypt the data as it is delivered to S3. By configuring a Lifecycle policy on the S3 bucket, the company can automatically transition data to Amazon S3 Glacier after 14 days, allowing the company to store the data for longer periods of time at a lower cost. This solution requires minimal management and provides high availability, making it the most operationally efficient choice.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757983,
          "date": "Tue 27 Dec 2022 01:44",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is not a right answer is Kinesis Firehose is not the right service to Ingest small 2KB events. Minimum Message Size for Kinesis Firehose is 5MB.  Kinesis Data Stream is the right service for this but as that is not given as option here, SQS with 14 Day retention is right answer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"A record can be as large as 1,000 KB. \\\" and the diagrams shown in this URL support A as the answer.<br>https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</li><li>Option A:<br>Thinking about this a more as Low operational overhead primary requirement option A will be better option but it will have higher Latency compared to using Kinesis Data Stream.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 761707,
          "date": "Fri 30 Dec 2022 07:10",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "\\\"A record can be as large as 1,000 KB. \\\" and the diagrams shown in this URL support A as the answer.<br>https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757992,
          "date": "Tue 27 Dec 2022 01:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A:<br>Thinking about this a more as Low operational overhead primary requirement option A will be better option but it will have higher Latency compared to using Kinesis Data Stream.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757475,
          "date": "Mon 26 Dec 2022 14:36",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "any data older than 14 days => can not D ! => A correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751335,
          "date": "Tue 20 Dec 2022 19:30",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "A, MOST operationally efficient solution = Kinesis Data Firehose, since it's a fully managed solution<br>B, more costly and more opp overhead compared to kinesis data firehose<br>C, not most cost-effective solution since it's data that's not actively being queried or analyzed after 14 days<br>D, designed for messaging rather than storage",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750459,
          "date": "Tue 20 Dec 2022 04:49",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is A: Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.<br><br>Amazon Kinesis Data Firehose is a fully managed service that makes it easy to load streaming data into data stores and analytics tools. It can continuously capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling real-time analytics with existing business intelligence tools and dashboards you're already using.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To meet the requirements of the company, you can create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts generated by the edge devices. You can then configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. This will provide a highly available solution that does not require the company to manage additional infrastructure.<br><br>To keep 14 days of data available for immediate analysis and archive any data older than 14 days, you can set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days. This will allow the company to store the data for long-term retention at a lower cost than storing it in S3.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750460,
          "date": "Tue 20 Dec 2022 04:49",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of the company, you can create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts generated by the edge devices. You can then configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. This will provide a highly available solution that does not require the company to manage additional infrastructure.<br><br>To keep 14 days of data available for immediate analysis and archive any data older than 14 days, you can set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days. This will allow the company to store the data for long-term retention at a lower cost than storing it in S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749985,
          "date": "Mon 19 Dec 2022 16:30",
          "username": "\t\t\t\tMorinator\t\t\t",
          "content": "A of course",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749493,
          "date": "Mon 19 Dec 2022 06:45",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 742494,
          "date": "Mon 12 Dec 2022 08:07",
          "username": "\t\t\t\tunbornfroyo\t\t\t",
          "content": "D as B is client-side encryption",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 732159,
          "date": "Thu 01 Dec 2022 02:39",
          "username": "\t\t\t\tDasCert\t\t\t",
          "content": "If we can't move data from standard s3 to glacier before 30 days, as described here:<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html<br>Then A is wrong.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723526,
          "date": "Mon 21 Nov 2022 14:11",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695818,
          "date": "Sun 16 Oct 2022 03:04",
          "username": "\t\t\t\tIncognito013\t\t\t",
          "content": "A<br><br>Stroring the data in S3 and assign a policy to transfer the data to Glacier after 14 days",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695494,
          "date": "Sat 15 Oct 2022 17:15",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "In most of the questions, first check the answers that are feasible and then check for the Well-Architected pillar emphasis in the question and hints pointing to it in solving Qs of SAA<br>D: SQS vs Kinesis <br>Both do support retention period 14 days , max record size [256KB and 1MB ]and <br> Total Data produced is 1TB/day <br><br>In Question there is \\\"store the alerts for future analysis\\\" \\\"highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure\\\"\\\"MOST operationally efficient solution \\\" <br>No requirement for real time and ordered processing. Also need for LEAST OPERATIONAL head. In Case of Kinesis one has to be watchful of shards capacity so no scope for Autoscaling like SQS and Cost Basis. No need for multi-consumers only one place to store S3.SQS- fully serverless<br>So I think its SQS . Incase there are even multi-consumers still consider SQS-SNS model.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What of high availability which translate to almost real time availability</li><li>D will significantly increase operational overhead.</li><li>Looks like a lot of contributors are forgetting that one cannot transition S3 objects that are less than 30 days old.<br>D is most appropriate.</li><li>Yes you can, <br>Minimum Days for Transition from S3 Standard or S3 Standard-IA to S3 Standard-IA or S3 One Zone-IA No where did they mention S3 glacier flexible or deep archive.<br>Using S3 Lifecycle configuration, you can transition objects to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes for archiving. When you choose the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3. You cannot access them directly through the separate Amazon S3 Glacier service<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html</li><li>You can create S3 =&gt; Buckets =&gt; Management =&gt; “Create Lifecycle Rule” to move objects to diff S3 class with any days you want to !!! &amp; I tested working.<br>So A is 100%Correct !!!</li><li>Nope, can't get immediate access to any data you want with SQS. Additionally, if you do somehow you have to stop calling the delete message API call for 14 hours, and then...</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752057,
          "date": "Wed 21 Dec 2022 10:16",
          "username": "\t\t\t\tQueTeddyJR\t\t\t",
          "content": "What of high availability which translate to almost real time availability",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741141,
          "date": "Sat 10 Dec 2022 18:32",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "D will significantly increase operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728875,
          "date": "Mon 28 Nov 2022 09:19",
          "username": "\t\t\t\tHarry_New\t\t\t",
          "content": "Looks like a lot of contributors are forgetting that one cannot transition S3 objects that are less than 30 days old.<br>D is most appropriate.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes you can, <br>Minimum Days for Transition from S3 Standard or S3 Standard-IA to S3 Standard-IA or S3 One Zone-IA No where did they mention S3 glacier flexible or deep archive.<br>Using S3 Lifecycle configuration, you can transition objects to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes for archiving. When you choose the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3. You cannot access them directly through the separate Amazon S3 Glacier service<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html</li><li>You can create S3 =&gt; Buckets =&gt; Management =&gt; “Create Lifecycle Rule” to move objects to diff S3 class with any days you want to !!! &amp; I tested working.<br>So A is 100%Correct !!!</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 744432,
          "date": "Tue 13 Dec 2022 21:02",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "Yes you can, <br>Minimum Days for Transition from S3 Standard or S3 Standard-IA to S3 Standard-IA or S3 One Zone-IA No where did they mention S3 glacier flexible or deep archive.<br>Using S3 Lifecycle configuration, you can transition objects to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes for archiving. When you choose the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3. You cannot access them directly through the separate Amazon S3 Glacier service<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737291,
          "date": "Wed 07 Dec 2022 00:49",
          "username": "\t\t\t\tJiang_aws1\t\t\t",
          "content": "You can create S3 => Buckets => Management => “Create Lifecycle Rule” to move objects to diff S3 class with any days you want to !!! & I tested working.<br>So A is 100%Correct !!!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 698321,
          "date": "Tue 18 Oct 2022 17:28",
          "username": "\t\t\t\tyd_h\t\t\t",
          "content": "Nope, can't get immediate access to any data you want with SQS. Additionally, if you do somehow you have to stop calling the delete message API call for 14 hours, and then...",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#41",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#41",
          "answers": [
            {
              "choice": "<p>A. Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703180,
          "date": "Mon 24 Oct 2022 17:59",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "This question just screams AppFlow (SaaS integration)<br>https://aws.amazon.com/appflow/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>configuring Auto-Scaling also takes time when compared to AppFlow,<br>in AWS's words \\\"in just a few clicks\\\"<br>&gt; Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks</li></ul>",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 703181,
          "date": "Mon 24 Oct 2022 18:02",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "configuring Auto-Scaling also takes time when compared to AppFlow,<br>in AWS's words \\\"in just a few clicks\\\"<br>> Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 783752,
          "date": "Sat 21 Jan 2023 22:47",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "This solution allows the EC2 instances to scale out as needed to handle the data processing and uploading, which will improve performance. Additionally, by configuring an S3 event notification to send a notification to an SNS topic when the upload is complete, the company can still receive the necessary notifications, but it eliminates the need for the same EC2 instance that is processing and uploading the data to also send the notifications, which further improves performance. This solution has less operational overhead as it only requires configuring S3 event notifications, SNS topic and AutoScaling group.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 768163,
          "date": "Sat 07 Jan 2023 01:50",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Amazon AppFlow is a fully managed integration service that enables the secure and easy transfer of data between popular software-as-a-service (SaaS) applications and AWS services. By using AppFlow, the company can easily set up integrations between SaaS sources and the S3 bucket, and the service will automatically handle the data transfer and transformation. The S3 event notification can then be used to send a notification to the user when the upload is complete, without the need to manage additional infrastructure or code. This solution would provide the required performance improvement and require minimal management, making it the most operationally efficient choice.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 764659,
          "date": "Tue 03 Jan 2023 14:05",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Appflow only",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759070,
          "date": "Tue 27 Dec 2022 22:17",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements with the least operational overhead, the company could consider the following solution:<br><br>Option B.  Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.<br><br>Amazon AppFlow is a fully managed service that enables you to easily and securely transfer data between your SaaS applications and Amazon S3. By creating an AppFlow flow to transfer the data between the SaaS sources and the S3 bucket, the company can improve the performance of the application by offloading the data transfer process to a managed service.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***INCORRECT ANSWERS***<br><br>Option A is incorrect because creating an Auto Scaling group and configuring an S3 event notification does not address the root cause of the slow application performance, which is related to the data transfer process.<br><br>Option C is incorrect because creating multiple EventBridge (CloudWatch Events) rules and configuring them to send events to an SNS topic is more complex and involves additional operational overhead.<br><br>Option D is incorrect because creating a Docker container and hosting it on ECS does not address the root cause of the slow application performance, which is related to the data transfer process.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759071,
          "date": "Tue 27 Dec 2022 22:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***INCORRECT ANSWERS***<br><br>Option A is incorrect because creating an Auto Scaling group and configuring an S3 event notification does not address the root cause of the slow application performance, which is related to the data transfer process.<br><br>Option C is incorrect because creating multiple EventBridge (CloudWatch Events) rules and configuring them to send events to an SNS topic is more complex and involves additional operational overhead.<br><br>Option D is incorrect because creating a Docker container and hosting it on ECS does not address the root cause of the slow application performance, which is related to the data transfer process.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 751358,
          "date": "Tue 20 Dec 2022 19:56",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "B, AppFlow is a fuly managed integration service that automatically handles data transfer and transformation, so it's the one that requires the least opp overhead",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749496,
          "date": "Mon 19 Dec 2022 06:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B.  App Flow usecase",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747538,
          "date": "Fri 16 Dec 2022 20:43",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "AppFlow = managed service SAAS",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747537,
          "date": "Fri 16 Dec 2022 20:42",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "AppFlow= managed service SAAS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723539,
          "date": "Mon 21 Nov 2022 14:26",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722413,
          "date": "Sun 20 Nov 2022 06:47",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Choosing B as it sounds simpler.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 710205,
          "date": "Thu 03 Nov 2022 04:02",
          "username": "\t\t\t\tpeneloco\t\t\t",
          "content": "AppFlow is made for SaaS",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 709367,
          "date": "Tue 01 Nov 2022 19:13",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "AppFlow , managed service SAAS-->Least effort",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 702831,
          "date": "Mon 24 Oct 2022 10:27",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "Appflow works very well with SaaS platforms, makes a lot more sense in this scenario. Using an ASG might improve the performance, but here it asks for THE BEST PERFORMANCE, hence ASG might not fix the underlying issue in an efficient manner.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701877,
          "date": "Sun 23 Oct 2022 05:09",
          "username": "\t\t\t\tdave9994\t\t\t",
          "content": "A is the answer, as it is the LEAST ops. overhead as asked. Minimal changes on current system.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698328,
          "date": "Tue 18 Oct 2022 17:46",
          "username": "\t\t\t\tyd_h\t\t\t",
          "content": "Amazon AppFlow is a bi-directional data transfer service; however, not all source-destination combinations are currently supported. The question does not imply any SaaS providers. It could beany SaaS provider (https://docs.aws.amazon.com/appflow/latest/userguide/requirements.html)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I will go with A.  LEAST operational overhead to add an ASG to the existing ec2 instances let S3 handle the notification part.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698330,
          "date": "Tue 18 Oct 2022 17:48",
          "username": "\t\t\t\tyd_h\t\t\t",
          "content": "I will go with A.  LEAST operational overhead to add an ASG to the existing ec2 instances let S3 handle the notification part.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 697038,
          "date": "Mon 17 Oct 2022 09:16",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a few clicks.<br>https://aws.amazon.com/appflow/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#42",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC.  The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges.<br>What is the MOST cost-effective way for the company to avoid Regional data transfer charges?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#42",
          "answers": [
            {
              "choice": "<p>A. Launch the NAT gateway in each Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Replace the NAT gateway with a NAT instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy a gateway VPC endpoint for Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision an EC2 Dedicated Host to run the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 768164,
          "date": "Sat 07 Jan 2023 01:54",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Deploying a gateway VPC endpoint for Amazon S3 is the most cost-effective way for the company to avoid Regional data transfer charges. A gateway VPC endpoint is a network gateway that allows communication between instances in a VPC and a service, such as Amazon S3, without requiring an Internet gateway or a NAT device. Data transfer between the VPC and the service through a gateway VPC endpoint is free of charge, while data transfer between the VPC and the Internet through an Internet gateway or NAT device is subject to data transfer charges. By using a gateway VPC endpoint, the company can reduce its data transfer costs by eliminating the need to transfer data through the NAT gateway to access Amazon S3. This option would provide the required connectivity to Amazon S3 and minimize data transfer charges.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 797342,
          "date": "Fri 03 Feb 2023 21:10",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "C - gateway VPC endpoint.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 761780,
          "date": "Fri 30 Dec 2022 09:43",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "'Regional' data transfer isn't clear but I think we have to assume this means the traffic stays in the region.<br>The two options that seem possible are NAT gateway per AZ vs privatelink gateway endpoints per AZ.<br>privatelink/endpoints do have costs (url below)<br>privatelink endpoint / LB costs look lower than NAT gateway costs<br>privatelink doesn't incur inter-AZ data transfer charges (if in the same region) as NAT gateways do which goes towards the key requirement stated<br><br>good writeup here : https://www.vantage.sh/blog/nat-gateway-vpc-endpoint-savings<br><br>https://aws.amazon.com/privatelink/pricing/<br>https://aws.amazon.com/vpc/pricing/<br>https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gateway-transfer-costs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751381,
          "date": "Tue 20 Dec 2022 20:17",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "C, privately connects vpc to aws services via privatelink. Doesn't require nat gateway, vpn or direct connect. Data doesn't leave amazon network so there are no data transfer charges<br>A, used to enable instances in private subnets to connect to internet or aws services, data transfered is charged <br>B, similar to nat gateway<br>D, not related to data transfer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750469,
          "date": "Tue 20 Dec 2022 04:58",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C (correct). Deploy a gateway VPC endpoint for Amazon S3.<br><br>A VPC endpoint for Amazon S3 allows you to access Amazon S3 resources within your VPC without using the Internet or a NAT gateway. This means that data transfer between your EC2 instances and S3 will not incur Regional data transfer charges.<br><br>Option A (wrong), launching a NAT gateway in each Availability Zone, would not avoid data transfer charges because the NAT gateway would still be used to access S3. <br><br>Option B (wrong), replacing the NAT gateway with a NAT instance, would also not avoid data transfer charges as it would still require using the Internet or a NAT gateway to access S3. <br><br>Option D (wrong), provisioning an EC2 Dedicated Host, would not affect data transfer charges as it only pertains to the physical host that the EC2 instances are running on and not the data transfer charges for accessing.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749997,
          "date": "Mon 19 Dec 2022 16:43",
          "username": "\t\t\t\tMorinator\t\t\t",
          "content": "VPC endpoint",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749497,
          "date": "Mon 19 Dec 2022 06:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734650,
          "date": "Sat 03 Dec 2022 20:15",
          "username": "\t\t\t\tshyam_yadav\t\t\t",
          "content": "Option is C bcz Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC.  Gateway endpoints do not enable AWS PrivateLink. There is no additional charge for using gateway endpoints",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 725169,
          "date": "Wed 23 Nov 2022 14:39",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "C is correct <br>https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723540,
          "date": "Mon 21 Nov 2022 14:27",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C isCorrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715785,
          "date": "Fri 11 Nov 2022 07:19",
          "username": "\t\t\t\tjustsaysid\t\t\t",
          "content": "This link clearly states that \\\"VPC gateway endpoints allow communication to Amazon S3 and Amazon DynamoDB without incurring data transfer charges within the same Region\\\". On the other hand NAT gateway incurs additional data processing charges. Hence, C is the correct answer.<br>https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 712968,
          "date": "Mon 07 Nov 2022 10:39",
          "username": "\t\t\t\tdduque10\t\t\t",
          "content": "Why not A?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>using the NAT gateway you will be charge for data transfer out. When VPC gateway endpoint in place for S3, the service will use internal route inside AWS to send data to S3 -&gt; no charge at all.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 727901,
          "date": "Sun 27 Nov 2022 03:51",
          "username": "\t\t\t\tTuLe\t\t\t",
          "content": "using the NAT gateway you will be charge for data transfer out. When VPC gateway endpoint in place for S3, the service will use internal route inside AWS to send data to S3 -> no charge at all.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 705521,
          "date": "Thu 27 Oct 2022 14:08",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "C is the answer",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 697785,
          "date": "Tue 18 Oct 2022 03:33",
          "username": "\t\t\t\tJahangeer_17\t\t\t",
          "content": "If we deploy VPC Gateway Endpoint then data will be transfer through AWS network only.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Though will it not incur regional data transfer cost ? Here the question is to avoid regional data transfer costs</li><li>Here it also says \\\"The company is concerned about data transfer charges\\\". They just want to reduce costs hence it is C. </li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 713421,
          "date": "Tue 08 Nov 2022 03:10",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "Though will it not incur regional data transfer cost ? Here the question is to avoid regional data transfer costs<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Here it also says \\\"The company is concerned about data transfer charges\\\". They just want to reduce costs hence it is C. </li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722415,
          "date": "Sun 20 Nov 2022 06:52",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Here it also says \\\"The company is concerned about data transfer charges\\\". They just want to reduce costs hence it is C. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 695144,
          "date": "Sat 15 Oct 2022 04:57",
          "username": "\t\t\t\tRachness\t\t\t",
          "content": "Gateway Endpoint",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692490,
          "date": "Wed 12 Oct 2022 01:31",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is C",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#43",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#43",
          "answers": [
            {
              "choice": "<p>A. Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694056,
          "date": "Thu 13 Oct 2022 17:44",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "A: VPN also goes through the internet and uses the bandwidth<br>C: daily Snowball transfer is not really a long-term solution when it comes to cost and efficiency<br>D: S3 limits don't change anything here<br><br>So the answer is B",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750478,
          "date": "Tue 20 Dec 2022 05:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B (correct). Establish a new AWS Direct Connect connection and direct backup traffic through this new connection.<br><br>AWS Direct Connect is a network service that allows you to establish a dedicated network connection from your on-premises data center to AWS. This connection bypasses the public Internet and can provide more reliable, lower-latency communication between your on-premises application and Amazon S3. By directing backup traffic through the AWS Direct Connect connection, you can minimize the impact on your internet bandwidth and ensure timely backups to S3.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A (wrong), establishing AWS VPN connections and proxying all traffic through a VPC gateway endpoint, would not necessarily minimize the impact on internet bandwidth as it would still utilize the public Internet to access S3. <br><br>Option C (wrong), using AWS Snowball devices, would not address the issue of internet bandwidth limitations as the data would still need to be transferred over the Internet to and from the Snowball devices. <br><br>Option D (wrong), submitting a support ticket to request the removal of S3 service limits, would not address the issue of internet bandwidth limitations and would not ensure timely backups to S3.</li><li>Option C is wrong so is your reason. you do not need internet to load data into Snowball Devices. if you are using snow cone for example, u will connect it to your on-premises device directly for loading and Aws will load it in the cloud. However, it not effective to do that everyday , hence option B is the better choice.</li><li>You're right Option B is the correct answer. I answered Option B as the correct answer above.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750479,
          "date": "Tue 20 Dec 2022 05:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A (wrong), establishing AWS VPN connections and proxying all traffic through a VPC gateway endpoint, would not necessarily minimize the impact on internet bandwidth as it would still utilize the public Internet to access S3. <br><br>Option C (wrong), using AWS Snowball devices, would not address the issue of internet bandwidth limitations as the data would still need to be transferred over the Internet to and from the Snowball devices. <br><br>Option D (wrong), submitting a support ticket to request the removal of S3 service limits, would not address the issue of internet bandwidth limitations and would not ensure timely backups to S3.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C is wrong so is your reason. you do not need internet to load data into Snowball Devices. if you are using snow cone for example, u will connect it to your on-premises device directly for loading and Aws will load it in the cloud. However, it not effective to do that everyday , hence option B is the better choice.</li><li>You're right Option B is the correct answer. I answered Option B as the correct answer above.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 818805,
          "date": "Thu 23 Feb 2023 05:51",
          "username": "\t\t\t\tBofi\t\t\t",
          "content": "Option C is wrong so is your reason. you do not need internet to load data into Snowball Devices. if you are using snow cone for example, u will connect it to your on-premises device directly for loading and Aws will load it in the cloud. However, it not effective to do that everyday , hence option B is the better choice.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're right Option B is the correct answer. I answered Option B as the correct answer above.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823280,
          "date": "Mon 27 Feb 2023 07:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "You're right Option B is the correct answer. I answered Option B as the correct answer above.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768166,
          "date": "Sat 07 Jan 2023 01:59",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Establishing a new AWS Direct Connect connection and directing backup traffic through this new connection would meet these requirements. AWS Direct Connect is a network service that provides dedicated network connections from on-premises data centers to AWS. It allows the company to bypass the public Internet and establish a direct connection to AWS, providing a more reliable and lower-latency connection for data transfer. By directing backup traffic through the Direct Connect connection, the company can reduce the impact on internet connectivity for internal users and improve the speed of backups to Amazon S3. This solution would provide a long-term solution for timely backups with minimal impact on internet connectivity.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 767346,
          "date": "Fri 06 Jan 2023 07:26",
          "username": "\t\t\t\tthensanity\t\t\t",
          "content": "Only B and C are the correct choices here, and C is more costly than B, so B is the correct answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 752107,
          "date": "Wed 21 Dec 2022 10:52",
          "username": "\t\t\t\tQueTeddyJR\t\t\t",
          "content": "I thought Direct Connect was or is used to connect directly to AWS from on premise machines and USERs are mentioned which means they might have users which are not on premise and need connecions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751410,
          "date": "Tue 20 Dec 2022 20:48",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "B, low-latency, dedicated network connections bw on-premises data center and AWS cloud. Directing backup traffic through direct connect would increase bandwidth and lower latency.<br>A, doesn't specifically address the needs of the backup traffic.<br>C, useful for transfering large amounts of data in short periods of time, not for ongoing backups<br>D, doesn't directly address the bandwidth contraints",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749500,
          "date": "Mon 19 Dec 2022 06:53",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723542,
          "date": "Mon 21 Nov 2022 14:28",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705525,
          "date": "Thu 27 Oct 2022 14:11",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "B is the answer",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 695507,
          "date": "Sat 15 Oct 2022 17:46",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "AWS Direct Connect and AWS Snowball Edge are primarily classified as \\\"Cloud Dedicated Network Connection\\\" and \\\"Data Transfer\\\" tools respectively.<br><br>Even if we say it takes 1/5th of cost for transfer of 250 TB data from on-prem to AWS in a week.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Direct Connect vs Snowball</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695509,
          "date": "Sat 15 Oct 2022 17:47",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Direct Connect vs Snowball",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 694858,
          "date": "Fri 14 Oct 2022 16:46",
          "username": "\t\t\t\toxfordcommaa\t\t\t",
          "content": "B.  <br>The keyword here is long term solution.<br>Direct connect is a dedicated connection between on-prem and AWS, this is the way to ensure stable network connectivity that will not wax and wane like internet connectivity.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 692491,
          "date": "Wed 12 Oct 2022 01:31",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#44",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion.<br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#44",
          "answers": [
            {
              "choice": "<p>A. Enable versioning on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable MFA Delete on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a bucket policy on the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Enable default encryption on the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a lifecycle policy for the objects in the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 690838,
          "date": "Mon 10 Oct 2022 09:49",
          "username": "\t\t\t\tUhrien\t\t\t",
          "content": "The correct solution is AB, as you can see here:<br><br>https://aws.amazon.com/it/premiumsupport/knowledge-center/s3-audit-deleted-missing-objects/<br><br>It states the following:<br><br>To prevent or mitigate future accidental deletions, consider the following features:<br><br>Enable versioning to keep historical versions of an object.<br>Enable Cross-Region Replication of objects.<br>Enable MFA delete to require multi-factor authentication (MFA) when deleting an object version.",
          "upvote_count": "33",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 827404,
          "date": "Thu 02 Mar 2023 21:30",
          "username": "\t\t\t\tGalileoEC2\t\t\t",
          "content": "There no need to add default S3 encryption this is alrady enabled<br>Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 are automatically encrypted at no additional cost and with no impact on performance. The automatic encryption status for S3 bucket default encryption configuration and for new object uploads is available in AWS CloudTrail logs, S3 Inventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API response header in the AWS Command Line Interface and AWS SDKs",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 822152,
          "date": "Sun 26 Feb 2023 09:05",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "A & B together solve this problem",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 768167,
          "date": "Sat 07 Jan 2023 02:00",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Enabling versioning on the S3 bucket and enabling MFA Delete on the S3 bucket will help protect the data from accidental deletion.<br><br>Versioning allows the company to store multiple versions of an object in the same bucket. When versioning is enabled, S3 automatically archives all versions of an object (including all writes and deletes) in the bucket. This means that if an object is accidentally deleted, it can be recovered by restoring an earlier version of the object.<br><br>MFA Delete adds an extra layer of protection by requiring users to provide additional authentication (through an MFA device) before they can permanently delete an object version. This helps prevent accidental or malicious deletion of objects by requiring users to confirm their intent to delete.<br><br>By using both versioning and MFA Delete, the company can protect the data in the S3 bucket from accidental deletion and provide a way to recover deleted objects if necessary.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 759366,
          "date": "Wed 28 Dec 2022 05:57",
          "username": "\t\t\t\tetikalas\t\t\t",
          "content": "As per white paper- \\\"versioning\\\" is one of the answer <br> https://d0.awsstatic.com/whitepapers/protecting-s3-against-object-deletion.pdf",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751420,
          "date": "Tue 20 Dec 2022 21:01",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "A, versioning is a way to protect buckets from accidental deletions<br>B, MFA is a way to protect bucket from accidental deletions",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 750489,
          "date": "Tue 20 Dec 2022 05:19",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***A.  Enable versioning on the S3 bucket.B.  Enable MFA Delete on the S3 bucket.<br><br>Enabling versioning on an S3 bucket allows you to store multiple versions of an object in the same bucket. This means that you can recover an object that was accidentally deleted or overwritten. When versioning is enabled, deleted objects are not permanently deleted, but are instead marked as deleted and stored as a new version of the object. <br><br>Enabling MFA (Multi-Factor Authentication) Delete on an S3 bucket adds an additional layer of security by requiring that you provide a valid MFA code before permanently deleting an object version. This can help prevent the accidental deletion of objects in the bucket.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option C, creating a bucket policy, would not directly protect the data from accidental deletion. <br>Option D, enabling default encryption, would help protect the data from unauthorized access but would not prevent accidental deletion. <br>Option E, creating a lifecycle policy, can be used to automate the deletion of objects based on specified criteria, but would not prevent accidental deletion in this case.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 750490,
          "date": "Tue 20 Dec 2022 05:19",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option C, creating a bucket policy, would not directly protect the data from accidental deletion. <br>Option D, enabling default encryption, would help protect the data from unauthorized access but would not prevent accidental deletion. <br>Option E, creating a lifecycle policy, can be used to automate the deletion of objects based on specified criteria, but would not prevent accidental deletion in this case.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749501,
          "date": "Mon 19 Dec 2022 06:54",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 747539,
          "date": "Fri 16 Dec 2022 20:46",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Enable versioning on the S3 bucket. Most Voted<br>Enable MFA Delete on the S3 bucket",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 746503,
          "date": "Thu 15 Dec 2022 22:08",
          "username": "\t\t\t\tNiceGuy1169\t\t\t",
          "content": "I would accept D if they would have mentioned \\\"sensitive\\\" but it is not... A & B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 739365,
          "date": "Thu 08 Dec 2022 20:08",
          "username": "\t\t\t\tparku\t\t\t",
          "content": "Versioning + MFA Delete.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 738336,
          "date": "Wed 07 Dec 2022 21:57",
          "username": "\t\t\t\tVJ_For_Azure_AWS\t\t\t",
          "content": "A should not be an answer because you can delete version of files, whenever you delete file which has versions it will delete top version so basically it is allowing you to delete, you can keep deleting versions until you delete old file.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734356,
          "date": "Sat 03 Dec 2022 11:50",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "A & B, THE CORRECT RESPONSE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 732831,
          "date": "Thu 01 Dec 2022 17:25",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "A and B, 100% CORRECT",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 723544,
          "date": "Mon 21 Nov 2022 14:29",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 710093,
          "date": "Wed 02 Nov 2022 22:41",
          "username": "\t\t\t\tSolarch\t\t\t",
          "content": "AB, Versioning keeps a copy and can be retrieved. MFA ensures you have proper authorization to delete an item.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 709279,
          "date": "Tue 01 Nov 2022 17:34",
          "username": "\t\t\t\tricenguyen208\t\t\t",
          "content": "AB for sure =))))))))))))))",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#45",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a data ingestion workflow that consists of the following:<br>• An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries<br>• An AWS Lambda function to process the data and record metadata<br>The company observes that the ingestion workflow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job.<br>Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#45",
          "answers": [
            {
              "choice": "<p>A. Deploy the Lambda function in multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Increase the CPU and memory that are allocated to the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Increase provisioned throughput for the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696243,
          "date": "Sun 16 Oct 2022 14:48",
          "username": "\t\t\t\tIncognito013\t\t\t",
          "content": "A, C, D options are out, since Lambda is fully managed service which provides high availability and scalability by its own<br><br>Answers are B and E<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>There are times you do have to increase lambda memory for improved performance though. But not in this case.</li></ul>",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 837382,
          "date": "Sun 12 Mar 2023 21:45",
          "username": "\t\t\t\tOluseun\t\t\t",
          "content": "There are times you do have to increase lambda memory for improved performance though. But not in this case.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 694057,
          "date": "Thu 13 Oct 2022 17:45",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "BE so that the lambda function reads the SQS queue and nothing gets lost",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 805294,
          "date": "Sat 11 Feb 2023 15:22",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Help <br>Can SQS Queue have multiple consumers so SNS and Lambda can consume at the same time?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788966,
          "date": "Thu 26 Jan 2023 18:25",
          "username": "\t\t\t\tLonojack\t\t\t",
          "content": "How come no one's acknowledged the connection issue? Obviously we know we need SQS as a buffer for messages when the system fails. But shouldn't we consider provisioned iops to handle the the connectivity so maybe it will be less likely to lose connectivity and fail in the first place?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What does connectivity have to do with Provisioned IOPS which is supposed to enhance I/O rate?</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 798590,
          "date": "Sun 05 Feb 2023 04:36",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "What does connectivity have to do with Provisioned IOPS which is supposed to enhance I/O rate?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 768171,
          "date": "Sat 07 Jan 2023 02:04",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To ensure that the Lambda function ingests all data in the future, the solutions architect can create an Amazon Simple Queue Service (Amazon SQS) queue and subscribe it to the SNS topic. This will allow the data notifications to be queued in the event of a network connectivity issue, rather than being lost. The solutions architect can then modify the Lambda function to read from the SQS queue, rather than from the SNS topic directly. This will allow the Lambda function to process any queued data as soon as the network connectivity issue is resolved, without the need for manual intervention.<br><br>By using an SQS queue as a buffer between the SNS topic and the Lambda function, the company can improve the reliability and resilience of the ingestion workflow. This approach will help ensure that the Lambda function ingests all data in the future, even when there are network connectivity issues.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 751580,
          "date": "Tue 20 Dec 2022 23:03",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "B and E, allow the data to be queued up in the event of a failure, rather than being lost, thenby reading from the queue, the Lambda function will be able to process the data<br>A,improves reliability but doesn't ensure all data is ingested<br>C and D, they improve performance but not ensure all data is ingested",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 750492,
          "date": "Tue 20 Dec 2022 05:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***B.  Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.E.  Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.<br><br>An Amazon Simple Queue Service (SQS) queue can be used to decouple the data ingestion workflow and provide a buffer for data deliveries. By subscribing the SQS queue to the SNS topic, you can ensure that notifications about new data deliveries are sent to the queue even if the Lambda function is unavailable or experiencing connectivity issues. When the Lambda function is ready to process the data, it can read from the SQS queue and process the data in the order in which it was received.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A, deploying the Lambda function in multiple Availability Zones, would not directly address the issue of connectivity failures. <br>Option C, increasing the CPU and memory that are allocated to the Lambda function, would not directly address the issue of connectivity failures. Option D, increasing provisioned throughput for the Lambda function, would not directly address the issue of connectivity failures.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 750493,
          "date": "Tue 20 Dec 2022 05:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, deploying the Lambda function in multiple Availability Zones, would not directly address the issue of connectivity failures. <br>Option C, increasing the CPU and memory that are allocated to the Lambda function, would not directly address the issue of connectivity failures. Option D, increasing provisioned throughput for the Lambda function, would not directly address the issue of connectivity failures.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749502,
          "date": "Mon 19 Dec 2022 06:56",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 723546,
          "date": "Mon 21 Nov 2022 14:31",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B and E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703186,
          "date": "Mon 24 Oct 2022 18:08",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "B andE is the obvious answer here,<br>SQS ensures that message does not get lost",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 700301,
          "date": "Thu 20 Oct 2022 23:05",
          "username": "\t\t\t\tD2w\t\t\t",
          "content": "Why not AB<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>lambda is serverless, it does not need to be multi-AZ..</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 703185,
          "date": "Mon 24 Oct 2022 18:06",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "lambda is serverless, it does not need to be multi-AZ..",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#46",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size.<br>Recently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included. The company wants administrators to be alerted if PII is shared again. The company also wants to automate remediation.<br>What should a solutions architect do to meet these requirements with the LEAST development effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#46",
          "answers": [
            {
              "choice": "<p>A. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain PII.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 716972,
          "date": "Sun 13 Nov 2022 00:17",
          "username": "\t\t\t\tGatt\t\t\t",
          "content": "I have a problem with answer B.  The question says: \\\"automate remediation\\\". B says that you inform the administrator and he removes the data manually, that's not automating remediation. Very weird, that would mean that D is correct - but it's so much harder to implement.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Pay attention to the entire question as in What should a solutions architect do to meet these requirements with the LEAST development effort? That is why Macie is used. Answer is B</li><li>\\\"The company wants administrators to be alerted \\\" the accessory follows the principal<br>the principale here is=&gt; wants administrators to be alerted</li><li>By \\\"automate remediation\\\", I thought it meant to use Amazon Macie to automate discovery onpersonally identifiable information.<br>https://aws.amazon.com/macie/<br>- Discover sensitive data across your S3 environment to increase visibility and automated remediation of data security risks.</li><li>That is correct, \\\"Automate remediation\\\" is not possible if you chose the B</li><li>what about LEAST development effort on <br> custom scanning algorithms andIf objects contain PII</li></ul>",
          "upvote_count": "16",
          "selected_answers": ""
        },
        {
          "id": 766836,
          "date": "Thu 05 Jan 2023 17:36",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Pay attention to the entire question as in What should a solutions architect do to meet these requirements with the LEAST development effort? That is why Macie is used. Answer is B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766104,
          "date": "Wed 04 Jan 2023 22:28",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": "\\\"The company wants administrators to be alerted \\\" the accessory follows the principal<br>the principale here is=> wants administrators to be alerted",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 754556,
          "date": "Fri 23 Dec 2022 22:29",
          "username": "\t\t\t\tronaldchow\t\t\t",
          "content": "By \\\"automate remediation\\\", I thought it meant to use Amazon Macie to automate discovery onpersonally identifiable information.<br>https://aws.amazon.com/macie/<br>- Discover sensitive data across your S3 environment to increase visibility and automated remediation of data security risks.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 729035,
          "date": "Mon 28 Nov 2022 12:35",
          "username": "\t\t\t\tHoraii\t\t\t",
          "content": "That is correct, \\\"Automate remediation\\\" is not possible if you chose the B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>what about LEAST development effort on <br> custom scanning algorithms andIf objects contain PII</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766105,
          "date": "Wed 04 Jan 2023 22:34",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": "what about LEAST development effort on <br> custom scanning algorithms andIf objects contain PII",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717333,
          "date": "Sun 13 Nov 2022 14:39",
          "username": "\t\t\t\tgrzeev\t\t\t",
          "content": "Amazon Macie is a data security and data privacy service that uses machine learning (ML) and pattern matching to discover and protect your sensitive data<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 717337,
          "date": "Sun 13 Nov 2022 14:44",
          "username": "\t\t\t\tgrzeev\t\t\t",
          "content": "Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 822411,
          "date": "Sun 26 Feb 2023 14:28",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "I think the question is vague....Macie will scan and detect sensitive data types including PII, so it points to B.  But the keywords automate remediation tells the Architect that he needs to do nothing when the problem is found. Then it points to D but how would a S3 Lifecycle removes PII data? The question doesn't ask about archiving or storing for a length of time.<br><br>I'm confused as to which answer is right....maybe B because Macie automates identifying of the data",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819956,
          "date": "Fri 24 Feb 2023 01:56",
          "username": "\t\t\t\thoazgazh\t\t\t",
          "content": "I asked ChatGPT:<br>I apologize for my previous response. You are correct that option B may not provide automatic remediation and would require manual intervention by administrators to remove the objects that contain PII. Therefore, option B would not be the best choice for meeting the requirement of automating remediation.<br><br>Option D would be the best choice to meet the requirement of automating remediation with the least development effort. This option involves implementing custom scanning algorithms in an AWS Lambda function and triggering the function when objects are loaded into the S3 bucket. If objects contain PII, the Lambda function can trigger an Amazon SES notification to alert the administrators and trigger an S3 Lifecycle policy to automatically remove the objects that contain PII.<br><br>Therefore, option D would be the best choice for meeting the requirement of automating remediation with the least development effort.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 800645,
          "date": "Tue 07 Feb 2023 08:16",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "B<br>I'm confused with D.  But I think S3 lifecycle policy does NOT remove an object by being triggerd by other AWS services.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 799905,
          "date": "Mon 06 Feb 2023 16:35",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "B.  The questions asks \\\"with the LEAST development effort?\\\"It does not ask for automation, so there will be some human effort involved. Now Macies job is to scan and identify PII which it then gives to a human who has to check instead of going through lets say 100GB of data now he will only get the ones that have people's information which might only be 1GB.  It simply finds the PII for you and all you have to do is make a final decision.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792539,
          "date": "Mon 30 Jan 2023 09:05",
          "username": "\t\t\t\txxenon\t\t\t",
          "content": "automate remediation -> D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 784389,
          "date": "Sun 22 Jan 2023 15:27",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "Why Amazon Macie?<br>Amazon Macie discovers sensitive data using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768181,
          "date": "Sat 07 Jan 2023 02:25",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To meet these requirements with the least development effort, the solutions architect can use an Amazon S3 bucket as a secure transfer point and use Amazon Macie to scan the objects in the bucket. Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. By using Macie, the company can quickly and easily scan the objects in the bucket for PII, without the need to develop custom scanning algorithms. If Macie detects PII in the objects, it can trigger an Amazon Simple Notification Service (Amazon SNS) notification to the administrators, alerting them to the presence of PII in the data. The administrators can then take action to remove the objects that contain PII. This solution would require minimal development effort and would provide automated remediation for PII in the data.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 765112,
          "date": "Wed 04 Jan 2023 00:37",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "I think question is way too confusing, Macie can do automatic remediation,why do you want admins to do it, just because of that choosing D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 764310,
          "date": "Tue 03 Jan 2023 09:12",
          "username": "\t\t\t\tspidy20\t\t\t",
          "content": "With Macie, we can do it with the least amount of effort.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 764206,
          "date": "Tue 03 Jan 2023 04:30",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "focus : LEAST development effort",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762616,
          "date": "Sat 31 Dec 2022 12:38",
          "username": "\t\t\t\tJohn_Zhuang\t\t\t",
          "content": "This question is about Amazon Macie: https://aws.amazon.com/macie/<br>Discover sensitive data across your S3 environment to increase visibility and automated remediation of data security risks.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762571,
          "date": "Sat 31 Dec 2022 09:49",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "D \\\"remove the meats\\\" - typo in the dump ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751677,
          "date": "Wed 21 Dec 2022 01:01",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "B, S3 can be the Secure transfer point, then use Macie to scan object in the bucket for any personally identifiable info and remove them if any, which can trigger SNS to send message to admins.<br>A, Inspector is a security assessment, but not designed for detecting PII<br>C, this involves creating custom management algorithms in a lambda functions, which would require a great amount of dev effort.<br>D, SES may be used to send a message to admins but doesn't automatically remove objects containing PII",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750494,
          "date": "Tue 20 Dec 2022 05:27",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>Option B.  Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.<br><br>Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in S3. You can use Macie to scan the objects in an S3 bucket and identify any that contain PII. If Macie finds objects that contain PII, you can configure it to trigger an Amazon Simple Notification Service (SNS) notification to alert the administrators. This solution requires minimal development effort, as it leverages the capabilities of Macie and SNS.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B doesn't meet the requirement to automate remediation.The only answer that meets both requirements is D even though it has a high development effort.<br>It's an awkward question.</li><li>You are correct that Answer B does not meet the requirement to automate remediation. Answer B only provides a notification to administrators to manually remove the objects containing PII. However, Answer D addresses both requirements by triggering an S3 Lifecycle policy to automatically remove the objects containing PII in addition to sending a notification to administrators.<br><br>Therefore, if the requirement to automate remediation is a must-have, Answer D is the correct choice, despite the higher development effort. Answer D is a more comprehensive solution that addresses both requirements, while Answer B only addresses one requirement. However, it is worth noting that Answer D will require more development effort to implement custom scanning algorithms and integrate with Amazon SES and S3 Lifecycle policies.</li><li>If both requirements are equally important, and development effort is not a constraint, then Answer D would be the correct answer because it addresses both requirements. Answer B only addresses the first requirement of alerting administrators if PII is shared again, but does not provide automation for remediation.<br><br>However, if the focus is on minimizing development effort and the priority is more on alerting administrators than automating remediation, then Answer B could be a viable option.<br><br>Ultimately, the choice between Answer B and Answer D depends on the specific requirements and priorities of the company and the project. But I stick to my answer (B).</li><li>***WRONG***<br>Option A, using Amazon Inspector to scan the objects in the bucket, would not provide the desired functionality as Amazon Inspector is designed for evaluating the security and compliance of infrastructure resources, rather than the contents of objects in an S3 bucket. <br><br>Option C, implementing custom scanning algorithms in an AWS Lambda function, would require more development effort compared to using Macie. <br>Option D, using Amazon Simple Email Service (SES) to trigger a notification, would also require more development effort compared to using SNS.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762574,
          "date": "Sat 31 Dec 2022 09:57",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "B doesn't meet the requirement to automate remediation.The only answer that meets both requirements is D even though it has a high development effort.<br>It's an awkward question.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are correct that Answer B does not meet the requirement to automate remediation. Answer B only provides a notification to administrators to manually remove the objects containing PII. However, Answer D addresses both requirements by triggering an S3 Lifecycle policy to automatically remove the objects containing PII in addition to sending a notification to administrators.<br><br>Therefore, if the requirement to automate remediation is a must-have, Answer D is the correct choice, despite the higher development effort. Answer D is a more comprehensive solution that addresses both requirements, while Answer B only addresses one requirement. However, it is worth noting that Answer D will require more development effort to implement custom scanning algorithms and integrate with Amazon SES and S3 Lifecycle policies.</li><li>If both requirements are equally important, and development effort is not a constraint, then Answer D would be the correct answer because it addresses both requirements. Answer B only addresses the first requirement of alerting administrators if PII is shared again, but does not provide automation for remediation.<br><br>However, if the focus is on minimizing development effort and the priority is more on alerting administrators than automating remediation, then Answer B could be a viable option.<br><br>Ultimately, the choice between Answer B and Answer D depends on the specific requirements and priorities of the company and the project. But I stick to my answer (B).</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823291,
          "date": "Mon 27 Feb 2023 07:41",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "You are correct that Answer B does not meet the requirement to automate remediation. Answer B only provides a notification to administrators to manually remove the objects containing PII. However, Answer D addresses both requirements by triggering an S3 Lifecycle policy to automatically remove the objects containing PII in addition to sending a notification to administrators.<br><br>Therefore, if the requirement to automate remediation is a must-have, Answer D is the correct choice, despite the higher development effort. Answer D is a more comprehensive solution that addresses both requirements, while Answer B only addresses one requirement. However, it is worth noting that Answer D will require more development effort to implement custom scanning algorithms and integrate with Amazon SES and S3 Lifecycle policies.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>If both requirements are equally important, and development effort is not a constraint, then Answer D would be the correct answer because it addresses both requirements. Answer B only addresses the first requirement of alerting administrators if PII is shared again, but does not provide automation for remediation.<br><br>However, if the focus is on minimizing development effort and the priority is more on alerting administrators than automating remediation, then Answer B could be a viable option.<br><br>Ultimately, the choice between Answer B and Answer D depends on the specific requirements and priorities of the company and the project. But I stick to my answer (B).</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823292,
          "date": "Mon 27 Feb 2023 07:41",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "If both requirements are equally important, and development effort is not a constraint, then Answer D would be the correct answer because it addresses both requirements. Answer B only addresses the first requirement of alerting administrators if PII is shared again, but does not provide automation for remediation.<br><br>However, if the focus is on minimizing development effort and the priority is more on alerting administrators than automating remediation, then Answer B could be a viable option.<br><br>Ultimately, the choice between Answer B and Answer D depends on the specific requirements and priorities of the company and the project. But I stick to my answer (B).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750495,
          "date": "Tue 20 Dec 2022 05:28",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, using Amazon Inspector to scan the objects in the bucket, would not provide the desired functionality as Amazon Inspector is designed for evaluating the security and compliance of infrastructure resources, rather than the contents of objects in an S3 bucket. <br><br>Option C, implementing custom scanning algorithms in an AWS Lambda function, would require more development effort compared to using Macie. <br>Option D, using Amazon Simple Email Service (SES) to trigger a notification, would also require more development effort compared to using SNS.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749507,
          "date": "Mon 19 Dec 2022 06:58",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#47",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week.<br>What should the company do to guarantee the EC2 capacity?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#47",
          "answers": [
            {
              "choice": "<p>A. Purchase Reserved Instances that specify the Region needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an On-Demand Capacity Reservation that specifies the Region needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Purchase Reserved Instances that specify the Region and three Availability Zones needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696245,
          "date": "Sun 16 Oct 2022 14:56",
          "username": "\t\t\t\tIncognito013\t\t\t",
          "content": "Reserved instances are for long term so on-demand will be the right choice - Answer D",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 750496,
          "date": "Tue 20 Dec 2022 05:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>Option D.  Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed.<br><br>An On-Demand Capacity Reservation is a type of Amazon EC2 reservation that enables you to create and manage reserved capacity on Amazon EC2. With an On-Demand Capacity Reservation, you can specify the Region and Availability Zones where you want to reserve capacity, and the number of EC2 instances you want to reserve. This allows you to guarantee capacity in specific Availability Zones in a specific Region.<br><br>***WRONG***<br>Option A, purchasing Reserved Instances that specify the Region needed, would not guarantee capacity in specific Availability Zones. <br>Option B, creating an On-Demand Capacity Reservation that specifies the Region needed, would not guarantee capacity in specific Availability Zones. <br>Option C, purchasing Reserved Instances that specify the Region and three Availability Zones needed, would not guarantee capacity in specific Availability Zones as Reserved Instances do not provide capacity reservations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Another reason as to why Reserved Instances aren't the solution here is that you have to commit to either a 1 year or 3 year term, not 1 week.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 780982,
          "date": "Thu 19 Jan 2023 10:53",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Another reason as to why Reserved Instances aren't the solution here is that you have to commit to either a 1 year or 3 year term, not 1 week.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799914,
          "date": "Mon 06 Feb 2023 16:42",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "D.  Reservations are used for long term. A minimum of 1 - 3 years making it cheaper. Whereas, on demand reservation is where you will always get access to CAPACITY it either be 1 week in advance or 1 month in an AZ but you pay On-Demand price meaning there is no discount.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780985,
          "date": "Thu 19 Jan 2023 10:55",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Correct answer is On-Demand Capacity Reservation: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768183,
          "date": "Sat 07 Jan 2023 02:28",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To guarantee EC2 capacity in specific Availability Zones, the company should create an On-Demand Capacity Reservation. On-Demand Capacity Reservations are a type of EC2 resource that allows the company to reserve capacity for On-Demand instances in a specific Availability Zone or set of Availability Zones. By creating an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed, the company can guarantee that it will have the EC2 capacity it needs for the upcoming event. The reservation will last for the duration of the event (1 week) and will ensure that the company has the capacity it needs to run its workloads.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751692,
          "date": "Wed 21 Dec 2022 01:13",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "D, specify the number of instances and AZs for a period of 1 weekand then use them whenever needed.<br>A and C, aren't designed to provide guaranteed capacity<br>B, doesn't guarantee that EC2 capacity will be available in the three specific AZs",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749508,
          "date": "Mon 19 Dec 2022 06:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747543,
          "date": "Fri 16 Dec 2022 20:53",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Answer D is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 732692,
          "date": "Thu 01 Dec 2022 15:07",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "Yes answer is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 727241,
          "date": "Sat 26 Nov 2022 04:36",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html#capacity-reservations-differences",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723550,
          "date": "Mon 21 Nov 2022 14:32",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721096,
          "date": "Fri 18 Nov 2022 07:14",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "Absolutely D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 719163,
          "date": "Tue 15 Nov 2022 23:22",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "D is the correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716305,
          "date": "Fri 11 Nov 2022 21:26",
          "username": "\t\t\t\tMyNameIsJulien\t\t\t",
          "content": "Ans D for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 709862,
          "date": "Wed 02 Nov 2022 15:28",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "D.  Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 695603,
          "date": "Sat 15 Oct 2022 20:23",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "Reserve instances: You will have to pay for the whole term (1 year or 3years) which is not cost effective<br>So answer is<br>D: on demand capacity region<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 695514,
          "date": "Sat 15 Oct 2022 18:00",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "on-demand Capacity reservation for a specific AZ for gamedays",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#48",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#48",
          "answers": [
            {
              "choice": "<p>A. Move the catalog to Amazon ElastiCache for Redis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy a larger EC2 instance with a larger instance store.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703192,
          "date": "Mon 24 Oct 2022 18:17",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "keyword is \\\"durable\\\" location<br>A and B is ephemeral storage <br>C takes forever so is not HA,<br>that leaves D",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692797,
          "date": "Wed 12 Oct 2022 09:07",
          "username": "\t\t\t\trajendradba\t\t\t",
          "content": "Elasticache is in Memory, EFS is for durability",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 834898,
          "date": "Fri 10 Mar 2023 12:29",
          "username": "\t\t\t\tmhmud12393\t\t\t",
          "content": "To make the catalog highly available and store it in a durable location, a solutions architect should move the catalog from the instance store to an Amazon EBS volume or an Amazon EFS file system. Option D is correct.<br><br>Option A, moving the catalog to Amazon ElastiCache for Redis, would improve performance by caching frequently accessed data, but it does not provide durability or high availability for the catalog data.<br><br>Option B, deploying a larger EC2 instance with a larger instance store, would not provide durability because data on an instance store is lost when the instance is stopped, terminated, or fails.<br><br>Option C, moving the catalog to Amazon S3 Glacier Deep Archive, would provide durability but not high availability, as it is designed for infrequent access and retrieval times of several hours.<br><br>Therefore, option D is the best solution to meet the company's requirements. Moving the catalog to an Amazon EBS volume or an Amazon EFS file system would provide durable storage and support high availability configurations.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 807801,
          "date": "Mon 13 Feb 2023 21:54",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "Amazon EFS is designed to be highly durable and highly available. https://aws.amazon.com/efs/faq/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 800695,
          "date": "Tue 07 Feb 2023 09:25",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "D.  Elastic cache is temporary, whereas EFS is regional so HA and durable.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 789341,
          "date": "Fri 27 Jan 2023 06:19",
          "username": "\t\t\t\tRkReddyViratStan\t\t\t",
          "content": "What's durable and HA here?<br><br>It must be EFS as Elastic Cache is a Ephemeral storage only.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 783020,
          "date": "Sat 21 Jan 2023 06:44",
          "username": "\t\t\t\tjainparag1\t\t\t",
          "content": "Must be A.  Not D since EFS is used for a very different purpose concurrently accessing data between a large number of Linux instances. For simple catalogue EFS will be a great waste.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768184,
          "date": "Sat 07 Jan 2023 02:30",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To make sure that the catalog is highly available and stored in a durable location, the solutions architect should move the catalog from the EC2 instance store to an Amazon Elastic File System (Amazon EFS) file system. Amazon EFS is a fully managed, elastic file storage service that is designed to scale up and down as needed, providing a durable and highly available storage solution for data that needs to be accessed concurrently from multiple Amazon EC2 instances. By moving the catalog to Amazon EFS, the company can ensure that the catalog is stored in a durable location and is highly available for access by the website.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EFS is Linux only. How can we be sure as it is not mentioned if it is Linux based?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 773904,
          "date": "Thu 12 Jan 2023 22:00",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "EFS is Linux only. How can we be sure as it is not mentioned if it is Linux based?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765114,
          "date": "Wed 04 Jan 2023 00:43",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Elastic Cache is not durable by default<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why did you vote for ElastiCache then?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 780987,
          "date": "Thu 19 Jan 2023 10:57",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Why did you vote for ElastiCache then?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763457,
          "date": "Mon 02 Jan 2023 03:16",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "The need is for cataloging. 2 Conditions: HA and Durability.<br>Choice A is correct: Redis Elastac Cache along with DynamoDB Streams are used for this purpose. Read Replicas can be provisioned for HA.  AOF persistence for every write operation by the server ensures replay and reconstruction of original dataset (High Durability). EFS is too heavy for just cataloging purpose. <br>Redis also has automatic sort algorithms for Leader Board feature.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754146,
          "date": "Fri 23 Dec 2022 12:26",
          "username": "\t\t\t\torionizzie\t\t\t",
          "content": "it cannot be other options",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751719,
          "date": "Wed 21 Dec 2022 01:44",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "D, meets high availability and durability requirement<br>A, It's an in-memory cache service, not a storage service<br>B, Doesn't meet durability requirement that S3 or EFS provide<br>C, S3 meets high availability and durability but onñy Standard, Standard IA and intelligent tiering, not Deep archive",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750498,
          "date": "Tue 20 Dec 2022 05:34",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>Option D.  Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.<br><br>An Amazon Elastic File System (EFS) is a fully managed, elastic file storage service that scales automatically to support the storage needs of your application. EFS is designed to be highly available and durable, making it a suitable storage location for data that needs to be highly available and stored in a durable location.<br><br>***WRONG***<br>Option A, moving the catalog to Amazon ElastiCache for Redis, would not provide a durable storage location for the catalog. <br>Option B, deploying a larger EC2 instance with a larger instance store, would not provide a highly available or durable storage location for the catalog. <br>Option C, moving the catalog to Amazon S3 Glacier Deep Archive, would provide a durable storage location but would not be suitable for data that needs to be highly available.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749511,
          "date": "Mon 19 Dec 2022 07:01",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744016,
          "date": "Tue 13 Dec 2022 13:51",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "D <br>EFS is a fully managed, scalable file storage service for EC2 instances. By moving the catalog to an EFS file system, the company can ensure that the catalog is highly available and that it is stored in a durable location. Moving the catalog to Amazon S3 Glacier Deep Archive would not provide the high availability that is required. so the correct answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 738040,
          "date": "Wed 07 Dec 2022 16:33",
          "username": "\t\t\t\tSujitshet\t\t\t",
          "content": "REDIS acts like a DB.  <br>HA 1 master-5 replicas, failover protection, Data Persistant",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 737437,
          "date": "Wed 07 Dec 2022 06:19",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Answer is D since EFS is durable by default",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#49",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.<br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#49",
          "answers": [
            {
              "choice": "<p>A. Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692941,
          "date": "Wed 12 Oct 2022 12:19",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I think the answer is B:<br>Users access the files randomly<br><br>S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns, independent of object size or retention period. You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, new applications, and user-generated content.<br><br>https://aws.amazon.com/fr/s3/storage-classes/intelligent-tiering/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What about if the file you have not accessed 360 days and intelligent tier moved the file to Glacier and on 364 day you want to access the file instantly ?<br><br>I think C is right choice</li><li>It says \\\"S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns\\\".<br>However, the statement says access pattern is predictable. It says there is frequent access about 1year.</li></ul>",
          "upvote_count": "24",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 825724,
          "date": "Wed 01 Mar 2023 11:45",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "What about if the file you have not accessed 360 days and intelligent tier moved the file to Glacier and on 364 day you want to access the file instantly ?<br><br>I think C is right choice",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817764,
          "date": "Wed 22 Feb 2023 12:31",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "It says \\\"S3 Intelligent-Tiering is the ideal storage class for data with unknown, changing, or unpredictable access patterns\\\".<br>However, the statement says access pattern is predictable. It says there is frequent access about 1year.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 692501,
          "date": "Wed 12 Oct 2022 01:44",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is B",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 823881,
          "date": "Mon 27 Feb 2023 16:21",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "I originally thought C but changed my mind to B. <br>Intelligent tiering will always only move object to object storage classes with milliseconf latency<br>https://aws.amazon.com/s3/storage-classes/<br>I was originally concerned a file would go to some storage class after several months but before a year to a storage class with higher latency but that is not the case.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 819788,
          "date": "Thu 23 Feb 2023 22:51",
          "username": "\t\t\t\tuser_deleted\t\t\t",
          "content": "I disagree with B, it says clearly access are less than 1-year-old as quickly as possible, use intelligent, if a data is not accessed after 3 months it will be moved to archive then you lose this requirement.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 819694,
          "date": "Thu 23 Feb 2023 21:15",
          "username": "\t\t\t\tAndrew123123\t\t\t",
          "content": "C is correct. B does not make any sense because the company want to grant users the ability to retrieve and query the files that are \\\"less than one year old as quickly as possible.\\\" Intelligent-tiering moves files that have been unassessed for 30 days to S3-IA, 90 days to S3 Glacier, and 180 days to S3 Glacier Deep Archive. This is problematic because Glacier and Glacier Deep Archive both have high retrieval times.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Edit: 90 days to Glacier Instant Access.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 820492,
          "date": "Fri 24 Feb 2023 14:08",
          "username": "\t\t\t\tAndrew123123\t\t\t",
          "content": "Edit: 90 days to Glacier Instant Access.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780997,
          "date": "Thu 19 Jan 2023 11:05",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Option B.  S3 Intelligent-Tiering seems best as files are accessed randomly in the first year. After 1 year, a delay in retrieving files is acceptable, so it makes sense to move them to Glacier Flexible Retrieval after 1 year. Archives can be restored for free from there using the bulk option. To query and retrieve files, S3 Select/Glacier Select and Athena are best suited and cheap, as you only pay for what you use.<br><br>A: Instant Retrieval is not the most cost-effective for the requirement, as requirements say a delay in retrieving files older than 1 year is acceptable.<br><br>C: Same as A - Instant Retrieval is not the most cost-effective solution for the requirements.<br><br>D: It's unnecessary to use RDS to query files when you have S3 Select, Glacier Select, Athena and Redshift Spectrum, all allowing you to query S3/Glacier, at varying levels of complexity.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 771217,
          "date": "Tue 10 Jan 2023 09:58",
          "username": "\t\t\t\tJohnnyBG\t\t\t",
          "content": "I would go against the majority and select D on this one. This is the most cost effective. Using S3 intelligent tiering is more costly and the delay to retrieve is acceptable.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768187,
          "date": "Sat 07 Jan 2023 02:36",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To meet these requirements in a cost-effective manner, the company can store individual files in Amazon S3 Intelligent-Tiering. Amazon S3 Intelligent-Tiering is a storage class that automatically moves data to the most cost-effective storage tier based on access patterns. By storing the files in Amazon S3 Intelligent-Tiering, the company can ensure that the files that are less than 1 year old are quickly and easily accessible to users, while still optimizing costs by automatically moving older files to a lower-cost storage tier. The company can use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. To query and retrieve the files, the company can use Amazon Athena to query and retrieve the files that are in Amazon S3, and S3 Glacier Select to query and retrieve the files that are in S3 Glacier.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752762,
          "date": "Wed 21 Dec 2022 22:18",
          "username": "\t\t\t\tQueTeddyJR\t\t\t",
          "content": "The answer is B because flexible retrival.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751729,
          "date": "Wed 21 Dec 2022 01:59",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "B, most cost-effective storage tier based on usage patterns (compared to the others here). Frequently accessed files within first year will remain in Standard (fast access), whereas infrequently accessed files after first year will move to Glacier Flexible retrieval tier. Lifecycle policy will automate the transition after 1 year. Athena allows you to analyze data stored in S3 with SQL, so it can be used along w Select (queries data stored in S3 glacier) to retrieve only the necessary data.<br>A, Data needs to be accessed within minutes, not for infrequent access after 1 year. <br>C, More expensive <br>D, More expensive",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750502,
          "date": "Tue 20 Dec 2022 05:44",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>Option B is the most cost-effective solution for meeting the requirements described.<br><br>In Option B, the files are stored in Amazon S3 Intelligent-Tiering, which automatically moves infrequently accessed data to a lower-cost storage tier based on usage patterns. This means that the files that are accessed frequently within the first year will be stored in the most efficient storage tier, while files that are not accessed as frequently can be moved to a lower-cost tier after 1 year.<br><br>Option B also uses S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year, which allows the company to store the files at a lower cost while still being able to retrieve them within a reasonable amount of time. The files can be queried and retrieved from S3 Glacier Flexible Retrieval using S3 Glacier Select, which allows for efficient querying of data stored in S3 Glacier.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>In comparison--- <br>Option A stores the files in S3 Glacier Instant Retrieval, which is a storage tier that is optimized for fast retrieval of data. This option may not be as cost-effective because it requires the data to be stored in a more expensive storage tier, even if it is not accessed frequently. <br>Option C stores the files in S3 Standard storage and moves them to S3 Glacier Instant Retrieval after 1 year, which is a similar approach to Option A and may not be as cost-effective. <br>Option D stores the files in S3 Standard storage and moves them to S3 Glacier Deep Archive after 1 year, which is the lowest-cost storage tier but may not meet the requirement for fast retrieval of files that are less than 1 year old.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750504,
          "date": "Tue 20 Dec 2022 05:45",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "In comparison--- <br>Option A stores the files in S3 Glacier Instant Retrieval, which is a storage tier that is optimized for fast retrieval of data. This option may not be as cost-effective because it requires the data to be stored in a more expensive storage tier, even if it is not accessed frequently. <br>Option C stores the files in S3 Standard storage and moves them to S3 Glacier Instant Retrieval after 1 year, which is a similar approach to Option A and may not be as cost-effective. <br>Option D stores the files in S3 Standard storage and moves them to S3 Glacier Deep Archive after 1 year, which is the lowest-cost storage tier but may not meet the requirement for fast retrieval of files that are less than 1 year old.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749723,
          "date": "Mon 19 Dec 2022 11:31",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "Query the data NOT metadata, so Athena with S3 intelligent tiering suits the requirement.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 740311,
          "date": "Fri 09 Dec 2022 18:49",
          "username": "\t\t\t\tAnny_Me\t\t\t",
          "content": "I think C is correct. \\\"retrieve files that are less than 1-year-old as quickly as possible\\\" hence Amazon S3 Standard is the correct one. S3 standard has 99.99% availability, and S3 Intelligent-Tiering has 99.9% availability. Details is here: https://aws.plainenglish.io/aws-s3-different-types-of-storage-types-available-in-s3-3550e0b87580<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Users access the files randomly<br>so B</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 761559,
          "date": "Fri 30 Dec 2022 00:32",
          "username": "\t\t\t\tIdriss10\t\t\t",
          "content": "Users access the files randomly<br>so B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 735980,
          "date": "Mon 05 Dec 2022 15:09",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "One point worth noting is that the question specified querying the files, not the file metadata, which makes C and D probably wrong",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 735788,
          "date": "Mon 05 Dec 2022 10:08",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "I choose C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 733325,
          "date": "Fri 02 Dec 2022 02:47",
          "username": "\t\t\t\tIncognito013\t\t\t",
          "content": "B - Keyword \\\"Random\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732878,
          "date": "Thu 01 Dec 2022 18:15",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "B is the correct and best choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#50",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#50",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Lambda function to apply the patch to all EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 716835,
          "date": "Sat 12 Nov 2022 18:31",
          "username": "\t\t\t\ttinyfoot\t\t\t",
          "content": "The primary focus of Patch Manager, a capability of AWS Systems Manager, is on installing operating systems security-related updates on managed nodes. By default, Patch Manager doesn't install all available patches, but rather a smaller set of patches focused on security. (Ref https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-how-it-works-selection.html)<br><br>Run Command allows you to automate common administrative tasks and perform one-time configuration changes at scale.(Ref https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html) <br><br>Seems like patch manager is meant for OS level patches and not 3rd party applications. And this falls under run command wheelhouse to carry out one-time configuration changes (update of 3rd part application) at scale.",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 744033,
          "date": "Tue 13 Dec 2022 14:06",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "D <br>AWS Systems Manager Run Command allows the company to run commands or scripts on multiple EC2 instances. By using Run Command, the company can quickly and easily apply the patch to all 1,000 EC2 instances to remediate the security vulnerability.<br><br>Creating an AWS Lambda function to apply the patch to all EC2 instances would not be a suitable solution, as Lambda functions are not designed to run on EC2 instances. Configuring AWS Systems Manager Patch Manager to apply the patch to all EC2 instances would not be a suitable solution, as Patch Manager is not designed to apply third-party software patches. Scheduling an AWS Systems Manager maintenance window to apply the patch to all EC2 instances would not be a suitable solution, as maintenance windows are not designed to apply patches to third-party software",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 811517,
          "date": "Fri 17 Feb 2023 06:52",
          "username": "\t\t\t\tFrankie193\t\t\t",
          "content": "D<br>System Manager Run Command giúp chạy một custom command và tải các bản vá về các EC2 instance. Đây là phương án hợp lý, phù hợp cho use case này.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>e đọc bộ dump bên mark4sure thì đáp án là B.  tí banh :')</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814838,
          "date": "Mon 20 Feb 2023 06:10",
          "username": "\t\t\t\tTtomm\t\t\t",
          "content": "e đọc bộ dump bên mark4sure thì đáp án là B.  tí banh :')",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780391,
          "date": "Wed 18 Jan 2023 21:15",
          "username": "\t\t\t\tShinobiGrappler\t\t\t",
          "content": "D = Third Party Workload. Use Run Command.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768188,
          "date": "Sat 07 Jan 2023 02:38",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To quickly apply a patch to the third-party software on all EC2 instances, the solutions architect can use AWS Systems Manager Run Command. Run Command is a feature of AWS Systems Manager that allows you to remotely and securely run shell scripts or Windows PowerShell commands on EC2 instances. By using Run Command, the solutions architect can quickly and easily apply the patch to all EC2 instances by running a custom command. This will allow the company to quickly and efficiently remediate the critical security vulnerability without the need to manually patch each instance or create a custom solution such as an AWS Lambda function or maintenance window.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765122,
          "date": "Wed 04 Jan 2023 01:21",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D quickest soluion.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763917,
          "date": "Mon 02 Jan 2023 19:48",
          "username": "\t\t\t\tzek\t\t\t",
          "content": "New answer is B : You can use Patch Manager to apply patches for both operating systems and applications",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 762584,
          "date": "Sat 31 Dec 2022 10:37",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "Could be either B or D :<br>A is not appropriate<br>B \\\"You can use Patch Manager to apply patches for both operating systems and applications.\\\"source https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html <br>As quickly as possible eliminates C<br>D is possible but B is made to deploy patches to fleets of EC2 instances.<br><br>Interesting CICD patch deployment here : https://aws.amazon.com/blogs/mt/software-patching-with-aws-systems-manager/<br>Notable quote from this URL supporting the use of Patch Manager for applications \\\"This solution provides a pathway to implement DevOps practices on monolith and legacy applications.\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"The primary focus of Patch Manager is applying patches to operating systems. However, you can also use Patch Manager to apply patches to some applications on your managed nodes.\\\"<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html<br><br>\\\"Approved patches\\\" may allow application of any patches but unclear if it's still restricted to MS applications (as Approval rules appears to be) or can be used for any applications :<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/create-baseline-console-windows.html</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 762586,
          "date": "Sat 31 Dec 2022 10:47",
          "username": "\t\t\t\tsecdaddy\t\t\t",
          "content": "\\\"The primary focus of Patch Manager is applying patches to operating systems. However, you can also use Patch Manager to apply patches to some applications on your managed nodes.\\\"<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-differences.html<br><br>\\\"Approved patches\\\" may allow application of any patches but unclear if it's still restricted to MS applications (as Approval rules appears to be) or can be used for any applications :<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/create-baseline-console-windows.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751763,
          "date": "Wed 21 Dec 2022 03:01",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "D, Use run command to run custom command to apply patch ASAP to a large number of instances<br>A, not designed to run long-performing tasks<br>B, automates process of patching instances to latest security updates, but it's timely<br>C, good choice but not the quickest one",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750505,
          "date": "Tue 20 Dec 2022 05:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The most appropriate solution to meet these requirements is Option D: Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances.<br><br>AWS Systems Manager Run Command is a feature that enables you to remotely and securely manage the configuration of your Amazon EC2 instances, on-premises servers, and virtual machines (VMs). You can use Run Command to run scripts or other common system administration tasks across large numbers of instances.<br><br>To patch the third-party software on all of the EC2 instances, you can use Run Command to run a custom command that applies the patch to all of the instances. This allows you to patch the software quickly and efficiently, without the need to manually log in to each instance and apply the patch manually.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A, creating an AWS Lambda function to apply the patch, would not be an appropriate solution because Lambda functions do not have the ability to directly access EC2 instances. <br>Option B, configuring AWS Systems Manager Patch Manager to apply the patch, would be an appropriate solution, but it may not be the quickest option because Patch Manager is designed for ongoing patch management rather than urgent patching. <br>Option C, scheduling a maintenance window to apply the patch, would also be an appropriate solution, but it may not be the quickest option because it requires scheduling and may take longer to complete than using Run Command.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750506,
          "date": "Tue 20 Dec 2022 05:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, creating an AWS Lambda function to apply the patch, would not be an appropriate solution because Lambda functions do not have the ability to directly access EC2 instances. <br>Option B, configuring AWS Systems Manager Patch Manager to apply the patch, would be an appropriate solution, but it may not be the quickest option because Patch Manager is designed for ongoing patch management rather than urgent patching. <br>Option C, scheduling a maintenance window to apply the patch, would also be an appropriate solution, but it may not be the quickest option because it requires scheduling and may take longer to complete than using Run Command.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749403,
          "date": "Mon 19 Dec 2022 04:07",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Looking at everything D is best option. 3rd Party patch may have different packing and installation procedure and may require customer script to install 3rd party patches so D is most suitable",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741166,
          "date": "Sat 10 Dec 2022 19:13",
          "username": "\t\t\t\tBaba_Eni\t\t\t",
          "content": "Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed nodes with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. <br><br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723556,
          "date": "Mon 21 Nov 2022 14:35",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721123,
          "date": "Fri 18 Nov 2022 08:01",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "Because system use third-party software, it needs custom command. D is right.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's incorrect</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 735997,
          "date": "Mon 05 Dec 2022 15:24",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "That's incorrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714353,
          "date": "Wed 09 Nov 2022 08:02",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "I was torn between B and D ad after reviewing docs, I choose D, and here is why;<br>\\\"For Linux-based operating system types that report a severity level for patches, Patch Manager uses the severity level reported by the software publisher for the update notice or individual patch. Patch Manager doesn't derive severity levels from third-party sources, such as the Common Vulnerability Scoring System (CVSS), or from metrics released by the National Vulnerability Database (NVD).\\\"<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 709896,
          "date": "Wed 02 Nov 2022 16:09",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "https://docs.aws.amazon.com/es_es/systems-manager/latest/userguide/execute-remote-commands.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 703212,
          "date": "Mon 24 Oct 2022 18:53",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with D here",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#51",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning.<br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#51",
          "answers": [
            {
              "choice": "<p>A. Configure the application to send the data to Amazon Kinesis Data Firehose.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696300,
          "date": "Sun 16 Oct 2022 16:46",
          "username": "\t\t\t\twhosawsome\t\t\t",
          "content": "You can use SES to format the report in HTML.<br>https://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 710854,
          "date": "Fri 04 Nov 2022 01:56",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "B&D are the only 2 correct options. If you are choosing option E then you missed the daily morning schedule requirement mentioned in the question which cant be achieved with S3 events for SNS. Event Bridge can used to configure scheduled events (every morning in this case). Option B fulfills the email in HTML format requirement (by SES) and D fulfills every morning schedule event requirement (by EventBridge)",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 781026,
          "date": "Thu 19 Jan 2023 11:42",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "You can't use SNS for HTML e-mails",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 779624,
          "date": "Wed 18 Jan 2023 06:22",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "https://kennbrodhagen.net/2016/01/31/how-to-return-html-from-aws-api-gateway-lambda/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 777427,
          "date": "Mon 16 Jan 2023 08:55",
          "username": "\t\t\t\tJohn_Zhuang\t\t\t",
          "content": "For anyone confused with Option E, I don't think the issue comes from the first part, i.e. using S3 notification every time in the morning. It may not be 100% right as the lambda function needs the help of EventBridge Rule to run on a schedule. But in general, the S3 notification can be triggered as the new object is uploaded by the lambda function.<br><br>The REAL problem comes from the second part of the statement, i.e. using SNS to send email. It is true that SNS can send emails, BUT it cannot be used to send HTML formatted emails as SNS could handle. <br>https://stackoverflow.com/questions/32241928/sending-html-content-in-aws-snssimple-notification-service-emails-notification",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 768190,
          "date": "Sat 07 Jan 2023 02:43",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To meet the requirements, the solutions architect can create an Amazon EventBridge (formerly known as Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data. The scheduled event can be configured to run at the desired time every morning. The Lambda function can be responsible for querying the API, formatting the data into an HTML format, and sending the report by email using Amazon Simple Email Service (Amazon SES).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 765303,
          "date": "Wed 04 Jan 2023 07:04",
          "username": "\t\t\t\tTrix786\t\t\t",
          "content": "Why is no one noticing the 'extract' key word? That's key for using Glue. Eventbridge can trigger Glue which extracts from the API and transforms the data to send it to SES.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Glue is always used for ETL processes that deal with unstructured data. When using Glue, usually the data will be sent to big data storage like Redshift. It is seldomly used for just sending email.<br>Lambda can easily get API data and do any filtering, let say some python code to extract JSON from API.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 769498,
          "date": "Sun 08 Jan 2023 15:09",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "AWS Glue is always used for ETL processes that deal with unstructured data. When using Glue, usually the data will be sent to big data storage like Redshift. It is seldomly used for just sending email.<br>Lambda can easily get API data and do any filtering, let say some python code to extract JSON from API.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 761518,
          "date": "Thu 29 Dec 2022 22:57",
          "username": "\t\t\t\tMars2k\t\t\t",
          "content": "With SNS you can't customize the body of the email message. The email delivery feature is intended to provide internal system alerts",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 752353,
          "date": "Wed 21 Dec 2022 14:26",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "D, Eventbridge = scheduled events, lambda = function that queries API for the data<br>B, SES (simple email service) = formats the data which then can be sent via email<br>A, Firehose = streaming<br>C, Glue = ETL service<br>E, S3 = SSS <br>A, C and E don't solve the problem of querying REST API for the data",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 751152,
          "date": "Tue 20 Dec 2022 16:50",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "D.  Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data.<br>B.  Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.<br><br>To meet the requirements, a solutions architect could create a scheduled event using Amazon EventBridge (formerly known as Amazon CloudWatch Events) that invokes an AWS Lambda function at a specific time every morning. The Lambda function could then query the application's API to retrieve the shipping statistics, format the data into an easy-to-read HTML format, and send the report by email using Amazon Simple Email Service (Amazon SES). This would allow the company to automate the process of retrieving and sending the shipping statistics report.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 750709,
          "date": "Tue 20 Dec 2022 10:26",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "BD is correct ANs <br>https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749410,
          "date": "Mon 19 Dec 2022 04:22",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D and E is better choice given the e-mail needs to contain a report data.<br>Can be done using integrating Lambda with SES but that will require some code to invoke SES from Lambda. SNS provides the e-mail as publishing functionality and it can even retry mechanism etc...",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 748735,
          "date": "Sun 18 Dec 2022 11:21",
          "username": "\t\t\t\tarseyam\t\t\t",
          "content": "This is a typical scenario for Extract-Transform-Load which means AWS GLUE<br><br>The below article shows how you can extract data from a web API<br>https://blog.clairvoyantsoft.com/extracting-data-from-a-web-service-via-aws-glue-570035b38988<br><br>You can start AWS Glue using AWS EventBridge<br>https://docs.aws.amazon.com/glue/latest/dg/starting-workflow-eventbridge.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 738272,
          "date": "Wed 07 Dec 2022 20:15",
          "username": "\t\t\t\tshw1981\t\t\t",
          "content": "B and D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 737299,
          "date": "Wed 07 Dec 2022 01:02",
          "username": "\t\t\t\treeba_908\t\t\t",
          "content": "D E, SES for the marketing email. DE combined will do the JOB. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723564,
          "date": "Mon 21 Nov 2022 14:53",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722510,
          "date": "Sun 20 Nov 2022 10:46",
          "username": "\t\t\t\tpepgua\t\t\t",
          "content": "We seem to agree option D is correct. The second choice is between B (SES) and E (SNS). SES is the best answer as it's specifically designed for Email service. SNS can also deliver notifications via email but it's not designed for that HTML format. BD is correct.<br>https://stackoverflow.com/questions/32241928/sending-html-content-in-aws-snssimple-notification-service-emails-notification",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#52",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires minimum operational overhead.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#52",
          "answers": [
            {
              "choice": "<p>A. Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698131,
          "date": "Tue 18 Oct 2022 12:58",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "EFS is a standard file system, it scales automatically and is highly available.",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 692956,
          "date": "Wed 12 Oct 2022 12:42",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I have absolutely no idea...<br><br>Output files that vary in size from tens of gigabytes to hundreds of terabytes<br><br>Simit size for a single object:<br>S3 5To TiB<br>https://aws.amazon.com/fr/blogs/aws/amazon-s3-object-size-limit/<br>EBS 64 Tib<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_constraints.html<br>EFS 47.9 TiB<br>https://docs.aws.amazon.com/efs/latest/ug/limits.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The answer to that is <br>Limit size for a single object:<br>S3, 5TiB is per object but you can have more than one object in a bucket, meaning infinity <br>https://aws.amazon.com/fr/blogs/aws/amazon-s3-object-size-limit/<br>EBS 64 Tib is per block of storage <br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_constraints.html<br>EFS 47.9 TiB per file and in the questions its says Files the 's'<br>https://docs.aws.amazon.com/efs/latest/ug/limits.html</li><li>None meets 100s of TB / file. Bit confusing / misleading</li><li>S3 and EBS are block storage but you are looking to store files, so EFS is the correct option.</li><li>S3 is object storage.</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 811638,
          "date": "Fri 17 Feb 2023 09:29",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "The answer to that is <br>Limit size for a single object:<br>S3, 5TiB is per object but you can have more than one object in a bucket, meaning infinity <br>https://aws.amazon.com/fr/blogs/aws/amazon-s3-object-size-limit/<br>EBS 64 Tib is per block of storage <br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/volume_constraints.html<br>EFS 47.9 TiB per file and in the questions its says Files the 's'<br>https://docs.aws.amazon.com/efs/latest/ug/limits.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742217,
          "date": "Mon 12 Dec 2022 00:48",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "None meets 100s of TB / file. Bit confusing / misleading",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 736018,
          "date": "Mon 05 Dec 2022 15:43",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "S3 and EBS are block storage but you are looking to store files, so EFS is the correct option.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>S3 is object storage.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774396,
          "date": "Fri 13 Jan 2023 12:42",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "S3 is object storage.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 806581,
          "date": "Sun 12 Feb 2023 17:50",
          "username": "\t\t\t\tharirkmusa\t\t\t",
          "content": "standard file system structure is the KEYWORD here, the S3 and EBS are not file based storage. EFS is. so the automatic answer is C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 789733,
          "date": "Fri 27 Jan 2023 16:25",
          "username": "\t\t\t\tNitiATOS\t\t\t",
          "content": "I will go with C as If the app is deployed in MultiAZ, computes are different but the Storage needs to be common. <br>EFS is easist way to configure shared storage as compared to SHARED EBS. <br> Hence C Suits the best.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 773225,
          "date": "Thu 12 Jan 2023 08:37",
          "username": "\t\t\t\tStrk18\t\t\t",
          "content": "C.  Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 768192,
          "date": "Sat 07 Jan 2023 02:49",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752852,
          "date": "Thu 22 Dec 2022 00:20",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "C = File storage system, Multi AZ ASG lets you maintain high availability<br>Not A, B or D because they don't meet the requirement of file system storage",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751161,
          "date": "Tue 20 Dec 2022 17:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.<br><br>To meet the requirements, a solution that would allow the company to migrate its on-premises application to AWS and scale automatically, be highly available, and require minimum operational overhead would be to migrate the application to Amazon Elastic Compute Cloud (Amazon EC2) instances in a Multi-AZ (Availability Zone) Auto Scaling group.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The Auto Scaling group would allow the application to automatically scale up or down based on demand, ensuring that the application has the required capacity to handle incoming requests. To store the data produced by the application, the company could use Amazon Elastic File System (Amazon EFS), which is a file storage service that allows the company to store and access file data in a standard file system structure. Amazon EFS is highly available and scales automatically to support the workload of the application, making it a good choice for storing the data produced by the application.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751162,
          "date": "Tue 20 Dec 2022 17:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The Auto Scaling group would allow the application to automatically scale up or down based on demand, ensuring that the application has the required capacity to handle incoming requests. To store the data produced by the application, the company could use Amazon Elastic File System (Amazon EFS), which is a file storage service that allows the company to store and access file data in a standard file system structure. Amazon EFS is highly available and scales automatically to support the workload of the application, making it a good choice for storing the data produced by the application.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749412,
          "date": "Mon 19 Dec 2022 04:27",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C.  Using EBS as storage is not a right option as it will not scale automatically.<br>Using ECS and EKS for running the application is not a requirement here and it is not clearly mentioned that applicationcan becontainerized or not.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 741515,
          "date": "Sun 11 Dec 2022 09:06",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "Highly available & Autoscales == Multi-AZ Auto Scaling group. <br>Standard File System == Amazon Elastic File System (Amazon EFS)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723566,
          "date": "Mon 21 Nov 2022 14:55",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715210,
          "date": "Thu 10 Nov 2022 13:23",
          "username": "\t\t\t\tpspinelli19\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/84147-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 700391,
          "date": "Fri 21 Oct 2022 01:57",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "standard file system => EFS rather than S3",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 698204,
          "date": "Tue 18 Oct 2022 14:00",
          "username": "\t\t\t\tKikiokiki\t\t\t",
          "content": "EBS doesn't offer high availability, data is stored in one AZ.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 697628,
          "date": "Mon 17 Oct 2022 20:37",
          "username": "\t\t\t\tqueen101\t\t\t",
          "content": "cCCCCCCCCCC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 694887,
          "date": "Fri 14 Oct 2022 17:53",
          "username": "\t\t\t\toxfordcommaa\t\t\t",
          "content": "chose this due to the key word \\\"standard file system\\\"",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#53",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#53",
          "answers": [
            {
              "choice": "<p>A. Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 835742,
          "date": "Sat 11 Mar 2023 09:33",
          "username": "\t\t\t\tathiha\t\t\t",
          "content": "Retention Period: A period is specified by Days & Years.<br>With Retention Compliance Mode, you can't change/adjust (even by the account root user) the retention mode during the retention period while all objects within the bucket are Locked.<br>With Retention Governance mode, a less restrictive mode, you can grant special permission to a group of users to adjust the Lock settings by using S3:BypassGovernanceRetention.<br><br>Legal Hold: It's On/Off setting on an object version. There is no retention period. If you enable Legal Hole on specific object version, you will not be able to delete or override that specific object version. It needs S:PutObjectLegalHole as a permission.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 823083,
          "date": "Mon 27 Feb 2023 01:37",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "S3 Glacier Deep Archive all day....",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 768196,
          "date": "Sat 07 Jan 2023 02:51",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754784,
          "date": "Sat 24 Dec 2022 10:19",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Use S3 Object Lock in compliance mode<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752902,
          "date": "Thu 22 Dec 2022 02:00",
          "username": "\t\t\t\tpazabal\t\t\t",
          "content": "C, A lifecycle set to transition from standard to Glacier deep archive and use lock for the delete requirement <br>A, B and D don't meet the requirements",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751165,
          "date": "Tue 20 Dec 2022 17:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.<br><br>To meet the requirements, the company could use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. S3 Glacier Deep Archive is Amazon's lowest-cost storage class, specifically designed for long-term retention of data that is accessed rarely. This would allow the company to store the records with maximum resiliency and at the lowest possible cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To ensure that the records are not deleted during the entire 10-year period, the company could use S3 Object Lock in compliance mode. S3 Object Lock allows the company to apply a retention period to objects in S3, preventing the objects from being deleted until the retention period expires. By using S3 Object Lock in compliance mode, the company can ensure that the records are not deleted by anyone, including administrative users and root users, during the entire 10-year period.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751166,
          "date": "Tue 20 Dec 2022 17:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To ensure that the records are not deleted during the entire 10-year period, the company could use S3 Object Lock in compliance mode. S3 Object Lock allows the company to apply a retention period to objects in S3, preventing the objects from being deleted until the retention period expires. By using S3 Object Lock in compliance mode, the company can ensure that the records are not deleted by anyone, including administrative users and root users, during the entire 10-year period.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749737,
          "date": "Mon 19 Dec 2022 11:50",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "A and B are ruled out as you need them to be accessible for 1 year and using control policy or IAM policies, the administrator or root still has the ability to delete them.<br>D is ruled out as it uses One Zone-IA, but requirement says max- resiliency. <br>SO- C should be the right answer.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749413,
          "date": "Mon 19 Dec 2022 04:30",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743396,
          "date": "Mon 12 Dec 2022 23:57",
          "username": "\t\t\t\tMarge_Simpson\t\t\t",
          "content": "They should've put Glacier Vault Lock into Option C to make it even more obvious",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 737496,
          "date": "Wed 07 Dec 2022 07:35",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "C is the answer that fulfill the requirements of immediate access for one year and data durability for 10 years",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723570,
          "date": "Mon 21 Nov 2022 14:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712221,
          "date": "Sun 06 Nov 2022 10:16",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "A-Wrong as the records must be immediately accessble for the first year.<br>B-The question never mentioned about the records can be deleted or modified after 10-year period.<br>D-It does not fulfill the condition of securing resiliency; you need multi-AZ to guarantee it.<br><br>Therefore, the answer is C. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 710123,
          "date": "Thu 03 Nov 2022 00:07",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "ans is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 700474,
          "date": "Fri 21 Oct 2022 04:47",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "sure for C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 697632,
          "date": "Mon 17 Oct 2022 20:41",
          "username": "\t\t\t\tqueen101\t\t\t",
          "content": "CCCCCCCCC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 695616,
          "date": "Sat 15 Oct 2022 20:50",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "This is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695546,
          "date": "Sat 15 Oct 2022 19:05",
          "username": "\t\t\t\tRachness\t\t\t",
          "content": "compliance lock cant be removed unlike governance",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#54",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#54",
          "answers": [
            {
              "choice": "<p>A. Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 768198,
          "date": "Sat 07 Jan 2023 02:52",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763486,
          "date": "Mon 02 Jan 2023 05:18",
          "username": "\t\t\t\tdan80\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/amazon-fsx-for-windows-file-server-update-new-enterprise-ready-features/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754788,
          "date": "Sat 24 Dec 2022 10:25",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "EFS is not supported on Windows instances<br>https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html<br>Amazon FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system.<br>https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751171,
          "date": "Tue 20 Dec 2022 17:14",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The best option to meet the requirements specified in the question is option D: Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.<br><br>Amazon EFS is a fully managed, elastic file storage service that scales on demand. It is designed to be highly available, durable, and secure, making it well-suited for hosting file shares. By using a Multi-AZ configuration, the file share will be automatically replicated across multiple Availability Zones, providing high availability and durability for the data.<br><br>To migrate the data, you can use a variety of tools and techniques, such as Robocopy or AWS DataSync. Once the data has been migrated to EFS, you can simply update the file share configuration on the existing EC2 instances to point to the EFS file system, and users will be able to access the files in the same way they currently do.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EFS is not support by windows.</li><li>You're 100% right Ello2023. I humbly acknowledged my first answer was WRONG. I am changing my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</li><li>Option A, migrating all the data to Amazon S3 and setting up IAM authentication for user access, would not preserve the current file share access methods and would require users to access the files in a different way. <br><br>Option B, setting up an Amazon S3 File Gateway, would not provide the high availability and durability needed for hosting file shares. <br><br>Option C, extending the file share environment to FSx for Windows File Server, would provide the desired high availability and durability, but would also require users to access the files in a different way.</li><li>EFS is for Linux only not Windows</li><li>You're right Ronald Chow. Thanks! Option D is incorrect because Amazon Elastic File System (EFS) is a file storage service that is not natively compatible with the Windows operating system, and would not preserve the existing access methods for users.<br><br>I am taking back my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 774408,
          "date": "Fri 13 Jan 2023 12:49",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "EFS is not support by windows.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're 100% right Ello2023. I humbly acknowledged my first answer was WRONG. I am changing my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 823304,
          "date": "Mon 27 Feb 2023 07:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "You're 100% right Ello2023. I humbly acknowledged my first answer was WRONG. I am changing my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 751172,
          "date": "Tue 20 Dec 2022 17:15",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, migrating all the data to Amazon S3 and setting up IAM authentication for user access, would not preserve the current file share access methods and would require users to access the files in a different way. <br><br>Option B, setting up an Amazon S3 File Gateway, would not provide the high availability and durability needed for hosting file shares. <br><br>Option C, extending the file share environment to FSx for Windows File Server, would provide the desired high availability and durability, but would also require users to access the files in a different way.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 754564,
          "date": "Fri 23 Dec 2022 22:53",
          "username": "\t\t\t\tronaldchow\t\t\t",
          "content": "EFS is for Linux only not Windows<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're right Ronald Chow. Thanks! Option D is incorrect because Amazon Elastic File System (EFS) is a file storage service that is not natively compatible with the Windows operating system, and would not preserve the existing access methods for users.<br><br>I am taking back my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759080,
          "date": "Tue 27 Dec 2022 22:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "You're right Ronald Chow. Thanks! Option D is incorrect because Amazon Elastic File System (EFS) is a file storage service that is not natively compatible with the Windows operating system, and would not preserve the existing access methods for users.<br><br>I am taking back my answer. \\\"The correct answer is Option C\\\". Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 749415,
          "date": "Mon 19 Dec 2022 04:32",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744076,
          "date": "Tue 13 Dec 2022 14:39",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "D<br>Amazon EFS is fully compatible with the SMB protocol that is used by Windows file shares, which means that users can continue to access the files in the same way they currently do. Extending the file share environment to FSx for Windows File Server with a Multi-AZ configuration would not be a suitable solution, as FSx for Windows File Server is not as scalable or cost-effective as Amazon EFS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723571,
          "date": "Mon 21 Nov 2022 14:58",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721803,
          "date": "Sat 19 Nov 2022 07:30",
          "username": "\t\t\t\tJuhith\t\t\t",
          "content": "EFS is only for Linux.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 721148,
          "date": "Fri 18 Nov 2022 08:44",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "EFS is only for Linux.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 720179,
          "date": "Thu 17 Nov 2022 04:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Windows file shares = Amazon FSx for Windows File Server<br>Hence, the correct answer is C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Taking back this answer. As explained in the latest update.<br><br>***CORRECT***<br>D: Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751175,
          "date": "Tue 20 Dec 2022 17:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Taking back this answer. As explained in the latest update.<br><br>***CORRECT***<br>D: Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 710628,
          "date": "Thu 03 Nov 2022 16:58",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "FSX---> SMB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 704319,
          "date": "Wed 26 Oct 2022 03:57",
          "username": "\t\t\t\tcark0728\t\t\t",
          "content": "C가 올바릅니다",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 696049,
          "date": "Sun 16 Oct 2022 09:08",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "FSx- Windows File Sharehttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-file-shares.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#55",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#55",
          "answers": [
            {
              "choice": "<p>A. Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a security group that denies inbound traffic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694071,
          "date": "Thu 13 Oct 2022 18:00",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "A: doesn't fully configure the traffic flow<br>B: security groups don't have deny rules<br>D: peering is mostly between VPCs, doesn't really help here<br><br>answer is C, most mainstream way",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 696061,
          "date": "Sun 16 Oct 2022 09:38",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Inside a VPC, traffic locallybetween different subnets cannot be restricted by routingbut incase they are in different VPCs then it would be possible. This is imp Gain in VPC<br>- So only method is Security Groups - like EC2 also RDS also has Security Groups to restrict traffic to database instances",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 826283,
          "date": "Wed 01 Mar 2023 22:36",
          "username": "\t\t\t\tGary_Phillips_2007\t\t\t",
          "content": "Just took the exam today and EVERY ONE of the questions came from this dump. Memorize it all. Good luck.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 768200,
          "date": "Sat 07 Jan 2023 02:55",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances. This will allow the EC2 instances in the private subnets to have access to the RDS databases while denying access to the EC2 instances in the public subnets.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751198,
          "date": "Tue 20 Dec 2022 17:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that meets the requirements described in the question is option C: Create a security group that allows inbound traffic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.<br><br>In this solution, the security group applied to the DB instances allows inbound traffic from the security group assigned to instances in the private subnets. This ensures that only EC2 instances running in the private subnets can have access to the RDS databases.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, creating a new route table that excludes the route to the public subnets' CIDR blocks and associating it with the database subnets, would not meet the requirements because it would block all traffic to the database subnets, not just traffic from the public subnets.<br><br>Option B, creating a security group that denies inbound traffic from the security group assigned to instances in the public subnets and attaching it to the DB instances, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.<br><br>Option D, creating a new peering connection between the public subnets and the private subnets and a different peering connection between the private subnets and the database subnets, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751199,
          "date": "Tue 20 Dec 2022 17:32",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, creating a new route table that excludes the route to the public subnets' CIDR blocks and associating it with the database subnets, would not meet the requirements because it would block all traffic to the database subnets, not just traffic from the public subnets.<br><br>Option B, creating a security group that denies inbound traffic from the security group assigned to instances in the public subnets and attaching it to the DB instances, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.<br><br>Option D, creating a new peering connection between the public subnets and the private subnets and a different peering connection between the private subnets and the database subnets, would not meet the requirements because it would allow all traffic from the private subnets to reach the DB instances, not just traffic from the security group assigned to instances in the private subnets.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749740,
          "date": "Mon 19 Dec 2022 11:56",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "The real trick is between B and C.  A and D are ruled out for obvious reasons.<br>B is wrong as you cannot have deny type rules in Security groups.<br>So- C is the right answer.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 726973,
          "date": "Fri 25 Nov 2022 18:07",
          "username": "\t\t\t\tashish_t\t\t\t",
          "content": "The key is \\\"Only EC2 instances that run in the private subnets can have access to the RDS databases\\\"<br>The answer is C. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723572,
          "date": "Mon 21 Nov 2022 15:00",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 710668,
          "date": "Thu 03 Nov 2022 18:03",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "Ans correct.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#56",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#56",
          "answers": [
            {
              "choice": "<p>A. Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route traffic to the API Gateway endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692984,
          "date": "Wed 12 Oct 2022 13:15",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "I think the answer is C.  we don't need to attach a certificate in us-east-1, if is not for cloudfront. In our case the target is ca-central-1.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think that is C too, the target would be the same Region.<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-regional-api-custom-domain-create.html</li></ul>",
          "upvote_count": "20",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 695590,
          "date": "Sat 15 Oct 2022 20:10",
          "username": "\t\t\t\tValero_\t\t\t",
          "content": "I think that is C too, the target would be the same Region.<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-regional-api-custom-domain-create.html",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 751214,
          "date": "Tue 20 Dec 2022 17:40",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution to meet these requirements is option C. <br><br>To design the API Gateway URL with the company's domain name and corresponding certificate, the company needs to do the following:<br><br>1. Create a Regional API Gateway endpoint: This will allow the company to create an endpoint that is specific to a region.<br><br>2. Associate the API Gateway endpoint with the company's domain name: This will allow the company to use its own domain name for the API Gateway URL.<br><br>3. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region: This will allow the company to use HTTPS for secure communication with its APIs.<br><br>4. Attach the certificate to the API Gateway endpoint: This will allow the company to use the certificate for securing the API Gateway URL.<br><br>5. Configure Route 53 to route traffic to the API Gateway endpoint: This will allow the company to use Route 53 to route traffic to the API Gateway URL using the company's domain name.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C includes all the necessary steps to meet the requirements, hence it is the correct solution. <br><br>Options A and D do not include the necessary steps to associate the API Gateway endpoint with the company's domain name and attach the certificate to the endpoint. <br><br>Option B includes the necessary steps to associate the API Gateway endpoint with the company's domain name and attach the certificate, but it imports the certificate into the us-east-1 Region instead of the ca-central-1 Region where the API Gateway is located.</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751216,
          "date": "Tue 20 Dec 2022 17:40",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C includes all the necessary steps to meet the requirements, hence it is the correct solution. <br><br>Options A and D do not include the necessary steps to associate the API Gateway endpoint with the company's domain name and attach the certificate to the endpoint. <br><br>Option B includes the necessary steps to associate the API Gateway endpoint with the company's domain name and attach the certificate, but it imports the certificate into the us-east-1 Region instead of the ca-central-1 Region where the API Gateway is located.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 834985,
          "date": "Fri 10 Mar 2023 13:46",
          "username": "\t\t\t\tgmehra\t\t\t",
          "content": "ACM is always in US east 1",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828110,
          "date": "Fri 03 Mar 2023 16:52",
          "username": "\t\t\t\tGalileoEC2\t\t\t",
          "content": "In the solution I provided, the region used for AWS Certificate Manager (ACM) is us-east-1, which is different from the ca-central-1 region used for Amazon API Gateway in the question. This is because ACM certificates can only be issued in the us-east-1 region, which is a global endpoint for ACM.<br><br>When creating a custom domain name in Amazon API Gateway and attaching an ACM certificate to it, the region of the certificate does not have to match the region of the API Gateway deployment. However, it's worth noting that there may be additional latency or costs associated with using a certificate from a different region.<br><br>In summary, the solution I provided is still valid and meets the requirements of the question, even though it uses a different region for ACM...pum!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781032,
          "date": "Thu 19 Jan 2023 11:52",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "It's C: You can use an ACM certificate in API Gateway.<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html<br><br>Certificates are regional and have to be uploaded in the same AWS Region as the service you're using it for. (If you're using a certificate with CloudFront, you have to upload it into US East (N. Virginia).)<br><br>https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750734,
          "date": "Tue 20 Dec 2022 10:50",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "Certificates in ACM are regional resources. To use a certificate with Elastic Load Balancing for the same fully qualified domain name (FQDN) or set of FQDNs in more than one AWS region, you must request or import a certificate for each region. For certificates provided by ACM, this means you must revalidate each domain name in the certificate for each region. You cannot copy a certificate between regions",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750730,
          "date": "Tue 20 Dec 2022 10:46",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "C correct ans <br>Edge-Optimized (default): For global clients<br>• Requests are routed through the CloudFront Edge locations<br>(improves latency)<br>• The API Gateway still lives in only one region<br>• The TLS Certificate must be in the same region as<br>CloudFront, in us-east-1<br>• Then setup CNAME or (better) A-Alias record in Route 53",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749421,
          "date": "Mon 19 Dec 2022 04:44",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C is the answer. As per the first line in question Route 53 already has registered DNS name forthe company so there is no additional steps needed in Route 53.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737112,
          "date": "Tue 06 Dec 2022 19:17",
          "username": "\t\t\t\tCertified101\t\t\t",
          "content": "Can't be D as an A record also can only point to IP address and not a domain name",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 726976,
          "date": "Fri 25 Nov 2022 18:17",
          "username": "\t\t\t\tashish_t\t\t\t",
          "content": "Cert should be in the same region.<br>Answer: C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 726836,
          "date": "Fri 25 Nov 2022 15:21",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "I choose D since the company wants its own domain name - should not be a regional one. Even though the answer does not mention edge-optimized custom domain name, this setup has to use it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You misunderstand the term regional. This has no impact on the domain name, but instead refers to Regional and Edge-Optimized are deployment options, see https://stackoverflow.com/questions/49826230/regional-edge-optimized-api-gateway-vs-regional-edge-optimized-custom-domain-nam</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736042,
          "date": "Mon 05 Dec 2022 16:19",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "You misunderstand the term regional. This has no impact on the domain name, but instead refers to Regional and Edge-Optimized are deployment options, see https://stackoverflow.com/questions/49826230/regional-edge-optimized-api-gateway-vs-regional-edge-optimized-custom-domain-nam",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 726834,
          "date": "Fri 25 Nov 2022 15:16",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "The only correct answer is D since the company wants to design its API Gateway URL with the company's domain name. Answer C supports only regional domain name.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723574,
          "date": "Mon 21 Nov 2022 15:01",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716439,
          "date": "Sat 12 Nov 2022 03:34",
          "username": "\t\t\t\tstudy_aws1\t\t\t",
          "content": "Will change my earlier selection to C). Reason -<br>• If using Edge-Optimized endpoint, then the certificate must be in us-east-1<br>• If using Regional endpoint, the certificate must be in the API Gateway region",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715885,
          "date": "Fri 11 Nov 2022 09:52",
          "username": "\t\t\t\tNirmal3331\t\t\t",
          "content": "Answer is C:<br><br>Regional custom domain names must use an SSL/TLS certificate that's in the same AWS Region as your API.<br><br>Edge-optimized custom domain names must use a certificate that's in the following Region: US East (N. Virginia) (us-east-1)./",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 714843,
          "date": "Thu 10 Nov 2022 02:30",
          "username": "\t\t\t\tluvincanada\t\t\t",
          "content": "The question states..company uses Amazon API Gateway in the ca-central-1 Region. Answer Dmentions region name as \\\"us-east-1\\\" Region. which does not match. Therefore C is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 710759,
          "date": "Thu 03 Nov 2022 20:40",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "same region",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#57",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#57",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694555,
          "date": "Fri 14 Oct 2022 08:51",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Good Answer is B :<br>https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html?pg=ln&sec=ft",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751221,
          "date": "Tue 20 Dec 2022 17:44",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The best solution to meet these requirements would be option B: Use Amazon Rekognition to detect inappropriate content, and use human review for low-confidence predictions.<br><br>Amazon Rekognition is a cloud-based image and video analysis service that can detect inappropriate content in images using its pre-trained label detection model. It can identify a wide range of inappropriate content, including explicit or suggestive adult content, violent content, and offensive language. The service provides high accuracy and low latency, making it a good choice for this use case.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using Amazon Comprehend, is not a good fit for this use case because Amazon Comprehend is a natural language processing service that is designed to analyze text, not images.<br><br>Option C, using Amazon SageMaker to detect inappropriate content, would require significant development effort to build and train a custom machine learning model. It would also require a large dataset of labeled images to train the model, which may be time-consuming and expensive to obtain.<br><br>Option D, using AWS Fargate to deploy a custom machine learning model, would also require significant development effort and a large dataset of labeled images. It may not be the most efficient or cost-effective solution for this use case.<br><br>In summary, the best solution is to use Amazon Rekognition to detect inappropriate content in images, and use human review for low-confidence predictions to ensure that all inappropriate content is detected.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751224,
          "date": "Tue 20 Dec 2022 17:44",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using Amazon Comprehend, is not a good fit for this use case because Amazon Comprehend is a natural language processing service that is designed to analyze text, not images.<br><br>Option C, using Amazon SageMaker to detect inappropriate content, would require significant development effort to build and train a custom machine learning model. It would also require a large dataset of labeled images to train the model, which may be time-consuming and expensive to obtain.<br><br>Option D, using AWS Fargate to deploy a custom machine learning model, would also require significant development effort and a large dataset of labeled images. It may not be the most efficient or cost-effective solution for this use case.<br><br>In summary, the best solution is to use Amazon Rekognition to detect inappropriate content in images, and use human review for low-confidence predictions to ensure that all inappropriate content is detected.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749422,
          "date": "Mon 19 Dec 2022 04:45",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744097,
          "date": "Tue 13 Dec 2022 14:55",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B<br>AWS Rekognition to detect inappropriate content and use human review for low-confidence predictions. This option minimizes development effort because Amazon Rekognition is a pre-built machine learning service that can detect inappropriate content. Using human review for low-confidence predictions allows for more accurate detection of inappropriate content.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723575,
          "date": "Mon 21 Nov 2022 15:02",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698159,
          "date": "Tue 18 Oct 2022 13:17",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option B. <br><br>https://docs.aws.amazon.com/rekognition/latest/dg/a2i-rekognition.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#58",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the critical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#58",
          "answers": [
            {
              "choice": "<p>A. Use Amazon EC2 instances, and install Docker on the instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694556,
          "date": "Fri 14 Oct 2022 08:53",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Good answer is C:<br>AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without having to manage servers. AWS Fargate is compatible with Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).<br><br>https://aws.amazon.com/fr/fargate/",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 768201,
          "date": "Sat 07 Jan 2023 03:04",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "ECS + Fargate",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763294,
          "date": "Sun 01 Jan 2023 19:13",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "AWS Fargate will hide all the complexity for you",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751230,
          "date": "Tue 20 Dec 2022 17:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.<br><br>AWS Fargate is a fully managed container execution environment that runs containers without the need to provision and manage underlying infrastructure. This makes it a good choice for companies that want to focus on maintaining their critical applications and do not want to be responsible for provisioning and managing the underlying infrastructure.<br><br>Option A involves installing Docker on Amazon EC2 instances, which would still require the company to manage the underlying infrastructure. Option B involves using Amazon ECS on Amazon EC2 worker nodes, which would also require the company to manage the underlying infrastructure. Option D involves using Amazon EC2 instances from an Amazon ECS-optimized Amazon Machine Image (AMI), which would also require the company to manage the underlying infrastructure.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749423,
          "date": "Mon 19 Dec 2022 04:46",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 741567,
          "date": "Sun 11 Dec 2022 10:33",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "Obviously anything with EC2 in the answer is wrong...",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 726980,
          "date": "Fri 25 Nov 2022 18:24",
          "username": "\t\t\t\tashish_t\t\t\t",
          "content": "The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload.<br>Fargate is serverless and no need to manage.<br>Answer: C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723576,
          "date": "Mon 21 Nov 2022 15:03",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712457,
          "date": "Sun 06 Nov 2022 17:15",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Agree Serverless Containerization Think Fargate",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 698164,
          "date": "Tue 18 Oct 2022 13:21",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C.  Fargate is serverless, no need to manage the underlying infrastructure.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#59",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day.<br>What should a solutions architect do to transmit and process the clickstream data?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#59",
          "answers": [
            {
              "choice": "<p>A. Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698169,
          "date": "Tue 18 Oct 2022 13:29",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option D.  <br><br>https://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-streaming-ingestion/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Unsure if this is right URL for this scenario. Option D is referring to S3 and then Redshift. Whereas URL discuss about eliminating S3 :- We're excited to launch Amazon Redshift streaming ingestion for Amazon Kinesis Data Streams, which enables you to ingest data directly from the Kinesis data stream without having to stage the data in Amazon Simple Storage Service (Amazon S3). Streaming ingestion allows you to achieve low latency in the order of seconds while ingesting hundreds of megabytes of data into your Amazon Redshift cluster.</li></ul>",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736674,
          "date": "Tue 06 Dec 2022 10:19",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "Unsure if this is right URL for this scenario. Option D is referring to S3 and then Redshift. Whereas URL discuss about eliminating S3 :- We're excited to launch Amazon Redshift streaming ingestion for Amazon Kinesis Data Streams, which enables you to ingest data directly from the Kinesis data stream without having to stage the data in Amazon Simple Storage Service (Amazon S3). Streaming ingestion allows you to achieve low latency in the order of seconds while ingesting hundreds of megabytes of data into your Amazon Redshift cluster.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751236,
          "date": "Tue 20 Dec 2022 17:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D is the most appropriate solution for transmitting and processing the clickstream data in this scenario.<br><br>Amazon Kinesis Data Streams is a highly scalable and durable service that enables real-time processing of streaming data at a high volume and high rate. You can use Kinesis Data Streams to collect and process the clickstream data in real-time.<br><br>Amazon Kinesis Data Firehose is a fully managed service that loads streaming data into data stores and analytics tools. You can use Kinesis Data Firehose to transmit the data from Kinesis Data Streams to an Amazon S3 data lake.<br><br>Once the data is in the data lake, you can use Amazon Redshift to load the data and perform analysis on it. Amazon Redshift is a fully managed, petabyte-scale data warehouse service that allows you to quickly and efficiently analyze data using SQL and your existing business intelligence tools.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, which involves using AWS Data Pipeline to archive the data to an Amazon S3 bucket and running an Amazon EMR cluster with the data to generate analytics, is not the most appropriate solution because it does not involve real-time processing of the data.<br><br>Option B, which involves creating an Auto Scaling group of Amazon EC2 instances to process the data and sending it to an Amazon S3 data lake for Amazon Redshift to use for analysis, is not the most appropriate solution because it does not involve a fully managed service for transmitting the data from the processing layer to the data lake.<br><br>Option C, which involves caching the data to Amazon CloudFront, storing the data in an Amazon S3 bucket, and running an AWS Lambda function to process the data for analysis when an object is added to the S3 bucket, is not the most appropriate solution because it does not involve a scalable and durable service for collecting and processing the data in real-time.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751238,
          "date": "Tue 20 Dec 2022 17:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, which involves using AWS Data Pipeline to archive the data to an Amazon S3 bucket and running an Amazon EMR cluster with the data to generate analytics, is not the most appropriate solution because it does not involve real-time processing of the data.<br><br>Option B, which involves creating an Auto Scaling group of Amazon EC2 instances to process the data and sending it to an Amazon S3 data lake for Amazon Redshift to use for analysis, is not the most appropriate solution because it does not involve a fully managed service for transmitting the data from the processing layer to the data lake.<br><br>Option C, which involves caching the data to Amazon CloudFront, storing the data in an Amazon S3 bucket, and running an AWS Lambda function to process the data for analysis when an object is added to the S3 bucket, is not the most appropriate solution because it does not involve a scalable and durable service for collecting and processing the data in real-time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749425,
          "date": "Mon 19 Dec 2022 04:48",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747383,
          "date": "Fri 16 Dec 2022 17:24",
          "username": "\t\t\t\tstudis\t\t\t",
          "content": "It is C. <br>The image in here https://aws.amazon.com/kinesis/data-firehose/ shows how kinesis can send data collected to firehose who can send it to Redshift. <br>It is also possible to use an intermediary S3 bucket between firehose and redshift. See image in here <br>https://aws.amazon.com/blogs/big-data/stream-transform-and-analyze-xml-data-in-real-time-with-amazon-kinesis-aws-lambda-and-amazon-redshift/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 733563,
          "date": "Fri 02 Dec 2022 09:06",
          "username": "\t\t\t\tsebasta\t\t\t",
          "content": "Why not A? <br>You can collect data with AWS Data Pipeline and then analyze it with EMR. Whats wrong with this option?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It's not A, the wording is tricky! It says \\\"to archive the data to S3\\\" - there is no mention of archiving in the question, so it has to be D :)</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 742193,
          "date": "Mon 12 Dec 2022 00:15",
          "username": "\t\t\t\tbearcandy\t\t\t",
          "content": "It's not A, the wording is tricky! It says \\\"to archive the data to S3\\\" - there is no mention of archiving in the question, so it has to be D :)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723578,
          "date": "Mon 21 Nov 2022 15:05",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712460,
          "date": "Sun 06 Nov 2022 17:16",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Click Stream & Analyse/ process- Think KDS,",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 700479,
          "date": "Fri 21 Oct 2022 05:22",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "D seems to make sense",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 700279,
          "date": "Thu 20 Oct 2022 22:25",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "Option D is correct... See the resource. Thank you Ariel",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#60",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS.<br>What should a solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#60",
          "answers": [
            {
              "choice": "<p>A. Update the ALB's network ACL to accept only HTTPS traffic.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a rule that replaces the HTTP in the URL with HTTPS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694562,
          "date": "Fri 14 Oct 2022 09:00",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Answer C : <br>https://docs.aws.amazon.com/fr_fr/elasticloadbalancing/latest/application/create-https-listener.html<br>https://aws.amazon.com/fr/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751254,
          "date": "Tue 20 Dec 2022 18:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.<br><br>To meet the requirement of forwarding all requests to the website so that the requests will use HTTPS, a solutions architect can create a listener rule on the ALB that redirects HTTP traffic to HTTPS. This can be done by creating a rule with a condition that matches all HTTP traffic and a rule action that redirects the traffic to the HTTPS listener. The HTTPS listener should already be configured to accept HTTPS traffic and forward it to the target group.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A.  Updating the ALB's network ACL to accept only HTTPS traffic is not a valid solution because the network ACL is used to control inbound and outbound traffic at the subnet level, not at the listener level.<br><br>Option B.  Creating a rule that replaces the HTTP in the URL with HTTPS is not a valid solution because this would not redirect the traffic to the HTTPS listener.<br><br>Option D.  Replacing the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is not a valid solution because it would not address the requirement to redirect HTTP traffic to HTTPS.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751255,
          "date": "Tue 20 Dec 2022 18:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A.  Updating the ALB's network ACL to accept only HTTPS traffic is not a valid solution because the network ACL is used to control inbound and outbound traffic at the subnet level, not at the listener level.<br><br>Option B.  Creating a rule that replaces the HTTP in the URL with HTTPS is not a valid solution because this would not redirect the traffic to the HTTPS listener.<br><br>Option D.  Replacing the ALB with a Network Load Balancer configured to use Server Name Indication (SNI) is not a valid solution because it would not address the requirement to redirect HTTP traffic to HTTPS.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 835247,
          "date": "Fri 10 Mar 2023 18:02",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "Configure an HTTPS listener on the ALB: This step involves setting up an HTTPS listener on the ALB and configuring the security policy to use a secure SSL/TLS protocol and cipher suite.<br><br>Create a redirect rule on the ALB: The redirect rule should be configured to redirect all incoming HTTP requests to HTTPS. This can be done by creating a redirect rule that redirects HTTP requests on port 80 to HTTPS requests on port 443.<br><br>Update the DNS record: The DNS record for the website should be updated to point to the ALB's DNS name, so that all traffic is routed through the ALB. <br><br>Verify the configuration: Once the configuration is complete, the website should be tested to ensure that all requests are being redirected to HTTPS. This can be done by accessing the website using HTTP and verifying that the request is redirected to HTTPS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749364,
          "date": "Mon 19 Dec 2022 03:06",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 744107,
          "date": "Tue 13 Dec 2022 15:07",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "C <br>To redirect HTTP traffic to HTTPS, a solutions architect should create a listener rule on the ALB to redirect HTTP traffic to HTTPS. Option A is not correct because network ACLs do not have the ability to redirect traffic. Option B is not correct because it does not redirect traffic, it only replaces the URL. Option D is not correct because a Network Load Balancer does not have the ability to handle HTTPS traffic.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723579,
          "date": "Mon 21 Nov 2022 15:06",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 691880,
          "date": "Tue 11 Oct 2022 09:57",
          "username": "\t\t\t\thanhdroid\t\t\t",
          "content": "Answer C: https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-alb/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#61",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#61",
          "answers": [
            {
              "choice": "<p>A. Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure the ability to fall back to previous values.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to grant access to the encrypted parameters.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696108,
          "date": "Sun 16 Oct 2022 10:49",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Secrets manager supports Autorotation unlike Parameter store.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Parameter store does not support autorotation.</li></ul>",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 700282,
          "date": "Thu 20 Oct 2022 22:28",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "Parameter store does not support autorotation.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 751263,
          "date": "Tue 20 Dec 2022 18:09",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is C.  Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.<br><br>AWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. By storing the database credentials as a secret in Secrets Manager, you can ensure that they are not hardcoded in the application and that they are automatically rotated on a regular basis. To grant the EC2 instance access to the secret, you can attach the required permission to the EC2 role. This will allow the application to retrieve the secret from Secrets Manager as needed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, storing the database credentials in the instance metadata and using a Lambda function to update them, would not meet the requirement of not hardcoding the credentials in the application. <br><br>Option B, storing the database credentials in an encrypted S3 bucket and using a Lambda function to update them, would also not meet this requirement, as the application would still need to access the credentials from the configuration file. <br><br>Option D, storing the database credentials as encrypted parameters in AWS Systems Manager Parameter Store, would also not meet this requirement, as the application would still need to access the encrypted parameters in order to use them.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751264,
          "date": "Tue 20 Dec 2022 18:09",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, storing the database credentials in the instance metadata and using a Lambda function to update them, would not meet the requirement of not hardcoding the credentials in the application. <br><br>Option B, storing the database credentials in an encrypted S3 bucket and using a Lambda function to update them, would also not meet this requirement, as the application would still need to access the credentials from the configuration file. <br><br>Option D, storing the database credentials as encrypted parameters in AWS Systems Manager Parameter Store, would also not meet this requirement, as the application would still need to access the encrypted parameters in order to use them.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 797817,
          "date": "Sat 04 Feb 2023 11:18",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "The right option is C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795808,
          "date": "Thu 02 Feb 2023 07:18",
          "username": "\t\t\t\tAdios_Amigo\t\t\t",
          "content": "C is the most correct answer. Automatic replacement must be performed by the secret manager.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749374,
          "date": "Mon 19 Dec 2022 03:18",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C - As the requirement is to rotate the secrets Secrets manager is the one that can support it.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723594,
          "date": "Mon 21 Nov 2022 15:21",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 700512,
          "date": "Fri 21 Oct 2022 06:44",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "AWS Secrets Manager is a newer service than SSM Parameter store",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 698174,
          "date": "Tue 18 Oct 2022 13:37",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C. <br><br>https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#62",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#62",
          "answers": [
            {
              "choice": "<p>A. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB.  Use the managed renewal feature to automatically rotate the certificate.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALUse the managed renewal feature to automatically rotate the certificate.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA.  Apply the certificate to the ALB.  Use the managed renewal feature to automatically rotate the certificate.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB.  Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 695435,
          "date": "Sat 15 Oct 2022 15:45",
          "username": "\t\t\t\tSinaneos\t\t\t",
          "content": "It's a third-party certificate, hence AWS cannot manage renewal automatically. The closest thing you can do is to send a notification to renew the 3rd party certificate.",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 712989,
          "date": "Mon 07 Nov 2022 12:05",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "It is D, because ACM does not manage the renewal process for imported certificates. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire.<br>Check this question on the link below:<br>Q: What types of certificates can I create and manage with ACM?<br>https://www.amazonaws.cn/en/certificate-manager/faqs/#Managed_renewal_and_deployment",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 797821,
          "date": "Sat 04 Feb 2023 11:21",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "Option D.  ACM cannot automatically renew imported certificates.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752203,
          "date": "Wed 21 Dec 2022 12:15",
          "username": "\t\t\t\tCSS85\t\t\t",
          "content": "D<br>https://aws.amazon.com/certificate-manager/faqs/<br>Imported certificates – If you want to use a third-party certificate with Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway, you may import it into ACM using the AWS Management Console, AWS CLI, or ACM APIs. ACM can not renew imported certificates, but it can help you manage the renewal process. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire. You can use ACM CloudWatch metrics to monitor the expiration dates of an imported certificates and import a new third-party certificate to replace an expiring one.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751268,
          "date": "Tue 20 Dec 2022 18:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is A.  Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB.  Use the managed renewal feature to automatically rotate the certificate.<br><br>AWS Certificate Manager (ACM) is a service that lets you easily provision, manage, and deploy Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS resources. ACM provides managed renewal for SSL/TLS certificates, which means that ACM automatically renews your certificates before they expire.<br><br>To meet the requirements for the web application, you should use ACM to issue an SSL/TLS certificate and apply it to the Application Load Balancer (ALB). Then, you can use the managed renewal feature to automatically rotate the certificate each year before it expires. This will ensure that the web application is always encrypted at the edge with a valid SSL/TLS certificate.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That is not good, because you are applying a new cert from AWS and discard the still valid cert from 3rd party,there might reason that they still want to use the 3rd party cert</li><li>I am taking back my answer after reading the AWS documentation. The correct answer is Option D.  Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB.  Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.<br><br>https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html</li><li>NOT ELIGIBLE if it is a private certificate issued by calling the AWS Private CA IssueCertificate API.<br><br>NOT ELIGIBLE if imported.<br><br>NOT ELIGIBLE if already expired.</li><li>Option D, using ACM to import an SSL/TLS certificate and manually rotating the certificate, would not meet the requirement to rotate the certificate before it expires each year. <br><br>Option C, using ACM Private Certificate Authority, is not necessary in this scenario because the requirement is to use a certificate issued by an external certificate authority. <br><br>Option B, importing the key material from the certificate, is not a valid option because ACM does not allow you to import key material for SSL/TLS certificates.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763301,
          "date": "Sun 01 Jan 2023 19:39",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "That is not good, because you are applying a new cert from AWS and discard the still valid cert from 3rd party,there might reason that they still want to use the 3rd party cert",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759094,
          "date": "Tue 27 Dec 2022 22:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "I am taking back my answer after reading the AWS documentation. The correct answer is Option D.  Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB.  Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually.<br><br>https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html<br><br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 757902,
          "date": "Mon 26 Dec 2022 23:40",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "NOT ELIGIBLE if it is a private certificate issued by calling the AWS Private CA IssueCertificate API.<br><br>NOT ELIGIBLE if imported.<br><br>NOT ELIGIBLE if already expired.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751270,
          "date": "Tue 20 Dec 2022 18:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D, using ACM to import an SSL/TLS certificate and manually rotating the certificate, would not meet the requirement to rotate the certificate before it expires each year. <br><br>Option C, using ACM Private Certificate Authority, is not necessary in this scenario because the requirement is to use a certificate issued by an external certificate authority. <br><br>Option B, importing the key material from the certificate, is not a valid option because ACM does not allow you to import key material for SSL/TLS certificates.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749384,
          "date": "Mon 19 Dec 2022 03:31",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741620,
          "date": "Sun 11 Dec 2022 12:04",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "Key phrase; external cert",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723597,
          "date": "Mon 21 Nov 2022 15:22",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 722931,
          "date": "Sun 20 Nov 2022 20:29",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "If issued by an external entity, the certificate must be imported.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 719591,
          "date": "Wed 16 Nov 2022 12:58",
          "username": "\t\t\t\tAck3rman\t\t\t",
          "content": "ACM certificates might be ineligible for renewal if:<br>The certificate isn't associated with another AWS service.<br>The certificate is expired.<br>The certificate is imported.<br>It's a private certificate issued with the IssueCertificate API call.<br><br>https://aws.amazon.com/tr/premiumsupport/knowledge-center/acm-certificate-ineligible/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 712988,
          "date": "Mon 07 Nov 2022 12:05",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "It is D, becauseACM does not manage the renewal process for imported certificates. You are responsible for monitoring the expiration date of your imported certificates and for renewing them before they expire.<br>Check this question on the link below: <br>Q: What types of certificates can I create and manage with ACM?<br>https://www.amazonaws.cn/en/certificate-manager/faqs/#Managed_renewal_and_deployment",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 703344,
          "date": "Mon 24 Oct 2022 22:39",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "When you have a cert issued by external CA, you can import and monitor for its expiration. AWS issued certificate contradicts the statement.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 703231,
          "date": "Mon 24 Oct 2022 19:25",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "> external certificate authority (CA)<br>answer is D here because question explicitly stated that they are using external CA",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 702726,
          "date": "Mon 24 Oct 2022 07:50",
          "username": "\t\t\t\tdave9994\t\t\t",
          "content": "D is the Answer. https://docs.aws.amazon.com/acm/latest/userguide/import-certificate.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 700620,
          "date": "Fri 21 Oct 2022 09:34",
          "username": "\t\t\t\tNIMIQ\t\t\t",
          "content": "It is A:https://www.amazonaws.cn/en/certificate-manager/faqs/#Managed_renewal_and_deployment",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700517,
          "date": "Fri 21 Oct 2022 06:56",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 700290,
          "date": "Thu 20 Oct 2022 22:38",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "It is option A<br>https://www.amazonaws.cn/en/certificate-manager/faqs/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>When you have a cert issued by external CA, you can import and monitor for its expiration. AWS issued certificate contradicts the statement.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703345,
          "date": "Mon 24 Oct 2022 22:39",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "When you have a cert issued by external CA, you can import and monitor for its expiration. AWS issued certificate contradicts the statement.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#63",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time.<br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#63",
          "answers": [
            {
              "choice": "<p>A. Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698179,
          "date": "Tue 18 Oct 2022 13:44",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option A.  Elastic BeanStalk is expensive, and DocumentDB has a 400KB max to upload files. So Lambda and S3 should be the one.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I'm thinking when you wrote DocumentDB you meant it as DynamoDB. ..yes?</li><li>Yes, DynamoDB has 400KB limit for the item.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html</li><li>In addition to this Lambda is paid only when used....</li><li>is lambda scalable as an EC2 ?</li></ul>",
          "upvote_count": "29",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751329,
          "date": "Tue 20 Dec 2022 19:20",
          "username": "\t\t\t\tmrbottomwood\t\t\t",
          "content": "I'm thinking when you wrote DocumentDB you meant it as DynamoDB. ..yes?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes, DynamoDB has 400KB limit for the item.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 754676,
          "date": "Sat 24 Dec 2022 04:14",
          "username": "\t\t\t\tbenjl\t\t\t",
          "content": "Yes, DynamoDB has 400KB limit for the item.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709395,
          "date": "Tue 01 Nov 2022 20:10",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "In addition to this Lambda is paid only when used....",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 706620,
          "date": "Fri 28 Oct 2022 18:27",
          "username": "\t\t\t\traffaello44\t\t\t",
          "content": "is lambda scalable as an EC2 ?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 768203,
          "date": "Sat 07 Jan 2023 03:15",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "This solution will meet the company's requirements in a cost-effective manner because it uses a serverless architecture with AWS Lambda to convert the files and store them in S3. The Lambda function will automatically scale to meet the demand for file conversions and S3 will automatically scale to store the original and converted files as needed.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751279,
          "date": "Tue 20 Dec 2022 18:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is the most cost-effective solution that meets the requirements.<br><br>In this solution, the .pdf files are saved to Amazon S3, which is an object storage service that is highly scalable, durable, and secure. S3 can store unlimited amounts of data at a very low cost.<br><br>The S3 PUT event triggers an AWS Lambda function to convert the .pdf files to .jpg format. Lambda is a serverless compute service that runs code in response to specific events and automatically scales to meet demand. This means that the conversion process can scale up or down as needed, without the need for manual intervention.<br><br>The converted .jpg files are then stored back in S3, which allows the company to store both the original .pdf files and the converted .jpg files in the same service. This reduces the complexity of the solution and helps to keep costs low.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C is also a valid solution, but it may be more expensive due to the use of EC2 instances, EBS storage, and an Auto Scaling group. These resources can add additional cost, especially if the demand for the conversion service grows rapidly.<br><br>Option D is not a valid solution because it uses Amazon EFS, which is a file storage service that is not suitable for storing large amounts of data. EFS is designed for storing and accessing files that are accessed frequently, such as application logs and media files. It is not designed for storing large files like .pdf or .jpg files.</li><li>EFS is optimized for a wide range of workloads and file sizes, and it can store files of any size up to the capacity of the file system. EFS scales automatically to meet your storage needs, and it can store petabyte-level capacity.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751280,
          "date": "Tue 20 Dec 2022 18:28",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C is also a valid solution, but it may be more expensive due to the use of EC2 instances, EBS storage, and an Auto Scaling group. These resources can add additional cost, especially if the demand for the conversion service grows rapidly.<br><br>Option D is not a valid solution because it uses Amazon EFS, which is a file storage service that is not suitable for storing large amounts of data. EFS is designed for storing and accessing files that are accessed frequently, such as application logs and media files. It is not designed for storing large files like .pdf or .jpg files.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EFS is optimized for a wide range of workloads and file sizes, and it can store files of any size up to the capacity of the file system. EFS scales automatically to meet your storage needs, and it can store petabyte-level capacity.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 768553,
          "date": "Sat 07 Jan 2023 13:39",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": "EFS is optimized for a wide range of workloads and file sizes, and it can store files of any size up to the capacity of the file system. EFS scales automatically to meet your storage needs, and it can store petabyte-level capacity.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749387,
          "date": "Mon 19 Dec 2022 03:33",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 736822,
          "date": "Tue 06 Dec 2022 14:01",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "This gives an example, using GET rather than PUT, but the idea is the same: https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723599,
          "date": "Mon 21 Nov 2022 15:23",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718618,
          "date": "Tue 15 Nov 2022 10:00",
          "username": "\t\t\t\tTonyghostR05\t\t\t",
          "content": "S3 is cost effective",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706061,
          "date": "Fri 28 Oct 2022 04:59",
          "username": "\t\t\t\tgoku58\t\t\t",
          "content": "For rapid scalability, B - DynamoDB looks to be a better solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It is not correct because the maximum item size in DynamoDB is 400 KB. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 709753,
          "date": "Wed 02 Nov 2022 12:09",
          "username": "\t\t\t\tludovikush\t\t\t",
          "content": "It is not correct because the maximum item size in DynamoDB is 400 KB. ",
          "upvote_count": "10",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#64",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day.<br>The company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#64",
          "answers": [
            {
              "choice": "<p>A. Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 693749,
          "date": "Thu 13 Oct 2022 10:45",
          "username": "\t\t\t\tsba21\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/83281-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 700533,
          "date": "Fri 21 Oct 2022 07:35",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "https://docs.aws.amazon.com/filegateway/latest/filefsxw/what-is-file-fsxw.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>From that shared doc: \\\"Amazon FSx File Gateway (FSx File Gateway) is a new File Gateway type that provides low latency and efficient access to in-cloud FSx for Windows File Server file shares from your on-premises facility. If you maintain on-premises file storage because of latency or bandwidth requirements, you can instead use FSx File Gateway for seamless access to fully managed, highly reliable, and virtually unlimited Windows file shares provided in the AWS Cloud by FSx for Windows File Server.\\\"</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744126,
          "date": "Tue 13 Dec 2022 15:27",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "From that shared doc: \\\"Amazon FSx File Gateway (FSx File Gateway) is a new File Gateway type that provides low latency and efficient access to in-cloud FSx for Windows File Server file shares from your on-premises facility. If you maintain on-premises file storage because of latency or bandwidth requirements, you can instead use FSx File Gateway for seamless access to fully managed, highly reliable, and virtually unlimited Windows file shares provided in the AWS Cloud by FSx for Windows File Server.\\\"",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 788202,
          "date": "Thu 26 Jan 2023 00:22",
          "username": "\t\t\t\tLoti2807\t\t\t",
          "content": "the company stated that they wanted to move the data from onprem to AWS with 'low latency' and 'no changes on current file access patterns', so FSx File Gateway is still needed in onprem to cache the data and then to the cloud, plus a secured data/file move. The Site2Site VPN is for users accessing the data from onprem and cloud within premise network. <br><br>Check on the Conclusion section for summary: https://aws.amazon.com/blogs/storage/accessing-your-file-workloads-from-on-premises-with-file-gateway/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779380,
          "date": "Tue 17 Jan 2023 23:47",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "D IS WRONG - Its used for caching. you cannot 'Move the on-premises file data to the FSx File Gateway.'which is stated in answer D.  It pretty sure AWS employee's are spamming this site with the wrong answers intentionally.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 768204,
          "date": "Sat 07 Jan 2023 03:21",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "This solution will meet the requirements because it allows the company to continue using a file server with minimal changes to the existing file access patterns. FSx for Windows File Server integrates with the on-premises Active Directory, so users can continue accessing the file data with their existing credentials. The Site-to-Site VPN connection can be used to establish low-latency connectivity between the on-premises file servers and FSx for Windows File Server on AWS. FSx for Windows File Server is also highly available and scalable, so it can handle the workloads' file storage needs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763307,
          "date": "Sun 01 Jan 2023 19:51",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "FSx is for windows file,other options like S3 certainly can handle files but might bring compatibility issue. and a FSx gateway might have sort of cache mechanism that make the users feel they are accessing local file system.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757905,
          "date": "Mon 26 Dec 2022 23:44",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "Benefits of using Amazon FSx File Gateway ****WINDOWS FILE SERVERS***<br><br>FSx File Gateway provides the following benefits:<br><br>Helps eliminate on-premises file servers and consolidates all their data in AWS to take advantage of the scale and economics of cloud storage.<br><br>Provides options that you can use for all your file workloads, including those that require on-premises access to cloud data.<br><br>Applications that need to stay on premises can now experience the same low latency and high performance that they have in AWS, without taxing your networks or impacting the latencies experienced by your most demanding applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754464,
          "date": "Fri 23 Dec 2022 20:41",
          "username": "\t\t\t\tkurinei021\t\t\t",
          "content": "I think it is C.  To meet these requirements, the solutions architect could recommend using AWS Storage Gateway to provide file-based storage access between the on-premises file servers and AWS.<br><br>AWS Storage Gateway is a hybrid storage service that connects on-premises storage environments with AWS storage infrastructure. It provides low-latency file-based storage access to AWS, enabling users and applications to access data in AWS as if it were stored on-premises.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751284,
          "date": "Tue 20 Dec 2022 18:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is C.  Deploy and configures an Amazon S3 File Gateway on-premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway, depending on each workload's location.<br><br>Amazon S3 is a highly durable and scalable object storage service that is well-suited for storing large amounts of file data. By moving the on-premises file data to Amazon S3, you can take advantage of its durability, scalability, and global availability, while still allowing users and applications to access the data using their existing file access patterns.<br><br>The Amazon S3 File Gateway can be deployed on-premises and configured to provide file-based access to data stored in Amazon S3. This allows users and applications to access the data stored in Amazon S3 as if it were stored on a local file server, while still taking advantage of the benefits of storing the data in Amazon S3.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, deploying and configuring Amazon FSx for Windows File Server on AWS, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.<br><br>Option B, deploying and configuring an Amazon S3 File Gateway on-premises and moving the on-premises file data to the S3 File Gateway, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.<br><br>Option D, deploying and configuring Amazon FSx for Windows File Server on AWS and an Amazon FSx File Gateway on-premises, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751286,
          "date": "Tue 20 Dec 2022 18:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, deploying and configuring Amazon FSx for Windows File Server on AWS, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.<br><br>Option B, deploying and configuring an Amazon S3 File Gateway on-premises and moving the on-premises file data to the S3 File Gateway, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.<br><br>Option D, deploying and configuring Amazon FSx for Windows File Server on AWS and an Amazon FSx File Gateway on-premises, would not meet the requirement to minimize operational overhead, as it would require significant changes to the existing file access patterns.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749388,
          "date": "Mon 19 Dec 2022 03:38",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744125,
          "date": "Tue 13 Dec 2022 15:27",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer C <br>Option C will provide low-latency access to the file data from both on-premises and AWS environments, and it will minimize operational overhead by requiring no significant changes to the existing file access patterns. Additionally, the use of the AWS Site-to-Site VPN connection will ensure secure and seamless connectivity between the on-premises and AWS environments. Option A is not correct because it only addresses the requirement to access file data on AWS, but it does not address the requirement to access file data on premises with minimal latency.Option D is not correct because it involves deploying and configuring two different file storage services (FSx for Windows File Server and FSx File Gateway), which would add complexity and operational overhead. It also does not provide a solution for accessing file data on premises with minimal latency.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"the company requires access to AWS and on-premises file storage\\\"C is excluding on premises needs.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 745973,
          "date": "Thu 15 Dec 2022 12:03",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "\\\"the company requires access to AWS and on-premises file storage\\\"C is excluding on premises needs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726878,
          "date": "Fri 25 Nov 2022 16:09",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "Answer A is correct. The company has a site to site VPN already.There is no need to install file gateway on-premise. https://docs.aws.amazon.com/fsx/latest/LustreGuide/mounting-on-premises.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I was confused with this one, but I would vote for D.  My thoughts:<br>You actually need the gateway... you would not need it in case of VPC peerig. Site to site vpn still requires the gateway to serve as endpoint.<br>https://bluexp.netapp.com/blog/aws-fsxo-blg-fsx-gateway-amazon-fsx-for-windows-at-on-premises-speed</li><li>You do not need the gateway if you have already VPN.<br>mazon FSx File Gateway is a way to access your Amazon FSx file system from on-premises servers or client devices over a Network File System (NFS) or Server Message Block (SMB) protocol. If you already have an AWS Site-to-Site VPN connection set up between your on-premises environment and your Amazon VPC, you can use that connection to access your Amazon FSx file system from on-premises without using the Amazon FSx File Gateway.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741193,
          "date": "Sat 10 Dec 2022 20:23",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "I was confused with this one, but I would vote for D.  My thoughts:<br>You actually need the gateway... you would not need it in case of VPC peerig. Site to site vpn still requires the gateway to serve as endpoint.<br>https://bluexp.netapp.com/blog/aws-fsxo-blg-fsx-gateway-amazon-fsx-for-windows-at-on-premises-speed<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You do not need the gateway if you have already VPN.<br>mazon FSx File Gateway is a way to access your Amazon FSx file system from on-premises servers or client devices over a Network File System (NFS) or Server Message Block (SMB) protocol. If you already have an AWS Site-to-Site VPN connection set up between your on-premises environment and your Amazon VPC, you can use that connection to access your Amazon FSx file system from on-premises without using the Amazon FSx File Gateway.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764343,
          "date": "Tue 03 Jan 2023 09:55",
          "username": "\t\t\t\tzek\t\t\t",
          "content": "You do not need the gateway if you have already VPN.<br>mazon FSx File Gateway is a way to access your Amazon FSx file system from on-premises servers or client devices over a Network File System (NFS) or Server Message Block (SMB) protocol. If you already have an AWS Site-to-Site VPN connection set up between your on-premises environment and your Amazon VPC, you can use that connection to access your Amazon FSx file system from on-premises without using the Amazon FSx File Gateway.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723602,
          "date": "Mon 21 Nov 2022 15:25",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718151,
          "date": "Mon 14 Nov 2022 19:04",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "ddddddddd",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 706069,
          "date": "Fri 28 Oct 2022 05:11",
          "username": "\t\t\t\tgoku58\t\t\t",
          "content": "Windows File server == FSx. <br>Since access from both on-prem and AWS is needed, A isn't sufficient. So D. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>and VPN S2S?</li><li>True, but the other requirement is no \\\"significant changes to the existing file access patterns\\\" which would mean mounting File Gateway shares in their on-premises location while they move their workloads to FSx during their migration. So D. </li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 716805,
          "date": "Sat 12 Nov 2022 17:37",
          "username": "\t\t\t\t17Master\t\t\t",
          "content": "and VPN S2S?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>True, but the other requirement is no \\\"significant changes to the existing file access patterns\\\" which would mean mounting File Gateway shares in their on-premises location while they move their workloads to FSx during their migration. So D. </li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 726297,
          "date": "Fri 25 Nov 2022 01:45",
          "username": "\t\t\t\trewdboy\t\t\t",
          "content": "True, but the other requirement is no \\\"significant changes to the existing file access patterns\\\" which would mean mounting File Gateway shares in their on-premises location while they move their workloads to FSx during their migration. So D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 693144,
          "date": "Wed 12 Oct 2022 15:58",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "dddddddddd",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 692247,
          "date": "Tue 11 Oct 2022 18:17",
          "username": "\t\t\t\toldcardigan\t\t\t",
          "content": "i think its D",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#65",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#65",
          "answers": [
            {
              "choice": "<p>A. Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from the extracted text.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 751296,
          "date": "Tue 20 Dec 2022 18:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is C: Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.<br><br>Option C: Using Amazon Textract to extract the text from the reports, and Amazon Comprehend Medical to identify the PHI from the extracted text, would be the most efficient solution as it would involve the least operational overhead. Textract is specifically designed for extracting text from documents, and Comprehend Medical is a fully managed service that can accurately identify PHI in medical text. This solution would require minimal maintenance and would not incur any additional costs beyond the usage fees for Textract and Comprehend Medical.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A: Using existing Python libraries to extract the text and identify the PHI from the text would require the hospital to maintain and update the libraries as needed. This would involve operational overhead in terms of keeping the libraries up to date and debugging any issues that may arise.<br><br>Option B: Using Amazon SageMaker to identify the PHI from the extracted text would involve additional operational overhead in terms of setting up and maintaining a SageMaker model, as well as potentially incurring additional costs for using SageMaker.<br><br>Option D: Using Amazon Rekognition to extract the text from the reports would not be an effective solution, as Rekognition is primarily designed for image recognition and would not be able to accurately extract text from PDF or JPEG files.</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751297,
          "date": "Tue 20 Dec 2022 18:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A: Using existing Python libraries to extract the text and identify the PHI from the text would require the hospital to maintain and update the libraries as needed. This would involve operational overhead in terms of keeping the libraries up to date and debugging any issues that may arise.<br><br>Option B: Using Amazon SageMaker to identify the PHI from the extracted text would involve additional operational overhead in terms of setting up and maintaining a SageMaker model, as well as potentially incurring additional costs for using SageMaker.<br><br>Option D: Using Amazon Rekognition to extract the text from the reports would not be an effective solution, as Rekognition is primarily designed for image recognition and would not be able to accurately extract text from PDF or JPEG files.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 755965,
          "date": "Sun 25 Dec 2022 20:47",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "Selected Answer: C<br>Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749390,
          "date": "Mon 19 Dec 2022 03:39",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 745716,
          "date": "Thu 15 Dec 2022 06:25",
          "username": "\t\t\t\tSONA_M_\t\t\t",
          "content": "WHY OPTION D IS WRONG<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B/C you use TextTract to extract text not Rekognition.</li><li>D is wrong only because Amazon Rekognition doesn't read text, only explicit image contents.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780622,
          "date": "Thu 19 Jan 2023 02:41",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "B/C you use TextTract to extract text not Rekognition.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 761879,
          "date": "Fri 30 Dec 2022 12:37",
          "username": "\t\t\t\ts_fun\t\t\t",
          "content": "D is wrong only because Amazon Rekognition doesn't read text, only explicit image contents.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 745119,
          "date": "Wed 14 Dec 2022 14:40",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 733935,
          "date": "Fri 02 Dec 2022 18:12",
          "username": "\t\t\t\tRameez1\t\t\t",
          "content": "C is correct<br>Textract- for extracting the text and Comprehend to identify the medical info<br>https://aws.amazon.com/comprehend/medical/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 723605,
          "date": "Mon 21 Nov 2022 15:27",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701773,
          "date": "Sat 22 Oct 2022 22:50",
          "username": "\t\t\t\tbansalhp\t\t\t",
          "content": "Textract -to extract textandComprehend -to identify Medicalinfo",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 700293,
          "date": "Thu 20 Oct 2022 22:45",
          "username": "\t\t\t\tJesseeS\t\t\t",
          "content": "Textract and Comprehend is HIPPA compliant<br> https://aws.amazon.com/blogs/machine-learning/amazon-textract-is-now-hipaa-eligible/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696120,
          "date": "Sun 16 Oct 2022 11:05",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Textract - Comprehend Medical for PHI info",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#66",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days.<br>Which storage solution is MOST cost-effective?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#66",
          "answers": [
            {
              "choice": "<p>A. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703237,
          "date": "Mon 24 Oct 2022 19:37",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "i think C should be the answer here, <br>> Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce<br><br>If they do not explicitly mention that they are using Glacier Instant Retrieval, we should assume that Glacier -> takes more time to retrieve and may not meet the requirements<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can make that assumption, but I think it would be wrong to make it. It does not state they are not using Glacier Instant Retrieval, and it's use would be the logical choice in this question, so I'm going for A</li><li>I think his assumption is correct because if you go to AWS documentation (https://aws.amazon.com/s3/storage-classes/glacier/) they clearly mention: \\\"S3 Glacier Flexible Retrieval (formerly S3 Glacier)\\\". So since this question doesn't specify the S3 Glacier class, then it would default to flexible retrieval (which ofc is not equal to Instant Retrieval).</li></ul>",
          "upvote_count": "42",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 736839,
          "date": "Tue 06 Dec 2022 14:21",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "You can make that assumption, but I think it would be wrong to make it. It does not state they are not using Glacier Instant Retrieval, and it's use would be the logical choice in this question, so I'm going for A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think his assumption is correct because if you go to AWS documentation (https://aws.amazon.com/s3/storage-classes/glacier/) they clearly mention: \\\"S3 Glacier Flexible Retrieval (formerly S3 Glacier)\\\". So since this question doesn't specify the S3 Glacier class, then it would default to flexible retrieval (which ofc is not equal to Instant Retrieval).</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 741344,
          "date": "Sun 11 Dec 2022 02:07",
          "username": "\t\t\t\tsyh_rapha\t\t\t",
          "content": "I think his assumption is correct because if you go to AWS documentation (https://aws.amazon.com/s3/storage-classes/glacier/) they clearly mention: \\\"S3 Glacier Flexible Retrieval (formerly S3 Glacier)\\\". So since this question doesn't specify the S3 Glacier class, then it would default to flexible retrieval (which ofc is not equal to Instant Retrieval).",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 696497,
          "date": "Sun 16 Oct 2022 21:57",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "Most COST EFFECTIVE<br>A:S3 Glacier Instant Retrieval is a new storage class that delivers the fastest access to archive storage, with the same low latency and high-throughput performance as the S3 Standard and S3 Standard-IA storage classes. You can save up to 68 percent on storage costs as compared with using the S3 Standard-IA storage class when you use the S3 Glacier Instant Retrieval storage class and pay a low price to retrieve data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Would agree if that was one of the answers, however many questions that are asked do have alternative solutions but again they are doing this on purpose to check your knowledge. Here C is best.</li><li>In the other hand, you need to chose a tier when going for glacier, so my previous comment is not stating well. The question is tricky, I change my mind: agree with you on this one</li><li>Instant Retrieval was never mentioned. The exams always mention the tier when needed to. To be A the answer given should at least include the step mentioning that instant retrieval would be used.</li><li>\\\"Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce\\\" is the key sentence. answer is C. </li><li>I agree with your key sentence..but the one zone infrequent doesn't fit for critical business and it is used for recreate..</li><li>But S3 Glacier Instant Retrieval\\\"is designed for rarely accessed data that still needs immediate access in performance-sensitive use cases\\\", so it offers lower cost and instant retrieval, so A</li></ul>",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 812096,
          "date": "Fri 17 Feb 2023 16:54",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "Would agree if that was one of the answers, however many questions that are asked do have alternative solutions but again they are doing this on purpose to check your knowledge. Here C is best.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741203,
          "date": "Sat 10 Dec 2022 20:37",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "In the other hand, you need to chose a tier when going for glacier, so my previous comment is not stating well. The question is tricky, I change my mind: agree with you on this one",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 741201,
          "date": "Sat 10 Dec 2022 20:32",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "Instant Retrieval was never mentioned. The exams always mention the tier when needed to. To be A the answer given should at least include the step mentioning that instant retrieval would be used.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 715643,
          "date": "Fri 11 Nov 2022 01:38",
          "username": "\t\t\t\tPamban\t\t\t",
          "content": "\\\"Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce\\\" is the key sentence. answer is C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I agree with your key sentence..but the one zone infrequent doesn't fit for critical business and it is used for recreate..</li><li>But S3 Glacier Instant Retrieval\\\"is designed for rarely accessed data that still needs immediate access in performance-sensitive use cases\\\", so it offers lower cost and instant retrieval, so A</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 793694,
          "date": "Tue 31 Jan 2023 06:40",
          "username": "\t\t\t\tBala75krish\t\t\t",
          "content": "I agree with your key sentence..but the one zone infrequent doesn't fit for critical business and it is used for recreate..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736836,
          "date": "Tue 06 Dec 2022 14:19",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "But S3 Glacier Instant Retrieval\\\"is designed for rarely accessed data that still needs immediate access in performance-sensitive use cases\\\", so it offers lower cost and instant retrieval, so A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837612,
          "date": "Mon 13 Mar 2023 04:55",
          "username": "\t\t\t\tlovelazur\t\t\t",
          "content": "Option A involves moving the files to S3 Glacier, which is a cheaper storage class but incurs additional retrieval costs and has a longer retrieval time. Since immediate accessibility is always required, this option may not be the best choice.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Think c should be the answer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837613,
          "date": "Mon 13 Mar 2023 04:56",
          "username": "\t\t\t\tlovelazur\t\t\t",
          "content": "Think c should be the answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 837597,
          "date": "Mon 13 Mar 2023 04:38",
          "username": "\t\t\t\tvi05\t\t\t",
          "content": "\\\"Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce.\\\"<br>Immediate accessibility --> Standard IA or Onezone IA<br>is not easy to reproduce --> Standard IA",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 835262,
          "date": "Fri 10 Mar 2023 18:21",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "S3 Standard-IA is designed for data that is accessed less frequently, but requires immediate access when needed. It has a lower storage cost than S3 Standard, and charges a retrieval fee when data is accessed. In this scenario, since the files are frequently accessed in the first 30 days of creation, it is likely that they will be accessed during that period and the retrieval fees will not be a significant cost.<br><br>Additionally, S3 Standard-IA has a minimum storage duration of 30 days. Since the files need to be stored for 4 years, the minimum storage duration requirement is met.<br><br>Overall, using S3 Standard-IA storage class would be the most cost-effective solution for storing these files while still meeting the company's policy requirements and accessibility needs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 829165,
          "date": "Sat 04 Mar 2023 18:04",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "B is most COST effective",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 821799,
          "date": "Sat 25 Feb 2023 21:12",
          "username": "\t\t\t\tJa13\t\t\t",
          "content": "As it says \\\"immediate access always\\\" you should choose infrequent access",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 806349,
          "date": "Sun 12 Feb 2023 14:10",
          "username": "\t\t\t\tjkmaws\t\t\t",
          "content": "Glacier instant retrieval would have been the correct question to ask and will suit the requirement. Glacier is ambiguous term. So with this ambigous question and given answers, C is most appropriate",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 800637,
          "date": "Tue 07 Feb 2023 07:58",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "The answer should be A, regardless of what the exam say.<br><br>Since the data needs to be saved for 4 years the minimum 90 days charge of glacier instant retrieval is irrelevant as opposed to the 30 minimum days of S3 Standard-IA<br><br>The files are approx 5MB in size so the minimum object size of 128KB doesn't matter here as well.<br><br>That leaves cost effectiveness - which means S3 Glacier Instant Retrieval is the correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 797823,
          "date": "Sat 04 Feb 2023 11:23",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "The question is tricky. They ommitted on purpose the Glacier storage class. <br><br>Here the philosophy is about : What is the most effective storage class to choose while the instant retreival is manadatory for the client along the 4 years.<br><br>Even Glacier Flexible Retrieval has a good retrieval duration but it's not instantly. So for the client it's not a good solution since the retrieval is not immediate. So C is the most optimal solution",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 796381,
          "date": "Thu 02 Feb 2023 20:46",
          "username": "\t\t\t\tegmiranda\t\t\t",
          "content": "Every time a question was releated to S3 Glacier Instant Retrieval they name it. In this case only talk about S3 Glacier. I choose C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793008,
          "date": "Mon 30 Jan 2023 16:58",
          "username": "\t\t\t\tluci1491\t\t\t",
          "content": "answer is A because we are rarely using file for 4 years after 30 days.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 786100,
          "date": "Tue 24 Jan 2023 03:56",
          "username": "\t\t\t\tLoti2807\t\t\t",
          "content": "Answer is B as explained by Buruguduystunstugudunstuy: https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/<br><br>\\\"With S3 One Zone-IA, customers can now store infrequently accessed data within a single Availability Zone at 20% lower cost than S3 Standard-IA.  In addition, S3 One Zone-IA can offer customers higher availability and durability than self-managed physical data centers, with the added benefit of having to pay only for what they use\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Distractor..S3 one zone IA doesn't help for critical business and not easy to reproduce</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 793713,
          "date": "Tue 31 Jan 2023 07:14",
          "username": "\t\t\t\tAjithKumar3\t\t\t",
          "content": "Distractor..S3 one zone IA doesn't help for critical business and not easy to reproduce",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781058,
          "date": "Thu 19 Jan 2023 12:39",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "The answer options seem to be outdated. The real answer should have been to move the files to Glacier Instant Retrieval, not IA. <br><br>Just \\\"Glacier\\\" refers to Glacier Flexible Retrieval, which does not give you immediate access.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779587,
          "date": "Wed 18 Jan 2023 04:49",
          "username": "\t\t\t\tnalindm\t\t\t",
          "content": "Choosing for a storage class after 30 days depends on the word \\\"Cost-effective\\\". Therefore, answer is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 775483,
          "date": "Sat 14 Jan 2023 15:10",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Glacier retrieval time is 12 hours and if bulk than 48 hours.<br>Infrequent Access is Expedite retrieval which is within 1 - 5 minutes<br>as the question asks Immediate accessibility then IA is best. <br>Answer C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Deep archive is 12 Hours. My choice was A.  still thinking...</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776100,
          "date": "Sun 15 Jan 2023 02:56",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "Deep archive is 12 Hours. My choice was A.  still thinking...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771265,
          "date": "Tue 10 Jan 2023 10:53",
          "username": "\t\t\t\tJohnnyBG\t\t\t",
          "content": "Most coest effective and respond to the criteria. S3-One zone not suitable for business critical data.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#67",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.<br>What should a solutions architect do to ensure messages are being processed once only?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#67",
          "answers": [
            {
              "choice": "<p>A. Use the CreateQueue API call to create a new queue.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use the AddPermission API call to add appropriate permissions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the ReceiveMessage API call to set an appropriate wait time.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the ChangeMessageVisibility API call to increase the visibility timeout.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696140,
          "date": "Sun 16 Oct 2022 11:27",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "In case of SQS - multi-consumers if one consumer has already picked the message and is processing, in meantime other consumer can pick it up and process the message there by two copies are added at the end. To avoid this the message is made invisible from the time its picked and deleted after processing. This visibility timeout is increased according to max time taken to process the message<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To add to this \\\"The VisibilityTimeout in SQS is a time frame that the message can be hidden so that no others can consume it except the first consumer who calls the ReceiveMessageAPI.\\\" The API ChangeMesssageVisibility changes this value.</li></ul>",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736851,
          "date": "Tue 06 Dec 2022 14:34",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "To add to this \\\"The VisibilityTimeout in SQS is a time frame that the message can be hidden so that no others can consume it except the first consumer who calls the ReceiveMessageAPI.\\\" The API ChangeMesssageVisibility changes this value.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 696126,
          "date": "Sun 16 Oct 2022 11:13",
          "username": "\t\t\t\tValero_\t\t\t",
          "content": "True, it's D. <br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777785,
          "date": "Mon 16 Jan 2023 15:37",
          "username": "\t\t\t\tdev1978\t\t\t",
          "content": "In theory, between reception and changing visibility, you can have multiple consumers. Question is not good as it won't guarantee not executing twice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769871,
          "date": "Sun 08 Jan 2023 23:27",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Increaseing visibilitytimeout makes sure message is not visible for time taken to process the message.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751333,
          "date": "Tue 20 Dec 2022 19:24",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To ensure that messages are being processed only once, a solutions architect should use the ChangeMessageVisibility API call to increase the visibility timeout which is Option D. <br><br>The visibility timeout determines the amount of time that a message received from an SQS queue is hidden from other consumers while the message is being processed. If the processing of a message takes longer than the visibility timeout, the message will become visible to other consumers and may be processed again. By increasing the visibility timeout, the solutions architect can ensure that the message is not made visible to other consumers until the processing is complete and the message can be safely deleted from the queue.<br><br>Option A (Use the CreateQueue API call to create a new queue) would not address the issue of duplicate message processing. <br><br>Option B (Use the AddPermission API call to add appropriate permissions) is not relevant to this issue. <br><br>Option C (Use the ReceiveMessage API call to set an appropriate wait time) is also not relevant to this issue.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>not relevant to this issue. ??? what is added value</li><li>Option B (Use the AddPermission API call to add appropriate permissions) is not relevant to this issue because it deals with setting permissions for accessing an SQS queue, which is not related to preventing duplicate records in the RDS table.<br><br>Option C (Use the ReceiveMessage API call to set an appropriate wait time) is not relevant to this issue because it is related to configuring how long the ReceiveMessage API call should wait for new messages to arrive in the SQS queue before returning an empty response. It does not address the issue of duplicate records in the RDS table.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768661,
          "date": "Sat 07 Jan 2023 15:54",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": "not relevant to this issue. ??? what is added value<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B (Use the AddPermission API call to add appropriate permissions) is not relevant to this issue because it deals with setting permissions for accessing an SQS queue, which is not related to preventing duplicate records in the RDS table.<br><br>Option C (Use the ReceiveMessage API call to set an appropriate wait time) is not relevant to this issue because it is related to configuring how long the ReceiveMessage API call should wait for new messages to arrive in the SQS queue before returning an empty response. It does not address the issue of duplicate records in the RDS table.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 823324,
          "date": "Mon 27 Feb 2023 08:13",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B (Use the AddPermission API call to add appropriate permissions) is not relevant to this issue because it deals with setting permissions for accessing an SQS queue, which is not related to preventing duplicate records in the RDS table.<br><br>Option C (Use the ReceiveMessage API call to set an appropriate wait time) is not relevant to this issue because it is related to configuring how long the ReceiveMessage API call should wait for new messages to arrive in the SQS queue before returning an empty response. It does not address the issue of duplicate records in the RDS table.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749394,
          "date": "Mon 19 Dec 2022 03:44",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723614,
          "date": "Mon 21 Nov 2022 15:31",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713010,
          "date": "Mon 07 Nov 2022 12:39",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "D is the correct choise, increasing the visibility timeout according to max time taken to process the message on the RDS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#68",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails.<br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#68",
          "answers": [
            {
              "choice": "<p>A. Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696192,
          "date": "Sun 16 Oct 2022 12:52",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "Direct Connect + VPN best of both",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 713014,
          "date": "Mon 07 Nov 2022 12:50",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "Direct Connect goes throught 1 Gbps, 10 Gbps or 100 Gbps and the VPN goes up to 1.25 Gbps. <br><br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793360,
          "date": "Mon 30 Jan 2023 22:24",
          "username": "\t\t\t\tdevonwho\t\t\t",
          "content": "With AWS Direct Connect + VPN, you can combine AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This solution combines the benefits of the end-to-end secure IPSec connection with low latency and increased bandwidth of the AWS Direct Connect to provide a more consistent network experience than internet-based VPN connections.<br><br>https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777787,
          "date": "Mon 16 Jan 2023 15:40",
          "username": "\t\t\t\tdev1978\t\t\t",
          "content": "Why not B? Two VPNs on different connections? Direct Connect costs a fortune?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The company requires a highly available connection with consistent low latency to an AWS Region, this is provided by Direct Connect as primary connection. The company allows a slower connection only for the backup option, so A is the right answer</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780911,
          "date": "Thu 19 Jan 2023 09:41",
          "username": "\t\t\t\tJ3nkinz\t\t\t",
          "content": "The company requires a highly available connection with consistent low latency to an AWS Region, this is provided by Direct Connect as primary connection. The company allows a slower connection only for the backup option, so A is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755427,
          "date": "Sun 25 Dec 2022 06:55",
          "username": "\t\t\t\tthanhch\t\t\t",
          "content": "DX for low latency connect and the company accept slower traffic if the primary connection fails. So we should choose VPN for backup purpose. And the question also mark : minimize cost.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751433,
          "date": "Tue 20 Dec 2022 21:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "This a tricky question but let's try to understand the requirements of the question.<br><br>The company requires VS The company needs.<br><br>The main difference between need and require is that needs are goals and objectives a business must achieve, whereas require or requirements are the things we need to do in order to achieve a need.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To meet the requirements specified in the question, the best solution is to provision two AWS Direct Connect connections to the same Region. This will provide a highly available connection with consistently low latency to the AWS Region and minimize costs by eliminating internet usage fees. Provisioning a second Direct Connect connection as a backup will ensure that there is a failover option available in case the primary connection fails.</li><li>Using VPN connections as a backup, as described in options A and B, is not the best solution because VPN connections are typically slower and less reliable than Direct Connect connections. Additionally, having two VPN connections to the same Region may not provide the desired level of availability and may not meet the company's requirement for low latency.<br><br>Option D, which involves using the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails, is not a valid option because the Direct Connect failover attribute is not available in the AWS CLI.</li><li>See pricing for more info.<br>https://aws.amazon.com/directconnect/pricing/</li><li>I love your comments!</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751436,
          "date": "Tue 20 Dec 2022 21:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements specified in the question, the best solution is to provision two AWS Direct Connect connections to the same Region. This will provide a highly available connection with consistently low latency to the AWS Region and minimize costs by eliminating internet usage fees. Provisioning a second Direct Connect connection as a backup will ensure that there is a failover option available in case the primary connection fails.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Using VPN connections as a backup, as described in options A and B, is not the best solution because VPN connections are typically slower and less reliable than Direct Connect connections. Additionally, having two VPN connections to the same Region may not provide the desired level of availability and may not meet the company's requirement for low latency.<br><br>Option D, which involves using the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails, is not a valid option because the Direct Connect failover attribute is not available in the AWS CLI.</li><li>See pricing for more info.<br>https://aws.amazon.com/directconnect/pricing/</li><li>I love your comments!</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 751437,
          "date": "Tue 20 Dec 2022 21:27",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Using VPN connections as a backup, as described in options A and B, is not the best solution because VPN connections are typically slower and less reliable than Direct Connect connections. Additionally, having two VPN connections to the same Region may not provide the desired level of availability and may not meet the company's requirement for low latency.<br><br>Option D, which involves using the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails, is not a valid option because the Direct Connect failover attribute is not available in the AWS CLI.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>See pricing for more info.<br>https://aws.amazon.com/directconnect/pricing/</li><li>I love your comments!</li></ul>",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 751441,
          "date": "Tue 20 Dec 2022 21:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "See pricing for more info.<br>https://aws.amazon.com/directconnect/pricing/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I love your comments!</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790374,
          "date": "Sat 28 Jan 2023 08:19",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "I love your comments!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749395,
          "date": "Mon 19 Dec 2022 03:47",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 727929,
          "date": "Sun 27 Nov 2022 06:21",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "A is rigth I thought wrong",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 727928,
          "date": "Sun 27 Nov 2022 06:09",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "I think VPN is not right solution for \\\"low latency\\\"<br>So how about C?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The question mention that \\\"The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails\\\" so VPN as secondary option is acceptable</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 738628,
          "date": "Thu 08 Dec 2022 05:51",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "The question mention that \\\"The company needs to minimize costs and is willing to accept slower traffic if the primary connection fails\\\" so VPN as secondary option is acceptable",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723615,
          "date": "Mon 21 Nov 2022 15:32",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#69",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data.<br>Which solution will meet these requirements with the LEAST operational effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#69",
          "answers": [
            {
              "choice": "<p>A. Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event of a failure.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696194,
          "date": "Sun 16 Oct 2022 12:58",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "RDS Proxy for Aurora https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 800639,
          "date": "Tue 07 Feb 2023 08:06",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "RDS Proxy is fully managed by AWS for RDS/Aurora. It is auto-scaling and highly available by default.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 768211,
          "date": "Sat 07 Jan 2023 03:34",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "By configuring the Auto Scaling group to use multiple Availability Zones, the application will be able to continue running even if one Availability Zone goes down. Configuring the database as Multi-AZ will also ensure that the database remains available in the event of a failure in one Availability Zone. Using an Amazon RDS Proxy instance for the database will allow the application to automatically route traffic to healthy database instances, further increasing the availability of the application. This solution will meet the requirements for high availability with minimal operational effort.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759107,
          "date": "Tue 27 Dec 2022 23:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is B: Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.<br><br>This solution will meet the requirements of high availability with minimum downtime and minimum loss of data with the least operational effort. By configuring the Auto Scaling group to use multiple Availability Zones, the web application will be able to withstand the failure of one Availability Zone without any disruption to the service. By configuring the database as Multi-AZ, the database will automatically failover to a standby instance in a different Availability Zone in the event of a failure, ensuring minimal downtime. Additionally, using an RDS Proxy instance will help to improve the performance and scalability of the database.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754804,
          "date": "Sat 24 Dec 2022 11:15",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Aurora PostgreSQL DB clusters don't support Aurora Replicas in different AWS Regions<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Replication.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749397,
          "date": "Mon 19 Dec 2022 03:50",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744308,
          "date": "Tue 13 Dec 2022 18:28",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer is B <br>it will ensure that the database is highly available by replicating the data to a secondary instance in a different Availability Zone. In the event of a failure, the secondary instance will automatically take over and continue servicing database requests without any data loss. Additionally, configuring an Amazon RDS Proxy instance for the database will help improve the availability and scalability of the database",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 727293,
          "date": "Sat 26 Nov 2022 06:57",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Why not A?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Here is why Option A is not the correct solution:<br><br>Option A: Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.<br><br>While this solution would provide high availability with minimum downtime, it would involve significant operational effort and may result in data loss. Placing the EC2 instances in different Regions would require significant infrastructure changes and could impact the performance of the application. Additionally, Aurora PostgreSQL Cross-Region Replication is designed to provide disaster recovery rather than high availability, and it may result in some data loss during the replication process.</li><li>maybe because of load balancer, diffrent region can't be answer.</li><li>\\\"The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones\\\". Why not A?</li><li>They need to be in the same Region</li><li>The question states multiple regions not multiple Availability Zones, a big difference!</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759112,
          "date": "Tue 27 Dec 2022 23:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Here is why Option A is not the correct solution:<br><br>Option A: Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect traffic. Use Aurora PostgreSQL Cross-Region Replication.<br><br>While this solution would provide high availability with minimum downtime, it would involve significant operational effort and may result in data loss. Placing the EC2 instances in different Regions would require significant infrastructure changes and could impact the performance of the application. Additionally, Aurora PostgreSQL Cross-Region Replication is designed to provide disaster recovery rather than high availability, and it may result in some data loss during the replication process.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727933,
          "date": "Sun 27 Nov 2022 06:38",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "maybe because of load balancer, diffrent region can't be answer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones\\\". Why not A?</li><li>They need to be in the same Region</li><li>The question states multiple regions not multiple Availability Zones, a big difference!</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 734085,
          "date": "Fri 02 Dec 2022 23:02",
          "username": "\t\t\t\tWZN\t\t\t",
          "content": "\\\"The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones\\\". Why not A?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>They need to be in the same Region</li><li>The question states multiple regions not multiple Availability Zones, a big difference!</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736664,
          "date": "Tue 06 Dec 2022 10:10",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "They need to be in the same Region",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 736862,
          "date": "Tue 06 Dec 2022 14:43",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "The question states multiple regions not multiple Availability Zones, a big difference!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723618,
          "date": "Mon 21 Nov 2022 15:33",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719620,
          "date": "Wed 16 Nov 2022 13:36",
          "username": "\t\t\t\tAck3rman\t\t\t",
          "content": "Important fact: EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions. So can't be D in case you are between B and D<br>https://aws.amazon.com/tr/ec2/autoscaling/faqs/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 715789,
          "date": "Fri 11 Nov 2022 07:25",
          "username": "\t\t\t\tAnji69659\t\t\t",
          "content": "MULTI-AZ FOR HIGH SCALABILITY .",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#70",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service.<br>The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#70",
          "answers": [
            {
              "choice": "<p>A. Enable HTTP health checks on the NLB, supplying the URL of the company's application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB.  Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697490,
          "date": "Mon 17 Oct 2022 16:56",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "I would choose A, as NLB supports HTTP and HTTPS Health Checks, BUT you can't put any URL (as proposed), only the node IP addresses. <br>So, the solution is C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>can you elaborate more pls</li><li>NLBs support HTTP, HTTPS and TCP health checks:<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html (check HealthCheckProtocol)<br><br>But NLBs only accept either selecting EC2 instances or IP addresses directly as targets. You can't provide a URL to your endpoints, only a health check path (if you're using HTTP or HTTPS health checks).</li></ul>",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 719626,
          "date": "Wed 16 Nov 2022 13:42",
          "username": "\t\t\t\tAck3rman\t\t\t",
          "content": "can you elaborate more pls<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>NLBs support HTTP, HTTPS and TCP health checks:<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html (check HealthCheckProtocol)<br><br>But NLBs only accept either selecting EC2 instances or IP addresses directly as targets. You can't provide a URL to your endpoints, only a health check path (if you're using HTTP or HTTPS health checks).</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 781067,
          "date": "Thu 19 Jan 2023 12:44",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "NLBs support HTTP, HTTPS and TCP health checks:<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html (check HealthCheckProtocol)<br><br>But NLBs only accept either selecting EC2 instances or IP addresses directly as targets. You can't provide a URL to your endpoints, only a health check path (if you're using HTTP or HTTPS health checks).",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 698197,
          "date": "Tue 18 Oct 2022 13:56",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C.  NLB works at Layer 4 so it does not support HTTP/HTTPS. The replacement for the ALB is the best choice.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's incorrect. NLB does support HTTP and HTTPS (and TCP) health checks.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html<br><br>There just isn't an answer option that reflects that. My guess is that the question and/or answer options are outdated.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 781069,
          "date": "Thu 19 Jan 2023 12:45",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "That's incorrect. NLB does support HTTP and HTTPS (and TCP) health checks.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html<br><br>There just isn't an answer option that reflects that. My guess is that the question and/or answer options are outdated.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 795996,
          "date": "Thu 02 Feb 2023 12:37",
          "username": "\t\t\t\tTony1980\t\t\t",
          "content": "Answer is C<br><br>A solution architect can use Amazon EC2 Auto Scaling health checks to automatically detect and replace unhealthy instances in the EC2 Auto Scaling group. The health checks can be configured to check the HTTP errors returned by the application and terminate the unhealthy instances. This will ensure that the application's availability is improved, without requiring custom scripts or code.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792971,
          "date": "Mon 30 Jan 2023 16:22",
          "username": "\t\t\t\taakashkumar1999\t\t\t",
          "content": "I will go with A as Network load balancer supports HTTP and HTTPS health checks, maybe the answer is outdated.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763681,
          "date": "Mon 02 Jan 2023 12:37",
          "username": "\t\t\t\tJohn_Zhuang\t\t\t",
          "content": "https://medium.com/awesome-cloud/aws-difference-between-application-load-balancer-and-network-load-balancer-cb8b6cd296a4<br>As NLB does not support HTTP health checks, you can only use ALB to do so.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's incorrect. NLB does support HTTP and HTTPS (and TCP) health checks.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html<br><br>Just a general tip: Medium is not a reliable resource. Anyone can create content there. Rely only on official AWS documentation.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 781070,
          "date": "Thu 19 Jan 2023 12:46",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "That's incorrect. NLB does support HTTP and HTTPS (and TCP) health checks.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html<br><br>Just a general tip: Medium is not a reliable resource. Anyone can create content there. Rely only on official AWS documentation.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 760169,
          "date": "Wed 28 Dec 2022 18:45",
          "username": "\t\t\t\tbenjl\t\t\t",
          "content": "Answer is C, and A is wrong because<br>In NLB, for HTTP or HTTPS health check requests, the host header contains the IP address of the load balancer node and the listener port, not the IP address of the target and the health check port.<br>https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 755651,
          "date": "Sun 25 Dec 2022 14:04",
          "username": "\t\t\t\tSilvestr\t\t\t",
          "content": "Correct answer - C<br>Network load balancers (Layer 4) allow to:<br>• Forward TCP & UDP traffic to your instances<br>• Handle millions of request per seconds<br>• Less latency ~100 ms (vs 400 ms for ALB)<br>Best choice for HTTP traffic - replace to Application load balancer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751610,
          "date": "Wed 21 Dec 2022 00:00",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The best option to meet the requirements is to enable HTTP health checks on the NLB by supplying the URL of the company's application. This will allow the NLB to automatically detect HTTP errors and take action, such as marking the target instance as unhealthy and routing traffic away from it.<br><br>Option A - Enable HTTP health checks on the NLB, supplying the URL of the company's application.<br>This is the correct solution as it allows the NLB to automatically detect HTTP errors and take action.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B - Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected, the application will restart.<br>This option involves writing custom scripts or code, which is not allowed by the requirements. Additionally, this solution may not be reliable or efficient, as it relies on checking the logs locally on each instance and may not catch all errors.<br><br>Option C - Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.<br>While this option may improve the availability of the application, it is not necessary to replace the NLB with an Application Load Balancer in order to enable HTTP health checks. The NLB can support HTTP health checks as well, and replacing it may involve additional effort and cost.</li><li>Option D - Create an Amazon CloudWatch alarm that monitors the UnhealthyHostCount metric for the NLB.  Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.<br>This option involves monitoring the UnhealthyHostCount metric, which only reflects the number of unhealthy targets that the NLB is currently routing traffic away from. It does not directly monitor the health of the application or detects HTTP errors. Additionally, this solution may not be sufficient to detect and respond to HTTP errors in a timely manner.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751613,
          "date": "Wed 21 Dec 2022 00:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B - Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected, the application will restart.<br>This option involves writing custom scripts or code, which is not allowed by the requirements. Additionally, this solution may not be reliable or efficient, as it relies on checking the logs locally on each instance and may not catch all errors.<br><br>Option C - Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.<br>While this option may improve the availability of the application, it is not necessary to replace the NLB with an Application Load Balancer in order to enable HTTP health checks. The NLB can support HTTP health checks as well, and replacing it may involve additional effort and cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D - Create an Amazon CloudWatch alarm that monitors the UnhealthyHostCount metric for the NLB.  Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.<br>This option involves monitoring the UnhealthyHostCount metric, which only reflects the number of unhealthy targets that the NLB is currently routing traffic away from. It does not directly monitor the health of the application or detects HTTP errors. Additionally, this solution may not be sufficient to detect and respond to HTTP errors in a timely manner.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751614,
          "date": "Wed 21 Dec 2022 00:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D - Create an Amazon CloudWatch alarm that monitors the UnhealthyHostCount metric for the NLB.  Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state.<br>This option involves monitoring the UnhealthyHostCount metric, which only reflects the number of unhealthy targets that the NLB is currently routing traffic away from. It does not directly monitor the health of the application or detects HTTP errors. Additionally, this solution may not be sufficient to detect and respond to HTTP errors in a timely manner.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749298,
          "date": "Mon 19 Dec 2022 00:36",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is very much a valid option as Autoscaling group can be configured to remove EC2 instances that fails http health check of NLB.  AWS NLB supports http based health check.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 726753,
          "date": "Fri 25 Nov 2022 13:38",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "A is the best option.<br>NLB support http healthcheck, so why do we need to move to ALB ?<br>moreover the sentence \\\"Configure an Auto Scaling action to replace unhealthy instances\\\" in C seems to be wrong, as auto scaling remove any unhealthy instance by default, you do not need to configure it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I would say A will not give you what you want. \\\"If you add a TLS listener to your Network Load Balancer, we perform a listener connectivity test.\\\" (https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html) So a check will be made to see that something is listening on port 443. What it will not check is the status of the application e.g. HTTP 200 OK. Now the Application Load Balancer HTTP health check using the URL of the company's application, will do this, so C is the correct answer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 736876,
          "date": "Tue 06 Dec 2022 14:57",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I would say A will not give you what you want. \\\"If you add a TLS listener to your Network Load Balancer, we perform a listener connectivity test.\\\" (https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.html) So a check will be made to see that something is listening on port 443. What it will not check is the status of the application e.g. HTTP 200 OK. Now the Application Load Balancer HTTP health check using the URL of the company's application, will do this, so C is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723619,
          "date": "Mon 21 Nov 2022 15:34",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713057,
          "date": "Mon 07 Nov 2022 14:12",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "C is the correct!<br>NLB does not handle HTTP (layer 7) listerns errors only TCP (layer 4) listeners.<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 710929,
          "date": "Fri 04 Nov 2022 07:09",
          "username": "\t\t\t\tSolarch\t\t\t",
          "content": "Answer is A<br>NLB is ideal for TPC and UDP Traffic and checks operating in layer 4. <br>ALB- Supports HTTP and HTTPs traffics. Hence the ELB needs to be changed from NLB to ALB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 700070,
          "date": "Thu 20 Oct 2022 17:16",
          "username": "\t\t\t\tAman54\t\t\t",
          "content": "NLB supports HTTP health checks, they are part of the target group and the setting is the same for ALB and NLB HTTP/HTTPS health checks.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A is incorrect. NLB cannot detect http errors. Adding health check only detects the healthiness of the instances, not http errors.</li><li>\\\"The company needs to improve the application's availability\\\"<br>Answer A does not address this. The auto scaling group in answer C does.</li><li>NLB is already configured with a target group supported by EC2 ASG \\\"NLB's target group is configured to use an Amazon EC2 Auto Scaling group\\\". NLB need to be configured to use http health check. Hence A</li><li>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html<br><br>Note<br>Unlike a Classic Load Balancer or an Application Load Balancer, a Network Load Balancer can't have application layer (layer 7) HTTP or HTTPS listeners. It only supports transport layer (layer 4) TCP listeners. HTTP and HTTPS traffic can be routed to your environment over TCP.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 726910,
          "date": "Fri 25 Nov 2022 16:51",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "A is incorrect. NLB cannot detect http errors. Adding health check only detects the healthiness of the instances, not http errors.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 704805,
          "date": "Wed 26 Oct 2022 17:21",
          "username": "\t\t\t\toxfordcommaa\t\t\t",
          "content": "\\\"The company needs to improve the application's availability\\\"<br>Answer A does not address this. The auto scaling group in answer C does.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>NLB is already configured with a target group supported by EC2 ASG \\\"NLB's target group is configured to use an Amazon EC2 Auto Scaling group\\\". NLB need to be configured to use http health check. Hence A</li><li>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html<br><br>Note<br>Unlike a Classic Load Balancer or an Application Load Balancer, a Network Load Balancer can't have application layer (layer 7) HTTP or HTTPS listeners. It only supports transport layer (layer 4) TCP listeners. HTTP and HTTPS traffic can be routed to your environment over TCP.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709910,
          "date": "Wed 02 Nov 2022 16:53",
          "username": "\t\t\t\tMaharaja\t\t\t",
          "content": "NLB is already configured with a target group supported by EC2 ASG \\\"NLB's target group is configured to use an Amazon EC2 Auto Scaling group\\\". NLB need to be configured to use http health check. Hence A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html<br><br>Note<br>Unlike a Classic Load Balancer or an Application Load Balancer, a Network Load Balancer can't have application layer (layer 7) HTTP or HTTPS listeners. It only supports transport layer (layer 4) TCP listeners. HTTP and HTTPS traffic can be routed to your environment over TCP.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 741212,
          "date": "Sat 10 Dec 2022 20:53",
          "username": "\t\t\t\twh1t4k3r\t\t\t",
          "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-nlb.html<br><br>Note<br>Unlike a Classic Load Balancer or an Application Load Balancer, a Network Load Balancer can't have application layer (layer 7) HTTP or HTTPS listeners. It only supports transport layer (layer 4) TCP listeners. HTTP and HTTPS traffic can be routed to your environment over TCP.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#71",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.<br>What should the solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#71",
          "answers": [
            {
              "choice": "<p>A. Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697522,
          "date": "Mon 17 Oct 2022 17:37",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A - DynamoDB global tables provides multi-Region, and multi-active database, but it not valid \\\"in case of data corruption\\\". In this case, you need a backup. This solutions isn't valid.<br>**B** - Point in Time Recovery is designed as a continuous backup juts to recover it fast. It covers perfectly the RPO, and probably the RTO.https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html<br>C - A daily export will not cover the RPO of 15min.<br>D - DynamoDB is serverless... so what are these EBS snapshots taken from???<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes, it is possible to take EBS snapshots of a DynamoDB table. The process for doing this involves the following steps:<br><br>Create a new Amazon Elastic Block Store (EBS) volume from the DynamoDB table.<br><br>Stop the DynamoDB service on the instance.<br><br>Detach the EBS volume from the instance.<br><br>Create a snapshot of the EBS volume.<br><br>Reattach the EBS volume to the instance.<br><br>Start the DynamoDB service on the instance.<br><br>You can also use AWS Data pipeline to automate the above process and schedule regular snapshots of your DynamoDB table.<br><br>Note that, if your table is large and you want to take a snapshot of it, it could take a long time and consume a lot of bandwidth, so it's recommended to use the Global Tables feature from DynamoDB in order to have a Multi-region and Multi-master DynamoDB table, and you can snapshot each region separately.</li></ul>",
          "upvote_count": "28",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 789421,
          "date": "Fri 27 Jan 2023 09:20",
          "username": "\t\t\t\tLionelSid\t\t\t",
          "content": "Yes, it is possible to take EBS snapshots of a DynamoDB table. The process for doing this involves the following steps:<br><br>Create a new Amazon Elastic Block Store (EBS) volume from the DynamoDB table.<br><br>Stop the DynamoDB service on the instance.<br><br>Detach the EBS volume from the instance.<br><br>Create a snapshot of the EBS volume.<br><br>Reattach the EBS volume to the instance.<br><br>Start the DynamoDB service on the instance.<br><br>You can also use AWS Data pipeline to automate the above process and schedule regular snapshots of your DynamoDB table.<br><br>Note that, if your table is large and you want to take a snapshot of it, it could take a long time and consume a lot of bandwidth, so it's recommended to use the Global Tables feature from DynamoDB in order to have a Multi-region and Multi-master DynamoDB table, and you can snapshot each region separately.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751623,
          "date": "Wed 21 Dec 2022 00:11",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The best solution to meet the RPO and RTO requirements would be to use DynamoDB point-in-time recovery (PITR). This feature allows you to restore your DynamoDB table to any point in time within the last 35 days, with a granularity of seconds. To recover data within a 15-minute RPO, you would simply restore the table to the desired point in time within the last 35 days.<br><br>To meet the RTO requirement of 1 hour, you can use the DynamoDB console, AWS CLI, or the AWS SDKs to enable PITR on your table. Once enabled, PITR continuously captures point-in-time copies of your table data in an S3 bucket. You can then use these point-in-time copies to restore your table to any point in time within the retention period.<br><br>***CORRECT***<br>Option B.  Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A (configuring DynamoDB global tables) would not meet the RPO requirement, as global tables are designed to replicate data to multiple regions for high availability, but they do not provide a way to restore data to a specific point in time.<br><br>Option C (exporting data to S3 Glacier) would not meet the RPO or RTO requirements, as S3 Glacier is a cold storage service with a retrieval time of several hours.<br><br>Option D (scheduling EBS snapshots) would not meet the RPO requirement, as EBS snapshots are taken on a schedule, rather than continuously. Additionally, restoring a DynamoDB table from an EBS snapshot can take longer than 1 hour, so it would not meet the RTO requirement.</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751624,
          "date": "Wed 21 Dec 2022 00:12",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A (configuring DynamoDB global tables) would not meet the RPO requirement, as global tables are designed to replicate data to multiple regions for high availability, but they do not provide a way to restore data to a specific point in time.<br><br>Option C (exporting data to S3 Glacier) would not meet the RPO or RTO requirements, as S3 Glacier is a cold storage service with a retrieval time of several hours.<br><br>Option D (scheduling EBS snapshots) would not meet the RPO requirement, as EBS snapshots are taken on a schedule, rather than continuously. Additionally, restoring a DynamoDB table from an EBS snapshot can take longer than 1 hour, so it would not meet the RTO requirement.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749344,
          "date": "Mon 19 Dec 2022 02:47",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 744326,
          "date": "Tue 13 Dec 2022 18:43",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B is correct<br>DynamoDB point-in-time recovery allows the solutions architect to recover the DynamoDB table to a specific point in time, which would meet the RPO of 15 minutes. This feature also provides an RTO of 1 hour, which is the desired recovery time objective for the application. Additionally, configuring DynamoDB point-in-time recovery does not require any additional infrastructure or operational effort, making it the best solution for this scenario.<br>Option D is not correct because scheduling Amazon EBS snapshots for the DynamoDB table every 15 minutes would not meet the RPO or RTO requirements. While EBS snapshots can be used to recover data from a DynamoDB table, they are not designed to provide real-time data protection or recovery capabilities",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723648,
          "date": "Mon 21 Nov 2022 15:55",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707105,
          "date": "Sat 29 Oct 2022 11:28",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 700582,
          "date": "Fri 21 Oct 2022 08:56",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "I think DynamoDB global tables also work here, but Point in Time Recovery is a better choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 698572,
          "date": "Wed 19 Oct 2022 02:14",
          "username": "\t\t\t\tKikiokiki\t\t\t",
          "content": "I THINK B.  <br> https://dynobase.dev/dynamodb-point-in-time-recovery/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696237,
          "date": "Sun 16 Oct 2022 14:37",
          "username": "\t\t\t\tpriya2224\t\t\t",
          "content": "answer is D<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>bhk gandu chutiye glt ans btata hai</li><li>Try communicate in English for audience</li><li>DynamoDB is serverless, so no storage snapshots available. https://aws.amazon.com/dynamodb/</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698867,
          "date": "Wed 19 Oct 2022 11:15",
          "username": "\t\t\t\t[Removed]\t\t\t",
          "content": "bhk gandu chutiye glt ans btata hai<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Try communicate in English for audience</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 711745,
          "date": "Sat 05 Nov 2022 13:33",
          "username": "\t\t\t\tAz900500\t\t\t",
          "content": "Try communicate in English for audience",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 697524,
          "date": "Mon 17 Oct 2022 17:40",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "DynamoDB is serverless, so no storage snapshots available. https://aws.amazon.com/dynamodb/",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#72",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs.<br>How can the solutions architect meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#72",
          "answers": [
            {
              "choice": "<p>A. Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696260,
          "date": "Sun 16 Oct 2022 15:32",
          "username": "\t\t\t\tKVK16\t\t\t",
          "content": "To reduce costs get rid of NAT Gateway , VPC endpoint to S3",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751627,
          "date": "Wed 21 Dec 2022 00:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The correct answer is Option D.  Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets.<br><br>By deploying an S3 VPC gateway endpoint, the application can access the S3 buckets over a private network connection within the VPC, eliminating the need for data transfer over the internet. This can help reduce data transfer fees as well as improve the performance of the application. The endpoint policy can be used to specify which S3 buckets the application has access to.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A, deploying Amazon API Gateway into a public subnet and adjusting the route table, would not address the issue of data transfer fees as the application would still be transferring data over the internet. <br><br>Option B, deploying a NAT gateway into a public subnet and attaching an endpoint policy, would not address the issue of data transfer fees either as the NAT gateway is used to enable outbound internet access for instances in a private subnet, rather than for connecting to S3. <br><br>Option C, deploying the application into a public subnet and allowing it to route through an internet gateway, would not reduce data transfer fees as the application would still be transferring data over the internet.</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 751628,
          "date": "Wed 21 Dec 2022 00:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, deploying Amazon API Gateway into a public subnet and adjusting the route table, would not address the issue of data transfer fees as the application would still be transferring data over the internet. <br><br>Option B, deploying a NAT gateway into a public subnet and attaching an endpoint policy, would not address the issue of data transfer fees either as the NAT gateway is used to enable outbound internet access for instances in a private subnet, rather than for connecting to S3. <br><br>Option C, deploying the application into a public subnet and allowing it to route through an internet gateway, would not reduce data transfer fees as the application would still be transferring data over the internet.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 794102,
          "date": "Tue 31 Jan 2023 13:12",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "To answer this question, I need to know the comparison of the types of gateway of costs, please give me a tip about that issue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749347,
          "date": "Mon 19 Dec 2022 02:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741854,
          "date": "Sun 11 Dec 2022 16:46",
          "username": "\t\t\t\t9014\t\t\t",
          "content": "The answer is D:- Actually, the Application (EC2) is running in the same region...instead of going to the internet, data can be copied through the VPC endpoint...so there will be no cost because data is not leaving the AWS infra",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 737556,
          "date": "Wed 07 Dec 2022 09:03",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Can somebody please explain this question? Are we assuming the application is running in AWS and that adding the gateway endpoint avoids the need for the EC2 instance to access the internet and thus avoid costs? Thanks a lot.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes correct</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 738897,
          "date": "Thu 08 Dec 2022 11:37",
          "username": "\t\t\t\tSR0611\t\t\t",
          "content": "Yes correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723649,
          "date": "Mon 21 Nov 2022 15:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701580,
          "date": "Sat 22 Oct 2022 16:30",
          "username": "\t\t\t\tyd_h\t\t\t",
          "content": "Selected Answer: D<br>FYI : <br>-There is no additional charge for using gateway endpoints.<br>-Interface endpoints are priced at ~ $0.01/per AZ/per hour. Cost depends on the Region<br>- S3 Interface Endpoints resolve to private VPC IP addresses and are routable from outside the VPC (e.g via VPN, Direct Connect, Transit Gateway, etc). S3 Gateway Endpoints use public IP ranges and are only routable from resources within the VPC. ",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 697537,
          "date": "Mon 17 Oct 2022 17:46",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Close question to the Question #4, with same solution.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#73",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC.  A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access.<br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#73",
          "answers": [
            {
              "choice": "<p>A. Replace the current security group of the bastion host with one that only allows inbound access from the application instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703268,
          "date": "Mon 24 Oct 2022 20:18",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "C because from on-prem network to bastion through internet (using on-prem resource's public IP),<br>D because bastion and ec2 is in same VPC, meaning bastion can communicate to EC2 via it's private IP address",
          "upvote_count": "17",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 822303,
          "date": "Sun 26 Feb 2023 12:31",
          "username": "\t\t\t\tSpiffaz\t\t\t",
          "content": "Why external and not internal?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Because the traffic goes through the public internet. In the public internet, public IP(external IP) is used.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 831584,
          "date": "Tue 07 Mar 2023 06:35",
          "username": "\t\t\t\tTariqKipkemei\t\t\t",
          "content": "Because the traffic goes through the public internet. In the public internet, public IP(external IP) is used.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 812926,
          "date": "Sat 18 Feb 2023 13:04",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "Application is in private subnet <br>Bastion Host is in public subnet <br><br>D does not make sense because the bastion host is in public subnet and they don't have a private IP but only a public IP address attached to them. The IP wanting to connect is Public as well. <br><br>Bastion host in public subnet allows external IP (via internet) of the company to access it. Which than leaves us to give permission to the application private subnet and for that the private subnet with the application accepts the IP coming from Bastion Host by changing its SG. C&E<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Bastion host in public subnet because it has a public IP and a NAT Gateway that can route traffic out of your AWS VPC but it does have the ability to access the private subnet using private IP since it's not leaving AWS to access the private subnet. So C&amp;D are the right answers.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 823191,
          "date": "Mon 27 Feb 2023 05:41",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "Bastion host in public subnet because it has a public IP and a NAT Gateway that can route traffic out of your AWS VPC but it does have the ability to access the private subnet using private IP since it's not leaving AWS to access the private subnet. So C&D are the right answers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773823,
          "date": "Thu 12 Jan 2023 20:24",
          "username": "\t\t\t\tswolfgang\t\t\t",
          "content": "I dont understand why not CE . Because question ask through internet connection to servers and bostion host.I understand they want to access both of from publıc. I mean not from the servers to bastion host.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 754859,
          "date": "Sat 24 Dec 2022 13:36",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/51356-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 751641,
          "date": "Wed 21 Dec 2022 00:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements, the solutions architect should take the following steps:<br>C.  Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company. This will allow the solutions architect to connect to the bastion host from the company's on-premises network through the internet connection.<br>E.  Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host. This will allow the solutions architect to connect to the application instances through the bastion host using SSH.<br><br>Note: It's important to ensure that the security groups for the bastion host and application instances are configured correctly to allow the desired inbound traffic, while still protecting the instances from unwanted access.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Here is why the other options are not correct:<br>A.  Replacing the current security group of the bastion host with one that only allows inbound access from the application instances would not allow the solutions architect to connect to the bastion host from the company's on-premises network through the internet connection. The bastion host needs to be accessible from the external network in order to allow the solutions architect to connect to it.<br>B.  Replacing the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company would not allow the solutions architect to connect to the bastion host from the company's on-premises network through the internet connection. The internal IP range is not accessible from the external network.</li><li>D.  Replacing the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host would not allow the solutions architect to connect to the application instances through the bastion host using SSH. The private IP address of the bastion host is not accessible from the external network, so the solutions architect would not be able to connect to it from the on-premises network.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 751644,
          "date": "Wed 21 Dec 2022 00:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Here is why the other options are not correct:<br>A.  Replacing the current security group of the bastion host with one that only allows inbound access from the application instances would not allow the solutions architect to connect to the bastion host from the company's on-premises network through the internet connection. The bastion host needs to be accessible from the external network in order to allow the solutions architect to connect to it.<br>B.  Replacing the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company would not allow the solutions architect to connect to the bastion host from the company's on-premises network through the internet connection. The internal IP range is not accessible from the external network.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D.  Replacing the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host would not allow the solutions architect to connect to the application instances through the bastion host using SSH. The private IP address of the bastion host is not accessible from the external network, so the solutions architect would not be able to connect to it from the on-premises network.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751645,
          "date": "Wed 21 Dec 2022 00:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "D.  Replacing the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host would not allow the solutions architect to connect to the application instances through the bastion host using SSH. The private IP address of the bastion host is not accessible from the external network, so the solutions architect would not be able to connect to it from the on-premises network.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749349,
          "date": "Mon 19 Dec 2022 02:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 723654,
          "date": "Mon 21 Nov 2022 15:57",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705729,
          "date": "Thu 27 Oct 2022 18:22",
          "username": "\t\t\t\tgcmrjbr\t\t\t",
          "content": "CD is Ok.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 699316,
          "date": "Wed 19 Oct 2022 21:49",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "why C? External?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Because the IP address exposed to the Bastian host will be the external not the internal IP address. Google \\\"whats my ip\\\" and you will see your IP address on the internet is NOT your internal IP.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 737594,
          "date": "Wed 07 Dec 2022 09:38",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Because the IP address exposed to the Bastian host will be the external not the internal IP address. Google \\\"whats my ip\\\" and you will see your IP address on the internet is NOT your internal IP.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 698218,
          "date": "Tue 18 Oct 2022 14:10",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C (allow access just from the external IP) and D (allow inbound SSH from the private IP of the bastion host).",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 696512,
          "date": "Sun 16 Oct 2022 22:11",
          "username": "\t\t\t\tninjawrz\t\t\t",
          "content": "CD is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        }
      ]
    },
    {
      "question_id": "#74",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company.<br>How should security groups be configured in this situation? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#74",
          "answers": [
            {
              "choice": "<p>A. Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 710799,
          "date": "Thu 03 Nov 2022 22:21",
          "username": "\t\t\t\tAthena\t\t\t",
          "content": "Web Server Rules: Inbound traffic from 443 (HTTPS) Source 0.0.0.0/0 - Allows inbound HTTPS access from any IPv4 address<br>Database Rules : 1433 (MS SQL)The default port to access a Microsoft SQL Server database, for example, on an Amazon RDS instancehttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 698241,
          "date": "Tue 18 Oct 2022 14:41",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "EC2 web on public subnets + EC2 SQL on private subnet + security is high priority. So, Option A to allow HTTPS from everywhere. Plus option C to allow SQL connection from the web instance.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 823197,
          "date": "Mon 27 Feb 2023 05:49",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "A & C are the correct answer. <br><br>Inbound traffic to the web tier on port 443 (HTTPS) <br>The web tier will then access the Database tier on port 1433 - inbound.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769997,
          "date": "Mon 09 Jan 2023 03:51",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "AC 443-http inbound and 1433-sql server<br>Security group => focus on inbound traffic since by default outboud traffic is allowed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 768558,
          "date": "Sat 07 Jan 2023 13:48",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Security group => focus on inbound traffic since by default outboud traffic is allowed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 754251,
          "date": "Fri 23 Dec 2022 14:31",
          "username": "\t\t\t\torionizzie\t\t\t",
          "content": "why both are inbound rules",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751670,
          "date": "Wed 21 Dec 2022 00:54",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The correct answers are C and E. <br><br>For security purposes, it is best practice to limit inbound and outbound traffic as much as possible. In this case, the web tier should only be able to access the database tier and not the other way around. Therefore, the security group for the web tier should only allow outbound traffic to the security group for the database tier on the necessary ports. Similarly, the security group for the database tier should only allow inbound traffic from the security group for the web tier on the necessary ports.<br><br>Answer C: Configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier. This is correct because the web tier needs to be able to connect to the database on port 1433 in order to access the data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This is WRONG. Browse to a website and type :443 at the end of it. IT will translate to HTTPS. HTTPS = 443. <br><br>answers are A and C</li><li>Answer E: Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier. This is correct because the web tier needs to be able to connect to the database on both port 443 and 1433 in order to access the data.<br><br>***WRONG***<br>Answer A: Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. This is not correct because the web tier should not allow inbound traffic from the internet. Instead, it should only allow outbound traffic to the security group for the database tier.</li><li>***WRONG***<br>Answer B: Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0. This is not correct because the web tier should not allow outbound traffic to the internet. Instead, it should only allow outbound traffic to the security group for the database tier.<br><br>Answer D: Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier. This is not correct because the database tier should not allow outbound traffic to the web tier. Instead, it should only allow inbound traffic from the security group for the web tier on the necessary ports.</li><li>Chatgpt is unreliable this answer from same.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 757915,
          "date": "Mon 26 Dec 2022 23:58",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "This is WRONG. Browse to a website and type :443 at the end of it. IT will translate to HTTPS. HTTPS = 443. <br><br>answers are A and C",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 751671,
          "date": "Wed 21 Dec 2022 00:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Answer E: Configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier. This is correct because the web tier needs to be able to connect to the database on both port 443 and 1433 in order to access the data.<br><br>***WRONG***<br>Answer A: Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0. This is not correct because the web tier should not allow inbound traffic from the internet. Instead, it should only allow outbound traffic to the security group for the database tier.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Answer B: Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0. This is not correct because the web tier should not allow outbound traffic to the internet. Instead, it should only allow outbound traffic to the security group for the database tier.<br><br>Answer D: Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier. This is not correct because the database tier should not allow outbound traffic to the web tier. Instead, it should only allow inbound traffic from the security group for the web tier on the necessary ports.</li><li>Chatgpt is unreliable this answer from same.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751672,
          "date": "Wed 21 Dec 2022 00:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Answer B: Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0. This is not correct because the web tier should not allow outbound traffic to the internet. Instead, it should only allow outbound traffic to the security group for the database tier.<br><br>Answer D: Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier. This is not correct because the database tier should not allow outbound traffic to the web tier. Instead, it should only allow inbound traffic from the security group for the web tier on the necessary ports.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Chatgpt is unreliable this answer from same.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769996,
          "date": "Mon 09 Jan 2023 03:49",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Chatgpt is unreliable this answer from same.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749352,
          "date": "Mon 19 Dec 2022 02:53",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 723656,
          "date": "Mon 21 Nov 2022 15:58",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705727,
          "date": "Thu 27 Oct 2022 18:20",
          "username": "\t\t\t\tgcmrjbr\t\t\t",
          "content": "Agree with AC. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 693639,
          "date": "Thu 13 Oct 2022 07:46",
          "username": "\t\t\t\tsrcshekhar\t\t\t",
          "content": "Very good questions",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#75",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application.<br>Which solution meets these requirements and is the MOST operationally efficient?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#75",
          "answers": [
            {
              "choice": "<p>A. Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 705733,
          "date": "Thu 27 Oct 2022 18:31",
          "username": "\t\t\t\tgcmrjbr\t\t\t",
          "content": "Agree with A>>> Lambda = serverless + autoscale (modernize), SQS= decouple (no more drops)",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 794863,
          "date": "Wed 01 Feb 2023 06:17",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "Must be D. <br>A is incorrect. Even though lambda could auto scale, SQS communication between tires is not addressing drop in transaction per se as SQS would allow to read (say serially with FIFO or NOT) in a controlled way, your application code determines how many threads are being spanned to process those messages. This could still overload the tier.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 793685,
          "date": "Tue 31 Jan 2023 06:26",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "The catch phrase is \\\"scale up when communication failures are detected\\\" Scaling should not be based on communication failures, that'll be crying over spilled milk ! or rather too late. So D is wrong.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>it says \\\"one tier becomes overloaded\\\" , Not communication failure...</li><li>D says: \\\"Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected\\\".</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 794859,
          "date": "Wed 01 Feb 2023 06:10",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "it says \\\"one tier becomes overloaded\\\" , Not communication failure...<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D says: \\\"Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected\\\".</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801310,
          "date": "Tue 07 Feb 2023 20:01",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "D says: \\\"Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781554,
          "date": "Thu 19 Jan 2023 20:50",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "D.  Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected. This solution allows for horizontal scaling of the application servers and allows for automatic scaling based on communication failures, which can help prevent transactions from being dropped when one tier becomes overloaded. It also provides a more modern and operationally efficient way to handle communication between application services compared to traditional RESTful services.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 770518,
          "date": "Mon 09 Jan 2023 15:29",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "Can be A only. Other 3 answers use CloudWatch, which does not make sense for this question.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 769998,
          "date": "Mon 09 Jan 2023 03:53",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Server less and de couple.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763396,
          "date": "Mon 02 Jan 2023 00:49",
          "username": "\t\t\t\tParsons\t\t\t",
          "content": "Serverless (Lambda) + Decouple (SQS) is a modernized application.<br>The flow looks like this:API Gateway --> SQS (act as decouple) -> Lambda functions (act as subscriber pull msg from the queue to process)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749356,
          "date": "Mon 19 Dec 2022 02:56",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741637,
          "date": "Sun 11 Dec 2022 12:47",
          "username": "\t\t\t\tbenaws\t\t\t",
          "content": "EC2 is not modern...<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>lmao...</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763691,
          "date": "Mon 02 Jan 2023 13:02",
          "username": "\t\t\t\tJohn_Zhuang\t\t\t",
          "content": "lmao...",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723658,
          "date": "Mon 21 Nov 2022 15:59",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718755,
          "date": "Tue 15 Nov 2022 14:39",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "https://serverlessland.com/patterns/apigw-http-sqs-lambda-sls",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 701827,
          "date": "Sun 23 Oct 2022 02:21",
          "username": "\t\t\t\tBoboChow\t\t\t",
          "content": "Serverless + decouple",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 700780,
          "date": "Fri 21 Oct 2022 13:17",
          "username": "\t\t\t\tcark0728\t\t\t",
          "content": "A가 올바른 정답이다",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#76",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive.<br>Which solution offers the MOST reliable data transfer?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#76",
          "answers": [
            {
              "choice": "<p>A. AWS DataSync over public internet<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. AWS DataSync over AWS Direct Connect<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. AWS Database Migration Service (AWS DMS) over public internet<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. AWS Database Migration Service (AWS DMS) over AWS Direct Connect<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698245,
          "date": "Tue 18 Oct 2022 14:44",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "DMS is for databases and here refers to “JSON files”. Public internet is not reliable. So best option is B. ",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751801,
          "date": "Wed 21 Dec 2022 03:38",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The most reliable solution for transferring the data in a secure manner would be option B: AWS DataSync over AWS Direct Connect.<br><br>AWS DataSync is a data transfer service that uses network optimization techniques to transfer data efficiently and securely between on-premises storage systems and Amazon S3 or other storage targets. When used over AWS Direct Connect, DataSync can provide a dedicated and secure network connection between your on-premises data center and AWS. This can help to ensure a more reliable and secure data transfer compared to using the public internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A, AWS DataSync over the public internet, is not as reliable as using Direct Connect, as it can be subject to potential network issues or congestion. <br><br>Option C, AWS Database Migration Service (DMS) over the public internet, is not a suitable solution for transferring large amounts of data, as it is designed for migrating databases rather than transferring large amounts of data from a storage area network (SAN). <br><br>Option D, AWS DMS over AWS Direct Connect, is also not a suitable solution, as it is designed for migrating databases and may not be efficient for transferring large amounts of data from a SAN.</li><li>explanation about D option is good</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751802,
          "date": "Wed 21 Dec 2022 03:38",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, AWS DataSync over the public internet, is not as reliable as using Direct Connect, as it can be subject to potential network issues or congestion. <br><br>Option C, AWS Database Migration Service (DMS) over the public internet, is not a suitable solution for transferring large amounts of data, as it is designed for migrating databases rather than transferring large amounts of data from a storage area network (SAN). <br><br>Option D, AWS DMS over AWS Direct Connect, is also not a suitable solution, as it is designed for migrating databases and may not be efficient for transferring large amounts of data from a SAN.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>explanation about D option is good</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 789484,
          "date": "Fri 27 Jan 2023 10:46",
          "username": "\t\t\t\tdoorahmie\t\t\t",
          "content": "explanation about D option is good",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749357,
          "date": "Mon 19 Dec 2022 02:58",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747697,
          "date": "Sat 17 Dec 2022 01:30",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B.  DMS is not needed as there is no Database migration requirement.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 728877,
          "date": "Mon 28 Nov 2022 09:21",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Public internet options automatically out being best-effort. DMS is for database migration service and here they have to just transfer the data to S3. Hence B. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723661,
          "date": "Mon 21 Nov 2022 16:00",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 701610,
          "date": "Sat 22 Oct 2022 17:21",
          "username": "\t\t\t\tyd_h\t\t\t",
          "content": "B<br><br>- A SAN presents storage devices to a host such that the storage appears to be locally attached. ( NFS is, or can be, a SAN- https://serverfault.com/questions/499185/is-san-storage-better-than-nfs )<br>- AWS Direct Connect does not encrypt your traffic that is in transit by default. But the connection is private (https://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html)",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#77",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#77",
          "answers": [
            {
              "choice": "<p>A. Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697556,
          "date": "Mon 17 Oct 2022 18:38",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "(A) - You don't need to deploy an EC2 instance to host an API - Operational overhead<br>(B) - Same as A<br>(**C**) - Is the answer<br>(D) - AWS Glue gets data from S3, not from API GW. AWS Glue could do ETL by itself, so don't need lambda. Non sense. https://aws.amazon.com/glue/",
          "upvote_count": "26",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 783885,
          "date": "Sun 22 Jan 2023 03:17",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "option C is the best solution. It uses Amazon Kinesis Data Firehose which is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3. This service requires less operational overhead as compared to option A, B, and D.  Additionally, it also uses Amazon API Gateway which is a fully managed service for creating, deploying, and managing APIs. These services help in reducing the operational overhead and automating the data ingestion process.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751804,
          "date": "Wed 21 Dec 2022 05:39",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C is the solution that meets the requirements with the least operational overhead.<br><br>In Option C, you can use Amazon API Gateway as a fully managed service to create, publish, maintain, monitor, and secure APIs. This means that you don't have to worry about the operational overhead of deploying and maintaining an EC2 instance to host the API.<br><br>Option C also uses Amazon Kinesis Data Firehose, which is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3. With Kinesis Data Firehose, you don't have to worry about the operational overhead of setting up and maintaining a data ingestion infrastructure.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Finally, Option C uses AWS Lambda, which is a fully managed service for running code in response to events. With AWS Lambda, you don't have to worry about the operational overhead of setting up and maintaining a server to run the data transformation code.<br><br>Overall, Option C provides a fully managed solution for real-time data ingestion with minimal operational overhead.</li><li>Option A is incorrect because it involves deploying an EC2 instance to host an API, which adds operational overhead in the form of maintaining and securing the instance.<br><br>Option B is incorrect because it involves deploying an EC2 instance to host an API and disabling source/destination checking on the instance. Disabling source/destination checking can make the instance vulnerable to attacks, which adds operational overhead in the form of securing the instance.</li><li>Option D is incorrect because it involves using AWS Glue to send the data to Amazon S3, which adds operational overhead in the form of maintaining and securing the AWS Glue infrastructure.<br><br>Overall, Option C is the best choice because it uses fully managed services for the API, data transformation, and data delivery, which minimizes operational overhead.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751805,
          "date": "Wed 21 Dec 2022 03:45",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Finally, Option C uses AWS Lambda, which is a fully managed service for running code in response to events. With AWS Lambda, you don't have to worry about the operational overhead of setting up and maintaining a server to run the data transformation code.<br><br>Overall, Option C provides a fully managed solution for real-time data ingestion with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it involves deploying an EC2 instance to host an API, which adds operational overhead in the form of maintaining and securing the instance.<br><br>Option B is incorrect because it involves deploying an EC2 instance to host an API and disabling source/destination checking on the instance. Disabling source/destination checking can make the instance vulnerable to attacks, which adds operational overhead in the form of securing the instance.</li><li>Option D is incorrect because it involves using AWS Glue to send the data to Amazon S3, which adds operational overhead in the form of maintaining and securing the AWS Glue infrastructure.<br><br>Overall, Option C is the best choice because it uses fully managed services for the API, data transformation, and data delivery, which minimizes operational overhead.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751806,
          "date": "Wed 21 Dec 2022 03:46",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because it involves deploying an EC2 instance to host an API, which adds operational overhead in the form of maintaining and securing the instance.<br><br>Option B is incorrect because it involves deploying an EC2 instance to host an API and disabling source/destination checking on the instance. Disabling source/destination checking can make the instance vulnerable to attacks, which adds operational overhead in the form of securing the instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D is incorrect because it involves using AWS Glue to send the data to Amazon S3, which adds operational overhead in the form of maintaining and securing the AWS Glue infrastructure.<br><br>Overall, Option C is the best choice because it uses fully managed services for the API, data transformation, and data delivery, which minimizes operational overhead.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751807,
          "date": "Wed 21 Dec 2022 03:46",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D is incorrect because it involves using AWS Glue to send the data to Amazon S3, which adds operational overhead in the form of maintaining and securing the AWS Glue infrastructure.<br><br>Overall, Option C is the best choice because it uses fully managed services for the API, data transformation, and data delivery, which minimizes operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749359,
          "date": "Mon 19 Dec 2022 02:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 747699,
          "date": "Sat 17 Dec 2022 01:33",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723663,
          "date": "Mon 21 Nov 2022 16:01",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 703977,
          "date": "Tue 25 Oct 2022 17:09",
          "username": "\t\t\t\tCristian93\t\t\t",
          "content": "C is correct answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#78",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years.<br>What is the MOST operationally efficient solution that meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#78",
          "answers": [
            {
              "choice": "<p>A. Use DynamoDB point-in-time recovery to back up the table continuously.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Backup to create backup schedules and retention policies for the table.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and to store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697564,
          "date": "Mon 17 Oct 2022 18:48",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Answer is B<br>\\\"Amazon DynamoDB offers two types of backups: point-in-time recovery (PITR) and on-demand backups. (==> D is not the answer)<br>PITR is used to recover your table to any point in time in a rolling 35 day window, which is used to help customers mitigate accidental deletes or writes to their tables from bad code, malicious access, or user error. (==> A isn't the answer)<br>On demand backups are designed for long-term archiving and retention, which is typically used to help customers meet compliance and regulatory requirements.<br>This is the second of a series of two blog posts about using AWS Backup to set up scheduled on-demand backups for Amazon DynamoDB.  Part 1 presents the steps to set up a scheduled backup for DynamoDB tables from the AWS Management Console.\\\" (==> Not the DynamoBD console and C isn't the answer either)<br>https://aws.amazon.com/blogs/database/part-2-set-up-scheduled-backups-for-amazon-dynamodb-using-aws-backup/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think the answer is C because of storage time.</li></ul>",
          "upvote_count": "29",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 783724,
          "date": "Sat 21 Jan 2023 21:45",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "I think the answer is C because of storage time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776506,
          "date": "Sun 15 Jan 2023 13:07",
          "username": "\t\t\t\tJiggs007\t\t\t",
          "content": "C is correct because wehave to store data in s3 and an S3 Lifecycle configuration for the S3 bucket for 7 year.and its used on-demand backup of the table by using the DynamoDB console because If you need to store backups of your data for longer than 35 days, you can use on-demand backup. On-demand provides you a fully consistent snapshot of your table data and stay around forever (even after the table is deleted).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think you are correct</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 783725,
          "date": "Sat 21 Jan 2023 21:45",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "I think you are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769549,
          "date": "Sun 08 Jan 2023 16:03",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "B.  Use AWS Backup to create backup schedules and retention policies for the table.<br><br>AWS Backup is a fully managed service that makes it easy to centralize and automate the backup of data across AWS resources. It can be used to create backup schedules and retention policies for DynamoDB tables, which will ensure that the data is retained for the desired period of 7 years. This solution will provide the most operationally efficient method for meeting the requirements because it requires minimal effort to set up and manage.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751810,
          "date": "Wed 21 Dec 2022 03:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most operationally efficient solution that meets these requirements would be to use option B, which is to use AWS Backup to create backup schedules and retention policies for the table.<br><br>AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS resources. It allows you to create backup policies and schedules to automatically back up your DynamoDB tables on a regular basis. You can also specify retention policies to ensure that your backups are retained for the required period of time. This solution is fully automated and requires minimal maintenance, making it the most operationally efficient option.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using DynamoDB point-in-time recovery, is also a viable option but it requires continuous backup, which may be more resource-intensive and may incur higher costs compared to using AWS Backup.<br><br>Option C, creating an on-demand backup of the table and storing it in an S3 bucket, is also a viable option but it requires manual intervention and does not provide the automation and scheduling capabilities of AWS Backup.<br><br>Option D, using Amazon EventBridge (CloudWatch Events) and a Lambda function to back up the table and store it in an S3 bucket, is also a viable option but it requires more complex setup and maintenance compared to using AWS Backup.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751811,
          "date": "Wed 21 Dec 2022 03:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using DynamoDB point-in-time recovery, is also a viable option but it requires continuous backup, which may be more resource-intensive and may incur higher costs compared to using AWS Backup.<br><br>Option C, creating an on-demand backup of the table and storing it in an S3 bucket, is also a viable option but it requires manual intervention and does not provide the automation and scheduling capabilities of AWS Backup.<br><br>Option D, using Amazon EventBridge (CloudWatch Events) and a Lambda function to back up the table and store it in an S3 bucket, is also a viable option but it requires more complex setup and maintenance compared to using AWS Backup.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749360,
          "date": "Mon 19 Dec 2022 03:01",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B AWS Backup",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747700,
          "date": "Sat 17 Dec 2022 01:36",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "AWS Backup",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723665,
          "date": "Mon 21 Nov 2022 16:01",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 721469,
          "date": "Fri 18 Nov 2022 18:58",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "We recommend you use AWS Backup to automatically delete the backups that you no longer need by configuring your lifecycle when you created your backup plan.<br>https://docs.aws.amazon.com/aws-backup/latest/devguide/deleting-backups.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 707559,
          "date": "Sun 30 Oct 2022 02:06",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "B is clear",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#79",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings. In the evenings, the read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very quickly.<br>What should a solutions architect recommend?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#79",
          "answers": [
            {
              "choice": "<p>A. Create a DynamoDB table in on-demand capacity mode.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a DynamoDB table with a global secondary index.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a DynamoDB table with provisioned capacity and auto scaling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707562,
          "date": "Sun 30 Oct 2022 02:15",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "On-demand mode is a good option if any of the following are true:<br>- You create new tables with unknown workloads.<br>- You have unpredictable application traffic.<br>- You prefer the ease of paying for only what you use.",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 697574,
          "date": "Mon 17 Oct 2022 18:59",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "**A** - On demand is the answer - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.OnDemand<br>B - not related with the unpredictable traffic<br>C - provisioned capacity is recommended for known patterns. Not the case here.<br>D - same as C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks. Your reference link perfectly supports the option \\\"A\\\". 100% correct</li></ul>",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829116,
          "date": "Sat 04 Mar 2023 17:27",
          "username": "\t\t\t\tNasosoAuxtyno\t\t\t",
          "content": "Thanks. Your reference link perfectly supports the option \\\"A\\\". 100% correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 835412,
          "date": "Fri 10 Mar 2023 21:30",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "Use on-demand capacity mode: With on-demand capacity mode, DynamoDB automatically scales up and down to handle the traffic without requiring any capacity planning. This way, the company only pays for the actual amount of read and write capacity used, with no minimums or upfront costs.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 813129,
          "date": "Sat 18 Feb 2023 15:52",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "A.  This is because the traffic spikes have no set time as they can happen at any time, it being morning or evening",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 784611,
          "date": "Sun 22 Jan 2023 20:03",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "C.  Create a DynamoDB table with provisioned capacity and auto scaling. This will allow the table to automatically scale its capacity based on usage patterns, which will help to optimize costs by reducing the amount of unused capacity during low traffic times and ensuring that sufficient capacity is available during traffic spikes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 783728,
          "date": "Sat 21 Jan 2023 21:53",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Use pattern is not unknown, it was well laid out in the question.I think C is the correct answer.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 781377,
          "date": "Thu 19 Jan 2023 18:12",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "I have a feeling that the need for cost-optimisation is a distractor, and that people will jump on \\\"provisioned with auto-scaling\\\" without considering that provisioned capacity mode is not a good fit for the requirements. On-demand may end up cheaper as you avoid over- or underprovisioning capacity (when using auto-scaling, you still need to define a min and max). You can later switch capacity mode once your usage pattern becomes stable (if it ever does).<br><br>AWS say that on-demand capacity mode is a good fit for:<br>- Unpredictable workloads with sudden spikes (mentioned in the requirements)<br>- Frequently idle workloads (where the DB isn't used at all; The requirements say that it won't be used most mornings)<br>- Events with unknown traffic (which this is - traffic in the evenings is unpredictable)<br><br>Whereas provisioned capacity mode is used for:<br>- Predictable workloads<br>- Gradual ramps (no sudden spikes, as auto-scaling isn't instant and can cause traffic to get throttled)<br>- Events iwth known traffic<br><br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778813,
          "date": "Tue 17 Jan 2023 12:02",
          "username": "\t\t\t\tdev1978\t\t\t",
          "content": "Initially I thought C but after reading comments and this page, I switch to A<br><br>Provisioned mode is a good option if any of the following are true:<br><br>You have predictable application traffic.<br><br>You run applications whose traffic is consistent or ramps gradually.<br>Here https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html it mentions for provisioned<br>> You can forecast capacity requirements to control costs.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 773807,
          "date": "Thu 12 Jan 2023 20:07",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Provisioned capacity is less expensive, the question says the time usage starts in the evening, which means I can provision for that time and auto scale up or down to address the usage spikes. I think this will be a better architecture than expensive \\\"on-demand\\\" architecture.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 771919,
          "date": "Wed 11 Jan 2023 01:41",
          "username": "\t\t\t\tmackeda\t\t\t",
          "content": "A, Please refer the following link https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770007,
          "date": "Mon 09 Jan 2023 04:17",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Answer is ondemand only,this fro. Aws text,This applies for scaling up or down the provisioned capacity of a DynamoDB table. In the case that you have an occasional usage spike auto scaling might not be able to react in time.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 770004,
          "date": "Mon 09 Jan 2023 04:12",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C is right The company is concerned about cost optimization.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 764348,
          "date": "Tue 03 Jan 2023 10:10",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "The correct answer is C: Create a DynamoDB table with provisioned capacity and auto-scaling.<br><br>In DynamoDB's provisioned capacity mode, you can specify the number of reads and writes you need for your table and pay for that capacity up front. However, if your table's read and write traffic is unpredictable and often experiences sudden spikes, it can be difficult to determine the correct amount of provisioned capacity for your table. In these cases, it is recommended that you use DynamoDB's automatic scaling, which allows you to automatically adjust the provisioned capacity of your table to adapt to changes in read and write traffic. In this way, you can ensure that your table always has the capacity it needs to handle the traffic without overpaying for capacity you don't use.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751817,
          "date": "Wed 21 Dec 2022 04:00",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***C.  Create a DynamoDB table with provisioned capacity and auto scaling.<br><br>Since the table will not be used on most mornings and the read and write traffic will often be unpredictable in the evenings, it would be more cost-effective to set the table to use provisioned capacity and enable auto scaling. This way, the table can scale up its capacity to handle increased traffic when needed, and scale down when traffic decreases, helping to optimize costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A, creating a DynamoDB table in on-demand capacity mode, would not be suitable in this case because on-demand capacity mode charges for every read and write request, which could become costly when traffic spikes occur.<br><br>Option B, creating a DynamoDB table with a global secondary index, would not directly address the concern of cost optimization. A global secondary index can be useful for querying data in different ways, but it does not affect the capacity or cost of the table.<br><br>Option D, creating a DynamoDB table in provisioned capacity mode and configuring it as a global table, could be a suitable option if the company needs to access the data from multiple regions. However, it would not address the concern of cost optimization.</li><li>Cost considerations was not mentioned in the question. answer is A</li><li>The company is concerned about cost optimization.mentioned in text</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751818,
          "date": "Wed 21 Dec 2022 04:01",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A, creating a DynamoDB table in on-demand capacity mode, would not be suitable in this case because on-demand capacity mode charges for every read and write request, which could become costly when traffic spikes occur.<br><br>Option B, creating a DynamoDB table with a global secondary index, would not directly address the concern of cost optimization. A global secondary index can be useful for querying data in different ways, but it does not affect the capacity or cost of the table.<br><br>Option D, creating a DynamoDB table in provisioned capacity mode and configuring it as a global table, could be a suitable option if the company needs to access the data from multiple regions. However, it would not address the concern of cost optimization.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Cost considerations was not mentioned in the question. answer is A</li><li>The company is concerned about cost optimization.mentioned in text</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766213,
          "date": "Thu 05 Jan 2023 03:43",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Cost considerations was not mentioned in the question. answer is A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The company is concerned about cost optimization.mentioned in text</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770005,
          "date": "Mon 09 Jan 2023 04:12",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "The company is concerned about cost optimization.mentioned in text",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749362,
          "date": "Mon 19 Dec 2022 03:01",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747705,
          "date": "Sat 17 Dec 2022 01:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is best option as load is unpreditable and during morning time it is very low. When difference in the peak vs lowest usage is very high and unpreditable on-demand is best and most cost effective.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 738697,
          "date": "Thu 08 Dec 2022 07:31",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "A is the answer since there is unpredictable access for short time.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#80",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots.<br>What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#80",
          "answers": [
            {
              "choice": "<p>A. Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698313,
          "date": "Tue 18 Oct 2022 17:16",
          "username": "\t\t\t\tSauran\t\t\t",
          "content": "Share the existing KMS key with the MSP external account because it has already been used to encrypt the AMI snapshot.<br><br>https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751825,
          "date": "Wed 21 Dec 2022 04:13",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***B.  Modify the launchPermission property of the AMI. <br><br>The most secure way for the solutions architect to share the AMI with the MSP Partner's AWS account would be to modify the launchPermission property of the AMI and share it with the MSP Partner's AWS account only. The key policy should also be modified to allow the MSP Partner's AWS account to use the key. This ensures that the AMI is only shared with the MSP Partner and is encrypted with a key that they are authorized to use.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, making the AMI and snapshots publicly available, is not a secure option as it would allow anyone with access to the AMI to use it. <br><br>Option C, modifying the key policy to trust a new KMS key owned by the MSP Partner, is also not a secure option as it would involve sharing the key with the MSP Partner, which could potentially compromise the security of the data encrypted with the key. <br><br>Option D, exporting the AMI to an S3 bucket in the MSP Partner's AWS account and encrypting the S3 bucket with a new KMS key owned by the MSP Partner, is also not the most secure option as it involves sharing the AMI and a new key with the MSP Partner, which could potentially compromise the security of the data.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751826,
          "date": "Wed 21 Dec 2022 04:13",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, making the AMI and snapshots publicly available, is not a secure option as it would allow anyone with access to the AMI to use it. <br><br>Option C, modifying the key policy to trust a new KMS key owned by the MSP Partner, is also not a secure option as it would involve sharing the key with the MSP Partner, which could potentially compromise the security of the data encrypted with the key. <br><br>Option D, exporting the AMI to an S3 bucket in the MSP Partner's AWS account and encrypting the S3 bucket with a new KMS key owned by the MSP Partner, is also not the most secure option as it involves sharing the AMI and a new key with the MSP Partner, which could potentially compromise the security of the data.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 749274,
          "date": "Mon 19 Dec 2022 00:00",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 716070,
          "date": "Fri 11 Nov 2022 14:23",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Must use and share the existing KMS key to decrypt the same key",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 714828,
          "date": "Thu 10 Nov 2022 01:53",
          "username": "\t\t\t\tflbcobra\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 705686,
          "date": "Thu 27 Oct 2022 17:17",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "If EBS snapshots are encrypted, then we need to share the same KMS key to partners to be able to access it. Read the note section in the link<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 699997,
          "date": "Thu 20 Oct 2022 15:57",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "MOST secure way should be C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 696416,
          "date": "Sun 16 Oct 2022 19:26",
          "username": "\t\t\t\tChunsli\t\t\t",
          "content": "MOST secure way should be C, with a separate key, not the one already used.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Must use and share the existing KMS key to decrypt the same key</li><li>A seperate/new key is not possible because it won't be able to decrypt the AMI snapshot which was already encrypted with the existing/old key.</li><li>This is truth</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716068,
          "date": "Fri 11 Nov 2022 14:22",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Must use and share the existing KMS key to decrypt the same key",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698314,
          "date": "Tue 18 Oct 2022 17:18",
          "username": "\t\t\t\tSauran\t\t\t",
          "content": "A seperate/new key is not possible because it won't be able to decrypt the AMI snapshot which was already encrypted with the existing/old key.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This is truth</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 704995,
          "date": "Wed 26 Oct 2022 22:14",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "This is truth",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#81",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored.<br>Which design should the solutions architect use?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#81",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 743450,
          "date": "Tue 13 Dec 2022 01:36",
          "username": "\t\t\t\tMarge_Simpson\t\t\t",
          "content": "decoupled = SQS<br>Launch template = AMI<br>Launch configuration = EC2",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751830,
          "date": "Wed 21 Dec 2022 04:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The correct design is Option C.  Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.<br><br>This design satisfies the requirements of the application by using Amazon Simple Queue Service (SQS) as durable storage for the job items and Amazon Elastic Compute Cloud (EC2) Auto Scaling to add and remove nodes based on the number of items in the queue. The processor application can be run in parallel on multiple nodes, and the use of launch templates allows for flexibility in the configuration of the EC2 instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A is incorrect because it uses Amazon Simple Notification Service (SNS) instead of SQS, which is not a durable storage solution. <br><br>Option B is incorrect because it uses CPU usage as the scaling trigger instead of the number of items in the queue. <br><br>Option D is incorrect for the same reasons as option A. </li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751831,
          "date": "Wed 21 Dec 2022 04:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A is incorrect because it uses Amazon Simple Notification Service (SNS) instead of SQS, which is not a durable storage solution. <br><br>Option B is incorrect because it uses CPU usage as the scaling trigger instead of the number of items in the queue. <br><br>Option D is incorrect for the same reasons as option A. ",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 747612,
          "date": "Fri 16 Dec 2022 22:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "SQS with EC2 autoscaling policy based number of messages in the queue.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 737125,
          "date": "Tue 06 Dec 2022 19:34",
          "username": "\t\t\t\tUhrien\t\t\t",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734583,
          "date": "Sat 03 Dec 2022 18:12",
          "username": "\t\t\t\tkelljons\t\t\t",
          "content": "what about the word \\\"coupled\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734470,
          "date": "Sat 03 Dec 2022 14:26",
          "username": "\t\t\t\tkewl\t\t\t",
          "content": "AWS strongly recommends that you do not use launch configurations hence answer is C<br>https://docs.amazonaws.cn/en_us/autoscaling/ec2/userguide/launch-configurations.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 730216,
          "date": "Tue 29 Nov 2022 11:10",
          "username": "\t\t\t\tHussamShokr\t\t\t",
          "content": "answer is C a there is nothing called \\\" Launch Configuration\\\" it's called \\\"Launch Template\\\" which is used by the autoscalling group to creat the new instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>There's launch configuration. Search</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 771313,
          "date": "Tue 10 Jan 2023 12:00",
          "username": "\t\t\t\tlulzsec2019\t\t\t",
          "content": "There's launch configuration. Search",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 726278,
          "date": "Fri 25 Nov 2022 00:46",
          "username": "\t\t\t\tLiliwood\t\t\t",
          "content": "I was not sure between Launch template and Launch configuration.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723733,
          "date": "Mon 21 Nov 2022 17:20",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713863,
          "date": "Tue 08 Nov 2022 14:31",
          "username": "\t\t\t\tdevopspro\t\t\t",
          "content": "answer is c",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 712149,
          "date": "Sun 06 Nov 2022 05:45",
          "username": "\t\t\t\tWilson_S\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/22139-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 707001,
          "date": "Sat 29 Oct 2022 08:04",
          "username": "\t\t\t\twookchan\t\t\t",
          "content": "It looks like C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706848,
          "date": "Sat 29 Oct 2022 02:49",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "Correct Answer: C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#82",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate.<br>What should a solutions architect recommend to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#82",
          "answers": [
            {
              "choice": "<p>A. Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificate will expire.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697381,
          "date": "Mon 17 Oct 2022 14:13",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "B<br>AWS Config has a managed rule<br>named acm-certificate-expiration-check<br>to check for expiring certificates<br>(configurable number of days)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer B and answer D are possible according to this article.<br>So, need to read B &amp; D carefully to determine the most suitable answer.<br><br>Reference: https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/</li><li>https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/</li></ul>",
          "upvote_count": "26",
          "selected_answers": ""
        },
        {
          "id": 837756,
          "date": "Mon 13 Mar 2023 09:14",
          "username": "\t\t\t\tChrisG1454\t\t\t",
          "content": "Answer B and answer D are possible according to this article.<br>So, need to read B & D carefully to determine the most suitable answer.<br><br>Reference: https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 697384,
          "date": "Mon 17 Oct 2022 14:22",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 705687,
          "date": "Thu 27 Oct 2022 17:23",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835161,
          "date": "Fri 10 Mar 2023 16:31",
          "username": "\t\t\t\tgmehra\t\t\t",
          "content": "Config is good to have but you can have Event bridge directly use and it can send the notification for straight 35 days (lets say certificate to be expire in 35 days)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 829681,
          "date": "Sun 05 Mar 2023 07:17",
          "username": "\t\t\t\tfkie4\t\t\t",
          "content": "All i wanna say is that I really hate this kind of question. The author of the question should have the question reviewed by some AWS experts. if there are 2 answers that are so close, he should consider changing the option, or abandon the question. I bet if all AWS expert on earth try to answer this question, the result will be 50-50",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 826517,
          "date": "Thu 02 Mar 2023 06:29",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "Both options are correct and viable. B and D . In D you just have to use Event Bridge and SNS whereas in B you have to user aws config additionaly to what we care using in D. <br>So most efficient wayis to go for D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809887,
          "date": "Wed 15 Feb 2023 20:35",
          "username": "\t\t\t\tenc_0343\t\t\t",
          "content": "I think D might be the correct answer here. https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796448,
          "date": "Thu 02 Feb 2023 22:10",
          "username": "\t\t\t\tStanislav4907\t\t\t",
          "content": "hate question like this .How do we suppose to know if certificate was self-signed or 3d party.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 795021,
          "date": "Wed 01 Feb 2023 11:51",
          "username": "\t\t\t\tcloudbusting\t\t\t",
          "content": "correct answer is D https://docs.aws.amazon.com/acm/latest/userguide/supported-events.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788629,
          "date": "Thu 26 Jan 2023 11:50",
          "username": "\t\t\t\tsassy2023\t\t\t",
          "content": "D seems like a better option<br>https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 788610,
          "date": "Thu 26 Jan 2023 11:24",
          "username": "\t\t\t\tamgice\t\t\t",
          "content": "AMC send event 45 days prior to expiration you can change de days in ACM so D is wrong.<br>AWS has a managed rule named acm-certificate-expiration-check configurable with numbers of days",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 779718,
          "date": "Wed 18 Jan 2023 08:45",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 773542,
          "date": "Thu 12 Jan 2023 15:11",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Solution overview<br>This solution provides a Lambda function that makes use of CloudWatch rules to report back those certificates that are due to expire within a pre-defined amount of time. The Lambda function is written to respond to CloudWatch events in two ways. When the event is time-based, the function looks for all certificates that have a DaysToExpiry metric. When the event is based on an event that is raised from a specific certificate, the function examines the single certificate. In both cases, the function logs the findings to Security Hub and sends out an SNS notification.<br>https://aws.amazon.com/blogs/security/how-to-monitor-expirations-of-imported-certificates-in-aws-certificate-manager-acm/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 771563,
          "date": "Tue 10 Jan 2023 16:32",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D & B BOTH Can do it, but B requires premium support,no such requirment withD. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>please ignore B is correct no premium support required.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 771566,
          "date": "Tue 10 Jan 2023 16:33",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "please ignore B is correct no premium support required.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 771332,
          "date": "Tue 10 Jan 2023 12:24",
          "username": "\t\t\t\tlulzsec2019\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/#:~:text=You%20can%20use%20AWS%20Config,are%20nearing%20the%20expiration%20date.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 771325,
          "date": "Tue 10 Jan 2023 12:13",
          "username": "\t\t\t\tlulzsec2019\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 771324,
          "date": "Tue 10 Jan 2023 12:12",
          "username": "\t\t\t\tlulzsec2019\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770574,
          "date": "Mon 09 Jan 2023 16:30",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "B.  I think D is incorrect, becuase the \\\"more\\\" correct operation shall be: ACM sends daily event to EventBridge, and EventBridge has rule to check expiry from these event. Not EventBridge actively scans for the cert expiry in ACM.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#83",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed.<br>What should the solutions architect recommend?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#83",
          "answers": [
            {
              "choice": "<p>A. Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Move the website to Amazon S3. Use Cross-Region Replication between Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon CloudFront with a custom origin pointing to the on-premises servers.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 763331,
          "date": "Sun 01 Jan 2023 21:16",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "Within few days you can not do more than using CloudFront",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 751842,
          "date": "Wed 21 Dec 2022 04:40",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***C.  Use Amazon CloudFront with a custom origin pointing to the on-premises servers.<br><br>Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML, CSS, JavaScript, images, and videos. By using CloudFront, the company can distribute the content of their website from edge locations that are closer to the users in Europe, reducing the loading times for these users.<br><br>To use CloudFront, the company can set up a custom origin pointing to their on-premises servers in the United States. CloudFront will then cache the content of the website at edge locations around the world and serve the content to users from the location that is closest to them. This will allow the company to optimize the loading times for their European users without having to move the backend of the website to a different region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>good explanation..thanks</li><li>***WRONG***<br>Option A (launch an Amazon EC2 instance in us-east-1 and migrate the site to it) would not address the issue of optimizing loading times for European users. <br><br>Option B (move the website to Amazon S3 and use Cross-Region Replication between Regions) would not be an immediate solution as it would require time to set up and migrate the website. <br><br>Option D (use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers) would not be suitable because it would not improve the loading times for users in Europe.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 832544,
          "date": "Wed 08 Mar 2023 06:02",
          "username": "\t\t\t\tTariqKipkemei\t\t\t",
          "content": "good explanation..thanks",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751843,
          "date": "Wed 21 Dec 2022 04:41",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A (launch an Amazon EC2 instance in us-east-1 and migrate the site to it) would not address the issue of optimizing loading times for European users. <br><br>Option B (move the website to Amazon S3 and use Cross-Region Replication between Regions) would not be an immediate solution as it would require time to set up and migrate the website. <br><br>Option D (use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers) would not be suitable because it would not improve the loading times for users in Europe.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 749280,
          "date": "Mon 19 Dec 2022 00:06",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 732506,
          "date": "Thu 01 Dec 2022 11:40",
          "username": "\t\t\t\tkajal1206\t\t\t",
          "content": "C is correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 728783,
          "date": "Mon 28 Nov 2022 05:47",
          "username": "\t\t\t\tkoreanmonkey\t\t\t",
          "content": "CloudFront = CDN Service",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 726284,
          "date": "Fri 25 Nov 2022 00:55",
          "username": "\t\t\t\tLiliwood\t\t\t",
          "content": "C. <br>S3 Cross region Replication minimize latency but also copies objects across Amazon S3 buckets in different AWS Regions(data has to remain in origin thou) so B wrong.<br>Route 53 geo, does not help reducing the latency.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 723735,
          "date": "Mon 21 Nov 2022 17:22",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709477,
          "date": "Wed 02 Nov 2022 00:52",
          "username": "\t\t\t\tHunkie\t\t\t",
          "content": "Same question with detailed explanation <br><br>https://www.examtopics.com/discussions/amazon/view/27898-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 698934,
          "date": "Wed 19 Oct 2022 12:47",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C, use CloudFront.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#84",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.<br>The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.<br>Which EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#84",
          "answers": [
            {
              "choice": "<p>A. Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697092,
          "date": "Mon 17 Oct 2022 10:06",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Spot blocks are not longer available, and you can't use spot instances on Prod machines 24x7, so option B should be valid.",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759745,
          "date": "Wed 28 Dec 2022 12:20",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "Well, AWS has DISCONTINUED the Spot-Block option. so that rules out the two options that use spot-block. Wait, this question must be from SAA-C02 or even 01. STALE QUESTION. I don't think this will feature in SAA-C03. Anyhow, the most cost-effective solution would be Option \\\"b\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755404,
          "date": "Sun 25 Dec 2022 05:26",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Choosing B as spot blocks (Spot instances with a finite duration) are no longer offered since July 2021",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751851,
          "date": "Wed 21 Dec 2022 04:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most cost-effective solution for the company's requirements would be to use Spot Instances for the development and test EC2 instances and Reserved Instances for the production EC2 instances.<br><br>Spot Instances are a cost-effective choice for non-critical, flexible workloads that can be interrupted. Since the development and test EC2 instances are only needed for at least 8 hours per day and can be stopped when not in use, they would be a good fit for Spot Instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The production EC2 instances run 24 hours a day.</li><li>Reserved Instances are a good fit for production EC2 instances that need to run 24 hours a day, as they offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.<br><br>Option A is the correct answer because it meets the company's requirements for cost-effectively running the development and test EC2 instances and the production EC2 instances.</li><li>Option B is not the most cost-effective solution because it suggests using On-Demand Instances for the development and test EC2 instances, which would be more expensive than using Spot Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Spot Instances.<br><br>Option C is not the correct solution because Spot blocks are a variant of Spot Instances that offer a guaranteed capacity and duration, but they are not available for all instance types and are not necessarily the most cost-effective option in all cases. In this case, it would be more cost-effective to use Spot Instances for the development and test EC2 instances, as they can be interrupted when not in use.</li><li>Can't use Spot instances for Production environment that needs to run 24/7. That should tell you that Production instances can't have a downtime. Spot instances are used when an application or service can allow disruption and 24/7 production environment won't allow that.</li><li>Option D is not the correct solution because it suggests using On-Demand Instances for the production EC2 instances, which would be more expensive than using Reserved Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Reserved Instances in the long run. Using Reserved Instances for the production EC2 instances would offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757925,
          "date": "Tue 27 Dec 2022 00:25",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "The production EC2 instances run 24 hours a day.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751852,
          "date": "Wed 21 Dec 2022 04:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Reserved Instances are a good fit for production EC2 instances that need to run 24 hours a day, as they offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.<br><br>Option A is the correct answer because it meets the company's requirements for cost-effectively running the development and test EC2 instances and the production EC2 instances.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not the most cost-effective solution because it suggests using On-Demand Instances for the development and test EC2 instances, which would be more expensive than using Spot Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Spot Instances.<br><br>Option C is not the correct solution because Spot blocks are a variant of Spot Instances that offer a guaranteed capacity and duration, but they are not available for all instance types and are not necessarily the most cost-effective option in all cases. In this case, it would be more cost-effective to use Spot Instances for the development and test EC2 instances, as they can be interrupted when not in use.</li><li>Can't use Spot instances for Production environment that needs to run 24/7. That should tell you that Production instances can't have a downtime. Spot instances are used when an application or service can allow disruption and 24/7 production environment won't allow that.</li><li>Option D is not the correct solution because it suggests using On-Demand Instances for the production EC2 instances, which would be more expensive than using Reserved Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Reserved Instances in the long run. Using Reserved Instances for the production EC2 instances would offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751853,
          "date": "Wed 21 Dec 2022 04:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B is not the most cost-effective solution because it suggests using On-Demand Instances for the development and test EC2 instances, which would be more expensive than using Spot Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Spot Instances.<br><br>Option C is not the correct solution because Spot blocks are a variant of Spot Instances that offer a guaranteed capacity and duration, but they are not available for all instance types and are not necessarily the most cost-effective option in all cases. In this case, it would be more cost-effective to use Spot Instances for the development and test EC2 instances, as they can be interrupted when not in use.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Can't use Spot instances for Production environment that needs to run 24/7. That should tell you that Production instances can't have a downtime. Spot instances are used when an application or service can allow disruption and 24/7 production environment won't allow that.</li><li>Option D is not the correct solution because it suggests using On-Demand Instances for the production EC2 instances, which would be more expensive than using Reserved Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Reserved Instances in the long run. Using Reserved Instances for the production EC2 instances would offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823229,
          "date": "Mon 27 Feb 2023 06:26",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "Can't use Spot instances for Production environment that needs to run 24/7. That should tell you that Production instances can't have a downtime. Spot instances are used when an application or service can allow disruption and 24/7 production environment won't allow that.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751854,
          "date": "Wed 21 Dec 2022 04:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D is not the correct solution because it suggests using On-Demand Instances for the production EC2 instances, which would be more expensive than using Reserved Instances. On-Demand Instances are a good choice for workloads that require a guaranteed capacity and can't be interrupted, but they are more expensive than Reserved Instances in the long run. Using Reserved Instances for the production EC2 instances would offer a significant discount compared to On-Demand Instances in exchange for a one-time payment and a commitment to use the instances for a certain period of time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749281,
          "date": "Mon 19 Dec 2022 00:09",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747693,
          "date": "Sat 17 Dec 2022 01:25",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747238,
          "date": "Fri 16 Dec 2022 14:43",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "Reserved instances for 24/7 production instances seems reasonable. By exclusion I will choose the on-demand for dev and test (despite thinking that Spot Flees may be even a better solution from a cost-wise perspective)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723739,
          "date": "Mon 21 Nov 2022 17:24",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717511,
          "date": "Sun 13 Nov 2022 21:25",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Reserved Instances and On-demand<br><br>Spot is out as the use case required continues instance running",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 712783,
          "date": "Mon 07 Nov 2022 04:18",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B is the answer<br><br>https://www.examtopics.com/discussions/amazon/view/80956-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#85",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored.<br>What should a solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#85",
          "answers": [
            {
              "choice": "<p>A. Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697639,
          "date": "Mon 17 Oct 2022 20:59",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "You can use S3 Object Lock to store objects using a write-once-read-many (WORM) model. Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use S3 Object Lock to meet regulatory requirements that require WORM storage, or add an extra layer of protection against object changes and deletion.<br>Versioning is required and automatically activated as Object Lock is enabled.<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 769562,
          "date": "Sun 08 Jan 2023 16:13",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Option A.  Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled. This will ensure that the documents cannot be modified or deleted after they are stored, and will meet the regulatory requirement. S3 Versioning allows you to store multiple versions of an object in the same bucket, and S3 Object Lock enables you to apply a retention policy to objects in the bucket to prevent their deletion.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751858,
          "date": "Wed 21 Dec 2022 05:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***A.  Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.<br><br>S3 Versioning allows multiple versions of an object to be stored in the same bucket. This means that when an object is modified or deleted, the previous version is preserved. S3 Object Lock adds additional protection by allowing objects to be placed under a legal hold or retention period, during which they cannot be deleted or modified. Together, S3 Versioning and S3 Object Lock can be used to meet the requirement of not allowing documents to be modified or deleted after they are stored.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option B, storing the documents in an S3 bucket and configuring an S3 Lifecycle policy to archive them periodically, would not prevent the documents from being modified or deleted. <br><br>Option C, storing the documents in an S3 bucket with S3 Versioning enabled and configuring an ACL to restrict all access to read-only, would also not prevent the documents from being modified or deleted, since an ACL only controls access to the object and does not prevent it from being modified or deleted. <br><br>Option D, storing the documents on an Amazon Elastic File System (Amazon EFS) volume and accessing the data in read-only mode, would prevent the documents from being modified, but would not prevent them from being deleted.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751859,
          "date": "Wed 21 Dec 2022 05:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option B, storing the documents in an S3 bucket and configuring an S3 Lifecycle policy to archive them periodically, would not prevent the documents from being modified or deleted. <br><br>Option C, storing the documents in an S3 bucket with S3 Versioning enabled and configuring an ACL to restrict all access to read-only, would also not prevent the documents from being modified or deleted, since an ACL only controls access to the object and does not prevent it from being modified or deleted. <br><br>Option D, storing the documents on an Amazon Elastic File System (Amazon EFS) volume and accessing the data in read-only mode, would prevent the documents from being modified, but would not prevent them from being deleted.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747694,
          "date": "Sat 17 Dec 2022 01:27",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A.  Object Lock will prevent modifications to documents",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 737364,
          "date": "Wed 07 Dec 2022 03:01",
          "username": "\t\t\t\tHarryZ\t\t\t",
          "content": "Why not C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Configure an ACL to restrict all access to read-only would be you could not write the docs to the bucket in the first place.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749615,
          "date": "Mon 19 Dec 2022 09:14",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Configure an ACL to restrict all access to read-only would be you could not write the docs to the bucket in the first place.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723741,
          "date": "Mon 21 Nov 2022 17:25",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714832,
          "date": "Thu 10 Nov 2022 02:02",
          "username": "\t\t\t\tflbcobra\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 699779,
          "date": "Thu 20 Oct 2022 12:04",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "aaaaaaaaa",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 699777,
          "date": "Thu 20 Oct 2022 12:04",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "aaaaaaaaaaa",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#86",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the web servers to connect to the database while meeting a security requirement to rotate user credentials frequently.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#86",
          "answers": [
            {
              "choice": "<p>A. Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM permissions to allow the web servers to access OpsCenter.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials and access the database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be able to decrypt the files and access the database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697643,
          "date": "Mon 17 Oct 2022 21:03",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Secrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise.<br>https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 814094,
          "date": "Sun 19 Feb 2023 14:06",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 765184,
          "date": "Wed 04 Jan 2023 03:59",
          "username": "\t\t\t\tthensanity\t\t\t",
          "content": "literally screams for AWS secrets manager to rotate the credentails",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751862,
          "date": "Wed 21 Dec 2022 05:09",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>Option A.  Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.<br><br>Option A is correct because it meets the requirements specified in the question: a secure method for the web servers to connect to the database while meeting a security requirement to rotate user credentials frequently. AWS Secrets Manager is designed specifically to store and manage secrets like database credentials, and it provides an automated way to rotate secrets every time they are used, ensuring that the secrets are always fresh and secure. This makes it a good choice for storing and managing the database user credentials in a secure way.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option B, storing the database user credentials in AWS Systems Manager OpsCenter, is not a good fit for this use case because OpsCenter is a tool for managing and monitoring systems, and it is not designed for storing and managing secrets.<br><br>Option C, storing the database user credentials in a secure Amazon S3 bucket, is not a secure option because S3 buckets are not designed to store secrets. While it is possible to store secrets in S3, it is not recommended because S3 is not a secure secrets management service and does not provide the same level of security and automation as AWS Secrets Manager.</li><li>Option D, storing the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system, is not a secure option because it relies on the security of the web server file system, which may not be as secure as a dedicated secrets management service like AWS Secrets Manager. Additionally, this option does not meet the requirement to rotate user credentials frequently because it does not provide an automated way to rotate the credentials.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751863,
          "date": "Wed 21 Dec 2022 05:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option B, storing the database user credentials in AWS Systems Manager OpsCenter, is not a good fit for this use case because OpsCenter is a tool for managing and monitoring systems, and it is not designed for storing and managing secrets.<br><br>Option C, storing the database user credentials in a secure Amazon S3 bucket, is not a secure option because S3 buckets are not designed to store secrets. While it is possible to store secrets in S3, it is not recommended because S3 is not a secure secrets management service and does not provide the same level of security and automation as AWS Secrets Manager.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D, storing the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system, is not a secure option because it relies on the security of the web server file system, which may not be as secure as a dedicated secrets management service like AWS Secrets Manager. Additionally, this option does not meet the requirement to rotate user credentials frequently because it does not provide an automated way to rotate the credentials.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 751864,
          "date": "Wed 21 Dec 2022 05:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D, storing the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system, is not a secure option because it relies on the security of the web server file system, which may not be as secure as a dedicated secrets management service like AWS Secrets Manager. Additionally, this option does not meet the requirement to rotate user credentials frequently because it does not provide an automated way to rotate the credentials.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749287,
          "date": "Mon 19 Dec 2022 00:12",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 734475,
          "date": "Sat 03 Dec 2022 14:37",
          "username": "\t\t\t\tkewl\t\t\t",
          "content": "Rotate credentials = Secrets Manager",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723744,
          "date": "Mon 21 Nov 2022 17:26",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720513,
          "date": "Thu 17 Nov 2022 15:15",
          "username": "\t\t\t\trenekton\t\t\t",
          "content": "Answer is A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#87",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon Aurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The result is that customer data is not recorded for some of the event.<br>A solutions architect needs to design a solution that stores customer data that is created during database upgrades.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#87",
          "answers": [
            {
              "choice": "<p>A. Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that stores the customer data in the database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new Lambda function that polls the queue and stores the customer data in the database.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 695461,
          "date": "Sat 15 Oct 2022 16:16",
          "username": "\t\t\t\tbrushek\t\t\t",
          "content": "https://aws.amazon.com/rds/proxy/<br><br>RDS Proxy minimizes application disruption from outages affecting the availability of your database by automatically connecting to a new database instance while preserving application connections. When failovers occur, RDS Proxy routes requests directly to the new database instance. This reduces failover times for Aurora and RDS databases by up to 66%.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Aurora supports RDS proxy!<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html</li><li>This isMySQL Database. RDS proxy = no no</li></ul>",
          "upvote_count": "26",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 725089,
          "date": "Wed 23 Nov 2022 12:43",
          "username": "\t\t\t\tattila9778\t\t\t",
          "content": "Aurora supports RDS proxy!<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 757932,
          "date": "Tue 27 Dec 2022 00:33",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "This isMySQL Database. RDS proxy = no no",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698238,
          "date": "Tue 18 Oct 2022 14:37",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "The answer is D. <br>RDS Proxy doesn't support Aurora DBs. See limitations at:<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Actually RDS Proxy supports Aurora DBs running on PostgreSQL and MySQL.<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.Aurora_Fea_Regions_DB-eng.Feature.RDS_Proxy.html<br><br>With RDS proxy, you only expose a single endpoint for request to hit and any failure of the primary DB in a Multi-AZ configuration is will be managed automatically by RDS Proxy to point to the new primary DB.  Hence RDS proxy is the most efficient way of solving the issue as additional code change is required. <br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html</li><li>It does, according to that link</li><li>You can use RDS Proxy with Aurora Serverless v2 clusters but not with Aurora Serverless v1 clusters. <br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html</li></ul>",
          "upvote_count": "14",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 722689,
          "date": "Sun 20 Nov 2022 15:20",
          "username": "\t\t\t\ttinyfoot\t\t\t",
          "content": "Actually RDS Proxy supports Aurora DBs running on PostgreSQL and MySQL.<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.Aurora_Fea_Regions_DB-eng.Feature.RDS_Proxy.html<br><br>With RDS proxy, you only expose a single endpoint for request to hit and any failure of the primary DB in a Multi-AZ configuration is will be managed automatically by RDS Proxy to point to the new primary DB.  Hence RDS proxy is the most efficient way of solving the issue as additional code change is required. <br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.howitworks.html",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 749620,
          "date": "Mon 19 Dec 2022 09:23",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "It does, according to that link",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 725714,
          "date": "Thu 24 Nov 2022 10:54",
          "username": "\t\t\t\tgcmrjbr\t\t\t",
          "content": "You can use RDS Proxy with Aurora Serverless v2 clusters but not with Aurora Serverless v1 clusters. <br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 833535,
          "date": "Thu 09 Mar 2023 03:13",
          "username": "\t\t\t\tViru_90\t\t\t",
          "content": "The answer is D. As question clearly suggest requirement is to store customer data that is created during database upgrades and not to minimize database upgrade or outage so only SQS queue before Lamda can store customer data and can be processed after database upgrade.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 832887,
          "date": "Wed 08 Mar 2023 12:28",
          "username": "\t\t\t\tmanthan_1211\t\t\t",
          "content": "Answer is A. <br><br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.howitworks.html#:~:text=Failover%20is%20a,costs%20are%20significant.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829423,
          "date": "Sat 04 Mar 2023 22:21",
          "username": "\t\t\t\tguau\t\t\t",
          "content": "RDS proxy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821861,
          "date": "Sat 25 Feb 2023 23:03",
          "username": "\t\t\t\tJa13\t\t\t",
          "content": "A according to this docs a rds proxy fixes connection errors: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 814104,
          "date": "Sun 19 Feb 2023 14:14",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 808511,
          "date": "Tue 14 Feb 2023 16:21",
          "username": "\t\t\t\tStanislav4907\t\t\t",
          "content": "A proposes to use an Amazon RDS proxy to sit between the Lambda functions and the database. However, using an RDS proxy alone does not solve the problem of Lambda functions not being able to establish database connections during database upgrades. The RDS proxy will only help to pool and manage connections to the database, which can improve performance and scalability, but it does not provide a solution to the underlying problem of database upgrades causing downtime for the Lambda functions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 801311,
          "date": "Tue 07 Feb 2023 20:02",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://www.learnaws.org/2020/12/13/aws-rds-proxy-deep-dive/<br>RDS proxy is currently available for Aurora MySQL, Aurora PostgreSQL, RDS MySQL and RDS PostgreSQL<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>100% correct. Thanks for the reference link. In addition, RDS Proxy for Aurora MySQL is suitable for:<br>Planned maintenance such as a DATABASE UPGRADE<br>A problem with the database instance itself</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829386,
          "date": "Sat 04 Mar 2023 21:28",
          "username": "\t\t\t\tNasosoAuxtyno\t\t\t",
          "content": "100% correct. Thanks for the reference link. In addition, RDS Proxy for Aurora MySQL is suitable for:<br>Planned maintenance such as a DATABASE UPGRADE<br>A problem with the database instance itself",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799451,
          "date": "Mon 06 Feb 2023 06:43",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "Probably A<br><br>RDS proxy can improve application availability in such a situation by waiting for the new database instance to be functional and maintaining any requests received from the application during this time. The end result is that the application is more resilient to issues with the underlying database.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 799126,
          "date": "Sun 05 Feb 2023 20:52",
          "username": "\t\t\t\tHappyHappyHippo\t\t\t",
          "content": "A is correct- RDS proxy Supports RDS (MySQL, PostgreSQL, MariaDB) and Aurora (MySQL, PostgreSQL<br>D is incorrect - no need for FIFO",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 794761,
          "date": "Wed 01 Feb 2023 01:37",
          "username": "\t\t\t\tmichy1234\t\t\t",
          "content": "If you're so confused about why A is the answer, checkout question 69 and you'll see that you can definitely use RDS proxy with aurora MYSQL or Postgres. Also SQS FIFO? I don't see the use in this case.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 793777,
          "date": "Tue 31 Jan 2023 08:39",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "RDS Proxy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 789317,
          "date": "Fri 27 Jan 2023 05:13",
          "username": "\t\t\t\tnetSound\t\t\t",
          "content": "RDS proxySupports RDS (MySQL, PostgreSQL, MariaDB) and Aurora (MySQL, PostgreSQL<br>The answer is A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782744,
          "date": "Fri 20 Jan 2023 21:04",
          "username": "\t\t\t\tHappyHappyHippo\t\t\t",
          "content": "https://www.learnaws.org/2020/12/13/aws-rds-proxy-deep-dive/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779459,
          "date": "Wed 18 Jan 2023 02:05",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "The maximum message size for a message in the SQS queue is only 256KB of text. You aren't going to be able to use that for storing DB data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777501,
          "date": "Mon 16 Jan 2023 10:27",
          "username": "\t\t\t\tJiggs007\t\t\t",
          "content": "A is the correct answer. <br>Because Amazon RDS Proxy is available for Amazon Aurora with MySQL compatibility.I don't see any need for FIFO for this question. I think AWS did it on purpose to make \\\"D\\\" wrong answer.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#88",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#88",
          "answers": [
            {
              "choice": "<p>A. Configure the Requester Pays feature on the company's S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703358,
          "date": "Mon 24 Oct 2022 23:04",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "this question is too vague imho<br>if the question is looking for a way to incur charges to the European company instead of the US company, then requester pay makes sense.<br><br>if they are looking to reduce overall data transfer cost, then B makes sense because the data does not leave the AWS network, thus data transfer cost should be lower technically?<br>A.  makes sense because the US company saves money, but the European company is paying for the charges so there is no overall saving in cost when you look at the big picture<br><br>I will go for B because they are not explicitly stating that they want the other company to pay for the charges<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I disagree. The question says, \\\"the company wants to ensure that ITS data transfer costs remain as low as possible\\\" -- 'it' being the US company. The question would have stayed \\\"ensure that data transfer costs\\\" (without the word 'its') if they meantthe overall data transfer cost.</li><li>I concur with your explanation 100%</li></ul>",
          "upvote_count": "24",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 760193,
          "date": "Wed 28 Dec 2022 19:22",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "I disagree. The question says, \\\"the company wants to ensure that ITS data transfer costs remain as low as possible\\\" -- 'it' being the US company. The question would have stayed \\\"ensure that data transfer costs\\\" (without the word 'its') if they meantthe overall data transfer cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I concur with your explanation 100%</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 832562,
          "date": "Wed 08 Mar 2023 06:29",
          "username": "\t\t\t\tTariqKipkemei\t\t\t",
          "content": "I concur with your explanation 100%",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698244,
          "date": "Tue 18 Oct 2022 14:42",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "\\\"Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur charges associated with others accessing the data. For example, you might use Requester Pays buckets when making available large datasets, such as zip code directories, reference data, geospatial information, or web crawling data.\\\"<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 826531,
          "date": "Thu 02 Mar 2023 06:58",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "B is correct. <br>Because it sates that \\\"The company has started to share the data with a European marketing firm that has S3 buckets. \\\" Eurpean marketing already has S3 buket.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 818654,
          "date": "Thu 23 Feb 2023 02:54",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "A is incorrect. A is not technical consideration. You can not just choose option A to reduce the companies cost. It is about contract.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 814110,
          "date": "Sun 19 Feb 2023 14:23",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "A is correct<br>The company wants to ensure that its data transfer costs remain as low as possible.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 811807,
          "date": "Fri 17 Feb 2023 12:39",
          "username": "\t\t\t\tAlhaz\t\t\t",
          "content": "It is Marketing firm where data is to be transferred, Obviously US company wants to save money and wants to Europe company will pay",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 805454,
          "date": "Sat 11 Feb 2023 18:19",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "The only reasonable answer. <br><br>Because: Cross-region replication is just possible for new objects not for existing objects.<br>Replication of the data to the other company would be just possible with the new S3 batch replication, gut also would cost money: <br>https://aws.amazon.com/blogs/aws/new-replicate-existing-objects-with-amazon-s3-batch-replication/<br>And open the bucket to the other company also cost us money if they download the data",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791320,
          "date": "Sun 29 Jan 2023 03:11",
          "username": "\t\t\t\tChiggaBoy\t\t\t",
          "content": "Ha to be A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Type *Has</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791321,
          "date": "Sun 29 Jan 2023 03:11",
          "username": "\t\t\t\tChiggaBoy\t\t\t",
          "content": "Type *Has",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790739,
          "date": "Sat 28 Jan 2023 16:38",
          "username": "\t\t\t\tLonojack\t\t\t",
          "content": "kEY wORD: \\\"Its\\\" meaning US company!This really is just a very poorly worded question. That ONE word took me from Answer \\\"B\\\" to Answer \\\"A\\\".Splitting Atoms here!!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 789105,
          "date": "Thu 26 Jan 2023 21:50",
          "username": "\t\t\t\tsipofa2049\t\t\t",
          "content": "\\\"With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket\\\"<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 788639,
          "date": "Thu 26 Jan 2023 12:03",
          "username": "\t\t\t\tsassy2023\t\t\t",
          "content": "Having requestor pay for transfer is the lowest one can go. literally xD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 788634,
          "date": "Thu 26 Jan 2023 11:58",
          "username": "\t\t\t\tkbaruu\t\t\t",
          "content": "The correct answer is A!",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 783105,
          "date": "Sat 21 Jan 2023 09:00",
          "username": "\t\t\t\tdexpos\t\t\t",
          "content": "With cross-region replication is it possible to transfer data cross accounts? TY",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782196,
          "date": "Fri 20 Jan 2023 13:10",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "The question says the company holding the original data wants to keep data transfer fees as low as possible. Requester Pays is a feature that makes the requester (as opposed to the bucket owner) pay for data transfer fees, IF the requester also has an AWS account (which in this case, they do, as the requester also has S3 buckets).<br>This fulfils the requirement.<br><br>Option B does not fit the requirements, as transfer fees incur when replicating data across AWS Regions. The company holding the original data and the marketing firm in Europe are 2 separate companies, hence the company with the original data does not care about reducing the marketing firm's costs (which option B would do).",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 777506,
          "date": "Mon 16 Jan 2023 10:32",
          "username": "\t\t\t\tJiggs007\t\t\t",
          "content": "A is the correct answer..<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774012,
          "date": "Fri 13 Jan 2023 03:05",
          "username": "\t\t\t\tedd270895\t\t\t",
          "content": "This is the one.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 770383,
          "date": "Mon 09 Jan 2023 14:04",
          "username": "\t\t\t\tRmnx\t\t\t",
          "content": "Option A- data transfer cost is born my 3rd party Mkt.co.<br>Refer to Guidance below.<br>Cost of Storage is still born and With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#89",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution.<br>What should a solutions architect do to secure the audit documents?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#89",
          "answers": [
            {
              "choice": "<p>A. Enable the versioning and MFA Delete features on the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698259,
          "date": "Tue 18 Oct 2022 15:12",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Same as Question #44",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778114,
          "date": "Mon 16 Jan 2023 19:43",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "only accidental deletion should be avoided. IAM policy will completely remove their access.hence, MFA is the right choice.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 770873,
          "date": "Mon 09 Jan 2023 22:09",
          "username": "\t\t\t\tkarbob\t\t\t",
          "content": "what about : IAM policies are used to specify permissions for AWS resources, and they can be used to allow or deny specific actions on those resources.<br>{<br>\\\"Version\\\": \\\"2012-10-17\\\",<br>\\\"Statement\\\": [<br>{<br>\\\"Sid\\\": \\\"DenyDeleteObject\\\",<br>\\\"Effect\\\": \\\"Deny\\\",<br>\\\"Action\\\": \\\"s3:DeleteObject\\\",<br>\\\"Resource\\\": [<br>\\\"arn:aws:s3:::my-bucket/my-object\\\",<br>\\\"arn:aws:s3:::my-bucket\\\"<br>]<br>}<br>]<br>}<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>only accidental deletion should be avoided. IAM policy will completely remove their access.hence, MFA is the right choice.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778112,
          "date": "Mon 16 Jan 2023 19:43",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "only accidental deletion should be avoided. IAM policy will completely remove their access.hence, MFA is the right choice.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759131,
          "date": "Tue 27 Dec 2022 23:46",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution architect should do Option A: Enable the versioning and MFA Delete features on the S3 bucket.<br><br>This will secure the audit documents by providing an additional layer of protection against accidental deletion. With versioning enabled, any deleted or overwritten objects in the S3 bucket will be preserved as previous versions, allowing the company to recover them if needed. With MFA Delete enabled, any delete request made to the S3 bucket will require the use of an MFA code, which provides an additional layer of security.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B: Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account, would not provide protection against accidental deletion. <br><br>Option C: Adding an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates, which would not provide protection against accidental deletion outside of the specified audit dates. <br><br>Option D: Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key, would not provide protection against accidental deletion.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759132,
          "date": "Tue 27 Dec 2022 23:46",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B: Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account, would not provide protection against accidental deletion. <br><br>Option C: Adding an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates, which would not provide protection against accidental deletion outside of the specified audit dates. <br><br>Option D: Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key, would not provide protection against accidental deletion.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 747605,
          "date": "Fri 16 Dec 2022 22:38",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is the right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723749,
          "date": "Mon 21 Nov 2022 17:29",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717506,
          "date": "Sun 13 Nov 2022 21:22",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Enable the versioning and MFA Delete features on the S3 bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#90",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours.<br>The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue.<br>Which solution will meet this requirement with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#90",
          "answers": [
            {
              "choice": "<p>A. Modify the DB instance to be a Multi-AZ deployment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a read replica of the database. Configure the script to query only the read replica.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Instruct the development team to manually export the entries in the database at the end of each day.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon ElastiCache to cache the common queries that the script runs against the database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696424,
          "date": "Sun 16 Oct 2022 19:48",
          "username": "\t\t\t\talvarez100\t\t\t",
          "content": "Elasti Cache if for reading common results. The script is looking for new movies added. Read replica would be the best choice.",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 713059,
          "date": "Mon 07 Nov 2022 14:15",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "• You have a production DB that is taking on a normal load<br> • You want to run a reporting application to run some analytics<br> • You create a read replica to run the new workload there<br> • The prod application is unaffected<br> • Read replicas are used for SELECT (=read) only kind of statements<br>Therefore I believe B to be the better answer.<br><br>As for \\\"D\\\" - ElastiCache use cases are:<br>1. Your data is slow or expensive to get when compared to cache retrieval.<br>2. Users access your data often.<br>3. Your data stays relatively the same, or if it changes quickly staleness is not a large issue.<br><br>1 - Somewhat true.<br>2 - Not true for our case.<br>3 - Also not true. The data changes throughout the day.<br><br>For my understanding, caching has to do with millisecond results, high-performance reads. These are not the issues mentioned in the questions, therefore B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I will support this by point to the question : \\\" with the LEAST operational overhead?\\\" <br><br>Configuring the read replica is much easier than configuring and integrating new service.</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 792714,
          "date": "Mon 30 Jan 2023 13:05",
          "username": "\t\t\t\tNitiATOS\t\t\t",
          "content": "I will support this by point to the question : \\\" with the LEAST operational overhead?\\\" <br><br>Configuring the read replica is much easier than configuring and integrating new service.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766125,
          "date": "Wed 04 Jan 2023 23:40",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Reason to have a Read Replica is improved performance (key word) which is native to RDS. Elastic Cache may have misses. <br><br>The other way of looking at this question is : Elastic Cache could be beneficial for development tasks (and hence improve the overall DB performance). But then, Option D mentions that the queries for scripts are cached, and not the DB content (or metadata). This may not necessarily improve the performance of the DB.  <br><br>So, Option B is the best answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 765426,
          "date": "Wed 04 Jan 2023 10:14",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "The correct answer would be option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759816,
          "date": "Wed 28 Dec 2022 13:38",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "D is incorrect. The requirement says LEAST OPERATIONAL OVERHEAD.  Here, using Elasticache you need to heavily modify your scripts/code to accommodate Elasticache into the architecture which is higher Operational overhead compared to turning DB into Muti-AZ mode.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751884,
          "date": "Wed 21 Dec 2022 05:58",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The best solution to meet the requirement with the least operational overhead would be to create a read replica of the database and configure the script to query only the read replica. Option B. <br><br>A read replica is a fully managed database that is kept in sync with the primary database. Read replicas allow you to scale out read-heavy workloads by distributing read queries across multiple databases. This can help improve the performance of the database and reduce the impact on the primary database.<br><br>By configuring the script to query the read replica, the development team can continue to use the primary database for development tasks, while the script's queries will be directed to the read replica. This will reduce the load on the primary database and improve its performance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***WRONG***<br>Option A (modifying the DB instance to be a Multi-AZ deployment) would not address the issue of the script's queries impacting the primary database. <br><br>Option C (instructing the development team to manually export the entries in the database at the end of each day) would not be an efficient solution as it would require manual effort and could lead to data loss if the export process is not done properly. <br><br>Option D (using Amazon ElastiCache to cache the common queries) could improve the performance of the script's queries, but it would not address the issue of the script's queries impacting the primary database.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751885,
          "date": "Wed 21 Dec 2022 05:58",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***WRONG***<br>Option A (modifying the DB instance to be a Multi-AZ deployment) would not address the issue of the script's queries impacting the primary database. <br><br>Option C (instructing the development team to manually export the entries in the database at the end of each day) would not be an efficient solution as it would require manual effort and could lead to data loss if the export process is not done properly. <br><br>Option D (using Amazon ElastiCache to cache the common queries) could improve the performance of the script's queries, but it would not address the issue of the script's queries impacting the primary database.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 750913,
          "date": "Tue 20 Dec 2022 14:13",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "bis correct <br>Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749870,
          "date": "Mon 19 Dec 2022 14:46",
          "username": "\t\t\t\tyoben84\t\t\t",
          "content": "D is not reducing operational overhead, since there is development effort to integrate the app to a cache. you have to manage the cluster of theelastic cache",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749865,
          "date": "Mon 19 Dec 2022 14:42",
          "username": "\t\t\t\tyoben84\t\t\t",
          "content": "It's a DB instance not managed instance so you can't use a read replica.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749863,
          "date": "Mon 19 Dec 2022 14:41",
          "username": "\t\t\t\tjuliansierra\t\t\t",
          "content": "The script makes two tasks. Firsts, the script runs queries RECORD the number of new movies that have been added to the database. In the second task, the script must report a final total. The question ask about how to improve the database behavior when this script is running. I don't know if B is a valid answer because you can not RECORD in a only-write database. But the other 3 options makes no sense for me too. So, it's difficult give a certain answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747607,
          "date": "Fri 16 Dec 2022 22:41",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B - Add read replica and run the script against read replica endpoints.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 740058,
          "date": "Fri 09 Dec 2022 12:58",
          "username": "\t\t\t\tshw1981\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 726990,
          "date": "Fri 25 Nov 2022 18:55",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "Caching works best for static contents. When you run a total, you need to go through all the records in a table. The question is not constructed properly. Best solution is to create an index on the added date, it won't take long, nor heavy io/cpu to get the total number of newly added total for the day. This approach takes minimal effort, does not incur any extra charge, better than both B and D. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I would choose B as the answer. For the stated type of queries It's better than D .</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726994,
          "date": "Fri 25 Nov 2022 19:00",
          "username": "\t\t\t\tVesperia\t\t\t",
          "content": "I would choose B as the answer. For the stated type of queries It's better than D .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723750,
          "date": "Mon 21 Nov 2022 17:30",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720378,
          "date": "Thu 17 Nov 2022 10:50",
          "username": "\t\t\t\thtangga\t\t\t",
          "content": "B is more make sense for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 713504,
          "date": "Tue 08 Nov 2022 06:29",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "Not D as apps have to be re-written to take advantage of elasticache APIs - that is too much overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 711935,
          "date": "Sat 05 Nov 2022 19:05",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "Even though B is correct, it says least operational overhead which is D.  Like the other person said, AWS used similar use cases.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#91",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has applications that run on Amazon EC2 instances in a VPC.  One of the applications needs to call the Amazon S3 API to store and read objects. According to the company's security regulations, no traffic from the applications is allowed to travel across the internet.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#91",
          "answers": [
            {
              "choice": "<p>A. Configure an S3 gateway endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an S3 bucket in a private subnet.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an S3 bucket in the same AWS Region as the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a NAT gateway in the same subnet as the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697103,
          "date": "Mon 17 Oct 2022 10:13",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC.  It should be option A. <br><br>https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 752651,
          "date": "Wed 21 Dec 2022 19:54",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The correct solution is Option A (Configure an S3 gateway endpoint.)<br><br>A gateway endpoint is a VPC endpoint that you can use to connect to Amazon S3 from within your VPC.  Traffic between your VPC and Amazon S3 never leaves the Amazon network, so it doesn't traverse the internet. This means you can access Amazon S3 without the need to use a NAT gateway or a VPN connection.<br><br>***WRONG***<br>Option B (creating an S3 bucket in a private subnet) is not a valid solution because S3 buckets do not have subnets.<br><br>Option C (creating an S3 bucket in the same AWS Region as the EC2 instances) is not a requirement for meeting the given security regulations.<br><br>Option D (configuring a NAT gateway in the same subnet as the EC2 instances) is not a valid solution because it would allow traffic to leave the VPC and travel across the Internet.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763334,
          "date": "Sun 01 Jan 2023 21:38",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "S3 Gateway Endpoint is a VPC endpoint,",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 737390,
          "date": "Wed 07 Dec 2022 04:31",
          "username": "\t\t\t\tlangiac\t\t\t",
          "content": "https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723757,
          "date": "Mon 21 Nov 2022 17:47",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#92",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC. <br>Which combination of steps should a solutions architect take to accomplish this? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#92",
          "answers": [
            {
              "choice": "<p>A. Configure a VPC gateway endpoint for Amazon S3 within the VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a bucket policy to make the objects in the S3 bucket public.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a bucket policy that limits access to only the application tier running in the VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 814136,
          "date": "Sun 19 Feb 2023 14:51",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "The key part that many miss out on is 'Combination' <br>The other answers are not wrong but <br>A works with C and not with the rest as they need an internet connection.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 814119,
          "date": "Sun 19 Feb 2023 14:33",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "AC is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 810000,
          "date": "Wed 15 Feb 2023 22:29",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-noauthentication/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 778130,
          "date": "Mon 16 Jan 2023 19:58",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "c & D for security. Aaddresses accessibility which is not a concern here imo",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 774636,
          "date": "Fri 13 Jan 2023 16:54",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "A & C.  <br>When the question is about security, do not select the answer that storing credential in EC2. This shall be done by using IAM policy + role or Secret Manager.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 764465,
          "date": "Tue 03 Jan 2023 11:50",
          "username": "\t\t\t\tmhmt4438\t\t\t",
          "content": "C and D <br>To provide secure access to the S3 bucket from the application tier running on EC2 instances inside a VPC, you should create a bucket policy that limits access to only the application tier running in the VPC.  This will ensure that only the application tier has access to the bucket and its contents.<br><br>Additionally, you should create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance. This will allow the EC2 instance to access the S3 bucket using the IAM user's permissions.<br><br>Option A, configuring a VPC gateway endpoint for Amazon S3 within the VPC, would not provide any additional security for the S3 bucket.<br><br>Option B, creating a bucket policy to make the objects in the S3 bucket public, would not provide sufficient security for sensitive user information.<br><br>Option E, creating a NAT instance and having the EC2 instances use the NAT instance to access the S3 bucket, would not provide any additional security for the S3 bucket",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757828,
          "date": "Mon 26 Dec 2022 21:34",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C is right among the choice. <br>But instead of having bucket policy for VPC access better option would be to create a role with specific S3 bucket access and attach that roleEC2 instances that needs access to S3 buckets.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 754892,
          "date": "Sat 24 Dec 2022 14:22",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "A & C looks correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 752657,
          "date": "Wed 21 Dec 2022 20:01",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>The solutions architect should take the following steps to accomplish secure access to the S3 bucket from the application tier running on Amazon EC2 instances inside a VPC:<br>C.  Create a bucket policy that limits access to only the application tier running in the VPC. D.  Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>After reviewing thoroughly the AWS documentation and the other answers in the discussion, I am taking back my previous answer. The correct answer for me is Option A and Option C. <br><br>To provide secure access to the S3 bucket from the application tier running on Amazon EC2 instances inside the VPC, the solutions architect should take the following combination of steps:<br><br> Option A: Configure a VPC gateway endpoint for Amazon S3 within the VPC. <br><br>Amazon S3 VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html<br><br>Option C: Create a bucket policy that limits access to only the application tier running in the VPC. <br><br>Amazon S3 Bucket Policies: https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html<br><br>AWS Identity and Access Management (IAM) Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</li><li>***INCORRECT***<br>Option C ensures that the S3 bucket is only accessible to the application tier running in the VPC, while Option D allows the EC2 instances to access the S3 bucket using the IAM credentials of the IAM user. This ensures that access to the S3 bucket is secure and controlled through IAM.<br><br>Option A is incorrect because configuring a VPC gateway endpoint for Amazon S3 does not directly control access to the S3 bucket.<br><br>Option B is incorrect because making the objects in the S3 bucket public would not provide secure access to the bucket.<br><br>Option E is incorrect because creating a NAT instance is not necessary to provide secure access to the S3 bucket from the application tier running on EC2 instances in the VPC. </li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 759137,
          "date": "Tue 27 Dec 2022 23:58",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "After reviewing thoroughly the AWS documentation and the other answers in the discussion, I am taking back my previous answer. The correct answer for me is Option A and Option C. <br><br>To provide secure access to the S3 bucket from the application tier running on Amazon EC2 instances inside the VPC, the solutions architect should take the following combination of steps:<br><br> Option A: Configure a VPC gateway endpoint for Amazon S3 within the VPC. <br><br>Amazon S3 VPC Endpoints: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html<br><br>Option C: Create a bucket policy that limits access to only the application tier running in the VPC. <br><br>Amazon S3 Bucket Policies: https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html<br><br>AWS Identity and Access Management (IAM) Policies: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 752659,
          "date": "Wed 21 Dec 2022 20:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***INCORRECT***<br>Option C ensures that the S3 bucket is only accessible to the application tier running in the VPC, while Option D allows the EC2 instances to access the S3 bucket using the IAM credentials of the IAM user. This ensures that access to the S3 bucket is secure and controlled through IAM.<br><br>Option A is incorrect because configuring a VPC gateway endpoint for Amazon S3 does not directly control access to the S3 bucket.<br><br>Option B is incorrect because making the objects in the S3 bucket public would not provide secure access to the bucket.<br><br>Option E is incorrect because creating a NAT instance is not necessary to provide secure access to the S3 bucket from the application tier running on EC2 instances in the VPC. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727727,
          "date": "Sat 26 Nov 2022 18:51",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "Option AC",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 723759,
          "date": "Mon 21 Nov 2022 17:48",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717503,
          "date": "Sun 13 Nov 2022 21:19",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "AC is the correct answer in the use case",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 715621,
          "date": "Fri 11 Nov 2022 00:45",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Options A and E<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Typo it should be A and C</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715623,
          "date": "Fri 11 Nov 2022 00:47",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Typo it should be A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 698937,
          "date": "Wed 19 Oct 2022 12:52",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Options A and C. <br><br>https://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-no-authentication/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AC"
        }
      ]
    },
    {
      "question_id": "#93",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability.<br>The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes.<br>A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#93",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 812361,
          "date": "Fri 17 Feb 2023 21:49",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 806122,
          "date": "Sun 12 Feb 2023 09:49",
          "username": "\t\t\t\tjohn2323\t\t\t",
          "content": "Database cloning is the best answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 757881,
          "date": "Mon 26 Dec 2022 23:03",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Database cloning is right answer here.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752672,
          "date": "Wed 21 Dec 2022 20:15",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The recommended solution is Option B: Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on-demand.<br><br>To alleviate the application latency issue, the recommended solution is to use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production, and use database cloning to create the staging database on-demand. This allows the development team to continue using the staging environment without delay, while also providing elasticity and availability for the production application.<br><br>Therefore, Options A, C, and D are not recommended<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A: Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populating the staging database by implementing a backup and restore process that uses the mysqldump utility is not the recommended solution because it involves taking a full export of the production database, which can cause unacceptable application latency.<br><br>Option C: Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Using the standby instance for the staging database is not the recommended solution because it does not give the development team the ability to continue using the staging environment without delay. The standby instance is used for failover in case of a production instance failure, and it is not intended for use as a staging environment.</li><li>Option D: Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populating the staging database by implementing a backup and restore process that uses the mysqqldump utility is not the recommended solution because it involves taking a full export of the production database, which can cause unacceptable application latency.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752674,
          "date": "Wed 21 Dec 2022 20:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A: Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populating the staging database by implementing a backup and restore process that uses the mysqldump utility is not the recommended solution because it involves taking a full export of the production database, which can cause unacceptable application latency.<br><br>Option C: Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Using the standby instance for the staging database is not the recommended solution because it does not give the development team the ability to continue using the staging environment without delay. The standby instance is used for failover in case of a production instance failure, and it is not intended for use as a staging environment.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D: Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populating the staging database by implementing a backup and restore process that uses the mysqqldump utility is not the recommended solution because it involves taking a full export of the production database, which can cause unacceptable application latency.</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 752675,
          "date": "Wed 21 Dec 2022 20:17",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D: Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populating the staging database by implementing a backup and restore process that uses the mysqqldump utility is not the recommended solution because it involves taking a full export of the production database, which can cause unacceptable application latency.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 747499,
          "date": "Fri 16 Dec 2022 19:33",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B is right.<br>You can not access Standby instance for Read in RDS Multi-AZ Deployments.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This is correct, stand by instances cannot be used for read/write and is for failover targets. Read Replicas can be used for that so B is correct.</li><li>In a RDS Multi-AZ deployment, you can use the standby instance for read-only purposes, such as running queries and reporting. This is known as a \\\"read replica.\\\" You can create one or more read replicas of a DB instance and use them to offload read traffic from the primary instance.<br>https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 751374,
          "date": "Tue 20 Dec 2022 20:10",
          "username": "\t\t\t\taadi7\t\t\t",
          "content": "This is correct, stand by instances cannot be used for read/write and is for failover targets. Read Replicas can be used for that so B is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 751369,
          "date": "Tue 20 Dec 2022 20:07",
          "username": "\t\t\t\taadi7\t\t\t",
          "content": "In a RDS Multi-AZ deployment, you can use the standby instance for read-only purposes, such as running queries and reporting. This is known as a \\\"read replica.\\\" You can create one or more read replicas of a DB instance and use them to offload read traffic from the primary instance.<br>https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 743419,
          "date": "Tue 13 Dec 2022 00:40",
          "username": "\t\t\t\t333666999\t\t\t",
          "content": "why not C",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 727730,
          "date": "Sat 26 Nov 2022 18:56",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 717757,
          "date": "Mon 14 Nov 2022 08:19",
          "username": "\t\t\t\tpspinelli19\t\t\t",
          "content": "Amazon Aurora Fast Database Cloning is what is required here.<br>https://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 707665,
          "date": "Sun 30 Oct 2022 08:22",
          "username": "\t\t\t\tKLLIM\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 697473,
          "date": "Mon 17 Oct 2022 16:39",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "B<br>Database cloning",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#94",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.<br>Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.<br>Which solution meets these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#94",
          "answers": [
            {
              "choice": "<p>A. Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707826,
          "date": "Sun 30 Oct 2022 14:22",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Option C<br>Dynamo DB is a NoSQL-JSON supported<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>also Use an AWS Lambda - serverless - less operational overhead</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 707827,
          "date": "Sun 30 Oct 2022 14:23",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "also Use an AWS Lambda - serverless - less operational overhead",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 762536,
          "date": "Sat 31 Dec 2022 08:40",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Dynamo DB is a NoSQL-JSON supported",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752680,
          "date": "Wed 21 Dec 2022 20:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C, Configuring Amazon S3 to send an event notification to an Amazon Simple Queue Service (SQS) queue and using an AWS Lambda function to read from the queue and process the data, would likely be the solution with the least operational overhead.<br><br>AWS Lambda is a serverless computing service that allows you to run code without the need to provision or manage infrastructure. When a new file is uploaded to Amazon S3, it can trigger an event notification which sends a message to an SQS queue. The Lambda function can then be set up to be triggered by messages in the queue, and it can process the data and store the resulting JSON file in Amazon DynamoDB. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Using a serverless solution like AWS Lambda can help to reduce operational overhead because it automatically scales to meet demand and does not require you to provision and manage infrastructure. Additionally, using an SQS queue as a buffer between the S3 event notification and the Lambda function can help to decouple the processing of the data from the uploading of the data, allowing the processing to happen asynchronously and improving the overall efficiency of the system.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752682,
          "date": "Wed 21 Dec 2022 20:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Using a serverless solution like AWS Lambda can help to reduce operational overhead because it automatically scales to meet demand and does not require you to provision and manage infrastructure. Additionally, using an SQS queue as a buffer between the S3 event notification and the Lambda function can help to decouple the processing of the data from the uploading of the data, allowing the processing to happen asynchronously and improving the overall efficiency of the system.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747511,
          "date": "Fri 16 Dec 2022 19:52",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C as JSON is supported by DynamoDB.  RDS or AuroraDB are not suitable for JSON data.<br>A - Because this is not a Bigdata analytics usecase.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743525,
          "date": "Tue 13 Dec 2022 03:43",
          "username": "\t\t\t\tgloritown\t\t\t",
          "content": "CCCCCCCC",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 739749,
          "date": "Fri 09 Dec 2022 05:11",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "Answer C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 730289,
          "date": "Tue 29 Nov 2022 12:10",
          "username": "\t\t\t\tHussamShokr\t\t\t",
          "content": "answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 728155,
          "date": "Sun 27 Nov 2022 12:28",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "cccccccccccc",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 727732,
          "date": "Sat 26 Nov 2022 19:00",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723760,
          "date": "Mon 21 Nov 2022 17:51",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717783,
          "date": "Mon 14 Nov 2022 09:06",
          "username": "\t\t\t\tPamban\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/67958-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 717501,
          "date": "Sun 13 Nov 2022 21:16",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "SQS + LAMDA + JSON to Dynamo DB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 709490,
          "date": "Wed 02 Nov 2022 01:36",
          "username": "\t\t\t\tHunkie\t\t\t",
          "content": "With explanations <br><br>https://www.examtopics.com/discussions/amazon/view/67958-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#95",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read traffic from write traffic. A solutions architect needs to optimize the application's performance quickly.<br>What should the solutions architect recommend?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#95",
          "answers": [
            {
              "choice": "<p>A. Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 752689,
          "date": "Wed 21 Dec 2022 20:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solutions architect should recommend option D: Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database.<br><br>Creating read replicas allows the application to offload read traffic from the source database, improving its performance. The read replicas should be configured with the same compute and storage resources as the source database to ensure that they can handle the read workload effectively.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 727733,
          "date": "Sat 26 Nov 2022 19:01",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723762,
          "date": "Mon 21 Nov 2022 17:53",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712840,
          "date": "Mon 07 Nov 2022 07:19",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "D<br><br>https://www.examtopics.com/discussions/amazon/view/46461-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709492,
          "date": "Wed 02 Nov 2022 01:42",
          "username": "\t\t\t\tHunkie\t\t\t",
          "content": "If you scale the source DB instance, also scale the read replicas.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 698963,
          "date": "Wed 19 Oct 2022 13:23",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "D is correct.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#96",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An Amazon EC2 administrator created the following policy associated with an IAM group containing several users:<br><img src=\"https://img.examtopics.com/aws-certified-solutions-architect-associate-saa-c03/image1.png\"><br>What is the effect of this policy?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#96",
          "answers": [
            {
              "choice": "<p>A. Users can terminate an EC2 instance in any AWS Region except us-east-1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 733433,
          "date": "Fri 02 Dec 2022 05:04",
          "username": "\t\t\t\tSubh_fidelity\t\t\t",
          "content": "C is correct.<br>0.0/24 , the following five IP addresses are reserved:<br>0.0: Network address.<br>0.1: Reserved by AWS for the VPC router.<br>0.2: Reserved by AWS. The IP address of the DNS server is the base of the VPC network range plus two. ...<br>0.3: Reserved by AWS for future use.<br>0.255: Network broadcast address.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 719187,
          "date": "Wed 16 Nov 2022 00:46",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Option C is the correct answer. <br><br> if CIDR block 10.100.100.0/24, then reserved IP addresses are:<br>• 10.100.100.0 – Network Address<br>• 10.100.100.1 – reserved by AWS for the VPC router<br>• 10.100.100.2 – reserved by AWS for mapping to Amazon-provided DNS<br>• 10.100.100.3 – reserved by AWS for future use<br>• 10.100.100.255 – Network Broadcast Address. AWS does not support broadcast in a VPC,<br><br>The above numbers cannot be used.This rules out the option B",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 800966,
          "date": "Tue 07 Feb 2023 14:58",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "IAM Conditions mean you can choose to grant/deny access to principals only if specified conditions are met.<br><br>In our case, StringNotEquals \\\"us-east-1\\\" means deny everything unless the region is us-east-1<br><br>An easier way to understand it but less effective ofcourse to achieve the same result would be configuring deny all ec2 if StringEquals: *state any other region except for us-east-1*<br><br>Correct answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 793905,
          "date": "Tue 31 Jan 2023 10:26",
          "username": "\t\t\t\tChalamalli\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777003,
          "date": "Sun 15 Jan 2023 20:40",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Deny overrules Allow. The first statement allows 100.100.254. but the second statement is denied which is the region us-east-1.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777002,
          "date": "Sun 15 Jan 2023 20:39",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "Deny overrules Allow. The first statement allows 100.100.254. but the second statement is denied which is the region us-east-1.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 772428,
          "date": "Wed 11 Jan 2023 12:22",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "Please disregard the initial answer. D is the CORRECT answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 772426,
          "date": "Wed 11 Jan 2023 12:19",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "C is the correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765848,
          "date": "Wed 04 Jan 2023 16:59",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "What the policy means:<br>1. Allow termination of any instance if user's source IP address is 100.100.254.<br>2. Deny termination of instances that are not in the us-east-1 Combining this two, you get:<br>“Allow instance termination in the us-east-1 region if the user's source IP address is 10.100.100.254. Deny termination operation on other regions.”",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 763205,
          "date": "Sun 01 Jan 2023 14:46",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "as the policy prevents anyone from doing any EC2 action on any region except us-east-1 and allows only users with source ip 10.100.100.0/24 to terminate instances. So user with source ip 10.100.100.254 can terminate instances in us-east-1 region.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 759197,
          "date": "Wed 28 Dec 2022 01:32",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "please read carefuly ,it says policy denies all EC2 actions in the if region doesn't not equals us-east-1 region,hence its deny for all regions except us-east-1.,now 1st deny is good but its not applicable for us-east-1,this deny is conditional,hence It will allow us-east-1 with source ip 10.100.100.254",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752698,
          "date": "Wed 21 Dec 2022 20:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is D.  Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.<br><br>The policy contains two statements. The first statement allows users to terminate any EC2 instance as long as the user's source IP address is within the range of 10.100.100.0/24. <br><br>The second statement denies all EC2 actions (indicated by the \\\"ec2:\\\" action) for all resources (\\\"\\\") except in the us-east-1 region. Since the second statement has a higher priority than the first statement, users who have a source IP address of 10.100.100.254 will not be able to terminate an EC2 instance in the us-east-1 region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>***Other Options are WRONG***<br><br>Option A is incorrect because the policy does not allow users to terminate EC2 instances in any region. Instead, the policy denies all EC2 actions in all regions except for the us-east-1 region.<br><br>Option B is incorrect because the policy does not restrict actions to a specific IP address or to the us-east-1 region. Instead, the policy allows users to terminate any EC2 instance as long as their source IP address is within the range of 10.100.100.0/24, and it denies all EC2 actions in all regions except for the us-east-1 region.<br><br>Option C is incorrect because the policy does not allow users to terminate EC2 instances in the us-east-1 region when their source IP is 10.100.100.254. Instead, the policy denies all EC2 actions in the us-east-1 region when the user's source IP is 10.100.100.254.</li><li>You are correct. \\\"Deny\\\" overrides \\\"Allow\\\". D is the definitely correct answer.<br>CIDR discussion is pointless.</li><li>please read carefuly ,it says policy denies all EC2 actions in the if region doesn't not equals us-east-1 region,hence its deny for all regions except us-east-1.,now 1st deny is good but its not applicable for us-east-1,this deny is conditional,hence It will allow us-east-1 with source ip 10.100.100.254</li><li>\\\"StringNotEquals\\\" is a condition operator used in AWS Identity and Access Management (IAM) policies. It checks if a string value is not equal to the specified string value in the policy statement. If the condition evaluates to true, the action in the policy statement is allowed. If the condition evaluates to false, the action is denied.<br>Hence, if the condition specified in the \\\"Condition\\\" block of a policy statement evaluates to true, then the action defined in the \\\"Effect\\\" block (Deny or Allow) will take effect.<br> Buruguduystunstugudunstuy is right D</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752700,
          "date": "Wed 21 Dec 2022 20:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***Other Options are WRONG***<br><br>Option A is incorrect because the policy does not allow users to terminate EC2 instances in any region. Instead, the policy denies all EC2 actions in all regions except for the us-east-1 region.<br><br>Option B is incorrect because the policy does not restrict actions to a specific IP address or to the us-east-1 region. Instead, the policy allows users to terminate any EC2 instance as long as their source IP address is within the range of 10.100.100.0/24, and it denies all EC2 actions in all regions except for the us-east-1 region.<br><br>Option C is incorrect because the policy does not allow users to terminate EC2 instances in the us-east-1 region when their source IP is 10.100.100.254. Instead, the policy denies all EC2 actions in the us-east-1 region when the user's source IP is 10.100.100.254.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are correct. \\\"Deny\\\" overrides \\\"Allow\\\". D is the definitely correct answer.<br>CIDR discussion is pointless.</li><li>please read carefuly ,it says policy denies all EC2 actions in the if region doesn't not equals us-east-1 region,hence its deny for all regions except us-east-1.,now 1st deny is good but its not applicable for us-east-1,this deny is conditional,hence It will allow us-east-1 with source ip 10.100.100.254</li><li>\\\"StringNotEquals\\\" is a condition operator used in AWS Identity and Access Management (IAM) policies. It checks if a string value is not equal to the specified string value in the policy statement. If the condition evaluates to true, the action in the policy statement is allowed. If the condition evaluates to false, the action is denied.<br>Hence, if the condition specified in the \\\"Condition\\\" block of a policy statement evaluates to true, then the action defined in the \\\"Effect\\\" block (Deny or Allow) will take effect.<br> Buruguduystunstugudunstuy is right D</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 758043,
          "date": "Tue 27 Dec 2022 02:29",
          "username": "\t\t\t\tTys\t\t\t",
          "content": "You are correct. \\\"Deny\\\" overrides \\\"Allow\\\". D is the definitely correct answer.<br>CIDR discussion is pointless.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>please read carefuly ,it says policy denies all EC2 actions in the if region doesn't not equals us-east-1 region,hence its deny for all regions except us-east-1.,now 1st deny is good but its not applicable for us-east-1,this deny is conditional,hence It will allow us-east-1 with source ip 10.100.100.254</li><li>\\\"StringNotEquals\\\" is a condition operator used in AWS Identity and Access Management (IAM) policies. It checks if a string value is not equal to the specified string value in the policy statement. If the condition evaluates to true, the action in the policy statement is allowed. If the condition evaluates to false, the action is denied.<br>Hence, if the condition specified in the \\\"Condition\\\" block of a policy statement evaluates to true, then the action defined in the \\\"Effect\\\" block (Deny or Allow) will take effect.<br> Buruguduystunstugudunstuy is right D</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759196,
          "date": "Wed 28 Dec 2022 01:32",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "please read carefuly ,it says policy denies all EC2 actions in the if region doesn't not equals us-east-1 region,hence its deny for all regions except us-east-1.,now 1st deny is good but its not applicable for us-east-1,this deny is conditional,hence It will allow us-east-1 with source ip 10.100.100.254<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"StringNotEquals\\\" is a condition operator used in AWS Identity and Access Management (IAM) policies. It checks if a string value is not equal to the specified string value in the policy statement. If the condition evaluates to true, the action in the policy statement is allowed. If the condition evaluates to false, the action is denied.<br>Hence, if the condition specified in the \\\"Condition\\\" block of a policy statement evaluates to true, then the action defined in the \\\"Effect\\\" block (Deny or Allow) will take effect.<br> Buruguduystunstugudunstuy is right D</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 792363,
          "date": "Mon 30 Jan 2023 04:57",
          "username": "\t\t\t\tgogod2\t\t\t",
          "content": "\\\"StringNotEquals\\\" is a condition operator used in AWS Identity and Access Management (IAM) policies. It checks if a string value is not equal to the specified string value in the policy statement. If the condition evaluates to true, the action in the policy statement is allowed. If the condition evaluates to false, the action is denied.<br>Hence, if the condition specified in the \\\"Condition\\\" block of a policy statement evaluates to true, then the action defined in the \\\"Effect\\\" block (Deny or Allow) will take effect.<br> Buruguduystunstugudunstuy is right D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749258,
          "date": "Sun 18 Dec 2022 23:43",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748301,
          "date": "Sat 17 Dec 2022 19:45",
          "username": "\t\t\t\tOuk\t\t\t",
          "content": "A : Should be 'Users can terminate an EC2 instance in us-east-1.' <br>B : 10.100.100.0 / 10.100.100.1 / 10.100.100.2 / 10.100.100.3 / 10.100.100.255 are reserved<br>C : correct<br>D : Users 'can' terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 723764,
          "date": "Mon 21 Nov 2022 17:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 717500,
          "date": "Sun 13 Nov 2022 21:14",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "should be Last IP 10.100.100.254<br><br>Not an option<br>10.100.100.1: Reserved by AWS for the VPC router",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 704585,
          "date": "Wed 26 Oct 2022 12:49",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/27814-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#97",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with Active Directory for access control.<br>Which solution will satisfy these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#97",
          "answers": [
            {
              "choice": "<p>A. Configure Amazon EFS storage and set the Active Directory domain for authentication.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 752705,
          "date": "Wed 21 Dec 2022 20:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "D.  Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication.<br><br>Amazon FSx for Windows File Server is a fully managed file storage service that is designed to be used with Microsoft Windows workloads. It is integrated with Active Directory for access control and is highly available, as it stores data across multiple availability zones. Additionally, FSx can be used to migrate data from on-premises Microsoft Windows file servers to the AWS Cloud. This makes it a good fit for the requirements described in the question.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750001,
          "date": "Mon 19 Dec 2022 16:45",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "FSx is for Windows",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749259,
          "date": "Sun 18 Dec 2022 23:44",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743318,
          "date": "Mon 12 Dec 2022 21:41",
          "username": "\t\t\t\txeun88\t\t\t",
          "content": "Im going for D as the answer because FXs is compatible with windows",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732542,
          "date": "Thu 01 Dec 2022 12:37",
          "username": "\t\t\t\tkajal1206\t\t\t",
          "content": "Answer is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723767,
          "date": "Mon 21 Nov 2022 17:58",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719397,
          "date": "Wed 16 Nov 2022 07:52",
          "username": "\t\t\t\tTonyghostR05\t\t\t",
          "content": "Window only available for using FSx",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 712855,
          "date": "Mon 07 Nov 2022 07:47",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "D.  Windows is the keyword<br><br>https://www.examtopics.com/discussions/amazon/view/29780-exam-aws-certified-solutions-architect-associate-saa-c02/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EFS is for Linux<br>FSx is for Windows</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 712856,
          "date": "Mon 07 Nov 2022 07:47",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "EFS is for Linux<br>FSx is for Windows",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 709497,
          "date": "Wed 02 Nov 2022 02:01",
          "username": "\t\t\t\tHunkie\t\t\t",
          "content": "DDDDDDDD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 706916,
          "date": "Sat 29 Oct 2022 05:26",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "Correct Answer:D <br>https://docs.aws.amazon.com/fsx/latest/WindowsGuide/aws-ad-integration-fsxW.html",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#98",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email.<br>Users report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages.<br>What should the solutions architect do to resolve this issue with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#98",
          "answers": [
            {
              "choice": "<p>A. Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 703369,
          "date": "Mon 24 Oct 2022 23:19",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "answer should be C,<br>users get duplicated messages because -> lambda polls the message, and starts processing the message.<br>However, before the first lambda can finish processing the message, the visibility timeout runs out on SQS, and SQS returns the message to the poll, causing another Lambda node to process that same message.<br>By increasing the visibility timeout, it should prevent SQS from returning a message back to the poll before Lambda can finish processing the message<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I am confused. If the email has been sent many times already why would they need more time? <br>I believe SQS Queue Fifo will keep in order and any duplicates with same ID will be deleted. Can you tell me where i am going wrong? Thanks</li><li>I tend to agree with you. See my comments above.</li></ul>",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777020,
          "date": "Sun 15 Jan 2023 20:58",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "I am confused. If the email has been sent many times already why would they need more time? <br>I believe SQS Queue Fifo will keep in order and any duplicates with same ID will be deleted. Can you tell me where i am going wrong? Thanks<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I tend to agree with you. See my comments above.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777299,
          "date": "Mon 16 Jan 2023 04:59",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "I tend to agree with you. See my comments above.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 693607,
          "date": "Thu 13 Oct 2022 06:37",
          "username": "\t\t\t\tbrushek\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html<br><br>this is important part:<br>Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 822559,
          "date": "Sun 26 Feb 2023 15:53",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Here https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html it says this for sqs standard.For standard queues, the visibility timeout isn't a guarantee against receiving a message twice. For more information, see At-least-once delivery.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777307,
          "date": "Mon 16 Jan 2023 05:17",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "the only thing that addresses deduplication is using a FIFO queue OR by coding idempotency into your code. Increasing the visibility timeout only means you can delete the message you were processing, it doesn't handle the duplicates and therefore doesn't answer the question of <br><br>\\\"What should the solutions architect do to resolve this issue \\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>the case is not about dups on the queue, but invoking the lambda function many times</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 787813,
          "date": "Wed 25 Jan 2023 16:29",
          "username": "\t\t\t\tces26015\t\t\t",
          "content": "the case is not about dups on the queue, but invoking the lambda function many times",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777297,
          "date": "Mon 16 Jan 2023 04:59",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "Increasing the visibility timeout only stops other consumers of the queue from seeing that message until it is handled and deleted.<br><br> However in our case <br><br>- there are duplicate messages in the queue!! So I do not see how increasing the visibility handles this issue. <br>- The question clearly calls out that a 'standard queue' is being usedso the reader will think of this issues caused by a standard queue... which is order of order and DEDUPLICATION.<br><br>The also do not mention performance as an issue, which might be a reason not to use FIFO.<br><br>The only issue I have with 'B' as an answer is it says <br><br>'change' the standard to FIFO.technically you cannot switch to a FIFO queue once its created... but you can at a higher level change the architecture to use a FIFO queue.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759204,
          "date": "Wed 28 Dec 2022 01:37",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C is right answer here<br>https://www.examtopics.com/discussions/amazon/view/83096-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757942,
          "date": "Tue 27 Dec 2022 00:42",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html<br><br>still valid<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>have a read of the page you linked too it states<br><br>\\\"For standard queues, the visibility timeout isn't a guarantee against receiving a message twice. For more information, see At-least-once delivery.\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777306,
          "date": "Mon 16 Jan 2023 05:13",
          "username": "\t\t\t\tMrAWS\t\t\t",
          "content": "have a read of the page you linked too it states<br><br>\\\"For standard queues, the visibility timeout isn't a guarantee against receiving a message twice. For more information, see At-least-once delivery.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755117,
          "date": "Sat 24 Dec 2022 21:05",
          "username": "\t\t\t\tpsr83\t\t\t",
          "content": "https://aws.amazon.com/sqs/faqs/<br>SQS - LongPolling decreases the number of API calls made to SQS while increasing the efficiency and reducing latency of your application <br>Long polling reduces the number of empty responses by allowing Amazon SQS to wait a specified time for a message to become available in the queue before sending a response. Also, long polling eliminates false empty responses by querying all of the servers instead of a sampling of server",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 754399,
          "date": "Fri 23 Dec 2022 18:52",
          "username": "\t\t\t\tnaabe\t\t\t",
          "content": "LEAST operational overhead Option D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752714,
          "date": "Wed 21 Dec 2022 21:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D, The solution architect should modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing. This is the least operationally overhead solution because it does not require any changes to the SQS queue or any additional configuration.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, setting up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds, could potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the latency of message delivery and potentially increase costs.<br><br>Option B, changing the SQS standard queue to an SQS FIFO queue and using the message deduplication ID to discard duplicate messages, would require changes to the queue and could potentially cause disruptions to the application if not implemented correctly. It may also require additional overhead to manage the message deduplication ID. </li><li>Option C, increasing the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout, could also potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the time it takes for messages to be available for processing again if the function fails. This could result in increased latency and potentially higher costs.</li><li>what happens if processing fails ???</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752715,
          "date": "Wed 21 Dec 2022 21:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, setting up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds, could potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the latency of message delivery and potentially increase costs.<br><br>Option B, changing the SQS standard queue to an SQS FIFO queue and using the message deduplication ID to discard duplicate messages, would require changes to the queue and could potentially cause disruptions to the application if not implemented correctly. It may also require additional overhead to manage the message deduplication ID. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, increasing the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout, could also potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the time it takes for messages to be available for processing again if the function fails. This could result in increased latency and potentially higher costs.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 752716,
          "date": "Wed 21 Dec 2022 21:03",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C, increasing the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout, could also potentially reduce the number of duplicate messages received by the Lambda function, but it would also increase the time it takes for messages to be available for processing again if the function fails. This could result in increased latency and potentially higher costs.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 759203,
          "date": "Wed 28 Dec 2022 01:36",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "what happens if processing fails ???",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750518,
          "date": "Tue 20 Dec 2022 06:17",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "At first I thought the answer should be B, since they specifically mentioned it is a Standard Queue and we know that in Std queue, we do get some duplicates. But the real catch over here is EVERY time the users are getting duplicate. So it must be the VisibilityTimeout issuewhich isn't long enough so EVERY time the message goes back on the queue before processing by one Lambda is completed and at the same time is being picked up by another function for processing.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750390,
          "date": "Tue 20 Dec 2022 02:57",
          "username": "\t\t\t\tRichaqua\t\t\t",
          "content": "Since SQS queue does not delete the message by default, Lambda function can be modified to delete the messages after it has been processed.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749262,
          "date": "Sun 18 Dec 2022 23:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C is the most probable case.<br>Though option B can also cause some duplicates but given this is happening for every request/users C seems to be real root cuase.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723768,
          "date": "Mon 21 Nov 2022 17:59",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709502,
          "date": "Wed 02 Nov 2022 02:09",
          "username": "\t\t\t\tHunkie\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/83096-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 700608,
          "date": "Fri 21 Oct 2022 09:21",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "Iexlude Polling because-->\\\"The maximum long polling wait time is 20 seconds\\\"<br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 692345,
          "date": "Tue 11 Oct 2022 20:50",
          "username": "\t\t\t\tMXB05\t\t\t",
          "content": "A can not be correct, long polling will only ensure that all images are retrieved from all SQS servers in one query. If the same message triggers the lambda function twice it is likely because the visibility timeout isn't long enough and lambda didn't repsond in time with a deletion of the message ->> C is correct",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#99",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#99",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698284,
          "date": "Tue 18 Oct 2022 16:09",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Answer is D. <br>Lustre in the question is only available as FSx<br>https://aws.amazon.com/fsx/lustre/",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752720,
          "date": "Wed 21 Dec 2022 21:08",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D.  Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.<br><br>Amazon FSx for Lustre is a fully managed file system that is designed for high-performance workloads, such as gaming applications. It provides a high-performance, scalable, and fully managed file system that is optimized for Lustre clients, and it is fully integrated with Amazon EC2. It is the only option that meets the requirements of being fully managed and able to support Lustre clients.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 835327,
          "date": "Fri 10 Mar 2023 19:56",
          "username": "\t\t\t\tfkie4\t\t\t",
          "content": "seriously? it spells out \\\"Lustre\\\" for you",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 798741,
          "date": "Sun 05 Feb 2023 11:20",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "D is the most logical solution. But still the app is OnPrem so AWS Fx for Lustre is not enough to connect the storage to the app, we'll need a File Gateway to use with the FSx Lustre",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793958,
          "date": "Tue 31 Jan 2023 11:16",
          "username": "\t\t\t\tChalamalli\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749265,
          "date": "Sun 18 Dec 2022 23:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723770,
          "date": "Mon 21 Nov 2022 18:01",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#100",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#100",
          "answers": [
            {
              "choice": "<p>A. Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696559,
          "date": "Sun 16 Oct 2022 22:47",
          "username": "\t\t\t\tChunsli\t\t\t",
          "content": "C makes a better sense. Between C (S3) and D (EBS), S3 is highly available with LEAST operational overhead.",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 692346,
          "date": "Tue 11 Oct 2022 20:53",
          "username": "\t\t\t\tMXB05\t\t\t",
          "content": "Correct Answer is C: EBS is not highly available<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EBS is Highly Available as it stores in multi AZ and S3 is regional.</li><li>EBS also has Multi-AZ capability, but it does not replicate the data across multiple availability zones by default. When Multi-AZ is enabled, it creates a replica of the EBS volume in a different availability zone and automatically failover to the replica in case of a failure. However, this requires additional configuration and management. In comparison, Amazon S3 automatically replicates data across multiple availability zones without any additional configuration. Therefore, storing the data on Amazon S3 provides a simpler and more efficient solution for high availability.</li><li>Per AWS: \\\"Amazon EBS volumes are designed to be highly available, reliable, and durable\\\"<br><br>https://aws.amazon.com/ebs/features/</li><li>Yes it is!</li></ul>",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 777027,
          "date": "Sun 15 Jan 2023 21:14",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "EBS is Highly Available as it stores in multi AZ and S3 is regional.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>EBS also has Multi-AZ capability, but it does not replicate the data across multiple availability zones by default. When Multi-AZ is enabled, it creates a replica of the EBS volume in a different availability zone and automatically failover to the replica in case of a failure. However, this requires additional configuration and management. In comparison, Amazon S3 automatically replicates data across multiple availability zones without any additional configuration. Therefore, storing the data on Amazon S3 provides a simpler and more efficient solution for high availability.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 786394,
          "date": "Tue 24 Jan 2023 11:07",
          "username": "\t\t\t\toguz11\t\t\t",
          "content": "EBS also has Multi-AZ capability, but it does not replicate the data across multiple availability zones by default. When Multi-AZ is enabled, it creates a replica of the EBS volume in a different availability zone and automatically failover to the replica in case of a failure. However, this requires additional configuration and management. In comparison, Amazon S3 automatically replicates data across multiple availability zones without any additional configuration. Therefore, storing the data on Amazon S3 provides a simpler and more efficient solution for high availability.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760295,
          "date": "Wed 28 Dec 2022 21:18",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "Per AWS: \\\"Amazon EBS volumes are designed to be highly available, reliable, and durable\\\"<br><br>https://aws.amazon.com/ebs/features/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749967,
          "date": "Mon 19 Dec 2022 16:13",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Yes it is!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777090,
          "date": "Sun 15 Jan 2023 22:43",
          "username": "\t\t\t\tAHUI\t\t\t",
          "content": "Ans is C: <br>Security certificates are just normal files.it is not SSL certificate etc… confusing !!!!!!!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774677,
          "date": "Fri 13 Jan 2023 17:45",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "Is this the real question from Exam? It is typically vague. Usually S3 would be chosen when the situation mentioned \\\"high availability\\\". But AWS official website states that EBS volume has 99.999% availability.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 774051,
          "date": "Fri 13 Jan 2023 03:58",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "EBS volumes are in one AZ and S3 buckets are a global resource.<br><br>Amazon EBS volumes are designed to be highly available, reliable, and durable. At no additional charge to you, Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>On 2nd thought, I'll change my answer to C</li><li>That was a hilarious change</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 774055,
          "date": "Fri 13 Jan 2023 04:02",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "On 2nd thought, I'll change my answer to C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That was a hilarious change</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 796304,
          "date": "Thu 02 Feb 2023 19:00",
          "username": "\t\t\t\tAk1009\t\t\t",
          "content": "That was a hilarious change",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765456,
          "date": "Wed 04 Jan 2023 10:53",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Users cannot terminate an EC2 instance in the us-east-1 Region",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765208,
          "date": "Wed 04 Jan 2023 04:36",
          "username": "\t\t\t\tthensanity\t\t\t",
          "content": "LEAST operational - S3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 762225,
          "date": "Fri 30 Dec 2022 19:32",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "Correct answer is C, <br><br>Least operational overhead is S3<br><br>Amazon S3 provides durability by redundantly storing the data across multiple Availability Zones whereas EBS provides durability by redundantly storing the data in a single Availability Zone.<br><br>Both S3 and EBS gives the availability of 99.99%, but the only difference that occurs is that S3 is accessed via the internet using API's and EBS is accessed by the single instance attached to EBS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759855,
          "date": "Wed 28 Dec 2022 14:05",
          "username": "\t\t\t\tNandan747\t\t\t",
          "content": "Well, they said Highly available. S3 is HA by default, EBS you need to ensure it's HA. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759207,
          "date": "Wed 28 Dec 2022 01:39",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752724,
          "date": "Wed 21 Dec 2022 21:15",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option C is the best solution that meets the requirements with the least operational overhead.<br><br>Option C is the best solution because it involves using AWS KMS to perform encryption operations and storing the encrypted data on Amazon S3. KMS provides a managed service for creating and controlling the encryption keys used to encrypt and decrypt data, which reduces the operational overhead of managing the encryption process. Amazon S3 is a highly available storage service, which meets the requirement of storing data in highly available storage. Additionally, allowing the EC2 role to use the KMS key for encryption operations means that the EC2 instance can access the key without requiring additional authentication, which further simplifies the process.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 750133,
          "date": "Mon 19 Dec 2022 19:14",
          "username": "\t\t\t\tyoben84\t\t\t",
          "content": "Since the solution is deployed in an EC2 instance, it's less operational overhead to have the data stored in EBS than S3.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749971,
          "date": "Mon 19 Dec 2022 16:18",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Which solution will meet these requirements with the LEAST operational overhead? rules out both A and B as these involve manual steps. If the EC2 instance is performing encryption then D allows you to write the encrypted data locally rather than to S3, so quicker, and the EBS volume can be a Solid State Drives (SSD) e.g. EBS Provisioned IOPS SSD (io2 Block Express) which provides \\\"Highest performance SSD volume designed for business-critical latency-sensitive transactional workloads\\\". This link explains why EBS should be used over EFS and S3: https://www.justaftermidnight247.com/insights/ebs-efs-and-s3-when-to-use-awss-three-storage-solutions/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749211,
          "date": "Sun 18 Dec 2022 22:46",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "There is some problem with way in which question is phrased. <br>In 1st part it talks about certificate to communicate other business services. This means it is talking about TLS certificate but later it talks about encrypting data stored in S3 buckets. <br>For S3 encryption KMS (option C) is right solution but keeping TLS (HTTPS) communication encryption keys Secrets managers may be the right option.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 743192,
          "date": "Mon 12 Dec 2022 19:43",
          "username": "\t\t\t\tbearcandy\t\t\t",
          "content": "D = near real time(EBS is faster than S3), not about cost savings",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 739668,
          "date": "Fri 09 Dec 2022 02:55",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "Should be D. EBS is HA up to 5 9s.It's also replicated across multiple servers in an AZ.S3 is max of 4 9s.https://aws.amazon.com/ebs/faqs/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 731534,
          "date": "Wed 30 Nov 2022 14:39",
          "username": "\t\t\t\tfabio3wz\t\t\t",
          "content": "Why do some people say that EBS is HA? how can an EBS volume be highly available??? S3 is the only option, and of course we need to use KMS, then the answer is clearly C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon says this too \\\"Amazon EBS volumes are designed to be highly available, reliable, and durable. At no additional charge to you, Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component.\\\" https://aws.amazon.com/ebs/features/</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749965,
          "date": "Mon 19 Dec 2022 16:12",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Amazon says this too \\\"Amazon EBS volumes are designed to be highly available, reliable, and durable. At no additional charge to you, Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component.\\\" https://aws.amazon.com/ebs/features/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    }
  ]
}